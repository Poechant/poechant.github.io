<!DOCTYPE html>
<html>

<head>
	<!-- Meta -->
	<meta charset="UTF-8"/>
	<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
	<meta name="generator" content="Jekyll">

	<title>自然语言处理的常见词表示法（Word Representation）</title>
  	<meta name="description" content="麦克船长对于技术、产品、商业等领域的分享|AI,A.I.,NLP,神经网络,人工智能,自然语言处理,BERT,GPT,ChatGPT,OpenAI,阿里巴巴,P9,运营,淘宝,天猫,总监,高管">

	<!-- CSS & fonts -->
	<link rel="stylesheet" href="/css/main.css">

	<!-- RSS -->
	<link href="/atom.xml" type="application/atom+xml" rel="alternate" title="ATOM Feed" />

  	<!-- Favicon -->
 	 <link rel="shortcut icon" type="image/png" href="/img/favicon.png">

 	 <!-- Syntax highlighter -->
  	<link rel="stylesheet" href="/css/syntax.css" />

  	<!--KaTeX-->
  	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
  	<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>
  	<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"></script>
  	<script>
  		document.addEventListener("DOMContentLoaded", function() {
  			renderMathInElement(document.body, {
  				// ...options...
  			});
  		});
  	</script>

  	

  	
  		<script async src="https://www.googletagmanager.com/gtag/js?id=G-CH4708X4R5"></script>
  		<script>
    		window.dataLayer = window.dataLayer || [];
    		function gtag(){dataLayer.push(arguments);}
    		gtag('js', new Date());

    		gtag('config', 'G-CH4708X4R5');
  		</script>
	


</head>

<body>
	<div id="wrap">
	  	
	  	<!-- Navigation -->
	  	<nav id="nav">
	<div id="nav-list">
		<a href="/">Home</a>

		<!-- Nav pages -->
	  <!-- 
	    
	  
	    
	      <a href="/about/" title="关于我">关于我</a>
	    
	  
	    
	  
	    
	  
	    
	      <a href="/booklist/" title="读书行路">读书行路</a>
	    
	  
	    
	  
	    
	  
	    
	  
	    
	      <a href="/categories/" title="Categories">Categories</a>
	    
	  
	    
	  
	    
	  
	    
	  
	    
	      <a href="/target/" title="目标感">目标感</a>
	    
	  
	    
	  
	    
	  
	    
	  
	    
	  
	    
	  
	    
	  
	    
	  
	    
	   -->

	  <!-- Tech category pages -->






  <a href="/category/ai" title="人工智能">人工智能</a>











  <a href="/category/energy" title="能源">能源</a>









  <a href="/category/rt_tech" title="实时技术">实时技术</a>







  <a href="/category/web" title="前端">前端</a>
















<!-- Non-tech category pages -->












  <a href="/category/business" title="商业">商业</a>



  <a href="/category/design" title="设计">设计</a>















  <a href="/category/thinking" title="思考与生活">思考与生活</a>

















	  
        
      
        
          <a href="/about/" title="关于我">关于我</a>
        
      
        
      
        
      
        
          <a href="/booklist/" title="读书行路">读书行路</a>
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
          <a href="/target/" title="目标感">目标感</a>
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    <!-- Nav links -->
	  <!-- <a href="https://github.com/thereviewindex/monochrome/archive/master.zip">Download</a>
<a href="https://github.com/thereviewindex/monochrome">Project on Github</a> -->

	</div>
  
  <!-- Nav footer -->
	
	  <footer>
	
	<span>version 1.0.0</span>

</footer>
	

</nav>

    
    <!-- Icon menu -->
	  <a id="nav-menu">
	  	<div id="menu"></div>
	  </a>

      <!-- Header -->
      
        <header id="header" class="parent justify-spaceBetween">
  <div class="inner w100 relative">
    <span class="f-left">  
      <a href="/">
        <h1>
          <span>Mike</span>Captain
        </h1>
      </a>
    </span>
    <span id="nav-links" class="absolute right bottom">

      <!-- Tech category pages -->






  <a href="/category/ai" title="人工智能">人工智能</a>











  <a href="/category/energy" title="能源">能源</a>









  <a href="/category/rt_tech" title="实时技术">实时技术</a>







  <a href="/category/web" title="前端">前端</a>
















<!-- Non-tech category pages -->












  <a href="/category/business" title="商业">商业</a>



  <a href="/category/design" title="设计">设计</a>















  <a href="/category/thinking" title="思考与生活">思考与生活</a>

















      &nbsp;&nbsp;&nbsp;丨&nbsp;

      <!-- Nav pages -->
      
        
      
        
          <a href="/about/" title="关于我">关于我</a>
        
      
        
      
        
      
        
          <a href="/booklist/" title="读书行路">读书行路</a>
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
          <a href="/target/" title="目标感">目标感</a>
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
      
      <!-- Nav links -->
      <!-- <a href="https://github.com/thereviewindex/monochrome/archive/master.zip">Download</a>
<a href="https://github.com/thereviewindex/monochrome">Project on Github</a> -->

    </span>
  </div>
</header>




      

    <!-- Main content -->
	  <div id="container">
		  
		<main>

			<article id="post-page">
	<h2>自然语言处理的常见词表示法（Word Representation）</h2>		
	<time datetime="2022-12-18T15:33:09+00:00" class="by-line">18 Dec 2022, 杭州 | 麦克船长 | 总计 6521 字</time>
	<div class="content">
		<h3 id="二静态词向量">二、静态词向量</h3>

<h4 id="1one-hot">1、One-Hot</h4>

<p>这就要讲到一个很有名也很基础的独热（One-Hot）编码。比如我们有一个 12 个不同词汇的语料库，按照 One-Hot 编码，我们需要构建一个 12x12 的矩阵，比如下面这样：
我喜欢梅西。</p>

<blockquote>
  <p>我喜欢阿根廷球队。
阿根廷拿下世界杯冠军。</p>
</blockquote>

<p>首先我们分次成「我 喜欢 梅西 带领 阿根廷 球队 赢得 2022 卡塔尔 世界杯 冠军 。」这些词，并根据整段话中每个两个词共同出现的频次（这就是为什么叫 Hot 的原因），构建如下矩阵：</p>

<table>
  <thead>
    <tr>
      <th> </th>
      <th>我</th>
      <th>喜欢</th>
      <th>梅西</th>
      <th>阿根廷</th>
      <th>球队</th>
      <th>拿下</th>
      <th>世界杯</th>
      <th>冠军</th>
      <th>。</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>我</td>
      <td>0</td>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2</td>
    </tr>
    <tr>
      <td>喜欢</td>
      <td>2</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2</td>
    </tr>
    <tr>
      <td>梅西</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <td>阿根廷</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
    </tr>
    <tr>
      <td>球队</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <td>拿下</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <td>世界杯</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <td>冠军</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <td>。</td>
      <td>2</td>
      <td>2</td>
      <td>1</td>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
    </tr>
  </tbody>
</table>

<p>这是一个词频共现表，这样每个词就被表示成了矩阵里的一行，也可以看成一个向量。这些词向量就组成了一个向量空间，空间中向量之间的关系可以表征词之间的关系，到此该方法的编码就结束了。</p>

<p>因为语言表达都有相似性，我们拿到语料用 One-Hot 编码方式表示的词，其实在后续其他 NLP 问题上一定程度可以复用，也就是这个方法是支持预训练（Pre-Traning）的，这大大节省了工程师的时间。但是从这样的分词和 One-Hot 编码表示词的方式里，你能看出什么问题吗？很明显这样的表示方式，是有损的，即有信息丢失的。都丢失了哪些信息？</p>

<ul>
  <li>问题 1：分词颗粒度粗，引发的子词信息丢失。把词进一步拆分能挖掘到的信息其实都丢失了，比如英文的词根（intelligence、intelligent、intelligen 其实有相关性，但是无法体现）。</li>
  <li>问题 2：有序结构背后的信息丢失。词的表示只考虑了两个词的共现频次，语言的有序结构是不是丢了，比如「我打了他一巴掌」和「他打了我一巴掌」在 One-Hot 编码里体现不出任何差异。</li>
  <li>问题 3：忽略一词多义。比如「东西」表示方向，也表示物品，也会被忽略。</li>
  <li>问题 4：高频低频词。另外，像「我、的、。」这种高频词会影响两个可能并不强关联的词在空间中的表达，反过来低频词的关联也变相地被弱化了，我们可以概括为高低频词的问题。</li>
  <li>问题 5：OOV（Out Of Vocabulary）问题，即不认识新词。</li>
  <li>问题 6：数据稀疏。基于当下的机器学习处理方法和工具，这个矩阵里的空洞太多了，也就是 0 太多了，有数据稀疏的问题，而且非常浪费存储空间、算力，尤其想象一个 100 万个词的语料库，这个矩阵非常的大。</li>
</ul>

<p>每个问题都是机会。比如高频低频词引发的问题，就引入了「缩小高频词权重、放大低频词权重」的思路，具体采用的指标常用到 PMI（Pointwise Mutual Information，点互信息）。我们继续往下看。</p>

<h4 id="2word2vec2013">2、Word2Vec（2013）</h4>

<h4 id="3glove2014">3、GloVe（2014）</h4>

<p>2014 年 Stanford 的 Jeff Pennington 等人在论文《GloVe: Global Vectors for Word Representation》中提出的 GloVe，是一种词表示法（word representation）。</p>

<p>从这里获取 GloVe 的最新代码：https://github.com/stanfordnlp/GloVe 。下载并解压后：</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cd </span>GloVe-master <span class="o">&amp;&amp;</span> make
</code></pre></div></div>

<p>正常运行将看到如下信息：</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">mkdir</span> <span class="nt">-p</span> build
gcc <span class="nt">-c</span> src/vocab_count.c <span class="nt">-o</span> build/vocab_count.o <span class="nt">-lm</span> <span class="nt">-pthread</span> <span class="nt">-O3</span> <span class="nt">-march</span><span class="o">=</span>native <span class="nt">-funroll-loops</span> <span class="nt">-Wall</span> <span class="nt">-Wextra</span> <span class="nt">-Wpedantic</span>
clang: warning: <span class="nt">-lm</span>: <span class="s1">'linker'</span> input unused <span class="o">[</span><span class="nt">-Wunused-command-line-argument</span><span class="o">]</span>
gcc <span class="nt">-c</span> src/cooccur.c <span class="nt">-o</span> build/cooccur.o <span class="nt">-lm</span> <span class="nt">-pthread</span> <span class="nt">-O3</span> <span class="nt">-march</span><span class="o">=</span>native <span class="nt">-funroll-loops</span> <span class="nt">-Wall</span> <span class="nt">-Wextra</span> <span class="nt">-Wpedantic</span>
clang: warning: <span class="nt">-lm</span>: <span class="s1">'linker'</span> input unused <span class="o">[</span><span class="nt">-Wunused-command-line-argument</span><span class="o">]</span>
gcc <span class="nt">-c</span> src/shuffle.c <span class="nt">-o</span> build/shuffle.o <span class="nt">-lm</span> <span class="nt">-pthread</span> <span class="nt">-O3</span> <span class="nt">-march</span><span class="o">=</span>native <span class="nt">-funroll-loops</span> <span class="nt">-Wall</span> <span class="nt">-Wextra</span> <span class="nt">-Wpedantic</span>
clang: warning: <span class="nt">-lm</span>: <span class="s1">'linker'</span> input unused <span class="o">[</span><span class="nt">-Wunused-command-line-argument</span><span class="o">]</span>
gcc <span class="nt">-c</span> src/glove.c <span class="nt">-o</span> build/glove.o <span class="nt">-lm</span> <span class="nt">-pthread</span> <span class="nt">-O3</span> <span class="nt">-march</span><span class="o">=</span>native <span class="nt">-funroll-loops</span> <span class="nt">-Wall</span> <span class="nt">-Wextra</span> <span class="nt">-Wpedantic</span>
clang: warning: <span class="nt">-lm</span>: <span class="s1">'linker'</span> input unused <span class="o">[</span><span class="nt">-Wunused-command-line-argument</span><span class="o">]</span>
gcc <span class="nt">-c</span> src/common.c <span class="nt">-o</span> build/common.o <span class="nt">-lm</span> <span class="nt">-pthread</span> <span class="nt">-O3</span> <span class="nt">-march</span><span class="o">=</span>native <span class="nt">-funroll-loops</span> <span class="nt">-Wall</span> <span class="nt">-Wextra</span> <span class="nt">-Wpedantic</span>
clang: warning: <span class="nt">-lm</span>: <span class="s1">'linker'</span> input unused <span class="o">[</span><span class="nt">-Wunused-command-line-argument</span><span class="o">]</span>
gcc build/vocab_count.o build/common.o <span class="nt">-o</span> build/vocab_count <span class="nt">-lm</span> <span class="nt">-pthread</span> <span class="nt">-O3</span> <span class="nt">-march</span><span class="o">=</span>native <span class="nt">-funroll-loops</span> <span class="nt">-Wall</span> <span class="nt">-Wextra</span> <span class="nt">-Wpedantic</span>
gcc build/cooccur.o build/common.o <span class="nt">-o</span> build/cooccur <span class="nt">-lm</span> <span class="nt">-pthread</span> <span class="nt">-O3</span> <span class="nt">-march</span><span class="o">=</span>native <span class="nt">-funroll-loops</span> <span class="nt">-Wall</span> <span class="nt">-Wextra</span> <span class="nt">-Wpedantic</span>
gcc build/shuffle.o build/common.o <span class="nt">-o</span> build/shuffle <span class="nt">-lm</span> <span class="nt">-pthread</span> <span class="nt">-O3</span> <span class="nt">-march</span><span class="o">=</span>native <span class="nt">-funroll-loops</span> <span class="nt">-Wall</span> <span class="nt">-Wextra</span> <span class="nt">-Wpedantic</span>
gcc build/glove.o build/common.o <span class="nt">-o</span> build/glove <span class="nt">-lm</span> <span class="nt">-pthread</span> <span class="nt">-O3</span> <span class="nt">-march</span><span class="o">=</span>native <span class="nt">-funroll-loops</span> <span class="nt">-Wall</span> <span class="nt">-Wextra</span> <span class="nt">-Wpedantic</span>
</code></pre></div></div>

<p>然后运行 demo 脚本：</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>./demo.sh
</code></pre></div></div>

<p>正常运行后将得到如下：</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">mkdir</span> <span class="nt">-p</span> build

<span class="nv">$ </span>build/vocab_count <span class="nt">-min-count</span> 5 <span class="nt">-verbose</span> 2 &lt; text8 <span class="o">&gt;</span> vocab.txt
BUILDING VOCABULARY
Processed 17005207 tokens.
Counted 253854 unique words.
Truncating vocabulary at min count 5.
Using vocabulary of size 71290.

<span class="nv">$ </span>build/cooccur <span class="nt">-memory</span> 4.0 <span class="nt">-vocab-file</span> vocab.txt <span class="nt">-verbose</span> 2 <span class="nt">-window-size</span> 15 &lt; text8 <span class="o">&gt;</span> cooccurrence.bin
COUNTING COOCCURRENCES
window size: 15
context: symmetric
max product: 13752509
overflow length: 38028356
Reading vocab from file <span class="s2">"vocab.txt"</span>...loaded 71290 words.
Building lookup table...table contains 94990279 elements.
Processed 17005207 tokens.
Writing cooccurrences to disk.........2 files <span class="k">in </span>total.
Merging cooccurrence files: processed 60666468 lines.

<span class="nv">$ </span>build/shuffle <span class="nt">-memory</span> 4.0 <span class="nt">-verbose</span> 2 &lt; cooccurrence.bin <span class="o">&gt;</span> cooccurrence.shuf.bin
Using random seed 1672234516
SHUFFLING COOCCURRENCES
array size: 255013683
Shuffling by chunks: processed 60666468 lines.
Wrote 1 temporary file<span class="o">(</span>s<span class="o">)</span><span class="nb">.</span>
Merging temp files: processed 60666468 lines.

<span class="nv">$ </span>build/glove <span class="nt">-save-file</span> vectors <span class="nt">-threads</span> 8 <span class="nt">-input-file</span> cooccurrence.shuf.bin <span class="nt">-x-max</span> 10 <span class="nt">-iter</span> 15 <span class="nt">-vector-size</span> 50 <span class="nt">-binary</span> 2 <span class="nt">-vocab-file</span> vocab.txt <span class="nt">-verbose</span> 2
TRAINING MODEL
Read 60666468 lines.
Initializing parameters...Using random seed 1672234542
<span class="k">done</span><span class="nb">.</span>
vector size: 50
vocab size: 71290
x_max: 10.000000
alpha: 0.750000
12/28/22 - 09:35.53PM, iter: 001, cost: 0.071355
12/28/22 - 09:36.03PM, iter: 002, cost: 0.052712
12/28/22 - 09:36.15PM, iter: 003, cost: 0.046689
12/28/22 - 09:36.26PM, iter: 004, cost: 0.043382
12/28/22 - 09:36.37PM, iter: 005, cost: 0.041456
12/28/22 - 09:36.47PM, iter: 006, cost: 0.040181
12/28/22 - 09:36.58PM, iter: 007, cost: 0.039276
12/28/22 - 09:37.11PM, iter: 008, cost: 0.038593
12/28/22 - 09:37.27PM, iter: 009, cost: 0.038052
12/28/22 - 09:37.39PM, iter: 010, cost: 0.037616
12/28/22 - 09:37.51PM, iter: 011, cost: 0.037249
12/28/22 - 09:38.04PM, iter: 012, cost: 0.036944
12/28/22 - 09:38.15PM, iter: 013, cost: 0.036681
12/28/22 - 09:38.26PM, iter: 014, cost: 0.036450
12/28/22 - 09:38.37PM, iter: 015, cost: 0.036244
<span class="nv">$ </span>python <span class="nb">eval</span>/python/evaluate.py
capital-common-countries.txt:
ACCURACY TOP1: 64.23% <span class="o">(</span>325/506<span class="o">)</span>
capital-world.txt:
ACCURACY TOP1: 26.68% <span class="o">(</span>951/3564<span class="o">)</span>
currency.txt:
ACCURACY TOP1: 4.19% <span class="o">(</span>25/596<span class="o">)</span>
city-in-state.txt:
ACCURACY TOP1: 25.45% <span class="o">(</span>593/2330<span class="o">)</span>
family.txt:
ACCURACY TOP1: 36.43% <span class="o">(</span>153/420<span class="o">)</span>
gram1-adjective-to-adverb.txt:
ACCURACY TOP1: 3.63% <span class="o">(</span>36/992<span class="o">)</span>
gram2-opposite.txt:
ACCURACY TOP1: 3.04% <span class="o">(</span>23/756<span class="o">)</span>
gram3-comparative.txt:
ACCURACY TOP1: 26.28% <span class="o">(</span>350/1332<span class="o">)</span>
gram4-superlative.txt:
ACCURACY TOP1: 9.68% <span class="o">(</span>96/992<span class="o">)</span>
gram5-present-participle.txt:
ACCURACY TOP1: 14.58% <span class="o">(</span>154/1056<span class="o">)</span>
gram6-nationality-adjective.txt:
ACCURACY TOP1: 53.58% <span class="o">(</span>815/1521<span class="o">)</span>
gram7-past-tense.txt:
ACCURACY TOP1: 13.01% <span class="o">(</span>203/1560<span class="o">)</span>
gram8-plural.txt:
ACCURACY TOP1: 25.08% <span class="o">(</span>334/1332<span class="o">)</span>
gram9-plural-verbs.txt:
ACCURACY TOP1: 5.75% <span class="o">(</span>50/870<span class="o">)</span>
Questions seen/total: 91.21% <span class="o">(</span>17827/19544<span class="o">)</span>
Semantic accuracy: 27.60%  <span class="o">(</span>2047/7416<span class="o">)</span>
Syntactic accuracy: 19.80%  <span class="o">(</span>2061/10411<span class="o">)</span>
Total accuracy: 23.04%  <span class="o">(</span>4108/17827<span class="o">)</span>
</code></pre></div></div>

<h4 id="4fasttext2016">4、fastText（2016）</h4>

<h3 id="二动态词向量">二、动态词向量</h3>


	</div>
</article>



	  </main>
		
		  <!-- Pagination links -->
      

	  </div>
	    
	    <!-- Footer -->
	    <footer>
	<span>
		-<br/><br/>
		船长还不会游泳 at 微信公众号/微博<br/>
		@麦克船长 at 即刻/知乎/小宇宙/掘金/小红书/微信读书<br/>
		@船长模玩 at Bilibili<br/>
		Copyright © 2011-2023, MikeCaptain.com
	</span>
</footer>


	    <!-- Script -->
      <script src="/js/main.js"></script>	


	</div>
</body>
</html>

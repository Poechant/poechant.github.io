<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.2">Jekyll</generator><link href="https://www.mikecaptain.com/feed.xml" rel="self" type="application/atom+xml" /><link href="https://www.mikecaptain.com/" rel="alternate" type="text/html" /><updated>2023-03-10T20:31:41+00:00</updated><id>https://www.mikecaptain.com/feed.xml</id><title type="html">麦克船长的技术、产品与商业博客</title><subtitle>麦克船长对于技术、产品、商业等领域的分享|AI,A.I.,NLP,神经网络,人工智能,自然语言处理,BERT,GPT,ChatGPT,OpenAI,阿里巴巴,P9,运营,淘宝,天猫,总监,高管</subtitle><author><name>Poechant</name><email>zhongchao.ustc@gmail.com</email></author><entry><title type="html">人工智能 LLM 革命破晓：一文读懂当下超大语言模型发展现状</title><link href="https://www.mikecaptain.com/2023/03/06/captain-aigc-2-llm/" rel="alternate" type="text/html" title="人工智能 LLM 革命破晓：一文读懂当下超大语言模型发展现状" /><published>2023-03-06T06:54:13+00:00</published><updated>2023-03-06T06:54:13+00:00</updated><id>https://www.mikecaptain.com/2023/03/06/captain-aigc-2-llm</id><content type="html" xml:base="https://www.mikecaptain.com/2023/03/06/captain-aigc-2-llm/">&lt;p&gt;&lt;img src=&quot;/img/src/2023/03/mikecaptain-llm-revolution-2.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;文本来自我的个人博客：&lt;a href=&quot;https://www.mikecaptain.com/&quot;&gt;MikeCaptain - 麦克船长的技术、产品与商业博客&lt;/a&gt;，初次成稿于 2023 年初春节期间对 LLM（Large Language Models，大型语言模型）的技术演变学习的笔记记录，后来又补充了一些近期的内容（LLaMA 发布、ChatGPT API 发布等）。本文是&lt;a href=&quot;https://www.mikecaptain.com/2023/01/22/captain-aigc-1-transformer/&quot;&gt;《人工智能 LLM 革命前夜：一文读懂横扫自然语言处理的 Transformer 模型》&lt;/a&gt;（以下简称《&lt;strong&gt;船涨：LLM 革命前夜&lt;/strong&gt;》）的续篇，也是前篇中提到过的，近日终于抽了些时间完成了排版、插图与参考内容的完善。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;作者：钟超（船涨）&lt;/li&gt;
  &lt;li&gt;微信：sinosuperman&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;前篇《船涨：LLM 革命前夜》主要讲的是 LLM 发展的神经网络模型一路如何走到 Transformer 的，是过去。本篇《船涨：LLM 革命破晓》讲的是 LLM 近年到现在进行时的演进关键点，是当下。再后续，我还将以商业为第一视角，与大家一起探讨下未来一些年内，AGI 在应用领域的发展路线可能会是怎样的，那将更多围绕推演和畅享。&lt;/p&gt;

&lt;h2 id=&quot;前言&quot;&gt;前言&lt;/h2&gt;

&lt;p&gt;在船涨上一篇文章&lt;a href=&quot;https://www.mikecaptain.com/2023/01/22/captain-aigc-1-transformer/&quot;&gt;《船涨：LLM 革命前夜》&lt;/a&gt;中，我们介绍了引领当下自然语言处理（Natural Language Processing，NLP）领域的基础模型 Transformer。但是 2017 年 Google 发表&lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot;&gt;《Attention is All You Need》&lt;/a&gt;论文介绍 Transformer 后，经过了五年才让我们用到 ChatGPT 这样惊艳的文本生成 AI 模型，这中间发生的变化值得我们一窥究竟。本文试图解答两个问题：1）&lt;strong&gt;从 Transformer 问世后，这几年是哪些关键技术出现使得主流语言模型（Language Models，LM）进化到 ChatGPT 这种程度？&lt;/strong&gt;2）&lt;strong&gt;近几年大型语言模型（Large LM，LLM）的整体进展是怎样的？&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;本文的受众：希望产品、运营等非技术的同学也看的懂，有少量计算机技术基础，且大致对&lt;a href=&quot;https://www.mikecaptain.com/2023/01/22/captain-aigc-1-transformer/&quot;&gt;《船涨：LLM 革命前夜》&lt;/a&gt;所述基础模型有粗浅了解的朋友。&lt;/strong&gt;对于 NLP 领域的学者来讲，本文只是一篇易于理解的解读综述。&lt;/p&gt;

&lt;p&gt;本文包括这几部分内容：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;第一章，初步了解自然语言处理的任务、方法和模型，其中方法部分介绍了近些年直到当下 NLP 的几个技术范式转换阶段，模型方面最好你有阅读过前篇&lt;a href=&quot;https://www.mikecaptain.com/2023/01/22/captain-aigc-1-transformer/&quot;&gt;《船涨：LLM 革命前夜》&lt;/a&gt;。&lt;/li&gt;
  &lt;li&gt;第二章，介绍了 2018 - 2021 年 LLM 关键发展节点与主流模型，从 Transformer 之后的 ELMo，到 GPT-1、BERT、GPT-2、T5、GPT-3，以及 Scaling Law。&lt;/li&gt;
  &lt;li&gt;第三章，探究上下文学习（In-Context Learning）的应用（Prompt Engineering）、底层假设和原理探究，包括思维链（CoT）。&lt;/li&gt;
  &lt;li&gt;第四章，介绍了 2022 - 2023 年初 LLM 发展关键节点与主流模型，延续前面对 GPT 的探讨基础上，一起看下 InstructGPT、ChatGPT 以及近期出现的一些其他 LLM。&lt;/li&gt;
  &lt;li&gt;第五章，未来技术趋势的关注重点，和值得翻阅的关键论文、值得关注的 AI 机构、值得关注的博客或 Twitter 账号。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;阅读本文，先对你过往的基础知识做了一些假设，如果你暂未了解，可能在阅读时遇到以下内容做一些简单地查询即可：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;对人工神经网络有一点点基础，可以通过阅读船涨的&lt;a href=&quot;https://www.mikecaptain.com/2023/01/22/captain-aigc-1-transformer/&quot;&gt;《船涨：LLM 革命前夜》&lt;/a&gt;第一章快速了解。&lt;/li&gt;
  &lt;li&gt;对 Transformer 模型有一点点基础，可以通过阅读船涨的&lt;a href=&quot;https://www.mikecaptain.com/2023/01/22/captain-aigc-1-transformer/&quot;&gt;《船涨：LLM 革命前夜》&lt;/a&gt;第二章快速了解。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;本文第一章是关于自然语言处理的任务、学习训练方法、语言模型基础的一些背景知识，对这些有了解的朋友可以直接跳到第二章开始阅读。希望本文能够以一种按图索骥、深入浅出的方式让大多数同学能了解到当下火热的 LLM 这几年都在发生什么。&lt;/p&gt;

&lt;p&gt;由于船涨近些年主职并非技术，因此本文内容抛砖引玉，若有错误纰漏，请大家联系我及时修正。期待与大家多多交流、学习。
&lt;!-- 声明：本文所有内容均来自公开论文、开源代码及模型、公开博客文章、社交媒体讨论、公开演示 DEMO，不涉及本人所就职机构的任何内部信息，且所发表言论仅代表个人，与本人所就职机构（聚划算$$^{\circledR}$$、淘宝$$^{\circledR}$$或阿里巴巴$$^{\circledR}$$集团）均无关。 --&gt;&lt;/p&gt;

&lt;h2 id=&quot;第一章--初步了解自然语言处理的任务方法和模型&quot;&gt;第一章 · 初步了解自然语言处理的任务、方法和模型&lt;/h2&gt;

&lt;h3 id=&quot;第-1-节--我们要解决什么问题nlp-任务&quot;&gt;第 1 节 · 我们要解决什么问题：NLP 任务&lt;/h3&gt;

&lt;p&gt;自然语言处理领域要实现的终极形态，就是让机器按照人类偏好的方式，与人类通过语言文字高效交互。而人类高等文明中「最高等」的那部分文明，全都是人类在发明了语言文字后才诞生的 —— 物理、数学、生物医药、金融体系、现代通信、航空航天、汽车工业、计算机科学等等。而绘画、音乐等视听刺激，更多来自本能。因此这里船涨再引用&lt;a href=&quot;https://www.mikecaptain.com/2023/01/22/captain-aigc-1-transformer/&quot;&gt;《船涨：LLM 革命前夜》&lt;/a&gt;中一段话：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;从第一性原理角度讲，生成图片的应用广度，远远小于生成文本。文本内容的本质是语言文字的理解与生成，人类历史有 600 万年，但是人类文明历史大概就 6000 年，文明的大发展出现在近 2000 多年的原因，主要来自 3500 多年前人类发明了文字。所以 AI 生成文本，意味着 AI 可以用人类熟悉的方式（语言文字）与人类高效协作，这必将引爆生产力革命。而这必将深入影响电商、内容、游戏、云计算、企业服务等众多领域。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;但是语言文字信息没有显式的结构化，其中还有大量的语言学问题，可以说是非常的复杂。也是在相当长时间里，NLP 领域企图完全解构人类的语言文字的所有细节，以期建立起一套理论，就像数学一样，能够精准描述我们在语言文字中显式、隐式包含的信息。这就是人工智能领域的&lt;strong&gt;符号主义（Symbolism）&lt;/strong&gt;路线，它期望通过「知其所以然」，进而「知其然」。&lt;/p&gt;

&lt;p&gt;而另一个路线则是希望机器像人类一样，给机器灌输一些文本信息后，机器能够自己抽取其中的特征信息，学会语言文字背后的知识。这一体系整体都基于&lt;strong&gt;人工神经网络（Artificial Neural Networks）&lt;/strong&gt;，将多个神经网络层以某种机制连接起来形成一套架构，每层神经网络中又包含数据的输入、输出、对输入的处理方法、处理这些数据所用到的大量参数。这就是人工智能领域的&lt;strong&gt;连结主义（Connectionism）&lt;/strong&gt;路线，它期望先让机器「知其然」，「所以然」这个问题以后再说。连结主义的底层逻辑是经验主义，其交付物是一大堆参数，但是 it works！这是典型的实验科学，如果 AI 的驱动是工业应用的话，这条路线的上游学术研究会被下游的「it works」驱动着跑起来。&lt;/p&gt;

&lt;p&gt;那么下游的任务都有什么呢？&lt;/p&gt;

&lt;h4 id=&quot;11相当长时间里nlp-领域都有大量细分任务&quot;&gt;1.1、相当长时间里，NLP 领域都有大量细分任务&lt;/h4&gt;

&lt;p&gt;由于 NLP 领域面对的问题太过庞杂，因此前面很多年 NLP 领域任务都被拆分的非常细，比如（下列任务大家看个感觉就好，暂时不搞懂细节不影响理解）：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;命名实体识别（Named Entity Recognition，NER）&lt;/strong&gt;：比如对于输入语句「擎天柱回到赛博坦」得到输出「B-PER, I-PER, E-PER, O, O, B-LOC, I-LOC, E-LOC」，其中 B、I、E 分别表示开始、中间、结束，PER、LOC 分别表示人物、地点，O 表示其他无关。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;文本蕴含（Text Entailment）&lt;/strong&gt;：比如对于文本 T「我在杭州」和如下三个假设 H1「我在浙江」、H2「我在上海」、H3「我是杭州人」之间的蕴含关系就是 Positive、Negative、Neutral，其实是个三分类问题。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;常识推理（Common Sense Reasoning）&lt;/strong&gt;：比如一个测试 LM 是否具备常识推理的例子，在句子 A「奖杯无法放进到箱子里，因为它太了」中的「它」指的谁？在句子 B「奖杯无法放进到箱子里，因为它大了」中「它」指的谁？。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;问答（Question Answering）&lt;/strong&gt;。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;词性标注（POS Tagging）&lt;/strong&gt;。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;情感分析（Sentiment Analysis，SA）&lt;/strong&gt;。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;自然语言推理（Natural Language Inference，NLI）&lt;/strong&gt;。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;总结摘要（Summarization）&lt;/strong&gt;。&lt;/li&gt;
  &lt;li&gt;……&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;太多细分任务了，这里不一一举例。每个任务也自然有它的测试数据集，由研究人员开发出新的任务解法（可能是模型创新，也可能是训练方法创新，甚至是一些小 tricks）后去「打榜」，也就是检验下目标任务在测试数据集上的表现如何。下面我引用&lt;a href=&quot;https://arxiv.org/abs/2302.09419&quot;&gt;《A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT》&lt;/a&gt;一文中对 NLP 领域数据集的汇总。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023/02/bert-official-blog-8.png&quot; alt=&quot;&quot; width=&quot;720&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;12我们始终在追求简洁优雅的技术路线技术理论&quot;&gt;1.2、我们始终在追求简洁优雅的技术路线/技术理论&lt;/h4&gt;

&lt;p&gt;对于上面那么多 NLP 任务类型，大致上我们可以把自然语言处理，分成&lt;strong&gt;自然语言理解（Natural Language Understanding，NLU）&lt;/strong&gt;和&lt;strong&gt;自然语言生成（Natural Language Generation，NLG）&lt;/strong&gt;两大类，而 NLU 任务都要输出判断作为结果，离散的判断就是分类（Classification）问题，连续的判断就是回归（Regression）问题，但是回归问题最终也基本会转换为分类问题。因此可以说 NLP 领域的任务主要就是分类、生成。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;分类任务（Classification Tasks）&lt;/strong&gt;更进一步细分，比如包括情感分析（Sentiment Analysis，SA）、命令实体识别（Named Entity Recognition，NER）、自然语言推理（Natural Language Inference，NLI）、文本蕴含（Text Entailment）、词性标注（POS Tagging）、常识推理（Common Sense Reasoning）等等。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;生成任务（Generation Tasks）&lt;/strong&gt;更进一步细分，比如包括摘要（Summarization）、机器翻译（Machine Translation）等等。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;一方面，这里很多任务，其实都不是最终任务，而属于中间任务。什么叫中间任务？就是为了完成现实中生产生活中的某项任务，而根据某种问题解决方法拆解出来的某个过程目标下的任务。NER、POS Tagging 等等都属于中间任务。从技术的优雅发展角度看，我们希望这些繁杂的技术路线都被替代掉。&lt;/p&gt;

&lt;p&gt;另一方面，即使是分类任务，最终也要有输出。那么如果对分类任务做一些改造，其输出就是另一种形式的「生成」。本篇会提到的 T5 模型在这条路线上的研究领域具有关键意义。&lt;/p&gt;

&lt;p&gt;所以回到本篇开头处的探讨，既然高等文明的各项成功都建立在语言文字诞生的基础上，那么回归我们的直觉，机器在交互形式层面，如果能对语言文字进行听说读写，也就能达到人类对自然语言处理领域的终极诉求。听和读是输入，说和写是输出。说和写就对应着文本生成，而能「正确地」说和写就意味着已经能「正确地」听和读了，那从这个意义上说，自然语言处理是不是就等价于文本生成问题？&lt;/p&gt;

&lt;p&gt;是这样吗？从近几年大型语言模型（Large Language Model，LLM）一路走过来看，似乎是这样。看过&lt;a href=&quot;http://localhost:4000/2023/01/22/captain-aigc-1-transformer/&quot;&gt;《船涨：LLM 革命前夜》&lt;/a&gt;文章的朋友们应该还记得，在连结主义的路线上，人工神经网络的基础模型架构是如何从统计语言模型（Statistical Language Models）发展为神经语言模型（Neural Language Models），并在神经语言模型的道路上经历了从 MLP 到最终 Transformer 的迭代演化。Transformer 问世后几乎统一了 NLP 领域的基础模型架构，而一代又一代的模型在这几年时间里大爆发，向通用人工智能（Artificial General Intelligence）方向加速推进了 NLP 的进程，直到大家觉得几乎要逼近奇点的 ChatGPT 出现。这过程中，解决 NLP 问题的技术思路是怎么发展过来的？&lt;/p&gt;

&lt;p&gt;已经说了「要解决什么问题」，所以接下来我们要开始探讨下「解决问题的思路」。既然沿着连结主义的人工神经网络往前走，回忆下&lt;a href=&quot;http://localhost:4000/2023/01/22/captain-aigc-1-transformer/&quot;&gt;《船涨：LLM 革命前夜》&lt;/a&gt;中我们提到的神经网络的基本研究范式。先要把文本转换为一组组的数字（文本表示，Word Representation），再由计算机寻找这些数字之间潜在关联（特征抽取，Feature Extraction）去优化参数（训练，Training）得到一个模型，然后就可以用它处理任务了。因此概括起来，解决问题的思路就变成了如何设计模型架构、设计学习方法。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;模型架构（Model Architecture）&lt;/strong&gt;的设计：这包括如何表示文本、抽取文本特征、以及围绕特征抽取对输入/输出的各种处理方法。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;学习方法（Learning Methods）&lt;/strong&gt;的设计：这包括优化目标函数的选定、训练数据的预处理、训练方法设计。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;这两个问题是相互交织在一起的。整体上 2017 年后的主流模型基本都以 Transformer 为基础架构做变体，但是模型如何学习则经历了很大的范式转换。&lt;/p&gt;

&lt;h3 id=&quot;第-2-节--解决问题的思路之方法神经网络模型学习方法的三个范式阶段&quot;&gt;第 2 节 · 解决问题的思路之方法：神经网络模型学习方法的三个范式阶段&lt;/h3&gt;

&lt;p&gt;如果你了解前篇&lt;a href=&quot;http://www.mikecaptain.com/2023/01/22/captain-aigc-1-transformer/&quot;&gt;《船涨：LLM 革命前夜》&lt;/a&gt;的 2.7 节中的「简述如何训练一个神经网络模型」的话，下面我们可以看下基于此，NLP 领域在几个阶段的不同学习范式。&lt;/p&gt;

&lt;p&gt;卡耐基梅隆大学几位学者在 2021 年的论文&lt;a href=&quot;https://arxiv.org/abs/2107.13586&quot;&gt;《Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing》&lt;/a&gt;中对 NLP 领域到那个时间点的学习范式演进做了总结，我在其基础上做了一些修改，写一下我自己的理解。&lt;/p&gt;

&lt;h4 id=&quot;21第一阶段完全监督学习fully-supervised-learning范式&quot;&gt;2.1、第一阶段：完全监督学习（Fully Supervised Learning）范式&lt;/h4&gt;

&lt;p&gt;先了解下什么是&lt;strong&gt;监督学习（Supervised Learning for NLP）&lt;/strong&gt;。拿「英语翻译成中文」这个机器翻译任务举例，最容易想到的就是如果我们知道很多正确的「输入的英语 x，对应的翻译中文 y」，其中 y 一般叫做输入数据的标签（label）。我们就可以让模型去学习一组 \({(x_i, y_i)}^n_{i=1}\) 这样的数据，来训练好模型中的所有参数。然后在使用模型时，直接面向目标任务的英文输入 x 通过模型输出中文翻译。这种输入数据有标签的方法就叫监督学习，而标签是需要人工标注的。&lt;/p&gt;

&lt;p&gt;这里顺便提一下，既然有监督学习，相应地还有：1）&lt;strong&gt;非监督学习（Unsupervised Learning）&lt;/strong&gt;，训练使用的输入数据是没有标记的；2）&lt;strong&gt;半监督学习（Semi-Supervised Learning）&lt;/strong&gt;，则是用少量有标记的数据和大量无标记的数据训练模型；3）&lt;strong&gt;自监督学习（Self-Supervised Learning）&lt;/strong&gt;则是训练所用的数据虽然没有标记，但是可以被模型生成标记，一般采用自监督学习的典型 NLP 任务是语言建模。&lt;/p&gt;

&lt;p&gt;早期的 NLP 模型还是统计模型时就有监督学习方法，那时很重要的一个环节是研究人员要自己定义特征模板，进而抽取特征。后来神经网络出现后，通过 MLP、CNN、RNN、LSTM、Transformer 等都可以通过训练自己的过程就顺便又比较黑盒地抽取了特征。&lt;/p&gt;

&lt;p&gt;这样完全监督学习大概也分成两个「子阶段」，其时间和核心工作如下：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;非神经网络的完全监督学习（Fully Supervised Learning base on Non-Neural Networks）&lt;/strong&gt;，大概在 2011 年之前，核心工作则聚焦在了特征工程（Feature Engineering）。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;基于神经网络的完全监督学习（Fully Supervised Learning base on Neural Networks）&lt;/strong&gt;，大概在 2011-2017 年间，这一阶段，我们把特征抽取这件事儿交给了模型自己（它的特征工程能力已远超人类），而核心工作变成了如何设计、优化出适合目标任务的模型架构，这其实是相对宏观地来调节特征抽取，例如 MLP、CNN、RNN、LSTM 等基础模型被提出，都有其各自特点适应不同的任务。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;22第二阶段预训练pre-train范式--为了更好的泛化性generalization&quot;&gt;2.2、第二阶段：预训练（Pre-train）范式 —— 为了更好的泛化性（Generalization）&lt;/h4&gt;

&lt;p&gt;完全监督学习的范式非常不优雅，一方面上游标注数据难搞（记作「短板一」），另一方面因为针对特定任务训练，所以下游使用时只能对特定任务起作用（记作「短板二」）。这两个显著的短板，引出下面几个研究的技术路线，跟船涨一起从逻辑上来理解一下。&lt;/p&gt;

&lt;p&gt;思路一：构建 NLP 自己的 ImageNet。针对「短板一」是比较为难的，因为自然语言处理 (NLP) 的最大挑战之一就是训练标注数据的匮乏，这与计算机视觉（CV）领域形成鲜明对比。由于 NLP 任务的多元化，因此研究人员都在面向不同任务领域，各领域也只能有各自的、围绕目标任务的标记数据，很难全领域形成合力构筑统一的、海量的训练标记数据。然而，近些年 NLP 深度学习发展到了比拼大模型的阶段，这些大模型对有标注的大数据量有极度的渴求。填补这个 GAP 的其中一种思路就出现了：搞个 NLP 人的 ImageNet。ImageNet 是包含了超过 1400 万张手动标注图像、涵盖 2 万多个类别的大型图像数据库，这一直让 NLP 领域研究人员非常羡慕搞 CV 的，这些年围绕「NLP 的 ImageNet 时刻到来」的说法不少，也有不少相关研究是朝这个方向努力，但其实一直无法真正和 ImageNet 相提并论。&lt;/p&gt;

&lt;p&gt;思路二：设计一套免标注的训练方法。仍然是沿着上面这个问题，但是填补这个 GAP 的想法是，能否搞出一套无标注数据（或者是少量标注数据）也能训练的方法。&lt;/p&gt;

&lt;p&gt;思路三：上游学习，下游迁移。针对「短板二」，针对特定任务训练的模型，泛化性都很差。那么能否把针对 \(\text{Task}_1\) 任务训练出的模型 \(\text{M}\)，用一些办法迁移到 \(\text{Task}_2\) 上也能跑呢？所以就有了「&lt;strong&gt;迁移学习（Transfer Learning）&lt;/strong&gt;」。&lt;/p&gt;

&lt;p&gt;思路四：上游多任务学习。在迁移学习的路线里，还延伸出一个思路，就是不要模型都训练好了再去迁移，而是一开始训练的时候，就让模型 \(\text{M}\) 对 \(\text{Task}_1\)、\(\text{Task}_2\) 甚至到 \(\text{Task}_n\) 一起训练，训练好后再去使用。这种方法就叫「&lt;strong&gt;多任务学习（Multi-Task Learning）&lt;/strong&gt;」。&lt;/p&gt;

&lt;h5 id=&quot;221这四个思路的交汇点预训练-微调学习范式&quot;&gt;2.2.1、这四个思路的交汇点：「预训练-微调」学习范式&lt;/h5&gt;

&lt;p&gt;本小节标题已经剧透了答案，但我们还是按照逻辑思路理一下。上船！&lt;/p&gt;

&lt;p&gt;我们先看「思路三」和「思路四」。两个思路都是把学习过程分成了两个阶段，三是希望学过某个任务的模型，下游稍微调整一下（也要更新模型参数）后也会解决另一个任务。但四是对三的进一步发展，希望上游学过一堆任务后，下游最好能经过一些调整能解决任何 NLP 任务。这样的两阶段学学习方法，&lt;strong&gt;第一阶段叫「预训练（Pre-train）」&lt;/strong&gt;，&lt;strong&gt;第二阶段叫「微调（Fine-tune）」&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;我们再说「思路二」，免除大量标注工作就能训练模型，NLP 里什么任务具备这个特点呢？就是文本生成任务，人类有海量的文本预料拿来把任意一段话、一句话做截断，前面部分就是 \(x\)，后面部分就是 \(y\)，天然可以作为文本生成的训练数据。这样在语料库中自己构建标记数据的方法，叫「&lt;strong&gt;自监督学习（Self-Supervised Learning）&lt;/strong&gt;」，后面要讲到的 GPT-1 就是这样做的。但是这样是不是只能解决「文本生成」任务？这就与我们前面在 1.2 节聊到的话题对上了，第二章开始讲的模型发展一直到 Google T5 提出的文本生成统一 NLP 任务，就是在回答这个问题。&lt;/p&gt;

&lt;p&gt;最后看下，如果沿着「思路一」搞下去，先说结论，应该不是做出一个类似 ImageNet 的什么 WordNet、TextNet 之类的数据库，而应该是语言建模。我们想一下，人类对图像、文本的理解使用是不同的。对于图像，与具备视觉感受器的生物的原始本能有关，认出它是啥是人类最朴素的目的，然后用于人脸识别、道路标记识别等等，核心是一种「识别」能力。而对于文本，根据不同输入目的，给予不同的输出反馈，例如分类、总结、回答等等，核心是一种「对话」能力。所以对于「聚焦识别」的 CV 领域，构建一个识别好（即标注好）的数据库 ImageNet 是好的解法，而不会去构建一个「人来发图、AI 回复个图」的「斗图对话」系统，或者基于其他什么核心能力的系统；而对于「聚焦对话」的 NLP 领域，构建一个识别好的文本数据库并非好的解法，这个数据库离最终任务距离还很远，而需要的是一个「能对话」的文本系统，所以用「语言建模」作为任务训练模型，然后下游再执行具体任务，这样的方法被提出并验证效果不错，此后逐渐成为预训练的基本范式。&lt;/p&gt;

&lt;p&gt;四个思路所提出的问题和逻辑推演，都 make sense 的话，这些就引导者学界和业界的人们走向了同一个方向：分「预训练-微调」两阶段的训练方法，且预训练阶段的任务是语言建模。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;预训练-微调（Pre-train and Fine-tune）&lt;/strong&gt;学习范式：具体说应该是「&lt;strong&gt;语言建模预训练-微调（Pre-train LM and Fine-tune）&lt;/strong&gt;」范式。一方面以 word2vec、GloVe 等一系列文本表示方法在推动「预训练」先用文本表示做语言建模，这是一股力量，ELMo 在 2018 年的出现是这种思路发展的一个巅峰；另一方面 2017 年出现的 Transformer 能够极强地抽取文本特征进而更好地通过语言建模完成预训练，GPT-1、BERT 等模型都是典型代表，尤其 BERT 把这种范式下的 NLP 研究推向了一个高潮，下面我在本篇第 7 节 BERT 部分会详细讲解。因此这种范式大概从 2017 年开始盛行，要注意微调本身也是一个训练过程，需要更新模型参数。这个阶段的重要研究工作，在于下游微调任务的目标优化，&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;因为语言建模的任务边界非常模糊，因为你很难孤立地用某一个语料集，说它训练后就是完备的，不需要任何其他语料补充。要实现一个使用体验较好的语言建模，需要的语料库是很大的，相应地，构建出来的语言模型的参数规模也要很大才能体现出语料库大的优势。因此，大语言模型（Large Language Models，LLM）就是在这一阶段被提出的。&lt;/p&gt;

&lt;h5 id=&quot;222解决-llm-微调成本高的问题预训练-提示学习范式&quot;&gt;2.2.2、解决 LLM 微调成本高的问题：「预训练-提示」学习范式&lt;/h5&gt;

&lt;p&gt;LLM 模型参数规模非常大，以大家熟悉的 2020 年出现的 GPT-3 为例，模型参数就达到了 1750 亿个。这样下游再去「微调」时，针对一个特定任务，都要更新一个巨大的模型，成本就有点高了。变相地拉高了「预训练-微调」范式的泛化成本。&lt;/p&gt;

&lt;p&gt;如果面对下游任务时，不用更新参数就好了 —— 也就是预训练之后不用微调。能实现吗？2018、2019 年在这方面的探索蛮多的，例如 &lt;a href=&quot;https://arxiv.org/abs/1806.08730&quot;&gt;Salesforce Research 在 2018 年 6 月&lt;/a&gt;、&lt;a href=&quot;https://arxiv.org/abs/1901.11373&quot;&gt;DeepMind 在 2019 年 1 月&lt;/a&gt;、&lt;a href=&quot;https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf&quot;&gt;OpenAI 在 2019 年 2 月&lt;/a&gt; 都提出了这方面类似的论断：语言模型应该可以解决一切文本任务。船涨会在本篇第 8 节 GPT-2 部分重点讲到。这里隐含着语言模型从大量语料里「隐式地」学到了很多任务该怎么做。既然学到了，下游可能就不用微调了，但是你可能需要给出一些「显式」的提醒来唤醒模型学到的「隐式」任务概念，这就是预训练范式下的第二个阶段：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;预训练-提示（Pre-train and Prompt）&lt;/strong&gt;学习范式：同样也可以具体说成是「语言建模预训练 - 提示」范式，举例来说比如你在使用模型时输入「北京，中国；纽约，美国；大阪，日本；悉尼，XXX。XXX 应该是什么？」，模型输出「澳大利亚」，这个输入的部分就是「提示（Prompt）」。Prompt 与 Fine-tune 最大的区别，是 Prompt 是不更新模型参数的，这是一种「上下文学习（In-Context Learning，ICL）」能力，这是一个非常重要的议题，因此本篇第四章将用一整章来探讨 ICL。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这样就从让预训练模型经过「&lt;strong&gt;微调&lt;/strong&gt;」的笨重方式去缩短与下游任务距离的范式，切换到了让下游任务经过「&lt;strong&gt;提示&lt;/strong&gt;」的轻便方式去缩短与预训练模型的距离。&lt;/p&gt;

&lt;p&gt;这个「预训练-提示」范式以 2019 年登场的 GPT-2 模型开始为代表，到 GPT-3 推出后被推向了顶峰，尤其到了 2021 年，甚至衍生出了「提示工程（Prompt Engineering）」。围绕于此的研究论文也井喷式出现（&lt;a href=&quot;http://pretrain.nlpedia.ai/&quot;&gt;下图引用自 pretrain.nlpedit.ai&lt;/a&gt;）：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023/2023-01-23-captain-aigc-2-llm-62.png&quot; alt=&quot;&quot; width=&quot;720&quot; /&gt;&lt;/p&gt;

&lt;h5 id=&quot;223特定领域的更优解预训练-微调-提示学习范式&quot;&gt;2.2.3、特定领域的更优解：「预训练-微调-提示」学习范式&lt;/h5&gt;

&lt;p&gt;对于现实中特定领域的实际应用来说，比如法律、客服、医疗等领域，在「预训练-提示」的范式上也加上针对特定领域的一些训练数据来微调，应用效果会更进一步提高，因此也出现了这样的范式：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;预训练-微调-提示（Pre-train, Fine-tune and Prompt）&lt;/strong&gt;学习范式：OpenAI 在开放 GPT-3 API 后，也推出了 fine-tune 的 API，不过后者价格就贵多了，而且把 fine-tune、使用两个环节分开收费，具体费用汇总可以看&lt;a href=&quot;https://mp.weixin.qq.com/s/xKExN-NbxTFn_eUcvUGR_w&quot;&gt;《AI 应用第一次大爆发来了：一文入门 ChatGPT 官方 API 文档解读》第一部分的第 5 小节「定价」&lt;/a&gt;。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;虽然会有上面说的「微调 LLM 成本高」的问题，但是对于商业化的应用领域，LLM 带来的 AI 性能显著提升，是值得用微调成本换取商业收益的，而且通常这些领域还有私有数据授权的问题，比如 2023 年 3 月 Meta 放出 LLaMA 并提供预训练好的模型下载后，业界很多人在其上开始使用自己的商业敏感数据进行微调再提供服务。&lt;/p&gt;

&lt;h4 id=&quot;23第三阶段预训练-人工反馈强化学习-提示pre-train-rlhf-and-prompt学习范式&quot;&gt;2.3、第三阶段：「预训练-人工反馈强化学习-提示（Pre-train, RLHF and Prompt）」学习范式&lt;/h4&gt;

&lt;p&gt;发展到「预训练」范式阶段，还有什么显而易见的问题？第一是预训练语言模型对数据、模型、算力的要求都太高了，就像当年计算机刚出来时候占满一栋楼一样。第二是 LLM 没有与人类的价值观、道德伦理等方面对齐，存在风险，因此需要推进 helpful、harmless、honest 的模型出现，这就是「对齐（Alignment）」议题。&lt;/p&gt;

&lt;p&gt;由于 LLM 能力太强，因此带来的负面影响亟需兜底策略，否则可能会对人工智能领域发展带来毁灭式的巨大风险，所以对齐问题的优先级从「预训练」范式蓬勃发展后就变得异常之高，也就引出了第三阶段的范式。而第二阶段的范式热潮几乎一直延续到现在（本篇初步成文于 2023 年 1 月春节期间，后又做内容补充发表），但是与之交叠在一起出现的新范式是 2022 上半年，在「预训练-提示」的基础上增加了「人工反馈强化学习」来通过人工干预解决对齐问题，主流模型里 InstructGPT 是率先这样训练的，我会在本篇的第 18 节 InstructGPT 部分详细介绍对齐相关议题。这里我们继续讨论范式。&lt;/p&gt;

&lt;p&gt;先了解下什么是「&lt;strong&gt;强化学习（Reinforcement Learning）&lt;/strong&gt;」。强化学习就类似人类的考试，是有一套标准答案（叫 &lt;strong&gt;「奖励模型（Reward Model）&lt;/strong&gt;」来评判 AI 学习效果的。监督学习在训练时，是模型针对 x 给出的输出值被拿去与标签 y 对比计算损失，进而优化模型参数；而强化学习在训练时，是模型针对 x 给出的输出被拿去由环境反馈评价，进而优化模型参数。&lt;/p&gt;

&lt;p&gt;要将人的伦理道德观念注入给 LLM，所以强化学习中的环境反馈就是「人工反馈」了，因此「&lt;strong&gt;人工反馈的强化学习（Reinforcement Learning with Human Feedback，RLHF）&lt;/strong&gt;」被引入「预训练-提示」范式中，变成了：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;预训练-人工反馈强化学习-提示（Pre-train, RLHF and Prompt）&lt;/strong&gt;学习范式：RLHF 方法最早由 OpenAI 在 2017 年论文&lt;a href=&quot;https://arxiv.org/abs/1706.03741&quot;&gt;《Deep reinforcement learning from human preferences》&lt;/a&gt;中提出，后来 GPT-2、GPT-3 相继发布后出现了虚假新闻、教唆犯罪、消极暗示等大量使用 GPT 系列的负面案例，于是 OpenAI 开始重视 Alignment 并最终在 2022 年上半年的 InstructGPT 上引入 RLHF 方法对齐人类道德伦理，起到很好的效果，后来这也被迭代回了 GPT-3 中以保障 API 调用时遵循人类道德伦理观念。这套范式也被用于大家熟悉的 ChatGPT 中，成为了目前的主流范式。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;与第二阶段类似的，对于特定领域应用，也可以下游的 fine-tune，以期进一步提升效果，即如下范式：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;预训练-人工反馈强化学习-微调-提示（Pre-train, RLHF, Fine-tune and Prompt）&lt;/strong&gt;学习范式：预训练-人工反馈强化学习，都是上游阶段，微调和提示属于下游阶段。目前提供 fine-tune 的 GPT API 背后的 GPT 模型也是有 RLHF 的，已经不是最初没有考虑 alignment 的 GPT 版本了。可以看到 GPT 官方 API 文档中有 Moderation API&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;第-3-节--解决问题的思路之模型初识预训练语言模型&quot;&gt;第 3 节 · 解决问题的思路之模型：初识预训练语言模型&lt;/h3&gt;

&lt;p&gt;语言模型（Language Model）就是词序列的概率分布，因为在给定词序列后能预测下一个词，所以语言模型本身任务就是文本生成。按照技术路线，可以分为统计语言模型（Statistical Language Model）和神经语言模型（Neural Language Model）。&lt;a href=&quot;https://www.mikecaptain.com/2023/01/22/captain-aigc-1-transformer/&quot;&gt;《船涨：LLM 革命前夜》&lt;/a&gt;中提到的 N 元文法就属于统计语言模型，MLP、CNN、RNN、LSTM、Transformer 等等都属于神经语言模型。&lt;/p&gt;

&lt;p&gt;本篇第 3 节提到，从第二阶段的「预训练」范式成为主流开始一直到当下，NLP 领域在模型架构上的探讨，就变成了如何打造一个更强有力抽取语言特征并生成文本的语言模型问题。目前主流语言模型已经全面拥抱 Transformer，甚至可以说 NLP 乃至很多其他 AI 领域也都拥抱了 Transformer。Transformer 变体而来的语言模型，有如下这几类（&lt;a href=&quot;https://arxiv.org/abs/2107.13586&quot;&gt;下图引用自 Liu et al., 2021&lt;/a&gt;）。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023/2023-01-23-captain-aigc-2-llm-63.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;有些文献里，比如 Google 的 T5 论文中（&lt;a href=&quot;https://arxiv.org/abs/1910.10683&quot;&gt;Raffel et al. 2019&lt;/a&gt;）没有把 Masked LM（或叫 Auto-Encoding LM）看做语言模型，只把最后有自左向右根据 x 输出 y 的当做了语言模型，从这点上说其他三个模型从右侧看都是这样。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;自回归语言模型（Auto-Regressive Language Models）&lt;/strong&gt;也被一些文献叫做「&lt;strong&gt;从左到右语言模型（Left-to-Right LM）&lt;/strong&gt;」会用已生成的预测结果做后续新的预测，简单说就是「&lt;strong&gt;Guess the Next&lt;/strong&gt;」，然后把生成的词再放回到输入里组成新的输入继续 Guess the Next，从而使模型的注意力都集中在前面的文本上，从注意力机制上说就是 Casual Multi-Head Self-Attention。TransformerXL、XLNet、OpenAI 的 GPT 系列都属于自回归模型。自回归模型需要学习各词、各词组的文本内部依赖关系。从自回归模型的定义你可以看出，文本生成就是一个典型的要使用自回归模型的任务。曾经一度 NLP 领域还会像上面那样把文本拆分的很细，在所有任务都可以转换成文本生成文本任务的思想统一 NLP 江湖后，你就知道自回归模型其实有着最强大的潜力。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The way Auto-Regression Models actually work is that after each token is produced, that token is added to the sequence of inputs. And that new sequence becomes the input to the model in its next step. This is an idea called “Auto-Regression”. —— Cited from jalammar.github.io&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;自编码语言模型（Auto-Encoding Language Models）&lt;/strong&gt;是通过破坏文本再尝试恢复来学习语言的，简单说就是「&lt;strong&gt;完形填空&lt;/strong&gt;」，会用掩码（Mask，跟计算机网络里说的掩码是一回事儿）挖掉训练预测的词，所以涉及到怎么挖（破坏方法 Corruption）、挖多少（破坏比率 Corruption Rate）、挖多长（破坏长度 Corruption Length），对这类 Corruption 的实验可以看本文 T5 模型的第 3 小节。也因此这类模型也叫 &lt;strong&gt;掩码语言模型（Masked Language Models，MLM）&lt;/strong&gt;，或者掩蔽语言模型。从注意力机制上说，是 Masked Multi-Head Self-Attention。相比自回归模型，自编码模型的学习过程，能看到待预测词的前后内容，所以对文本的理解是更深入的，在同等成本的情况下理论上自编码模型对文本的分类、回归方面的 NLU 问题会有更好性能表现。典型的自编码模型有 BERT、ERNIE、ALBERT、RoBERTa、DistilBERT、ConvBERT、XLM、XLM-RoBERTa、FlauBERT、ELECTRA、Funnel Transformer。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;前缀语言模型（Prefix LM）&lt;/strong&gt;：也算是 Left-to-Right 模式的，但是与自回归语言模型相比，前缀语言模型在抽取输入文本特征时用了 Fully-Visible Mask（Encoder 用的掩码，能看到「过去」和「未来」）而不是 Future Mask（Decoder 用的掩码，只能看到「过去」），而生成本文部分则与自回归语言模型一样，只看到左侧。从注意力机制上讲，用到了 Masked Multi-Head Self-Attention 和 Casual Multi-Head Self-Attention。采用此架构的模型有 UniLM（&lt;a href=&quot;https://arxiv.org/abs/1905.03197&quot;&gt;Dong et al., 2019&lt;/a&gt;）、UniLMv2（&lt;a href=&quot;https://arxiv.org/abs/2002.12804&quot;&gt;Bao et al., 2020&lt;/a&gt;）、ERNIE-M（&lt;a href=&quot;https://arxiv.org/abs/2012.15674&quot;&gt;Ouyang et al., 2020&lt;/a&gt;）。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;编码器-解码器语言模型（Encoder-Decoder LM）&lt;/strong&gt;是把 Transformer 的 Encoder 和 Decoder 都包括在内。其注意力机制，Encoder 部分是 Masked Multi-Head Self-Attention，Decoder 部分是 Casual Multi-Head Cross-Attention 和 Casual Multi-Head Self-Attention 兼具。典型的 Encoder-Decoder 语言模型有 BART（&lt;a href=&quot;https://arxiv.org/abs/1910.13461&quot;&gt;Lewis et al., 2020a&lt;/a&gt;）、T5（&lt;a href=&quot;https://arxiv.org/abs/1910.10683&quot;&gt;Raffel et al, 2020&lt;/a&gt;）、MASS（&lt;a href=&quot;https://arxiv.org/abs/1905.02450&quot;&gt;Song et al., 2019&lt;/a&gt;）。&lt;/p&gt;

&lt;h3 id=&quot;本小节参考&quot;&gt;本小节参考&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;https://arxiv.org/abs/2107.13586&lt;/li&gt;
  &lt;li&gt;https://zhuanlan.zhihu.com/p/395115779&lt;/li&gt;
  &lt;li&gt;http://pretrain.nlpedia.ai&lt;/li&gt;
  &lt;li&gt;https://arxiv.org/abs/1905.02450&lt;/li&gt;
  &lt;li&gt;http://pretrain.nlpedia.ai/data/pdf/plm.pdf&lt;/li&gt;
  &lt;li&gt;https://lifearchitect.ai/chatgpt/&lt;/li&gt;
  &lt;li&gt;https://huggingface.co/docs/transformers/model_summary&lt;/li&gt;
  &lt;li&gt;https://zhuanlan.zhihu.com/p/608047052&lt;/li&gt;
  &lt;li&gt;https://arxiv.org/abs/1905.03197&lt;/li&gt;
  &lt;li&gt;https://arxiv.org/abs/2002.12804&lt;/li&gt;
  &lt;li&gt;https://zhuanlan.zhihu.com/p/424631681&lt;/li&gt;
  &lt;li&gt;https://arxiv.org/abs/1910.10683&lt;/li&gt;
  &lt;li&gt;https://zhuanlan.zhihu.com/p/198964217&lt;/li&gt;
  &lt;li&gt;https://arxiv.org/abs/1910.13461&lt;/li&gt;
  &lt;li&gt;https://zhuanlan.zhihu.com/p/395115779&lt;/li&gt;
  &lt;li&gt;https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1184/lectures/lecture17.pdf&lt;/li&gt;
  &lt;li&gt;https://speech.ee.ntu.edu.tw/~hylee/ml/2021-spring.php&lt;/li&gt;
  &lt;li&gt;https://arxiv.org/abs/1301.3781&lt;/li&gt;
  &lt;li&gt;https://arxiv.org/abs/2012.15674&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;第二章--llm-关键发展节点与主流模型2018---2021&quot;&gt;第二章 · LLM 关键发展节点与主流模型（2018 - 2021）&lt;/h2&gt;

&lt;h3 id=&quot;第-4-节--transformer2017-年-6-月&quot;&gt;第 4 节 · Transformer（2017 年 6 月）&lt;/h3&gt;

&lt;p&gt;船涨在&lt;a href=&quot;/2023/01/22/captain-aigc-1-transformer/&quot;&gt;《船涨：LLM 革命前夜》&lt;/a&gt;中讲述了 NLP 领域基础模型如何发展出 Transformer 的。Transformer 最初只不过想解决机器翻译问题，输入 A 语言的文本序列，输出 B 语言的文本序列，但其强大的能力让其几乎正在统一 AI 个研究领域。Transformer 模型的最大亮点有两方面：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;特征提取能力强：捕获更大范围内的语言结构（Capture longer range linguistic structure）。&lt;/li&gt;
  &lt;li&gt;非常好的并行性。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Transformer 由 Encoder 和 Decoder 组成，其诞生之后各主流语言模型要么基于 Encoder，大致结构示意如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023/2023-01-23-captain-aigc-2-llm-54.png&quot; alt=&quot;&quot; width=&quot;640&quot; /&gt;&lt;/p&gt;

&lt;p&gt;要么基于 Decoder，大致结构示意如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023/2023-01-23-captain-aigc-2-llm-55.png&quot; alt=&quot;&quot; width=&quot;640&quot; /&gt;&lt;/p&gt;

&lt;p&gt;要么基于 Encoder 和 Decoder 整体，比如 Google 推出的 T5。如果想搞懂 Transformer Encoder-Decoder 结构再继续阅读本文的话，建议先看&lt;a href=&quot;/2023/01/22/captain-aigc-1-transformer/&quot;&gt;《船涨：LLM 革命前夜》&lt;/a&gt;。如果你已经大致了解了，后续提到 Encoder、Decoder 部分则不会有疑问，请跟随船涨继续下去啦。&lt;/p&gt;

&lt;h3 id=&quot;第-5-节--elmo词所在的上下文很重要2018-年-2-月&quot;&gt;第 5 节 · ELMo：词所在的上下文很重要（2018 年 2 月）&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023/2023-01-23-captain-aigc-2-llm-35.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;尽管在 2017 年的 Transformer 之后第一个在 NLP 圈子里爆火的模型是 ELMo，但它并不是基于 Transformer 架构。甚至现在回头看它的核心价值都不是完成目标任务方面的出色程度，而是词的表示（Word Presentation），但它对于双向语言模型、词向量等的后续发展都产生了很大影响。&lt;/p&gt;

&lt;p&gt;需要提一下的是，ELMo 的作者们在 2017 年时提出过「TagLM」模型，其思路在 ELMo 推出后回头来看算是一个过渡阶段的思考方案，ELMo 对其思路方向是一个完善。这一阶段的一系列模型（TagLM、ELMo、ULMFiT 等），概括起来都是在从 Word Embeddings 升级为更先进的 Embeddings。&lt;/p&gt;

&lt;p&gt;我们还是重点讲解下最受认可且结构简单清爽的 ELMo 语言模型，先看下 ELMo 解决的最核心问题：一词多义。&lt;/p&gt;

&lt;h4 id=&quot;51一词多义问题&quot;&gt;5.1、一词多义问题&lt;/h4&gt;

&lt;p&gt;无论是 word2vec 还是 GloVe，对于多义词始终没有处理好。这些词嵌入（Word Embeddings）方法都是在语料库中词之间的共现（co-occurrence）统计进行预训练的。比如下面这个例子中，king 和 queen 就会得到完全一样的 word embedding：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023/2023-01-23-captain-aigc-2-llm-27.png&quot; alt=&quot;&quot; width=&quot;490&quot; /&gt;&lt;/p&gt;

&lt;p&gt;尽管这被 GloVe 方法考虑更大范围词频共现后一定程度解决了（没有完全解决）。然而另外一个问题才是最致命的，就是对任何一个词，这类方法都是采用固定的向量，完全没有考虑上下文语境 —— 换句话说，一词多义问题无法解决，比如：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023/2023-01-23-captain-aigc-2-llm-28.png&quot; alt=&quot;&quot; width=&quot;490&quot; /&gt;&lt;/p&gt;

&lt;p&gt;那为什么不考虑上下文呢？实际上，到 2018 年初那个时间点，主流的语言模型基本都只看单向的上下文，即上文（Left Context）或者下文（Right Context）。这与人类在自然语言理解（Natural Language Understanding，NLU）上的真正方式 —— 上下文双向理解 —— 是不一样的。于是很自然的问题就是，实现一个双向的语言模型呀？那为什么一直到 2018 年语言模型还都是单向的？&lt;/p&gt;

&lt;h4 id=&quot;52see-themselves-问题&quot;&gt;5.2、See-Themselves 问题&lt;/h4&gt;

&lt;p&gt;BERT 作者 Jacob Devlin 总结过「到 2018 年时为什么主流语言模型一直未出现双向」的原因：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Words can “see themselves”.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;这个问题就是 See-Themselve 问题，也被一些人叫做 See-Itself 问题，就是在预训练的词向量生成过程中，当学习输入的文本语料时，其中已经包含了自己，则在词表示上形成了递归，如下图则是对这个问题的抽象表示（左侧是单向 LM，右侧是双向 LM）：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023/2023-01-23-captain-aigc-2-llm-30.png&quot; alt=&quot;&quot; width=&quot;650&quot; /&gt;&lt;/p&gt;

&lt;p&gt;那么是否存在可以解决 See-Themselves 问题的双向模型呢？&lt;/p&gt;

&lt;h4 id=&quot;53elmo基于两个单向-lstm-的语言模型&quot;&gt;5.3、ELMo：基于两个单向 LSTM 的语言模型&lt;/h4&gt;

&lt;p&gt;2018 年 2 月 AI2（Allan Institution for AI）在论文&lt;a href=&quot;https://arxiv.org/abs/1802.05365&quot;&gt;《Deep contextualized word representations》&lt;/a&gt;中提出了 ELMo 模型。该论文同时被 ICLR、NAACL 接受，并获得 NAACL 最佳论文奖。ELMo 是「Embeddings from Language Model」的意思，也是呼应美国知名儿童电视节目《芝麻街》里的角色 Elmo，这样给模型起名是比较容易口口相传的。&lt;/p&gt;

&lt;p&gt;ELMo  会&lt;strong&gt;根据上下文信息&lt;/strong&gt;给一个词编码出一个词向量。这样类似下面的 bank 在两个句子中含义不同，则有不同的词向量：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023/2023-01-23-captain-aigc-2-llm-29.png&quot; alt=&quot;&quot; width=&quot;490&quot; /&gt;&lt;/p&gt;

&lt;p&gt;这样的词表示方法在&lt;a href=&quot;https://arxiv.org/abs/1903.08855&quot;&gt;《Linguistic Knowledge and Transferability of Contextual Representations》&lt;/a&gt;中被称为「Contextual Word Representations，CWRs」，后来出现的 GPT、BERT 也都是用关注上下文的方法做词表示（只不过关注的范围有差异），到 2023 年初我写文本的当下，关注上下文的词表示法已经是共识（从语言学上，向这个方向演化也是对的）。&lt;/p&gt;

&lt;p&gt;本节要介绍的这个 ELMo 模型，在解决了一词多义问题的同时，也避开了 See-Themselves 问题。如本小节的剧透，ELMo 用了 BiLSTM（快速了解 BiLSTM，看&lt;a href=&quot;https://www.mikecaptain.com/2023/01/22/captain-aigc-1-transformer/&quot;&gt;《人工智能 LLM 革命前夜：一文读懂横扫自然语言处理的 Transformer 模型》这篇文章的 4.4、4.5 节&lt;/a&gt;）。其结构很好理解，对于任何一个输入文本，用一个 Left-to-Right 的单向 LSTM 模型和一个 Right-to-Left 的单向 LSTM 模型，从而避免了「See-Themselves」问题，如下所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023/2023-01-23-captain-aigc-2-llm-31.png&quot; alt=&quot;&quot; width=&quot;490&quot; /&gt;&lt;/p&gt;

&lt;p&gt;这样每个词的词向量表示都考虑了上下文。这样用 ELMo 得到的文本词向量作为预训练词嵌入（Pre-trained Embeddings），可以作为词嵌入输入给其他流行的模型架构：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023/2023-01-23-captain-aigc-2-llm-32.png&quot; alt=&quot;&quot; width=&quot;260&quot; /&gt;&lt;/p&gt;

&lt;p&gt;下面是一个 ELMo 执行文本预测任务的过程，以此来说明 ELMo 的模型结构（这里只展示了 Left-to-Right 单向 LSTM 部分，Right-to-Left 与之相似略去）：对输入文本先 &lt;strong&gt;Word Embedding 提取单词特征&lt;/strong&gt;，然后经过两层 LSTM，研究表名&lt;strong&gt;第一层 LSTM 能够捕获句法特征&lt;/strong&gt;，&lt;strong&gt;第二层 LSTM 能够提取到语义特征&lt;/strong&gt;。最后到输出层（FFNN 前馈神经网络 + Softmax 回归，这两个概念也在船涨的&lt;a href=&quot;https://www.mikecaptain.com/2023/01/22/captain-aigc-1-transformer/&quot;&gt;《人工智能 LLM 革命前夜：一文读懂横扫自然语言处理的 Transformer 模型》&lt;/a&gt;的 2.8、2.5 中有简洁讲述）得到预测概率最高的词。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023/2023-01-23-captain-aigc-2-llm-33.png&quot; alt=&quot;&quot; width=&quot;490&quot; /&gt;&lt;/p&gt;

&lt;p&gt;更详细地结构描述中，补充一点，即在 ELMo 的第一第二层之间，还有一个 ResNet（ResNet 的快速了解可参考船涨上一篇&lt;a href=&quot;https://www.mikecaptain.com/2023/01/22/captain-aigc-1-transformer/&quot;&gt;《人工智能 LLM 革命前夜：一文读懂横扫自然语言处理的 Transformer 模型》&lt;/a&gt;）的第 9 节），这样可以增加模型的稳定性。&lt;/p&gt;

&lt;h4 id=&quot;54elmo-训练及使用&quot;&gt;5.4、ELMo 训练及使用&lt;/h4&gt;

&lt;p&gt;第一阶段是预训练，就是灌入大量语料给 ELMo，目标是构建语言模型。首先是对输入词的 Embedding，然后经过两层 LSTM，分别提取目标词的 Context-Before 和 Context-After 的特征，之后输出层的细节就不展开了。剩下的就是神经网络基本的反向传播、梯度更新、优化目标函数。预训练结束后我们就得到了一个 ELMo，其包含大量被预训练好的参数，等待用于下游任务。&lt;/p&gt;

&lt;p&gt;第二阶段是下游任务使用。一个具体的下游任务的输入 X，经过第一步预训练好的 ELMo 得到 Pre-trained Embeddings 作为新的输入，给到下游任务模型。因此新的输入 Pre-trained Embeddings 比只是 Word Embeddings 要提取了更多特征，所以第一步的这种预训练也叫 Feature-based Pre-training。&lt;/p&gt;

&lt;h4 id=&quot;55双向语言模型是灯塔吗&quot;&gt;5.5、双向语言模型，是灯塔吗？&lt;/h4&gt;

&lt;p&gt;尽管 AI2 官方称，ELMo 在问答（Question Answering，QA）、文本蕴含（Textual Entailment，TE）、情感分析（sentiment analysis）三类 NLP 任务上取得 SOTA。但是我们回顾下本节前面提到的：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;在 2018 年初那个时间点，主流的语言模型基本都只看单向的上下文，即上文（Left Context）或者下文（Right Context）。这与人类在自然语言理解（Natural Language Understanding，NLU）上的真正方式 —— 上下文双向理解 —— 是不一样的。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;注意这里提到的是 NLU，不是 NLG。对于 NLU，我们在处理文本内容时，通常都是通过上下文来理解的（双向）。而对于 NLG，通常通过上文来影响下文（单向）。但是从自然语言的任务角度说，NLU 和 NLG 其实并没有明显的界限。一个 NLU 任务，最终也要有输出，那么这类任务就可以对输入进行改造变成 NLG 的「形式」。这里先按下不表，在下面 T5 模型时我们会再次提到。&lt;/p&gt;

&lt;p&gt;所以双向语言模型是灯塔吗？上下文一定要都知道才对吗？更具体地说，Left-to-Right 和 Right-to-Left 都要兼顾到的模型才是唯一正确的方向吗？我们继续往下把本章看完再讨论这个话题。&lt;/p&gt;

&lt;h4 id=&quot;56elmo-的局限性&quot;&gt;5.6、ELMo 的局限性&lt;/h4&gt;

&lt;p&gt;在 2018 年 Transformer 已经发布并引起普遍关注的情况下，ELMo 依然选择了基于 LSTM 的架构。因此要讲 ELMo 的局限性，其实主要就是这个视角。相比这一点，对其训练方法、数据集等等的关注显得避重就轻了。所以以下说两点：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;LSTM 特征提取能力的局限：尽管 ELMo 在特征提取上，已经显著优于此前仅依靠 Word Embedding 方法的 word2vec、GloVe 等。但是回头看，后续一系列 Transformer 架构之上的模型表现，都远优于 LSTM 架构的模型，两者的特征抽取能力差距显著。也是因为 Transformer 如此的优秀，后续基本统一了 NLP 的技术路线。&lt;/li&gt;
  &lt;li&gt;双向 LSTM 拼接的局限：EMLo 采用了双向模型，但是没有采用 Transformer Encoder 的架构，而是用双向 LSTM 拼接的方式，相比 Transformer 效果要差。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;本小节参考-1&quot;&gt;本小节参考&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;https://allenai.org/allennlp/software/elmo&lt;/li&gt;
  &lt;li&gt;https://jalammar.github.io/illustrated-bert/&lt;/li&gt;
  &lt;li&gt;https://arxiv.org/abs/1802.05365&lt;/li&gt;
  &lt;li&gt;https://nlp.stanford.edu/seminar/details/jdevlin.pdf&lt;/li&gt;
  &lt;li&gt;https://zhuanlan.zhihu.com/p/51679783&lt;/li&gt;
  &lt;li&gt;https://arxiv.org/abs/1903.08855&lt;/li&gt;
  &lt;li&gt;https://zhuanlan.zhihu.com/p/72839501&lt;/li&gt;
  &lt;li&gt;https://www.jianshu.com/p/81dddec296fa&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;第-6-节--gpt2018-年-6-月&quot;&gt;第 6 节 · GPT（2018 年 6 月）&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023/2023-01-23-captain-aigc-2-llm-34.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;由于 GPT 系列模型已经成为当下阶段 LLM 领域的绝对领导者，因此本文中关于 GPT 系列的每个模型细节的着墨都会稍多一些。&lt;/p&gt;

&lt;h4 id=&quot;61gpt-出现的背景监督学习模型为主但标注和泛化成为卡点&quot;&gt;6.1、GPT 出现的背景：监督学习模型为主，但标注和泛化成为卡点&lt;/h4&gt;

&lt;p&gt;在介绍 GPT 之前，我们说一下它出现的那个时间点的 NLP 模型发展情况。那时 NLP 领域绝大多数的 SOTA 模型，都是针对特定类型任务进行监督学习训练得到的，而监督学习模型有两个严重的限制：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;标注成本极高&lt;/strong&gt;：面对特定任务，需要大量的标注数据用于训练。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;泛化能力极差&lt;/strong&gt;：除了训练过的特定任务，模型很难泛化去做其他任务。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;这两个问题不解决，AI 技术在 NLP 领域很难带来应用的广泛性，更不要提准确性问题了。但是 GPT 的出现，拉开了 NLP 领域预训练大模型对任务大一统的大幕。而后 T5 模型则明确提出了这个断言，而后续 GPT-3 基本实现了这一点。&lt;/p&gt;

&lt;p&gt;OpenAI 官方所称的 GPT，根据其官方定义是 Generative Pre-trained Transformer 缩写，这里大家注意并不是一些文中误传的 Generative Pre-Trained，因为架构的核心理念是 Transformer。&lt;/p&gt;

&lt;h4 id=&quot;62gpt-1基于-transformer-decoder-的自监督训练语言模型&quot;&gt;6.2、GPT-1：基于 Transformer Decoder 的自监督训练语言模型&lt;/h4&gt;

&lt;p&gt;在 2018 年 1 月，Google Brain 团队在文章&lt;a href=&quot;https://arxiv.org/abs/1801.10198&quot;&gt;《Generating Wikipedia by Summarizing Long Sequences》&lt;/a&gt;中提出了一种基于 Transformer 改进，但只有 Decoder 架构的模型，也可以用于构建语言模型。相应地，因为没有 Encoder，这种架构里自然去掉了 Decoder 中的 Encoder-Decoder 注意力层。Google AI Language 团队在&lt;a href=&quot;https://arxiv.org/abs/1808.04444&quot;&gt;《Character-Level Language Modeling with Deeper Self-Attention》&lt;/a&gt;论文的工作中验证了用类似架构构建的语言模型可以逐一生成词。&lt;/p&gt;

&lt;p&gt;OpenAI 在 2018 年 6 月其博客上发布一篇名为&lt;a href=&quot;https://openai.com/blog/language-unsupervised/&quot;&gt;《Improving Language Understanding with Unsupervised Learning》&lt;/a&gt;的文章提出了一种模型，该模型的打造方法包括&lt;strong&gt;生成式预训练（Generative Pre-training）&lt;/strong&gt;和&lt;strong&gt;判别式微调（Discriminative Fine-tuning）&lt;/strong&gt;两个关键阶段，并在一系列不同语言任务上获得了 SOTA 的结果。其实这种方法，早在 GPT-1 推出几年前在 CV（计算机视觉）领域就已经很主流了，其中预训练环节在 CV 领域用的都是大名鼎鼎的 ImageNet。而因为 NLP 领域没有类似 ImageNet 这种海量标注数据可用。最初，大家甚至不知道该称呼它叫什么，所以那段时间提到它的相关文章里你会看到 &lt;strong&gt;Fine-tune Transformer&lt;/strong&gt;、&lt;strong&gt;Fine-tuned Transformer&lt;/strong&gt;、&lt;strong&gt;OpenAI Transformer&lt;/strong&gt; 等等，后来 GPT-2 推出后，大家也就叫 2018 年这个为 GPT-1 了，下文也用该说法。&lt;/p&gt;

&lt;!-- 主要讲的是：在 Google 的 Transformer 架构之上采用无监督预训练方法得到了一个可扩展的、任务无偏（也叫任务无关）的语言模型 --&gt;

&lt;p&gt;GPT-1 与当时主流的 NLP 模型最大的区别是什么？&lt;/p&gt;

&lt;p&gt;首先是基础架构（Architecture），与 Google Brain 团队 2018 年 1 月提出的模式一样，下图是 GPT 采用的 Transformer 模型变体，也用的是 Transformer Decoder，同样因为没有 Encoder 自然也就移除了 Encoder-Decoder Attention，而只采用多头自注意力（Multi-Headed Self-Attention，关于此的介绍可见&lt;a href=&quot;https://www.mikecaptain.com/2023/01/22/captain-aigc-1-transformer/&quot;&gt;《船涨：LLM 革命前夜》&lt;/a&gt;的第 7、8 节），中间的 transformer blocks 一共用了 12 层（作为对比，后续的 GPT-2、GPT-3 分别达到了 48 层、96 层）。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023/2023-01-23-captain-aigc-2-llm-41.png&quot; alt=&quot;&quot; width=&quot;200&quot; /&gt;&lt;/p&gt;

&lt;p&gt;其次是核心方法（Method），GPT-1 采用的是「无监督预训练 + 监督微调」，当时 OpenAI 把这种方法仍然归类为半监督学习，但后来学界和业界都把这种叫做「自监督学习（Self-Supervised Learning）」，后面要介绍的 BERT 也采用这种方法。&lt;/p&gt;

&lt;p&gt;关于架构和这两段式的训练方法，我们分四个小节来看下。&lt;/p&gt;

&lt;h4 id=&quot;63gpt-1-为什么用-transformer-而不是-lstm&quot;&gt;6.3、GPT-1 为什么用 Transformer 而不是 LSTM？&lt;/h4&gt;

&lt;p&gt;回到 2017 年的背景下（虽然是 2018 年初发布，但 GPT-1 的研发是从 2017 年 Transformer 发布后就开始的），哪怕知道 Transformer 是个不错的基础模型，但是当时一个新的语言模型采用 LSTM/RNN 还是 Transformer 并没有现在这么显而易见。为什么选择 Transformer？OpenAI 给出了两个原因。但是船涨认为，按学界的通常情况，都是试出了结果再尝试归因，肯定不是分析出哪个模型更有效然后指哪打哪的。这并非吐槽，因为与理论科学不同，实验科学其实就要这样。&lt;/p&gt;

&lt;p&gt;我们来看下 OpenAI 给出的归因。&lt;/p&gt;

&lt;p&gt;首先一个原因，是 Transformer 有更结构化的记忆可以处理长距离依赖关系（可以理解为更能搞定长文本），这样就意味着不仅是句子维度，甚至段落维度、文章维度上的信息也可以被 Transformer 学习到。&lt;/p&gt;

&lt;p&gt;其次，OpenAI 在做迁移学习的时候，同样不做监督微调的情况下，发现 Transformer 比 LSTM 架构有更好的表现。如下图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023/2023-01-23-captain-aigc-2-llm-71.png&quot; alt=&quot;&quot; width=&quot;480&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;64gpt-1-的无监督预训练unsupervised-pre-training&quot;&gt;6.4、GPT-1 的无监督预训练（Unsupervised Pre-training）&lt;/h4&gt;

&lt;p&gt;能用未标注的数据做无监督的预训练，已经很不容易了，尽管 GPT-1 并不是第一个这样做的（2014 年的 Word2Vec 词嵌入模型也是用的大量无标注文本）。下面我们来理解一下无监督预训练的过程。\(\mathcal{U}={u_1, ..., u_n}\) 是一个无监督词序列语料，那么语言模型给出这样一个词序列的概率是：&lt;/p&gt;

\[\begin{aligned}
P(\mathcal{U}) = P(u_1)P(u_2|u_1)P(u_3|u_1,u_2)...p(u_n|u_1,u_{n-1}) = \prod_i^n P(u_i|u_1, ..., u_{i-1})
\end{aligned}\]

&lt;p&gt;如果模型的上下文窗口（Context Windows）大小是 \(k\) 的话，则上式可近似转化为：&lt;/p&gt;

\[\begin{aligned}
P(\mathcal{U}) = \prod_i P(u_i | u_{i-k}, ..., u_{i-1})
\end{aligned}\]

&lt;p&gt;我们的目标就是让这个概率 \(P(\mathcal{U})\) 最大化，因此我们定义一下目标，即最大化对数似然函数。再将模型的参数 \(\Theta\) 也考虑进来，则其定义如下：&lt;/p&gt;

\[\begin{aligned}
L_1(\mathcal{U}) = \sum_{i} \log P(u_i | u_{i-k}, ..., u_{i-1}; \Theta)
\end{aligned}\]

&lt;p&gt;明确了上面目标函数后，我们来看下 GPT-1 预训练模型。\(U = (u_{-k}, ..., u_{-1})\) 是考虑了上下文的输入词向量矩阵，\(W_e\) 是词嵌入矩阵，\(W_p\) 是位置编码（或叫位置嵌入）矩阵。所有隐藏层都是 transformer_block，第一个隐藏层的输入是 \(h_0\)，每 i 个隐藏层的输出是 \(h_i\)。那么 GPT-1 预训练模型可以表示为：&lt;/p&gt;

\[\begin{aligned}
h_0 &amp;amp;= U W_e + W_p \\
h_l &amp;amp;= \operatorname{transformer\_block}(h_{l-1}) \quad \forall i\in [1, n] \\
P(u) &amp;amp;= \operatorname{softmax}(h_n W_e^T)
\end{aligned}\]

&lt;p&gt;如果你在&lt;a href=&quot;http://www.mikecaptain.com/2023/01/22/captain-aigc-1-transformer/&quot;&gt;《船涨：LLM 革命前夜》&lt;/a&gt;中已经理解了位置编码（Positional Encoding），那么在 GPT-1 模型里，\(U\) 经过 \(W_e\) 处理后每一行就得到了特征（特征抽取）。文本的有序性决定了「位置」本身就是有信息量的，因此叠加 \(W_p\) 则保留了位置相关（position-wise）信息。&lt;/p&gt;

&lt;p&gt;以最大化 \(L_1\) 为目标，经过这样学习大量文本语料后，就得到了一个预训练模型。&lt;/p&gt;

&lt;h4 id=&quot;65gpt-1-的监督微调supervised-fine-tuningsft&quot;&gt;6.5、GPT-1 的监督微调（Supervised Fine-Tuning，SFT）&lt;/h4&gt;

&lt;p&gt;现在我们已经有了一个预训练好的模型了，这一步就是要 fine-tune 它的参数，来适应下游的监督学习任务。对于不同的任务，在 fine-tune 阶段将所有任务的输入结构都转换成 token 序列，喂给已经预训练好的模型来 fine-tune，然后再接一个 linear+softmax。流程结构上表达如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023/2023-01-23-captain-aigc-2-llm-42.png&quot; alt=&quot;&quot; width=&quot;640&quot; /&gt;&lt;/p&gt;

&lt;p&gt;设我们有一个标注过的数据集 \(\mathcal{C}\)，组成它的每个样本都包括一个文本序列 \(x = x^1, ..., x^m\) 和一个标签 \(y\)。微调时，就是把输入 \(x\) 经过预训练模型后在最后一个 Decoder 输出的 \(y\)，进行线性变换和 softmax 回归：&lt;/p&gt;

\[\begin{aligned}
P(y|x^1, ..., x^m) = \operatorname{softmax}(y W_y)
\end{aligned}\]

&lt;p&gt;这个过程中，就学习到了 \(W_l \in \mathbb{R}^{m\times c}\) 参数矩阵，其中 \(c\) 是下游任务目标类别的数量，比如情感分类（positive、neutral、negative）的 \(c\) 为 3。在模型使用时，最后得到的 \(\operatorname{softmax}(y W_y)\) 就能得到一组关于目标类别的概率分布了，其中最大的概率值即可看做是结果。&lt;/p&gt;

&lt;p&gt;监督微调的目标，也是最大化对数似然函数：&lt;/p&gt;

\[\begin{aligned}
L_2(\mathcal{C}) = \sum_{(x,y)}\log P(y|x^1, ..., x^m)
\end{aligned}\]

&lt;p&gt;这样整体看，我们把两个训练过程（无监督预训练、监督训练）联合起来。其中在无监督预训练过程中，我们顺手完成了语言建模，它其实相当于我们的一个辅助目标。我们发现这个辅助目标有两个好处：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;提升了监督模型的泛化能力；&lt;/li&gt;
  &lt;li&gt;加速模型收敛。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;这并非 GPT-1 首次提出，此前剑桥大学的 Marek Rei 在 2017 年 4 月 24 日发表的论文&lt;a href=&quot;https://arxiv.org/abs/1704.07156&quot;&gt;《Semi-supervised Multitask Learning for Sequence Labeling》&lt;/a&gt;中就得出过同样的结论：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;We found that the additional language modeling objective provided consistent performance improvements on every benchmark.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;同样在 2017 年，AI2 的 Matthew E. Peters 等四位学者在 4 月 29 日发表的论文&lt;a href=&quot;https://arxiv.org/abs/1705.00108&quot;&gt;《Semi-supervised sequence tagging with bidirectional language models》&lt;/a&gt; 中也提到了半监督预训练一个语言模型后在 NER 和 Chunking 数据集上都有显著的表现提升：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;In this paper, we proposed a simple and general semi-supervised method using pre-trained neural language models to augment token representations in sequence tagging models. Our method significantly outperforms current state of the art models in two popular datasets for NER and Chunking.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;在这样的「无监督预训练 + 监督训练」方法下，目标函数就是最大化下面这个组合（引入一个 \(\lambda\) 超参数控制无监督预训练权重）：&lt;/p&gt;

\[\begin{aligned}
L_3(\mathcal{C}) = L_2(\mathcal{C}) + \lambda * L_1(\mathcal{C})
\end{aligned}\]

&lt;p&gt;以上整个架构与方法，为后来 GPT 的发展确定了基本的模式，甚至包括后来的商业化。OpenAI 在 2020 年 6 月开放了 GPT 的 API（不过那时候已经不是 GPT-1 了）后，其实提供的是预训练后的模型，另外还给开发者提供了 SFT 的 API。&lt;/p&gt;

&lt;h4 id=&quot;66gpt-1-的预训练数据集&quot;&gt;6.6、GPT-1 的预训练数据集&lt;/h4&gt;

&lt;p&gt;在最初 GPT-1 的论文中，对于预训练数据集的来源和内容，只轻描淡写地提了一句：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;We use the BooksCorpus dataset of training the language model.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;论文中所说的这个 BooksCorpus，其实是把 BookCorpus 拼写错误了。BookCorpus 也被称为 Toronto BookCorpus，是一个包含未出版的、免费的书籍内容的数据集，这些书籍都是来自 SmashWords 电子书网站（这个网站自称是全球最大独立电子书分销商，独立电子书的概念可以类比独立电影、独立游戏）。2018 年 OpenAI 训练 GPT-1 时，OpenAI 称该数据集包含 7000 多本未出版的书籍，4.6 GB 数据。&lt;/p&gt;

&lt;p&gt;为什么用未出版的小说书籍训练呢？在船涨的理解里，这是为了在一个相对隔离的数据集上训练，然后在真实世界中我们可能遇到的问题上做测试，这样可以更好地检验模型的泛化能力。因为 BookCorpus 这些书都是未公开的，而且小说又不像其他书籍，理论上构成的语料也都是原创性的，这样就能更好地检验泛化能力。&lt;/p&gt;

&lt;p&gt;在 2021 年时，一份对 BookCorpus 当时 11038 本书籍各类目分布的统计分析如下（来自 Alan D. Thompson, March 2022, What’S IN MY AI? ），从这个分布里大概能推测 GPT-1 用其中 7000 本书都学了什么：&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;书籍类别&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;书籍数&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;占比&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;书籍类别&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;书籍数&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;占比&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;言情&lt;/td&gt;
      &lt;td&gt;2880&lt;/td&gt;
      &lt;td&gt;26.1%&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;11&lt;/td&gt;
      &lt;td&gt;Adventure&lt;/td&gt;
      &lt;td&gt;390&lt;/td&gt;
      &lt;td&gt;3.5%&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;Fantasy&lt;/td&gt;
      &lt;td&gt;1502&lt;/td&gt;
      &lt;td&gt;13.6%&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;12&lt;/td&gt;
      &lt;td&gt;Other&lt;/td&gt;
      &lt;td&gt;360&lt;/td&gt;
      &lt;td&gt;3.3%&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;科幻&lt;/td&gt;
      &lt;td&gt;823&lt;/td&gt;
      &lt;td&gt;7.5%&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;13&lt;/td&gt;
      &lt;td&gt;Literature&lt;/td&gt;
      &lt;td&gt;330&lt;/td&gt;
      &lt;td&gt;3.0%&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;New Adult&lt;/td&gt;
      &lt;td&gt;766&lt;/td&gt;
      &lt;td&gt;6.9%&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;14&lt;/td&gt;
      &lt;td&gt;Humor&lt;/td&gt;
      &lt;td&gt;265&lt;/td&gt;
      &lt;td&gt;2.4%&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;Young Adult&lt;/td&gt;
      &lt;td&gt;758&lt;/td&gt;
      &lt;td&gt;6.8%&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;15&lt;/td&gt;
      &lt;td&gt;历史&lt;/td&gt;
      &lt;td&gt;178&lt;/td&gt;
      &lt;td&gt;1.6%&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;Thriller&lt;/td&gt;
      &lt;td&gt;646&lt;/td&gt;
      &lt;td&gt;5.9%&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;16&lt;/td&gt;
      &lt;td&gt;Themes&lt;/td&gt;
      &lt;td&gt;51&lt;/td&gt;
      &lt;td&gt;0.5%&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;Mystery&lt;/td&gt;
      &lt;td&gt;621&lt;/td&gt;
      &lt;td&gt;5.6%&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;Total&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;11038&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;100%&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;吸血鬼&lt;/td&gt;
      &lt;td&gt;600&lt;/td&gt;
      &lt;td&gt;5.4%&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;Horror&lt;/td&gt;
      &lt;td&gt;448&lt;/td&gt;
      &lt;td&gt;4.1%&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;Teen&lt;/td&gt;
      &lt;td&gt;430&lt;/td&gt;
      &lt;td&gt;3.9%&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h4 id=&quot;67小结&quot;&gt;6.7、小结&lt;/h4&gt;

&lt;p&gt;从性能表现上来看，在如下这些数据集上的表现大都是超越此前的模型的：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023/2023-01-23-captain-aigc-2-llm-45.png&quot; alt=&quot;&quot; width=&quot;720&quot; /&gt;&lt;/p&gt;

&lt;p&gt;GPT-1 给 NLP 领域带来了两个重要启示与指引：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;GPT-1 基于 Transformer 架构，不同于当时主流模型采用的 LSTM。具体说，是 Transformer 的 Decoder 部分，并移除 Transformer 定义的 Encoder-Decoder Attention（毕竟没有 Encoder）。这样的架构，先天地可以实现无监督训练，让世界上所有自然语言（甚至代码）都有了成为其语料的可能。&lt;/li&gt;
  &lt;li&gt;尽管并非 GPT-1 首创，但是它采用自监督学习的训练方法，具体是语言建模的无监督预训练 + 监督微调训练，为模型带来了更强的泛化能力、更快的收敛速度。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;虽然那个时间点学界与业界觉得 GPT-1 规模不小，但现在回头看它的各维度都还不算暴力。预训练数据量为 4.6GB、上下文滑动窗口为 512 tokens、drop rate 为 0.1，其他基本信息如下：&lt;/p&gt;

&lt;div class=&quot;table-wrapper&quot;&gt;

  &lt;table&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;&lt;strong&gt;模型&lt;/strong&gt;&lt;/td&gt;
        &lt;td&gt;&lt;strong&gt;参数量&lt;/strong&gt;&lt;/td&gt;
        &lt;td&gt;&lt;strong&gt;层数&lt;/strong&gt;&lt;/td&gt;
        &lt;td&gt;&lt;strong&gt;词向量长度&lt;/strong&gt;&lt;/td&gt;
        &lt;td&gt;&lt;strong&gt;注意力头数&lt;/strong&gt;&lt;/td&gt;
        &lt;td&gt;&lt;strong&gt;头宽度&lt;/strong&gt;&lt;/td&gt;
        &lt;td&gt;&lt;strong&gt;Batch Size&lt;/strong&gt;&lt;/td&gt;
        &lt;td&gt;&lt;strong&gt;Learning Rate&lt;/strong&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
        &lt;td&gt;GPT-1&lt;/td&gt;
        &lt;td&gt;117M&lt;/td&gt;
        &lt;td&gt;12&lt;/td&gt;
        &lt;td&gt;768&lt;/td&gt;
        &lt;td&gt;12&lt;/td&gt;
        &lt;td&gt;64&lt;/td&gt;
        &lt;td&gt;64&lt;/td&gt;
        &lt;td&gt;不超过 2.5E-4&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;

&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;另外，OpenAI 也公布了 &lt;a href=&quot;https://github.com/openai/finetune-transformer-lm&quot;&gt;GPT-1 的源码和训练好的模型&lt;/a&gt;，那时的 OpenAI 还是很 Open 的。&lt;/p&gt;

&lt;p&gt;而就在 OpenAI 发布 GPT-1 后没多久，提出 Transformer 模型的 Google 发布了后来几年产生深远影响的、基于 Transformer Encoder 架构的语言模型 —— &lt;strong&gt;BERT&lt;/strong&gt;。&lt;/p&gt;

&lt;h4 id=&quot;本小节参考-2&quot;&gt;本小节参考&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;https://openai.com/blog/language-unsupervised/&lt;/li&gt;
  &lt;li&gt;https://transformer.huggingface.co/doc/gpt&lt;/li&gt;
  &lt;li&gt;https://huggingface.co/docs/transformers/model_doc/openai-gpt&lt;/li&gt;
  &lt;li&gt;https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf&lt;/li&gt;
  &lt;li&gt;https://lifearchitect.ai/whats-in-my-ai/&lt;/li&gt;
  &lt;li&gt;https://zhuanlan.zhihu.com/p/362929145&lt;/li&gt;
  &lt;li&gt;https://medium.com/the-artificial-impostor/notes-improving-language-understanding-by-generative-pre-training-4c9d4214369c&lt;/li&gt;
  &lt;li&gt;https://arxiv.org/abs/1705.00108&lt;/li&gt;
  &lt;li&gt;https://arxiv.org/abs/1704.07156&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;第-7-节--bert2018-年-10-月&quot;&gt;第 7 节 · BERT（2018 年 10 月）&lt;/h3&gt;

&lt;p&gt;OpenAI 发布 GPT-1 后，Google 大受震撼。在其发布 4 个月后的 2018 年 10 月，Google 终于推出了 BERT，它有两个版本 BERT-Base 和 BERT-Large。从性能表现上看，参数规模相当的情况下，BERT-Base 超越 GPT-1，而参数规模更大的 BERT-Large 又远好于 BERT-Base，可以说 Google 又穿上了黄色领骑衫。&lt;/p&gt;

&lt;p&gt;BERT 的名字，在船涨看来也是作者们硬凑出来的，就是为了呼应 ELMo 这种芝麻街角色名字系列。BERT 是 Bidirectional Encoder Representations from Transformers 的缩写。下面是 BERT 的基本信息。&lt;/p&gt;

&lt;div class=&quot;table-wrapper&quot;&gt;

  &lt;table&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;&lt;strong&gt;模型&lt;/strong&gt;&lt;/td&gt;
        &lt;td&gt;&lt;strong&gt;参数量&lt;/strong&gt;&lt;/td&gt;
        &lt;td&gt;&lt;strong&gt;层数&lt;/strong&gt;&lt;/td&gt;
        &lt;td&gt;&lt;strong&gt;词向量长度&lt;/strong&gt;&lt;/td&gt;
        &lt;td&gt;&lt;strong&gt;注意力头数&lt;/strong&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
        &lt;td&gt;BERT-Large&lt;/td&gt;
        &lt;td&gt;3.4 亿&lt;/td&gt;
        &lt;td&gt;24 层&lt;/td&gt;
        &lt;td&gt;1024&lt;/td&gt;
        &lt;td&gt;16&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
        &lt;td&gt;BERT-Base&lt;/td&gt;
        &lt;td&gt;1.1 亿&lt;/td&gt;
        &lt;td&gt;12 层&lt;/td&gt;
        &lt;td&gt;768&lt;/td&gt;
        &lt;td&gt;12&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
        &lt;td&gt;GPT-1&lt;/td&gt;
        &lt;td&gt;1.17 亿&lt;/td&gt;
        &lt;td&gt;12 层&lt;/td&gt;
        &lt;td&gt;768&lt;/td&gt;
        &lt;td&gt;12&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;

&lt;/div&gt;

&lt;p&gt;BERT 公布了&lt;a href=&quot;https://github.com/google-research/bert&quot;&gt;源码和训练好的模型参数供下载&lt;/a&gt;。Google 团队希望 BERT 能够让业内人士，用几个小时甚至几十分钟，就能训练好一个 SOTA 小模型。Google 在论文中称可以在 11 个 NLP 任务上取得 SOTA 结果，甚至包括非常挑战的 SQuAD v1.1 数据集。&lt;/p&gt;

&lt;p&gt;BERT 发布一年半后的 2020 年 3 Google 又为 BERT 发布了&lt;a href=&quot;https://storage.googleapis.com/bert_models/2020_02_20/all_bert_models.zip&quot;&gt;一系列 24 种小模型&lt;/a&gt;共各种场景使用，开源精神一直都贯穿在 Google 的技术路线上。这些小模型有不同的层数和自注意力头数，具体如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023/2023-01-23-captain-aigc-2-llm-60.png&quot; alt=&quot;&quot; width=&quot;640&quot; /&gt;&lt;/p&gt;

&lt;p&gt;以下阅读前需要你初步了解 &lt;strong&gt;Transformer 基本架构&lt;/strong&gt;。如果你此前不了解 Transformer Encoder、Decoder 的架构特点，可以看&lt;a href=&quot;http://www.mikecaptain.com/2023/01/22/captain-aigc-1-transformer/&quot;&gt;《船涨：LLM 革命前夜》&lt;/a&gt;中的第二章。&lt;/p&gt;

&lt;p&gt;下面我们来看看在 Transformer 模型基础上研发的 BERT 吧。&lt;/p&gt;

&lt;h4 id=&quot;71bert-的模型架构双向&quot;&gt;7.1、BERT 的模型架构：双向&lt;/h4&gt;

&lt;p&gt;与 GPT 不同，BERT 采用的是 Transformer 的编码器。但是这样在技术路线上选择的分野，带来的影响非常的大。Transformer 的编码器就像完形填空，在预测每个词时，是知道前后（过去和未来）的文本内容的；但是 Transformer 的解码器仅知道前面的文本（过去）来预测词，相当于在预测未来。&lt;/p&gt;

&lt;p&gt;在 ELMo 一节我们介绍过双向语言模型，这里将单向与双向作对比，举例来说，对于同一句话「I accessed the bank account」，GPT 单向语言模型的学习方法是：&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;I -&amp;gt; I accessed
I accessed -&amp;gt; I access the
I accessed the -&amp;gt; I accessed the bank
I accessed the bank -&amp;gt; I accessed the bank account
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;而 BERT、ELMo 双向语言模型的学习方法是：&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;I accessed the [MASK] account -&amp;gt; [MASK]=bank
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;这里就涉及到两个关键点：首先，上面在 ELMo 提到的，双向语言模型需要解决的「&lt;strong&gt;See-Itself&lt;/strong&gt; 或 &lt;strong&gt;See-Themselves&lt;/strong&gt;」问题。其次，BERT 如何挖词来做完形填空，即 Corruption Technique。&lt;/p&gt;

&lt;p&gt;可以说 OpenAI 做了一个价值更大、难度更大的技术选型，因此如果在类似数据规模、模型规模、训练方法的情况下，GPT 是难有超过 BERT 的表现的。BERT 问世后的几年内，学界与业界的很多人都以为 BERT 是一统江湖的正途，甚至都认为 OpenAI 的 GPT 选择错了技术路线还硬着头皮坚持。这与 2022 年底 ChatGPT 彻底轰动世界形成了鲜明的对比。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023/2023-01-23-captain-aigc-2-llm-61.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;上图展示了 BERT 与当时另外两大主流 NLP 模型 GPT-1、ELMo 的对比。BERT 与 GPT 的共同点是都基于 Transformer 架构，而 BERT 与 ELMo 的共同点是都用了双向架构。&lt;/p&gt;

&lt;h5 id=&quot;711bert-是深度双向deeply-bidirectionalelmo-是浅度双向shallowly-bidrectional&quot;&gt;7.1.1、BERT 是深度双向（Deeply Bidirectional），ELMo 是浅度双向（Shallowly Bidrectional）&lt;/h5&gt;

&lt;p&gt;由于自注意力层的加持，Transformer 有着极其强大的特征提取能力，这使得 Google 在其官方博客上有底气说 BERT 是深度双向模型，而 ELMo 基于双向 LSTM 提取特征的能力只算是浅度双向模型，提出 ELMo 的 AI2 对此也无法辩驳。&lt;/p&gt;

&lt;p&gt;同样基于 Transformer、得益于自注意力，双向模型比单向模型对自然语言有更好的理解，因此在 NLU（Natural Langauge Understanding，NLU）问题上可以轻松取得比单向模型好得多的表现，这也是 GPT-1 相对吃亏的地方。&lt;/p&gt;

&lt;h5 id=&quot;712基于-transformer-encoder-之上-bert-做了哪些架构改进&quot;&gt;7.1.2、基于 Transformer Encoder 之上 BERT 做了哪些架构改进&lt;/h5&gt;

&lt;p&gt;首先是 Input Embedding 处理得到优化，BERT 的 Input Embedding 是三种 Embedding 的求和。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023/02/bert-official-blog-5.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;单词嵌入（Token Embedding）。&lt;/li&gt;
  &lt;li&gt;位置嵌入（Position Embedding）：在 NLP 任务中，词的位置信息非常有影响。&lt;/li&gt;
  &lt;li&gt;片段嵌入（Segment Embedding）：或者叫「句子嵌入」，增加对句子结构的理解。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;72bert-的训练方法&quot;&gt;7.2、BERT 的训练方法&lt;/h4&gt;

&lt;p&gt;BERT 也是采用「无监督预训练 + 监督微调」的方法，与 GPT-1 相同。但毕竟是双向语言模型，BERT 的预训练任务与 GPT-1 不同，有如下两个：Masked Language Modeling（在某些文献中也叫 Mask Language Modeling，MLM）和 Next Sentence Prediction（NSP）&lt;/p&gt;

&lt;h5 id=&quot;721masked-language-modelingmlm预训练任务&quot;&gt;7.2.1、Masked Language Modeling（MLM）预训练任务&lt;/h5&gt;

&lt;p&gt;BERT 具体采用的方法是，随机选择 15% 的 tokens 出来，但是并非把它们全部都 MASK 掉，而是：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;其中的 80% 被替换为 [MASK]，类似 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;my dog is hairy -&amp;gt; my dog is [MASK]&lt;/code&gt;。&lt;/li&gt;
  &lt;li&gt;其中的 10% 被替换为一个随机 token，类似 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;my dog is hairy -&amp;gt; my dog is apple&lt;/code&gt;。&lt;/li&gt;
  &lt;li&gt;剩余的 10% 不变。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这个 80-10-10 是怎么定出来的，Google 团队也是脑拍了几种数字组合试出来的，如下图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023/02/bert-official-blog-6.png&quot; alt=&quot;&quot; width=&quot;480&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;MNLI 任务：Multi-Genre Natural Language Inference 是一个大规模的众包蕴含（entailment）分类任务（&lt;a href=&quot;https://arxiv.org/abs/1704.05426&quot;&gt;Williams et al., 2018&lt;/a&gt;）。给定一对句子，预测第二个句子相对于第一个句子是蕴含、矛盾还是中性。&lt;/li&gt;
  &lt;li&gt;NER 任务：Named Entity Recognition 命名实体识别任务，比如对于输入语句「擎天柱回到赛博坦」得到输出「B-PER, I-PER, E-PER, O, O, B-LOC, I-LOC, E-LOC」，其中 B、I、E 分别表示开始、中间、结束，PER、LOC 分别表示人物、地点，O 表示其他无关。&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;722next-sentence-predictionnsp预训练任务&quot;&gt;7.2.2、Next Sentence Prediction（NSP）预训练任务&lt;/h5&gt;

&lt;p&gt;许多 NLP 任务（比如问答、推理等等）都涉及到句子之间关系的理解，这不会被一般性的语言建模过程学习到。因此 Google 想用预训练阶段的 NSP 任务来解决这个痛点。NSP 预训练任务所准备的数据，是从单一语种的语料库中取出两个句子 \(S_i\) 和 \(S_j\)，其中 50% 的情况下 B 就是实际跟在 A 后面的句子，50% 的情况下 B 是随机取的。这样语言模型就是在面对一个二元分类问题进行预训练，例如：&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;INPUT: [CLS] the man went to [MASK] store [SEP]
       he bought a gallon [MASK] milk [SEP]
LABEL: IsNext
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;INPUT: [CLS] the man [MASK] to the store [SEP]
       penguin [MASK] are flight ##less birds [SEP]
LABEL: NotNext
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;CLS 是一个表示「classification」的 token，SEP 是一个表示「separate」的 token。这样的预训练任务，让 BERT 在词维度的语言知识外，也让 BERT 学习到一些句子维度的语言结构。&lt;/p&gt;

&lt;h4 id=&quot;73bert-的哪些改进是带来最显著性能提升的&quot;&gt;7.3、BERT 的哪些改进是带来最显著性能提升的？&lt;/h4&gt;

&lt;p&gt;BERT 与其他几个主流模型的性能对比如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023/2023-01-23-captain-aigc-2-llm-46.png&quot; alt=&quot;&quot; width=&quot;720&quot; /&gt;&lt;/p&gt;

&lt;p&gt;可以看到 BERT 在当时有着极其出色的表现。那么对于这么出色的表现，从上面 BERT 的架构特色到训练方法，到底什么改进对 BERT 性能的积极影响是最大的？这就要依赖&lt;strong&gt;消融研究&lt;/strong&gt;（Ablation Studies，也可以叫消融实验）了。什么是消融研究？你在一些论文中会经常看到，就是指&lt;strong&gt;删除模型或算法的某些「功能」并查看其如何影响性能&lt;/strong&gt;，也就是物理实验中大家最熟悉的「控制变量」法，我们下面具体看下 Google 对 BERT 做的消融研究实验：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023/02/bert-official-blog-7.png&quot; alt=&quot;&quot; width=&quot;480&quot; /&gt;&lt;/p&gt;

&lt;p&gt;上表中对如下四项做了对比：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;BERT-Base：这个是 baseline，其他所有「变量」都基于此。&lt;/li&gt;
  &lt;li&gt;No NSP：在 BERT-Base 上移除「Next Sentence Prediction」预训练任务。&lt;/li&gt;
  &lt;li&gt;LTR &amp;amp; No NSP：LTR 就是 Left To Right，也就是变成了 GPT 的 Auto-Regression Model 架构，同时也把 NSP 预训练任务移除。&lt;/li&gt;
  &lt;li&gt;+BiLSTM：在 fine-tuning 期间，基于「LTR &amp;amp; No NSP」架构之上增加随机初始化的 BiLSTM。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;可以看到，「LTR &amp;amp; No NSP」与「No NSP」对比，在 5 个任务中的 4 个都显著大幅下降，说明&lt;strong&gt;双向结构的正向影响是最显著的&lt;/strong&gt;。而单独移除 NSP 后，各任务上的表现只小幅下降，但其中在 QNLI 任务上大幅下降（QNLI，Question Natural Language Inference 是基于 Stanford Question Dataset 之上的一个测试推理能力的二元分类任务），这说明&lt;strong&gt;增加句子维度的学习对「推理」有帮助&lt;/strong&gt;。再看在「LTR &amp;amp; No NSP」上加「BiLSTM」也未能拯救性能（只有 SQuAD 提升多一些），说明 &lt;strong&gt;Transformer 特征抽取能力比 BiLSTM 强很多&lt;/strong&gt;。&lt;/p&gt;

&lt;h4 id=&quot;74bert-的数据集&quot;&gt;7.4、BERT 的数据集&lt;/h4&gt;

&lt;p&gt;BERT 比 GPT-1 用的训练数据集要大得多。BERT 同样也用了 BookCorpus（并且继承了 GPT-1 在论文中的拼写谬误「BooksCorpus」）约含 8 亿个词，以及英文维基百科（English Wikipedia）约含 25 亿词。整体来看，BERT 训练数据集大小差不多是 GPT-1 训练数据集的 4 倍左右。&lt;/p&gt;

&lt;h4 id=&quot;75bert-小节&quot;&gt;7.5、BERT 小节&lt;/h4&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;用足够硬的打榜成绩夯实了「预训练 + 微调」学习范式&lt;/strong&gt;：过去我们都是针对某个任务进行训练，让模型成为这个任务领域的专家。但是 NLP 的很多知识是有交叉的，比如语言知识、推理能力等等，各个任务的边界并不泾渭分明，因此总是为了更好解决特定任务而要学习补充其他知识。逐渐地，领域知识的边界越来越模糊，知识范围越来越广，就逐渐自然地向着大语言模型的方向发展了，于是就出现了 GPT、BERT 这种「&lt;strong&gt;预训练 + 微调&lt;/strong&gt;」的学习范式。但是 BERT 对特定任务微调后，由于参数被更新，相应地在其他一些任务上的表现可能就会下降，这就导致&lt;strong&gt;模型的泛化能力受到局限&lt;/strong&gt;。而后来的 GPT-3、InstructGPT 到 ChatGPT，则是在预训练完成后并不针对任何下游任务更新参数。这样的好处是模型泛化能力很好，但是针对到特定任务身上，很肯定没有监督微调的 BERT 好，尤其是在 NLU 类型的任务上。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;开源并开放各种规格的模型下载&lt;/strong&gt;：成为了 2018 到几乎 ChatGPT 出现之前 NLP 领域研究的核心模型。&lt;/li&gt;
  &lt;li&gt;Transformer Encoder 双向模型的特征抽取能力，被充分认可。但其实双向语言模型在生成类任务上并不符合人类自然的语言文字「从前向后」的交互模式，这也为后来 GPT 系列反超埋下伏笔。&lt;/li&gt;
  &lt;li&gt;掀起了模型轻量化的研究热点，尤其在 2020 年推出 24 个小模型后。&lt;/li&gt;
  &lt;li&gt;NSP 预训练任务增加了句子层面的语言结构理解。&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;76动手小实践&quot;&gt;7.6、动手小实践&lt;/h4&gt;

&lt;p&gt;BERT 是主流大模型里，开放源代码和模型参数最好的。我们在本小节用 bert-as-service 来跑个简单的例子，为了让大家在任何个人电脑上都跑的起来，这个例子比较小，我们主要是为了简单实践一下找找感觉。&lt;/p&gt;

&lt;h5 id=&quot;761安装-bert-所需要的各种依赖&quot;&gt;7.6.1、安装 BERT 所需要的各种依赖&lt;/h5&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;conda &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;tensorflow&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;1.14.0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;验证 tensorflow 是否安装正确：&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;tensorflow&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__version__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h5 id=&quot;762下载一个预训练pre-train过的-bert-模型&quot;&gt;7.6.2、下载一个预训练（Pre-Train）过的 BERT 模型&lt;/h5&gt;

&lt;p&gt;官方的模型在这里浏览：https://github.com/google-research/bert#pre-trained-models&lt;/p&gt;

&lt;p&gt;也有一些中文的模型，以下是 ChatGPT 推荐的三个：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;BERT-Base, Chinese：这是 Google 官方提供的中文 BERT 模型，在中文 NLP 任务中表现良好。你可以从 这里下载这个模型。&lt;/li&gt;
  &lt;li&gt;ERNIE：这是由中科院自然语言所提供的中文 BERT 模型，包含了额外的语义信息。你可以从 这里下载这个模型。&lt;/li&gt;
  &lt;li&gt;RoBERTa-wwm-ext：这是由清华大学自然语言处理实验室提供的中文 BERT 模型，在多种中文 NLP 任务中表现良好。你可以从 这里下载这个模型。&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;763安装-bert-的服务端和客户端&quot;&gt;7.6.3、安装 BERT 的服务端和客户端&lt;/h5&gt;

&lt;p&gt;这里我们使用 bert-as-service，bert-as-service 是一种将 BERT 模型部署为服务的方式。该工具使用 TensorFlow Serving 来运行 BERT 模型，并允许通过 REST API 进行调用。根据 bert-as-service 的文档，它已经在 TensorFlow 1.14.0 上测试过。&lt;/p&gt;

&lt;p&gt;在你激活的 conda 环境里，安装 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bert-as-service&lt;/code&gt;：&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# 安装服务端和客户端&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# 更多关于 bert-serving-server 的信息可以参考：https://bert-serving.readthedocs.io/en/latest/index.html&lt;/span&gt;
conda &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;bert-serving-server bert-serving-client 
验证 bert-as-service 是否安装成功
bert-serving-start &lt;span class=&quot;nt&quot;&gt;-h&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h5 id=&quot;764启动-bert-服务端&quot;&gt;7.6.4、启动 BERT 服务端&lt;/h5&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# 命令行下启动BERT服务&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# -num_worker 表示启动几个worker服务，即可以处理几个并发请求，超过这个数字的请求将会在LBS（负载均衡器）中排队等待&lt;/span&gt;
bert-serving-start &lt;span class=&quot;nt&quot;&gt;-model_dir&lt;/span&gt; /模型/的/绝对/路径 &lt;span class=&quot;nt&quot;&gt;-num_worker&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;4
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h5 id=&quot;765编写程序实现-bert-客户端&quot;&gt;7.6.5、编写程序实现 BERT 客户端&lt;/h5&gt;

&lt;p&gt;这里有一个客户端例子可以参考：https://cloud.tencent.com/developer/article/1886981&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;bert_serving.client&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BertClient&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 定义类
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;BertModel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;try&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bert_client&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BertClient&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ip&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;127.0.0.1&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;port&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5555&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;port_out&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5556&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# 创建客户端对象
&lt;/span&gt;            &lt;span class=&quot;c1&quot;&gt;# 注意：可以参考API，查看其它参数的设置
&lt;/span&gt;            &lt;span class=&quot;c1&quot;&gt;# 127.0.0.1 表示本机IP，也可以用localhost
&lt;/span&gt;        &lt;span class=&quot;k&quot;&gt;except&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;raise&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;Exception&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;cannot create BertClient&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;close_bert&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bert_client&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;close&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# 关闭服务
&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;sentence_embedding&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&apos;&apos;&apos;对输入文本进行embedding
          Args:
            text: str, 输入文本
          Returns:
            text_vector: float, 返回一个列表，包含text的embedding编码值
        &apos;&apos;&apos;&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;text_vector&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bert_client&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;text_vector&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# 获取输出结果
&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;caculate_similarity&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vec_1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vec_2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&apos;&apos;&apos;根据两个语句的vector，计算它们的相似性
          Args:
            vec_1: float, 语句1的vector
            vec_2: float, 语句2的vector
          Returns:
            sim_value: float, 返回相似性的计算值
        &apos;&apos;&apos;&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# 根据cosine的计算公式
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;v1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vec_1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;v2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vec_2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;cosine&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cosine&lt;/span&gt;


&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__name__&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;__main__&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# 创建bert对象
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;bert&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BertModel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# --- 输入语句 ----
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;input_a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;请输入语句1: &apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;N&quot;&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;or&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;n&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;bert&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;close_bert&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# 关闭服务
&lt;/span&gt;            &lt;span class=&quot;k&quot;&gt;break&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;input_b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;请输入语句2: &apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# --- 对输入语句进行embedding ---
&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;a_vec&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bert&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sentence_embedding&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;a_vec shape : &apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a_vec&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;b_vec&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bert&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sentence_embedding&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;b_vec shape : &apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b_vec&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# 计算两个语句的相似性
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;cos&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bert&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;caculate_similarity&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a_vec&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b_vec&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;cosine value : &apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cos&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# 如果相似性值大于0.85，则输出相似，否则，输出不同
&lt;/span&gt;        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cos&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.85&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;2个语句的含义相似&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;不相似&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h5 id=&quot;766测试效果&quot;&gt;7.6.6、测试效果&lt;/h5&gt;

&lt;p&gt;在使用 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bert-serving-client&lt;/code&gt; 连接 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bert-serving-server&lt;/code&gt; 时，你需要确保 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bert-serving-server&lt;/code&gt; 使用的模型和 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bert-serving-client&lt;/code&gt; 使用的模型是匹配的，否则会出现错误。&lt;/p&gt;

&lt;p&gt;程序正常运行后，将要求你输入两句话，然后 BERT 计算两句话的相似性。&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;请输入语句1: 
请输入语句2: 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;两句输入好确认后，得到如下形式的结果：&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;a_vec shape :  (768,)
b_vec shape :  (768,)
cosine value :  0.8691698561422959
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h4 id=&quot;本小节参考-3&quot;&gt;本小节参考&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;https://nlp.stanford.edu/seminar/details/jdevlin.pdf&lt;/li&gt;
  &lt;li&gt;https://zhuanlan.zhihu.com/p/49271699&lt;/li&gt;
  &lt;li&gt;https://arxiv.org/abs/2302.09419&lt;/li&gt;
  &lt;li&gt;https://zhuanlan.zhihu.com/p/530524533&lt;/li&gt;
  &lt;li&gt;https://arxiv.org/abs/1704.05426&lt;/li&gt;
  &lt;li&gt;https://github.com/hanxiao/bert-as-service&lt;/li&gt;
  &lt;li&gt;https://cloud.tencent.com/developer/article/1886981&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;第-8-节--gpt-22019-年-2-月&quot;&gt;第 8 节 · GPT-2（2019 年 2 月）&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023/2023-01-23-captain-aigc-2-llm-36.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;虽然 BERT 似乎在结果上「打败」了 GPT-1，但是用 Transformer Encoder（更容易）并且数据规模和参数规模都显著提升，多少有点「胜之不武」，OpenAI 自然不服。BERT 发布后又过了 4 个月，OpenAI 发布了比 BERT 更大的 GPT-2，俨然进入了军备竞赛。前面船涨介绍 GPT-1 时仍以结构理念、训练方法为主，现在介绍这个扩展自 GPT-1 的 GPT-2，则我们主要以它在结构、方法、数据等方面改进了什么为讲解线索。&lt;/p&gt;

&lt;h4 id=&quot;81gpt-2-是对-gpt-1-的直接扩展但更笃定地追逐通用语言模型的理想&quot;&gt;8.1、GPT-2 是对 GPT-1 的直接扩展，但更笃定地追逐「通用语言模型」的理想&lt;/h4&gt;

&lt;p&gt;在 2018 年 6 月，Salesforce Research 团队在论文&lt;a href=&quot;https://arxiv.org/abs/1806.08730&quot;&gt;《The Natural Language Decathlon: Multitask Learning as Question Answering》&lt;/a&gt;中提出「&lt;strong&gt;通用 NLP 模型是无法诞生于一个只着眼在单一度量、数据集和任务的范式中的&lt;/strong&gt;」，同时提出将 NLP 的十项全能任务（Natural Language Decathlon，decaNLP），主张所有 NLP 任务可以转换成问答任务，并且提出了一个基于该思路的实验模型 MQAN 来挑战 decaNLP，尽管性能表现还有差距。&lt;/p&gt;

&lt;p&gt;DeepMind 团队对 Salesforce Research 提出的假设非常认同，并在 2019 年 1 月发表的论文&lt;a href=&quot;https://arxiv.org/abs/1901.11373&quot;&gt;《Learning and Evaluating General Linguistic Intelligence》&lt;/a&gt;中提到：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;A perfect language model should in theory be able to do any linguistic task.&lt;br /&gt;理论上，完美的语言模型应该能够执行任何语言任务。&lt;br /&gt;&lt;br /&gt;We believe that continued progress on generative language models will drive progress on general linguistic intelligence.&lt;br /&gt;我们相信，对生成式语言模型的持续进展，将推动通用语言智能的发展。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;OpenAI 也认同这些理念假设，所以期待构建出能解决任何 NLP 任务的语言模型。OpenAI 在 GPT-2 的工作中，把这些理念假设概括为「由于（所训练的）这些任务是通用语言建模的子集，因此我们可以预期随着更多算力和训练数据的使用，性能将进一步提高。」&lt;/p&gt;

&lt;p&gt;2019 年的情人节，OpenAI 在其官方发布了一篇 blog&lt;a href=&quot;https://openai.com/blog/better-language-models/&quot;&gt;《Better Language Models and Their Implications》&lt;/a&gt;，后又发布了介绍 GPT-2 的论文&lt;a href=&quot;https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf&quot;&gt;《Language Models are Unsupervised Multitask Learners》&lt;/a&gt;。GPT-2 是 GPT-1 的直接扩展，因此还是基于 Transformer Decoder 架构，但是参数扩 10 倍左右，训练数据集扩 10 倍左右。GPT-2 的训练目标也很简单，就是基于一段文本中前面的所有词，预测下一个词。训练方法上，GPT-2 没有对任何任务的任何数据集做针对性的训练，都是直接评估并作为最终结果。&lt;/p&gt;

&lt;p&gt;GPT-2 模型的基本信息如下表，其中可以看出 117M 参数版本的 GPT-2 是对标 BERT-Base，15 亿参数版本的 GPT-2 是对标 BERT-Large。&lt;/p&gt;

&lt;div class=&quot;table-wrapper&quot;&gt;

  &lt;table&gt;
    &lt;tbody&gt;
      &lt;tr&gt;
        &lt;td&gt;Model Name&lt;/td&gt;
        &lt;td&gt;\(n_{params}\)&lt;/td&gt;
        &lt;td&gt;\(n_{layers}\)&lt;/td&gt;
        &lt;td&gt;\(d_{model}\)&lt;/td&gt;
        &lt;td&gt;\(n_{heads}\)&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
        &lt;td&gt;GPT-1&lt;/td&gt;
        &lt;td&gt;117M&lt;/td&gt;
        &lt;td&gt;12&lt;/td&gt;
        &lt;td&gt;768&lt;/td&gt;
        &lt;td&gt;12&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
        &lt;td&gt;&lt;strong&gt;GPT-2 Small&lt;/strong&gt;&lt;/td&gt;
        &lt;td&gt;&lt;strong&gt;117M&lt;/strong&gt;&lt;/td&gt;
        &lt;td&gt;&lt;strong&gt;12&lt;/strong&gt;&lt;/td&gt;
        &lt;td&gt;&lt;strong&gt;768&lt;/strong&gt;&lt;/td&gt;
        &lt;td&gt;&lt;strong&gt;12&lt;/strong&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
        &lt;td&gt;&lt;strong&gt;GPT-2 Medium&lt;/strong&gt;&lt;/td&gt;
        &lt;td&gt;&lt;strong&gt;345M&lt;/strong&gt;&lt;/td&gt;
        &lt;td&gt;&lt;strong&gt;24&lt;/strong&gt;&lt;/td&gt;
        &lt;td&gt;&lt;strong&gt;1024&lt;/strong&gt;&lt;/td&gt;
        &lt;td&gt;&lt;strong&gt;16&lt;/strong&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
        &lt;td&gt;&lt;strong&gt;GPT-2 Large&lt;/strong&gt;&lt;/td&gt;
        &lt;td&gt;&lt;strong&gt;762M&lt;/strong&gt;&lt;/td&gt;
        &lt;td&gt;&lt;strong&gt;36&lt;/strong&gt;&lt;/td&gt;
        &lt;td&gt;&lt;strong&gt;1280&lt;/strong&gt;&lt;/td&gt;
        &lt;td&gt;&lt;strong&gt;16&lt;/strong&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
        &lt;td&gt;&lt;strong&gt;GPT-2 Extra Large&lt;/strong&gt;&lt;/td&gt;
        &lt;td&gt;&lt;strong&gt;1542M&lt;/strong&gt;&lt;/td&gt;
        &lt;td&gt;&lt;strong&gt;48&lt;/strong&gt;&lt;/td&gt;
        &lt;td&gt;&lt;strong&gt;1600&lt;/strong&gt;&lt;/td&gt;
        &lt;td&gt;&lt;strong&gt;24&lt;/strong&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
        &lt;td&gt;BERT-Base&lt;/td&gt;
        &lt;td&gt;110M&lt;/td&gt;
        &lt;td&gt;12&lt;/td&gt;
        &lt;td&gt;768&lt;/td&gt;
        &lt;td&gt;12&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
        &lt;td&gt;BERT-Large&lt;/td&gt;
        &lt;td&gt;340M&lt;/td&gt;
        &lt;td&gt;24&lt;/td&gt;
        &lt;td&gt;1024&lt;/td&gt;
        &lt;td&gt;16&lt;/td&gt;
      &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;

&lt;/div&gt;

&lt;p&gt;在 AI2 网站上可以在线试用基于 GPT-2 的 Next-Token 的语言模型：&lt;a href=&quot;https://demo.allennlp.org/next-token-lm&quot;&gt;https://demo.allennlp.org/next-token-lm&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023/2023-01-23-captain-aigc-2-llm-53.png&quot; alt=&quot;&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;p&gt;GPT-2 模型架构在 GPT-1 做了一些优化，如下几点：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Layer Normalization 移到了输入部分，并在自注意力之后也加了 Layer Normalization。&lt;/li&gt;
  &lt;li&gt;残差层（Residual Layers）的初始化权重用 \(1/\sqrt{N}\) 缩放，其中 \(N\) 是残差层数。&lt;/li&gt;
  &lt;li&gt;词汇表扩展至 50,257.&lt;/li&gt;
  &lt;li&gt;上下文窗口从 512 扩展至 1024.&lt;/li&gt;
  &lt;li&gt;GPT-2 的 batch size 是 512.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;但更重要的亮点在于其对更先进训练方法的成功验证。&lt;/p&gt;

&lt;h4 id=&quot;82gpt-2-大幅改进训练方法&quot;&gt;8.2、GPT-2 大幅改进训练方法&lt;/h4&gt;

&lt;p&gt;GPT-2 的核心亮点，都体现在其论文标题「Language Models are Unsupervised Multitask Learners」中。第一个亮点即「Unsupervised」，可不只是预训练过程无监督，整个学习过程都可以无监督。第二个亮点是「Multitask」，在无监督的情况下还可以把多种不同的任务混合起来学。&lt;/p&gt;

&lt;h5 id=&quot;821zero-shot无需监督微调即可执行下游任务不用-fine-tune&quot;&gt;8.2.1、Zero-Shot：无需监督微调即可执行下游任务，不用 fine-tune&lt;/h5&gt;

&lt;p&gt;GPT-2 的首个重要改进，就是其论文摘要中的前两句话总结：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on task-specific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;GPT-1 及当时所有语言模型的局限性在于，即便采取无监督预训练，仍然需要对特定任务进行监督微调。而 OpenAI 在 GPT-2 上验证了基于数以百万计网页上的无监督学习后就可以执行多种语言任务，比如问答、及其翻译、阅读理解、文本摘要。&lt;/p&gt;

&lt;h5 id=&quot;822multitask-learning多任务学习共享参数更新&quot;&gt;8.2.2、Multitask Learning：多任务学习共享参数更新&lt;/h5&gt;

&lt;p&gt;卡耐基梅隆大学 Rich Caruana 在 1997 年提出了 &lt;a href=&quot;https://link.springer.com/article/10.1023/A:1007379606734&quot;&gt;Multitask Learning（多任务学习）&lt;/a&gt; 这样一个提升模型泛化能力的学习框架，但是经过了二十年发展，NLP 在多任务的训练探索上仍然不成熟。&lt;/p&gt;

&lt;p&gt;不过有两个取得了一定突破的技术路线，值得关注。一个是，2018 年 OpenAI 提出的 GPT-1 与 Google 提出的 BERT 都验证了「不需针对特定任务而只需要增加自注意力即可」的架构可行性。但是这样的技术方案，依然需要用到监督学习，泛化性依然受局限。另一个是，在无监督或极少量监督数据的情况下，在特定任务上也都能取得很好表现，例如常识推理（华盛顿大学几位学者于 2017 年在&lt;a href=&quot;https://maartensap.com/pdfs/schwartz2017story.pdf&quot;&gt;《Story Cloze Task: UW NLP System》&lt;/a&gt;研究中验证）、情感分析（OpenAI 团队 2017 年在&lt;a href=&quot;https://arxiv.org/abs/1704.01444&quot;&gt;《Learning to Generate Reviews and Discovering Sentiment》&lt;/a&gt;研究中验证）。&lt;/p&gt;

&lt;p&gt;OpenAI 团队受到两条研究路线的启发并采用更通用的迁移方法，实现无监督预训练后，在 Zero-Shot 情况下完成多任务，多有任务共享更新同一个 Transformer Decoder 架构之上的模型参数。&lt;/p&gt;

&lt;h4 id=&quot;83gpt-2-的预训练数据集高质量多样性的-webtext&quot;&gt;8.3、GPT-2 的预训练数据集：高质量、多样性的 WebText&lt;/h4&gt;

&lt;p&gt;GPT-1 是拿一堆书预训练的，其实很明显多样性是不足的，尤其只用了小说。&lt;/p&gt;

&lt;p&gt;GPT-2 则用了 40GB 的 WebText 语料（800 万个网页）。具体地，这些网页都是来自 Reddit 的网页中包含的出站链接（Outbound Links），并且获得了至少 3 个 karma，这两点门槛让 OpenAI 认为得到了一些比较高质量的网页（明显质量比 CommonCrawl 整来那些乱七八糟的要高不少）。而且这样得到的数据集，具有非常好的多样性，因此很多任务的示例会自然地被学习到。OpenAI 在论文中提到：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;…… These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.&lt;br /&gt;…… 这些发现为构建语言处理系统指明了一条道路，就是从文本语料中自然出现的样本示例来学习并完成任务。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;当 OpenAI 在研发 GPT-2 时这样认为，已经预示着两点：1）ICL（In-Context Learning），甚至 Prompt Engineering，势必会在语言模型通用性变得更强后，成为一个人人都能参与的研究热点。2）如果各类任务的模式是「隐式」出现在语料中的，那么大规模训练数据就意味着可以覆盖更多任务类型，进而暴力军备竞赛有了理论上的动力。&lt;/p&gt;

&lt;p&gt;从实验表现上，用 WebText 预训练的 GPT-2 优于在 Wikipedia、新闻或书籍上训练的其他语言模型，而无需使用这些训练数据集。&lt;/p&gt;

&lt;h4 id=&quot;84如果预训练直接喂生数据最终的效果怎样&quot;&gt;8.4、如果预训练直接喂生数据，最终的效果怎样？&lt;/h4&gt;

&lt;p&gt;对于预训练数据集的处理，GPT-2 采用了最简单直接、符合我们目标期待的方式：不作任何处理，直接喂生数据（raw text）。&lt;/p&gt;

&lt;h5 id=&quot;841生文本隐式包含任务模式上下文显式提示具象任务&quot;&gt;8.4.1、生文本「隐式」包含任务模式，上下文「显式」提示具象任务&lt;/h5&gt;

&lt;p&gt;GPT-2 直接从原始文本开始学习，而不需要针对任务准备的训练数据，也不需要任何微调，尤其对于问答、阅读理解、summarization、translation 这些任务上，只需要以正确的方式提示经过训练的模型就能得到令人惊讶的结果。当然离 SOTA 还有区别，但作者们表示，从实验表现上看，如果有足够大的未标记数据集和算力，模型在这些任务上也会取得领先表现。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023/2023-01-23-captain-aigc-2-llm-43.png&quot; alt=&quot;&quot; /&gt;
↑ 标注 (+) 的项，表示分数越高越好；标注 (–) 的项，表示分数越低越好。&lt;/p&gt;

&lt;p&gt;这里已经基本预示着，投喂生文本数据，让模型「囫囵吞枣」地学会了不少东西，就像一个小孩子到父亲的书房里翻了很多很多书，知识都学杂了。但是如果启发教育问的好，给一些上下文提示语，模型就能给出很不错的响应。这也就引出了 In-Context Learning、Prompt Engineering 等一系列话题。关于这些的探讨，我们将在「In-Context Learning」那一章详细介绍这方面的研究发展和技术尝试。&lt;/p&gt;

&lt;p&gt;但是显然在 GPT-2 这个阶段，其表现还没有让 OpenAI 这么笃定这件事。比如整体看，OpenAI 发现需要对 GPT-2 多尝试几次才能获得好的样本，尝试的次数取决于模型对上下文的熟悉程度：1）当提示数据中有非常具体的主题（比如英国脱欧、指环王等）时，GPT-2 能在一半的时间内生成合理的样本。2）对于高度技术性或深奥类型的内容，GPT-2 就表现不太行了。比如对于数据集 Natural Questions 上，OpenAI 给出测试问答的例子：&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;任务&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;Question Answering&lt;/strong&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;数据集&lt;/td&gt;
      &lt;td&gt;Natural Questions&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;示例&lt;/td&gt;
      &lt;td&gt;Who wrote the book the origin of species?&lt;br /&gt;&lt;br /&gt;&lt;strong&gt;Correct answer: Charles Darwin&lt;/strong&gt;&lt;br /&gt;Model answer: Charles Darwin&lt;br /&gt;&lt;br /&gt;What is the largest state in the U.S. by land mass?&lt;br /&gt;&lt;br /&gt;&lt;strong&gt;Correct answer: Alaska&lt;/strong&gt;&lt;br /&gt;Model answer: California&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h5 id=&quot;842llm-军备竞赛的序幕拉开&quot;&gt;8.4.2、LLM 军备竞赛的序幕拉开&lt;/h5&gt;

&lt;p&gt;整体来看，GPT-2 在以下这些数据集上执行的对应任务，虽然很多没到 SOTA，但效果还可以，毕竟是没有针对性的任务数据拿来训练的。还是上面提到的那句，OpenAI 认为「由于（所训练的）这些任务是通用语言建模的子集，因此我们可以预期随着更多算力和训练数据的使用，性能将进一步提高」。这也为 GPT-3 留下了巨大的空间：&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;任务&lt;/th&gt;
      &lt;th&gt;数据集&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Summarization: summarize news articles&lt;/td&gt;
      &lt;td&gt;CNN and Daily Mail dataset&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Machine Translation: translate French sentences to English&lt;/td&gt;
      &lt;td&gt;WMT-14 Fr-En&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Reading Comprehension: answer questions about given passages&lt;/td&gt;
      &lt;td&gt;CoQA&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Common Sense Reasoning: resolution of an ambiguous pronoun&lt;/td&gt;
      &lt;td&gt;Winograd Schema Challenge&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Question Answering&lt;/td&gt;
      &lt;td&gt;Natural Questions&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Language Modeling of Broad Contexts: predict the last word of a passage&lt;/td&gt;
      &lt;td&gt;LAMBADA&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;在 GPT-2 的开发与公布阶段，OpenAI 就已经跃跃欲试 LLM 下一阶段的可能范式：基于扩展性极好的 Transformer Decoder 架构上（撑得起巨量参数规模）构建模型，并投喂足够多的数据（海量数据已经潜在包括各种任务模式）进行无监督预训练（所有任务都「隐式」地变成从左至右的生成训练） —— 从而拉开了「大模型、大数据、大算力」的军备竞赛大幕。&lt;/p&gt;

&lt;h4 id=&quot;85openai-初步预见了-llm-可能带来的影响&quot;&gt;8.5、OpenAI 初步预见了 LLM 可能带来的影响&lt;/h4&gt;

&lt;p&gt;其实如果大家对 2019 年的科技新闻还有印象的话，一定记得当时说有一家美国公司搞了个 AI 模型编造假新闻给大家忽悠得一愣一愣的，也就是 GPT-2。另外 GPT-2 在发布时只放出来一个小模型，所以也就是从这时开始 OpenAI 被人调侃为 ClosedAI 的。&lt;/p&gt;

&lt;p&gt;当时 OpenAI 认为 LLM 正变得越来越可扩展、可定制、生成连贯，而这可以给各行各业带来很大积极价值，也能拿来作恶。因此 OpenAI 最初发布博客和论文时没有放出完整的模型，只给出了一个用于研究的实验小版本：https://github.com/openai/gpt-2 。具体地，GPT-2 没有放出数据集、训练代码、GPT-2 模型参数。OpenAI 认为那些可能的危害是必须预防的，所以要预期到。&lt;/p&gt;

&lt;h5 id=&quot;851openai-在-2019-年就倡议政府监管&quot;&gt;8.5.1、OpenAI 在 2019 年就倡议政府监管&lt;/h5&gt;

&lt;blockquote&gt;
  &lt;p&gt;It is not possible to control research in these domains without slowing down the progress of AI as a whole.&lt;br /&gt;如果管控这些领域的研究，就不可能不减缓整个人工智能的进展。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;OpenAI 希望大家对此要有预期，不要抵触可能出现的管控。甚至 OpenAI 早在 GPT-2 出现的 2019 年这个时间点就提出倡议：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;政府应考虑扩大或启动更系统地监测 AI 技术的社会影响和扩散，并要能够量化 AI 系统能力的迭代发展情况。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;这其实就预示着后来关于「对齐（Alignment）」这个议题被更重视的提出，顺带必然要面对的「对齐税（Alignment Tax）」，也因此有为了 Alignment 而出现的 InstructGPT，进而孕育出 ChatGPT。&lt;/p&gt;

&lt;h5 id=&quot;852同年-5-月公布-345-亿参数版本并暗示了后来微软对-openai-不-open-的影响&quot;&gt;8.5.2、同年 5 月公布 3.45 亿参数版本，并暗示了后来微软对 OpenAI 不 Open 的影响&lt;/h5&gt;

&lt;p&gt;同年 5 月，OpenAI 公开发布了 GPT-2 的 3.45 亿参数版本，因为 OpenAI 认为很多机构已经能够训练同等能力的模型，所以风险不大。&lt;/p&gt;

&lt;p&gt;另外，对那些致力于促进社会为 LLM 的广泛影响做好准备的合作伙伴与安全社区，OpenAI 开放了 7.62 亿参数和 15 亿参数的版本。这里 OpenAI 提到：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;These research partnerships will be a key input to our decision-making on larger models.&lt;br /&gt;这些研究方面的合作关系将是我们在决定发布更大模型时的关键因素。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;这个在 2019 年时间节点上的表态，暗示了此后 OpenAI 与 Microsoft 的合作关系，对于 OpenAI 后续模型的公开程度的影响，给各方在打预防针。后来在 2023 年 1 月 23 日 OpenAI 博客上也发布了&lt;a href=&quot;https://openai.com/blog/openai-and-microsoft-extend-partnership/&quot;&gt;《OpenAI 与 Microsoft 扩大合作伙伴关系》&lt;/a&gt;一文提到 Microsoft 从 2019 年开始对 OpenAI 的投资。&lt;/p&gt;

&lt;p&gt;而在 5 月这次发布的内容，还包括&lt;a href=&quot;https://github.com/openai/gpt-2-output-dataset&quot;&gt;GPT-2 output dataset&lt;/a&gt;。&lt;/p&gt;

&lt;h4 id=&quot;86gpt-2-小节&quot;&gt;8.6、GPT-2 小节&lt;/h4&gt;

&lt;p&gt;GPT-2 的亮点和洞察：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;自回归语言模型虽然难但正确，可以隐式地从语料中学到各类任务概念（有研究认为其包含隐式马尔科夫模型，详见本篇第 14 节），继续显著提高性能及泛化能力的方法，就是加大训练数据规模、模型参数规模。&lt;/li&gt;
  &lt;li&gt;无需监督微调阶段：处理下游任务时不需要 fine-tune，预示未来在 LLM 上以 Prompt 方式完成任务可能成为一种新范式。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;本小节参考-4&quot;&gt;本小节参考&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;https://openai.com/blog/better-language-models/&lt;/li&gt;
  &lt;li&gt;https://jalammar.github.io/illustrated-gpt2/&lt;/li&gt;
  &lt;li&gt;https://github.com/openai/gpt-2&lt;/li&gt;
  &lt;li&gt;https://github.com/openai/gpt-2-output-dataset&lt;/li&gt;
  &lt;li&gt;https://zhuanlan.zhihu.com/p/56869079&lt;/li&gt;
  &lt;li&gt;https://demo.allennlp.org/next-token-lm&lt;/li&gt;
  &lt;li&gt;https://openai.com/blog/openai-and-microsoft-extend-partnership/&lt;/li&gt;
  &lt;li&gt;https://link.springer.com/article/10.1023/A:1007379606734&lt;/li&gt;
  &lt;li&gt;https://arxiv.org/abs/1806.08730&lt;/li&gt;
  &lt;li&gt;https://arxiv.org/abs/1901.11373&lt;/li&gt;
  &lt;li&gt;https://maartensap.com/pdfs/schwartz2017story.pdf&lt;/li&gt;
  &lt;li&gt;https://arxiv.org/abs/1704.01444&lt;/li&gt;
  &lt;li&gt;https://www.mikecaptain.com/2023/01/22/captain-aigc-1-transformer/&lt;/li&gt;
  &lt;li&gt;https://zhuanlan.zhihu.com/p/350017443&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;第-9-节--t5提出所有-nlp-任务可统一为文本生成任务2019-年-10-月&quot;&gt;第 9 节 · T5：提出所有 NLP 任务可统一为文本生成任务（2019 年 10 月）&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023/2023-01-23-captain-aigc-2-llm-38.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Google 团队在 2019 年 10 月发布了一个对 NLP 任务大一统的 T5 模型&lt;a href=&quot;https://arxiv.org/abs/1910.10683&quot;&gt;《Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer》&lt;/a&gt;。T5 模型是基于 Transformer 整体 Encoder-Decoder 架构的。这篇论文有三个后来被大家常提起的关注点 —— &lt;strong&gt;对下游 NLP 任务的统一&lt;/strong&gt;、&lt;strong&gt;新开源数据集 C4 的引入&lt;/strong&gt;和&lt;strong&gt;多到惊人的实验数据&lt;/strong&gt;。&lt;/p&gt;

&lt;h4 id=&quot;91对下游-nlp-任务的统一&quot;&gt;9.1、对下游 NLP 任务的统一&lt;/h4&gt;

&lt;p&gt;最直接的价值，就是提出「&lt;strong&gt;所有基于文本的语言问题，都可以转换成文本生成文本（text-to-text）形式&lt;/strong&gt;」：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;具体地，只要在编码前，加上任务提示（比如 summarize、translate English to German）就可以在模型输出得到想要的结果。下面这张图可以说是提到 T5 模型必会引用的图。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023/2023-01-23-captain-aigc-2-llm-12.png&quot; alt=&quot;&quot; width=&quot;640&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;该图里举例了语言翻译、分类问题（来自 cola 数据集）、回归问题的语义文本相似评测（STSB）、摘要生成。除了图中提到这四种任务，T5 模型所有下游任务都可以转化（transfer）为 text-to-text 任务，所以这也是 T5 模型名称的由来：Text-To-Text Transfer Transformer。&lt;/p&gt;

&lt;p&gt;而且这个方法并不局限于 Transformer Encodoer-Decoder 架构，其他架构的模型也可以。&lt;/p&gt;

&lt;h4 id=&quot;92新开源数据集-c4colossal-clean-crawled-corpus&quot;&gt;9.2、新开源数据集 C4（Colossal Clean Crawled Corpus）&lt;/h4&gt;

&lt;p&gt;作者对从互联网上爬取的公开网页数据集 Common Crawl（来自 commoncrawl.org）的 6.1TB 数据过滤处理为 745GB 的「庞大而干净的爬取语料库」，也就是它名字（Colossal Clean Crawled Corpus）的由来，但其实军事迷或爱玩射击游戏的朋友们都知道，作者主要是为了凑成「C4」炸药这个缩写。&lt;/p&gt;

&lt;p&gt;过滤处理的主要手段包括：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;只保留以句号、感叹号、问号或结束引号结尾的行。&lt;/li&gt;
  &lt;li&gt;干掉了少于 5 个句子的页面，只保留了至少包含 3 个单词的行。&lt;/li&gt;
  &lt;li&gt;干掉了任何包含「&lt;a href=&quot;https://github.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words&quot;&gt;List of Dirty, Naughty, Obscene or Otherwise Bad Words&lt;/a&gt;」语料库中任意词语的页面。&lt;/li&gt;
  &lt;li&gt;干掉了所有包含 Javascript 这个词儿的行（因为许多爬取的页面都包含类似「请启用 JavaScript」这样的警告）。&lt;/li&gt;
  &lt;li&gt;干掉了所有包含印刷及排版领域常用的哑元文本（又叫乱数假文）lorem ipsum —— 你可能在调试打印机时见过。&lt;/li&gt;
  &lt;li&gt;干掉了所有包含代码的页面。特别低，因为编程语言常用的大括号 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;{&lt;/code&gt; 其实不出现在自然语言中，于是所有包含大括号的页面也都被干掉了。&lt;/li&gt;
  &lt;li&gt;为了删除数据集的重复数据，我们丢弃了除在数据集中出现不止一次的任何三句跨度中的一个以外的所有内容。&lt;/li&gt;
  &lt;li&gt;干掉了所有非英文页面（是否为英文页面由 &lt;a href=&quot;https://pypi.org/project/langdetect/&quot;&gt;langdetect&lt;/a&gt; 来做分类，概率门槛设定为 0.99）。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这里值得说的一点是，后来 GPT 系列模型里的 text-davinci-002、text-davinci-003、ChatGPT 具备推理能力，主要就是靠喂代码喂出来的，尽管代码段与 AI 学习自然语言的逻辑推理能力表面上看似无关。&lt;/p&gt;

&lt;p&gt;过滤清洗过的 C4，从表现上来看比没有过滤的数据集，能让模型结果表现更好。对于 C4 数据集，目前你可以通过如下链接访问该数据集：&lt;a href=&quot;https://www.tensorflow.org/datasets/catalog/c4&quot;&gt;https://www.tensorflow.org/datasets/catalog/c4&lt;/a&gt;。&lt;/p&gt;

&lt;h4 id=&quot;93大量实验覆盖模型数据算力的多维度因素&quot;&gt;9.3、大量实验覆盖模型、数据、算力的多维度因素&lt;/h4&gt;

&lt;p&gt;最为学界与业界啧啧称奇、惊叹不已的，是 T5 在实验算力投入上的一掷千金。T5 作者们对不同的架构、多种训练方法、不同文本破坏设置、不同的数据集、不同迁移方法、不同模型规模与训练时长组合等多维度都进行了实验，实验数据就是下面这张震撼的表格：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023/2023-01-23-captain-aigc-2-llm-13.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;这些因素里，其中有几点，我们摘出来看下。首先，T5 作者们解释了三种不同的自注意力（Self-Attention）机制：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023/2023-01-23-captain-aigc-2-llm-16.png&quot; alt=&quot;&quot; width=&quot;490&quot; /&gt;&lt;/p&gt;

&lt;p&gt;输入输出全部有关联的注意力，即 Fully-Visible Attention；输出只与当前输入、过去输入有关的注意力机制，叫 Casusal Attention；在 Causal Attention 基础上，让输出文本的前缀部分与所有输入文本有关，是 Causal Attention with Prefix。&lt;/p&gt;

&lt;p&gt;基于 Transformer 模型架构，T5 作者们进行了几个不同变种：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023/2023-01-23-captain-aigc-2-llm-15.png&quot; alt=&quot;&quot; width=&quot;490&quot; /&gt;&lt;/p&gt;

&lt;p&gt;第一个是就是 Transformer 的 Encoder-Decoder，Encoder 用 fully-visible attention，Decoder 用 causal attention；第二个相当于 Transformer 的 Decoder 部分，即只用 causal attention；第三个是在 Decoder 基础上前缀部分用 fully-visible attention，其余部分用 causal attention。&lt;strong&gt;实验表明第一种 Transformer 架构表现更好&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;另外再看下多样预训练方法、不同文本破坏设置的对比：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023/2023-01-23-captain-aigc-2-llm-14.png&quot; alt=&quot;&quot; width=&quot;490&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;第一步对比「&lt;strong&gt;预训练方法（Pre-Training Method）&lt;/strong&gt;」。从好理解的角度讲，图中第一个的 language modeling 指的是「续写」式的 GPT（或者说 Transformer 中的 Decoder，自左向右让 AI 预测后面是啥）来做预训练，第二个是「完形填空」式的 BERT（或者说 Transformer 的 Encoder，左右双向文本都知道，AI 来预测中间是啥），第三个是「乱序还原」式的 Deshuffling（先打乱顺序，再让 AI 来还原）。从实验表现上，第二种 BERT-Style 完形填空时的胜出了。&lt;/li&gt;
  &lt;li&gt;第二步对比「&lt;strong&gt;破坏方法（Corruption Scheme）&lt;/strong&gt;」。Mask 方法是每个 token 都被挖掉替换成掩码特殊字符；Replace Spans 方法是在 Mask 基础上如果有连续被挖掉的，则连续的几个合为一个整体掩码特殊字符；Drop 则是直接丢弃，不插入掩码特殊字符。实验表明 Replace Spans 表现更好。&lt;/li&gt;
  &lt;li&gt;第三步对比「&lt;strong&gt;破坏比率（Corruption Rate）&lt;/strong&gt;」，就是挖掉百分之多少的 token 来做「完形填空」，实验表明 15% 是最好的。&lt;/li&gt;
  &lt;li&gt;第四步对比「&lt;strong&gt;破坏长度（Corrupted Length）&lt;/strong&gt;」，就是一次挖多长的空比较好，实验表明 3% 是最好的。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;因此 T5 模型就是通过 BERT-Style 预训练方法（具体地是用文本破坏方法 Replace-Spans、文本破坏比率 15%、文本破坏长度 3）得到的 Transformer 架构 NLP 模型。&lt;/p&gt;

&lt;h4 id=&quot;94t5-小节&quot;&gt;9.4、T5 小节&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;尽管此前已经有人提出 NLP 的统一模型理念，比如 Salesforce Research 团队、DeepMind 团队、OpenAI 团队在发布 GPT-2 时。但 NLP 领域比较确定性地进入通用任务的语言模型阶段，是从 T5 正式发布后。&lt;/li&gt;
  &lt;li&gt;ELMo 那一节提到的「NLU 和 NLG 其实并没有明显的界限」，以及 GPT-2 在致力探索的「通用语言模型」，到 T5 断言「所有基于文本的语言问题，都可以转换成文本生成文本」，可以说 NLP 主流研究及前沿应用全面向「语言建模预训练-提示」范式转换，其中语言建模预训练就是将任务全部统一为文本生成。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;本小节参考-5&quot;&gt;本小节参考&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html&lt;/li&gt;
  &lt;li&gt;https://arxiv.org/abs/1910.10683&lt;/li&gt;
  &lt;li&gt;https://github.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words&lt;/li&gt;
  &lt;li&gt;https://www.tensorflow.org/datasets/catalog/c4&lt;/li&gt;
  &lt;li&gt;https://commoncrawl.org/the-data/&lt;/li&gt;
  &lt;li&gt;https://zhuanlan.zhihu.com/p/88438851&lt;/li&gt;
  &lt;li&gt;https://github.com/google-research/t5x&lt;/li&gt;
  &lt;li&gt;https://zhuanlan.zhihu.com/p/589869911&lt;/li&gt;
  &lt;li&gt;https://towardsdatascience.com/understanding-t5-model-text-to-text-transfer-transformer-model-69ce4c165023&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;第-10-节--缩放定律scaling-lawai-时代的摩尔定律2020-年-1-月&quot;&gt;第 10 节 · 缩放定律（Scaling Law）：AI 时代的摩尔定律（2020 年 1 月）&lt;/h3&gt;

&lt;p&gt;2020 年 1 月 OpenAI 在论文&lt;a href=&quot;https://arxiv.org/abs/2001.08361v1&quot;&gt;《Scaling Laws for Neural Language Models》&lt;/a&gt;中提出了 LLM 的「Scaling Law」，这是一个经验性的总结，但此后学界与业界基本对此形成共识。&lt;/p&gt;

&lt;p&gt;具体地，OpenAI 指出，随着算力（具体说就是训练迭代次数）、数据规模（训练集的 tokens 数）、参数规模的指数增长，测试集上验证的损失是线性下降的（即可理解为性能线性提升）。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023/2023-01-23-captain-aigc-2-llm-10.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;这也就是当下 LLM 的三要素。用船涨自己的理解概括 Scaling Law 就是说：LLM 三要素（算力、数据规模、参数规模）中任一要素的指数增长，都会带来模型性能的线性增长。但别急，这还没完，请往下看完。&lt;/p&gt;

&lt;p&gt;但尽管我们堆叠 LLM 三要素中任何一个，都能带来性能提升，但是三者的增长收益是不同的，我们看下面的图。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023/2023-01-23-captain-aigc-2-llm-9.png&quot; alt=&quot;&quot; width=&quot;640&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;这张图说明，提升参数规模带来的性能提升最显著（当然不能无脑堆参数，也要优化），提升训练数据规模带来收益次之，最差的是提升算力消耗（对应训练迭代次数）。因此之后再提 LLM 的 Scaling Law，大家一般指的是「参数规模的指数增长，可以带来模型性能的线性增长」。&lt;/p&gt;

&lt;p&gt;这就像 AI 界的摩尔定律一样，谁要是想要性能提升多少，根据 Scaling Law 预估下模型参数规模就可以了。于是一场原本就在进行的 AI 领域模型参数规模的军备竞赛，进入了显性竞争阶段，似乎 AGI（通用人工智能）即将到来的大幕正在伴随模型参数规模的提升而徐徐拉开。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023/2023-01-23-captain-aigc-2-llm-11.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;上图数据来自&lt;a href=&quot;https://arxiv.org/abs/1910.01108&quot;&gt;《DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter》&lt;/a&gt;和&lt;a href=&quot;https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/&quot;&gt;《
Turing-NLG: A 17-billion-parameter language model by Microsoft》&lt;/a&gt;。当下语言模型的参数大型化趋势已经成为共识（但这反而值得我们警惕），例如 OpenAI 的 GPT-3 规模为 1750 亿，Google 的 LaMDA 规模为 1370 亿，PaLM 的规模为 5400 亿，DeepMind 的 Gogher 规模为 2800 亿.&lt;/p&gt;

&lt;h3 id=&quot;第-11-节--gpt-32020-年-5-月&quot;&gt;第 11 节 · GPT-3（2020 年 5 月）&lt;/h3&gt;

&lt;p&gt;在 GPT-2 发布 1 年零 3 个月后的 2020 年 5 月，OpenAI 团队发布 GPT-3，从其论文&lt;a href=&quot;https://arxiv.org/abs/2005.14165&quot;&gt;《Language Models are Few-Shot Learners》&lt;/a&gt;可以看到，其最大亮点是数据规模、参数规模都比 GPT-2 大 100 倍。这么简单粗暴的办法，带来的是 GPT-3 表现上的炸裂。而其论文标题也点出了本文的主题：语言模型就是要无监督预训练 + Few-Shot Prompt。&lt;/p&gt;

&lt;h4 id=&quot;111gpt-3-表明-openai-进一步收紧其技术开放度&quot;&gt;11.1、GPT-3 表明 OpenAI 进一步收紧其技术开放度&lt;/h4&gt;

&lt;p&gt;首先要明白 OpenAI 在技术开放上的收紧策略，自 GPT-2 发布时就已经给公众打了预防针了，这点船涨在本篇 8.5.2 中已经提到。因此我们能看到 GPT 系列上 OpenAI 变得越来越 Closed：GPT-1、GPT-2 发布时，OpenAI 都在其官网发布了 blog，到了表现远超过 GPT-1、GPT-2 的 GPT-3 时，OpenAI 反而连一篇博客都没写。而其论文更是对关键的训练、模型、数据做了哪些重要工程表现得轻描淡写，花大篇幅着墨于实验及结果上。&lt;/p&gt;

&lt;p&gt;GPT-3 没有放出源码、预训练好的模型参数等等，俨然变成了各国网友们调侃的 ClosedAI 了。&lt;/p&gt;

&lt;p&gt;对于模型架构，OpenAI 声称 GPT-3 与 GPT-2 是一样的。GPT-3 依然延续了此前 GPT-2 的基本架构和预训练方法：构建基于 Transformer Decoder 的自回归语言模型，然后进行无监督预训练，无针对特定任务的微调。&lt;/p&gt;

&lt;h4 id=&quot;112gpt-3-参数规模直接比-gpt-2-放大-100-倍&quot;&gt;11.2、GPT-3 参数规模：直接比 GPT-2 放大 100 倍&lt;/h4&gt;

&lt;p&gt;我们可以看到共 8 个参数规模的模型如下。&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Model Name&lt;/td&gt;
      &lt;td&gt;\(n_{params}\)&lt;/td&gt;
      &lt;td&gt;\(n_{layers}\)&lt;/td&gt;
      &lt;td&gt;\(d_{model}\)&lt;/td&gt;
      &lt;td&gt;\(n_{heads}\)&lt;/td&gt;
      &lt;td&gt;\(d_{head}\)&lt;/td&gt;
      &lt;td&gt;Batch Size&lt;/td&gt;
      &lt;td&gt;Learning Rate&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;GPT-3 Small&lt;/td&gt;
      &lt;td&gt;125M&lt;/td&gt;
      &lt;td&gt;12&lt;/td&gt;
      &lt;td&gt;768&lt;/td&gt;
      &lt;td&gt;12&lt;/td&gt;
      &lt;td&gt;64&lt;/td&gt;
      &lt;td&gt;0.5M&lt;/td&gt;
      &lt;td&gt;6.0E-4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;GPT-3 Medium&lt;/td&gt;
      &lt;td&gt;350M&lt;/td&gt;
      &lt;td&gt;24&lt;/td&gt;
      &lt;td&gt;1024&lt;/td&gt;
      &lt;td&gt;16&lt;/td&gt;
      &lt;td&gt;64&lt;/td&gt;
      &lt;td&gt;0.5M&lt;/td&gt;
      &lt;td&gt;3.0E-4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;GPT-3	Large&lt;/td&gt;
      &lt;td&gt;760M&lt;/td&gt;
      &lt;td&gt;24&lt;/td&gt;
      &lt;td&gt;1536&lt;/td&gt;
      &lt;td&gt;16&lt;/td&gt;
      &lt;td&gt;96&lt;/td&gt;
      &lt;td&gt;0.5M&lt;/td&gt;
      &lt;td&gt;2.5E-4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;GPT-3 XL&lt;/td&gt;
      &lt;td&gt;1.3B&lt;/td&gt;
      &lt;td&gt;24&lt;/td&gt;
      &lt;td&gt;2048&lt;/td&gt;
      &lt;td&gt;24&lt;/td&gt;
      &lt;td&gt;128&lt;/td&gt;
      &lt;td&gt;1M&lt;/td&gt;
      &lt;td&gt;2.0E-4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;GPT-3	2.7B&lt;/td&gt;
      &lt;td&gt;2.7B&lt;/td&gt;
      &lt;td&gt;32&lt;/td&gt;
      &lt;td&gt;2560&lt;/td&gt;
      &lt;td&gt;32&lt;/td&gt;
      &lt;td&gt;80&lt;/td&gt;
      &lt;td&gt;1M&lt;/td&gt;
      &lt;td&gt;1.6E-4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;GPT-3	6.7B&lt;/td&gt;
      &lt;td&gt;6.7B&lt;/td&gt;
      &lt;td&gt;32&lt;/td&gt;
      &lt;td&gt;4096&lt;/td&gt;
      &lt;td&gt;32&lt;/td&gt;
      &lt;td&gt;128&lt;/td&gt;
      &lt;td&gt;2M&lt;/td&gt;
      &lt;td&gt;1.2E-4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;GPT-3	13B&lt;/td&gt;
      &lt;td&gt;13.0B&lt;/td&gt;
      &lt;td&gt;40&lt;/td&gt;
      &lt;td&gt;5140&lt;/td&gt;
      &lt;td&gt;40&lt;/td&gt;
      &lt;td&gt;128&lt;/td&gt;
      &lt;td&gt;2M&lt;/td&gt;
      &lt;td&gt;1.0E-4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;GPT-3	175B&lt;/td&gt;
      &lt;td&gt;175.0B&lt;/td&gt;
      &lt;td&gt;96&lt;/td&gt;
      &lt;td&gt;12288&lt;/td&gt;
      &lt;td&gt;96&lt;/td&gt;
      &lt;td&gt;128&lt;/td&gt;
      &lt;td&gt;3.2M&lt;/td&gt;
      &lt;td&gt;0.6E-4&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;在模型&lt;strong&gt;参数规模&lt;/strong&gt; \(n_{params}\) 的军备竞赛方面，可以看到 GPT-3 Small 是与 GPT-1（1.17 亿参数）、BERT-Base（1.1 亿参数）对应的；GPT-3 Medium 是与 BERT-Large（3.4 亿参数）对应的；其余六个的参数规模都直接超过了这些大模型，尤其 GPT-3 1750 亿参数的版本，直接从 GPT-2 最大的 15.4 亿版本基础上拉升了 100 多倍规模！也比以往任何一个非稀疏模型至少大 10 倍！&lt;/p&gt;

&lt;p&gt;模型的 Transformer decoder block 的&lt;strong&gt;层数&lt;/strong&gt; \(n_{layers}\)，也常叫&lt;strong&gt;模型深度&lt;/strong&gt;。可以看到随着模型参数规模的急速拉升，OpenAI 团队并没有把模型深度急剧增加，与 GPT-2 152M 版本相比，比它大一百多倍的 GPT-3 175B 的层数也只不过是其两倍（96 vs. 48）。&lt;/p&gt;

&lt;p&gt;模型的&lt;strong&gt;词向量长度&lt;/strong&gt;，也常叫&lt;strong&gt;模型宽度&lt;/strong&gt;，一般表示为 \(d_{model}\)，区分于注意力头的宽度 \(d_{head}\)。与 GPT-2 对比就能看到，类似层数的模型情况下，GPT-3 要宽很多，可以说整体 GPT-3 的模型要胖一些，而不是在提升规模时更多去加深（加层数）。&lt;/p&gt;

&lt;p&gt;GPT-3 的 &lt;strong&gt;batch size&lt;/strong&gt; 都非常大，哪怕 Small 版本也有 0.5M，175B 版本更是达到 3.2M，这对内存要求非常高。如果 batch 处理并行的话，那么内存就是就是 3.2M 除以并行的数量。注意一般 batch 指的是每次训练迭代中模型处理的样本数，而不是具体的数据量，即 3,200,000 个样本，而不是 3.2MB 数据。把 batch size 做大的好处是，降低模型训练时的通讯量。不过对于小模型，batch size 太大的话很容易过拟合。对于参数量大的模型，batch size 自然也要相匹配的变大。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;LR（Learning Rate，学习率）&lt;/strong&gt;在 GPT-3 里是随着 batch size 增加而下降的，这与当时一些研究的结果是相反的。&lt;/p&gt;

&lt;h4 id=&quot;113gpt-3-的训练数据&quot;&gt;11.3、GPT-3 的训练数据&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023/2023-01-23-captain-aigc-2-llm-6.png&quot; alt=&quot;&quot; width=&quot;640&quot; /&gt;&lt;/p&gt;

&lt;p&gt;上面这个表格，是 GPT-3 用到的训练数据集。关于 GPT-3 的训练数据集解读，很多中文文章里说的是错的，主要是对论文中该表的数据理解错误，下面我们分别来看下各个数据集和整体数据处理做的核心工作。&lt;/p&gt;

&lt;h5 id=&quot;1131训练数据集&quot;&gt;11.3.1、训练数据集&lt;/h5&gt;

&lt;p&gt;GPT-3 的数据源来自五部分组成，包括一个大型的来自 CommonCrawl.org 数据集、扩展的 WebText 数据集、两个互联网上的书籍语料库（一般认为书籍的语料质量是非常高的）和英文的维基百科。表中各列的含义如下：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;第二列 Quantity&lt;/strong&gt;：每个数据源本身的数据规模，单位是 tokens（根据 OpenAI 官方文档里提到的一个大概的经验是，通常英文文本里 1 token 有 4 个字母或者 0.75 个单词）。&lt;/li&gt;
  &lt;li&gt;训练期间从给定数据集中提取的部分，就是&lt;strong&gt;第三列 Weight in training mix（数据混合训练时的权重）&lt;/strong&gt;，因为不同数据集质量不同，基于此考虑 OpenAI 结合数据质量因素做的配比。最后，GPT-3 整体上是设定了 3000 亿 tokens 的训练数据集。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;第四列&lt;/strong&gt;表示各数据集在&lt;strong&gt;训练时出现的次数&lt;/strong&gt;，最高的 wikipedia 是 3.4 次。这样其实是为了高质量训练，稍微接受一点点的过拟合。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;说到这里，不得不提中文 LLM 的数据集问题。中文整体上是缺乏这些非营利组织语料库、高质量文本内容社区/百科平台内容的。类似 StackOverflow 这种技术问答社区、WikiPedia 高质量百科。&lt;/p&gt;

&lt;h5 id=&quot;1132提高数据质量的处理准备工作&quot;&gt;11.3.2、提高数据质量的处理准备工作&lt;/h5&gt;

&lt;p&gt;在上述数据集基础上，GPT-3 使用如下方式提高了数据集的质量。&lt;/p&gt;

&lt;p&gt;首先，用一个高质量数据集作为正例，用 LogisticRegression 过滤了 CommonCrawl 的数据。这个高质量数据集是什么呢？还记得 GPT-2 里采用的 WebText 数据（Reddit 外链，且 Karma 大于 3）吗？OpenAI 认为这是质量比较高的数据，就是将其作为正例来过滤的。过滤前的文本压缩规模是 45TB，过滤后的是 570GB。&lt;/p&gt;

&lt;p&gt;其次，在文档级别上用 LSH 算法去除重复的数据。LSH 是 Locality Sensitive Hashing（局部敏感哈希），是信息检索领域的一个常用算法，可以快速判断一个词集合（文章就是一个词集合）和一个很大集合之间的相似度。&lt;/p&gt;

&lt;p&gt;第三，再额外加一些高质量的数据，上面表格中 WebText2 就是基于 GPT-2 用的 WebText 扩展而来的，另外还有 English Wikipedia、两个电子书数据集。&lt;/p&gt;

&lt;h4 id=&quot;114gpt-3-的训练开销&quot;&gt;11.4、GPT-3 的训练开销&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023/2023-01-23-captain-aigc-2-llm-58.png&quot; alt=&quot;&quot; width=&quot;720&quot; /&gt;&lt;/p&gt;

&lt;p&gt;上图展示了 GPT-3 与 BERT、RoBERTa、T5 的训练算力对比。可以看到 GPT-3 的算力开销有多么惊人。注意图标的纵轴是非线性的、指数级间隔。那么消耗这么多的算力，性能表现如何呢？我们看下面这张表。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023/2023-01-23-captain-aigc-2-llm-59.png&quot; alt=&quot;&quot; width=&quot;640&quot; /&gt;&lt;/p&gt;

&lt;p&gt;上表中横轴是算力（对应上上表中的纵轴，注意同样是指数增长），纵轴是训练期间的验证损失（注意也是指数增长）。可以看到每一条曲线都有一个大致的拐点，在拐点之后继续增加算力并不能显著提升性能。所有这个最优拐点连起来，是完全符合 Scaling Law 的（关于 Scaling Law 可以看本文的「第 10 节」）。&lt;/p&gt;

&lt;p&gt;OpenAI 官方并没有公开讲过花了多少钱训练 GPT-3，市面上流传的说法「460 万美元」目前考证来看是一家云服务厂商用其最低价格 GPU 云服务估算而写的一篇软广，并不可信。也有一些其他组织或个人做了测算，整体上都是想表达训练一次很贵。至少从 OpenAI 的论文里我们能看出来，这个训练花费已经贵到研究人员即使发现多个 bug 也没舍得重新训练的地步：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;论文第 9 页：Unfortunately, a &lt;strong&gt;bug&lt;/strong&gt; in the filtering caused us to ignore some overlaps, and &lt;strong&gt;due to the cost&lt;/strong&gt; of training it was not feasible to retrain the model.&lt;br /&gt;中文翻译：不幸的是，过滤中的一个 BUG 导致我们忽略了一些重叠，而考虑到训练成本，重新训练模型是不可行的。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;论文第 31 页：Unfortunately, a &lt;strong&gt;bug&lt;/strong&gt; resulted in only partial removal of all detected overlaps from the training data. Due to the cost of training, it wasn’t feasible to retrain the model.&lt;br /&gt;中文翻译：不幸的是，一个 BUG 导致仅删除了训练数据中检测到的所有重叠的一部分。考虑到训练成本，重新训练模型是不可行的。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;论文第 44 页：Due to a &lt;strong&gt;bug&lt;/strong&gt; revealed by this analysis, filtering described above failed on long documents such as books. Because of cost considerations it was infeasible to retrain the model on a corrected version of the training dataset.&lt;br /&gt;中文翻译：由于此分析揭示的 BUG，上述过滤在长文档（比如书籍）上是失败的。出于成本考虑，在训练数据集的修正版本上重新训练模型是不可行的。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;作为对比，我们看下：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Meta AI 在同样参数规模的&lt;a href=&quot;https://arxiv.org/abs/2205.01068&quot;&gt;《OPT: Open Pre-trained Transformer Language Models》&lt;/a&gt; 模型上，用约 1000 个 80G A100 GPU 上训练至少两个月时间，就可想而知这花费有多高昂了。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;2022 年 7 月，为了训练拥有 1760 亿个参数的开源模型 Bloom，Hugging Face 的研究人员耗时三个月，使用了 384 个英伟达 A100 GPU，数据来自其论文&lt;a href=&quot;https://arxiv.org/abs/2211.05100&quot;&gt;《BLOOM: A 176B-Parameter Open-Access Multilingual Language Model》&lt;/a&gt;。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;115in-context-learning&quot;&gt;11.5、In-Context Learning&lt;/h4&gt;

&lt;p&gt;OpenAI 在 GPT-3 发布中显式地提出了 In-Context Learning，即在无监督训练好的 GPT-3，使用时用少量示例就可以得到有较好的输出反馈，这就叫 Few-Shot Prompt。只有 1 个示例的时候就叫 One-Shot Prompt，没有示例的时候就叫 Zero-Shot。对于在使用时出现在输入中的这些示例，模型是不会更新其参数来做 fine-tune 的。那么模型是怎么从这些示例学到东西的呢？我们把这样的学习方法叫 In-Context Learning，即模型从无监督的训练文本上下文里，完成了非显性的学习。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023/2023-01-23-captain-aigc-2-llm-57.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In-Context Learning 与 Fine-Tune 两者区别在于是否更新模型参数。In-Context Learning 这种神奇的能力为什么会 work，船涨将和你在本文的「第 14 节 · 语言模型是如何具备 ICL 能力的？」一起来初步探索下（目前这个领域还没有完全清晰明确的理论证明）。OpenAI 评估模型性能时，对示例不同做了区分：&lt;/p&gt;

&lt;p&gt;Few-Shot Learning，对每个子任务提供 10-100 个样本。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023/2023-01-23-captain-aigc-2-llm-51.png&quot; alt=&quot;&quot; width=&quot;480&quot; /&gt;&lt;/p&gt;

&lt;p&gt;One-Shot，Few-Shot 一个特殊情况是只给 1 个样本，我们叫 One-Shot Learning。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023/2023-01-23-captain-aigc-2-llm-50.png&quot; alt=&quot;&quot; width=&quot;480&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Zero-Shot Learning 顾名思义无样本。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023/2023-01-23-captain-aigc-2-llm-49.png&quot; alt=&quot;&quot; width=&quot;480&quot; /&gt;&lt;/p&gt;

&lt;p&gt;对位对比，Fine-Tune 如下图示例，能看到要进行多次梯度更新。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023/2023-01-23-captain-aigc-2-llm-48.png&quot; alt=&quot;&quot; width=&quot;480&quot; /&gt;&lt;/p&gt;

&lt;p&gt;GPT-3 得出了一些与 ICL 有关的一些实验结论。模型性能表现，随着示例样本数增加而增加，如下图所示。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023/2023-01-23-captain-aigc-2-llm-56.png&quot; alt=&quot;&quot; width=&quot;640&quot; /&gt;&lt;/p&gt;

&lt;p&gt;无论是 Few-Shot、One-Shot 还是 Zero-Shot，模型的性能表现都随着参数规模的增加而增加，如下图所示。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023/2023-01-23-captain-aigc-2-llm-47.png&quot; alt=&quot;&quot; width=&quot;640&quot; /&gt;&lt;/p&gt;

&lt;p&gt;关于 Few-Shot Prompt，沿着 Few Shot 的方向，机器学习的过程有点类似于人类的学习：大量无监督输入，针对特定任务只需极少量有监督输入。&lt;/p&gt;

&lt;p&gt;更多与 In-Context Learning 有关的探讨，船涨在本篇专门安排了一个章节（第三章），会探讨如何利用好 ICL 的能力、ICL 能力的底层是什么、为什么 LLM 具备 ICL 能力、ICL 具体是如何起作用的。&lt;/p&gt;

&lt;h4 id=&quot;116gpt-3-api&quot;&gt;11.6、GPT-3 API&lt;/h4&gt;

&lt;p&gt;具体大家可以查看 OpenAI GPT API 文档，但是这里船涨罗列几个值得一提的点。&lt;/p&gt;

&lt;h5 id=&quot;1161gpt-一些基本概念&quot;&gt;11.6.1、GPT 一些基本概念&lt;/h5&gt;

&lt;p&gt;关于 prompt 和 completion：OpenAI 提到一个理念：「设计提示语，就相当于在用一些指令和少量例子给模型编程」。另外 OpenAI 还强调了在目标任务上的区别，就是 OpenAI 的 NLP 模型与其他 NLP 模型很大的一个区别是，它不是设计用来解决单一类型任务的，而是可以解决几乎各种类型的 NLP 任务，包括但不限于文本生成（content generation）、代码生成（code generation）、总结（summarization）、扩写（expansion）、对话（conversation）、创意写作（creative wrting）、风格转换（style transfer）等。&lt;/p&gt;

&lt;p&gt;关于 token：我们理解和处理文本，是把文本先打碎成 token。以英文文本为例，token 可以是单词，也可以词根（一些字母组合），比如单词「hamburger」可能会被打碎成「ham」、「bur」、「ger」这几个 tokens。再比如「pear」这个单词，可能就会单独作为一个 token 不再打碎了。还有些 token 可能会以「空格」开头，比如「 hello」、「 bye」。一个大概的经验是，通常英文文本里 1 token 有 4 个字母或者 0.75 个单词。使用时的一个限制是，最好你的提示（prompt）或生成内容，不要超过 2048 个 tokens，大概相当于 1500 个单词。&lt;/p&gt;

&lt;p&gt;关于 model：目前 OpenAI 有基于 GPT-3.5 的基础模型 Turbo 和这些基于 GPT-3 的基础模型 Davinci、Curie、Babbage、Ada 开放 API，另外 Codex 系列是 GPT-3 的后代，是用「自然语言 + 代码」训练的。&lt;/p&gt;

&lt;h5 id=&quot;1162gpt-3-的几个基础模型&quot;&gt;11.6.2、GPT-3 的几个基础模型&lt;/h5&gt;

&lt;p&gt;注意：因为 2023.3 OpenAI 发布了 GPT-3.5 支持的 API，所以原有的模型已经有一些变化了，此处列出的并非 GPT 全部 API，详细信息可以看船涨的一份入门解读&lt;a href=&quot;http://www.mikecaptain.com/2023/03/02/chatgpt-api/&quot;&gt;《AI 应用第一次大爆发来了：一文入门 ChatGPT 官方 API 文档解读》&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;先介绍下基础模型系列如下：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Davinci&lt;/strong&gt;：目前已经有 GPT-3.5 版本的 Davinci 了，但 Davinci 在 GPT-3 中就已经存在，是最有能力的模型系列，可以执行其他模型（Ada、Curie 和 Babbage）可以执行的任何任务，而且通常只需要很少的 instruction。 对于需要对内容有大量理解的应用程序，例如针对特定受众的 summarization 和创意内容生成，Davinci 将产生最佳结果。 这些增加的功能需要更多的计算资源，因此 Davinci 每次 API 调用更贵，并且不如其他模型那么快。Davinci 的另一个亮点是理解文本的意图。Davinci 擅长解决多种逻辑问题和解释人物的动机。 达芬奇已经能够解决一些涉及因果关系的最具挑战性的人工智能问题。&lt;strong&gt;适合：复杂的意图、因果分析、Summarization for Audience&lt;/strong&gt;。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Curie&lt;/strong&gt;：Curie 在 GPT-3 中就已经存在，也非常强大，速度也比较快。 虽然 Davinci 在分析复杂文本方面更强大，但 Curie 能够胜任许多细微的任务，例如情感分类和摘要。 Curie 还非常擅长回答问题和执行问答以及作为通用服务聊天机器人。&lt;strong&gt;适合：机器翻译、复杂分类任务、情感分析、Summarization&lt;/strong&gt;。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Babbage&lt;/strong&gt;：同样是 GPT-3 里就存在的。Babbage 可以执行简单的任务，例如简单的分类。在语义搜索方面，它也非常有能力对文档与搜索查询的匹配程度进行排名。&lt;strong&gt;适合：适度分类、语义搜索分类&lt;/strong&gt;。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Ada&lt;/strong&gt;：也是 GPT-3 时期就有的。Ada 通常是最快的模型，可以执行解析文本、地址更正和不需要太多细微差别的某些分类任务等任务。 Ada 的性能通常可以通过提供更多上下文来提高。&lt;strong&gt;适合：解析文本、简单分类、地址修正、关键词&lt;/strong&gt;。需要注意的是：由像 Ada 这样更快的模型执行的任何任务都可以由像 Curie 或 Davinci 这样更强大的模型执行。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;基于这些基础模型系列，目前 GPT-3 目前可用的模型包括下面这些，与 InstructGPT 背后的模型是一样的，它们的最大请求都是 2048 tokens，训练数据也都是 up to 2019 年 10 月：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;text-curie-001&lt;/code&gt;：比 davinci 要弱一点，但是速度更快、更便宜。&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;text-babbage-001&lt;/code&gt;：一些比较直接的任务（straightforward tasks），比 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;text-curie-001&lt;/code&gt; 更快、更便宜。&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;text-ada-001&lt;/code&gt;：一些非常简单的任务，这些模型里最快、最便宜的。&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;davinci&lt;/code&gt;：目前最强的 GPT-3 模型，任何其他模型能做的任务，&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;davinci&lt;/code&gt; 都可以做。&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;curie&lt;/code&gt;：就是 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;text-curie-001&lt;/code&gt;。&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;babbage&lt;/code&gt;：就是 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;text-babbage-001&lt;/code&gt;。&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ada&lt;/code&gt;：就是 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;text-ada-001&lt;/code&gt;。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;117gpt-3-小节&quot;&gt;11.7、GPT-3 小节&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;GPT-3 比较充分展示了训练范式上不用微调的可信性。&lt;/li&gt;
  &lt;li&gt;GPT-3 跟很多深度学习模型一样，都是无法解释的。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;LMM 一定是下一个军备竞赛打卡点&lt;/strong&gt;。LMM 即 Large Multimodal Models（大型多模态模型），目前的 LLM 只是在读书（读文本），缺少其他体验，比如视频到底是什么鬼，比如真实物理世界的交互；而 LMM 就不同了，可以与人类进行不同模态的交互，可以读懂人类给它的文本、视频、语音等等模态内容，也能根据需要给人类生成文档、图片、视频等等。&lt;/li&gt;
  &lt;li&gt;GPT-3 论文试图重新定义 Meta Learning，但发表后并没有引起大家的认同。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;GPT-3 的局限性也很明显：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;训练成本太高。算力能耗太高，不环保。当然一开始大型计算机出来的时候也是很夸张的用电量和占地空间。&lt;/li&gt;
  &lt;li&gt;样本有效性差，目前训练方法还是让 GPT-3 看了太多数据了（相比人类一生能看的文本量，GPT 看的量太大了）。未来提高样本有效性也是一个重要工作。&lt;/li&gt;
  &lt;li&gt;缺少 Alignment，可能导致被用来撒布不实消息、生成垃圾邮件或者钓鱼、造假论文；内容可能带有性别偏见、种族评价、宗教歧视；缺少对政治敏感的兼容，但这也是最复杂的。&lt;/li&gt;
  &lt;li&gt;GPT 在 few-shot learning 时到底是现学的，还是找到原来学过的相似的东西找出来。如果是后者，那真的是在拼训练数据大小了。但是对比人类，我们应该要做到前者才对。&lt;/li&gt;
  &lt;li&gt;如果要补全一段，还可以。如果要一直续写小说，GPT 可能不太行。&lt;/li&gt;
  &lt;li&gt;训练学习时，对每个词都是一样对待的，就不像人类其实是有重点的。&lt;/li&gt;
  &lt;li&gt;这里也有问题：1）当你在下游任务真有一大组样本（比如 1 万条）想给模型时，Few-Shot 真的给模型那么多数据么，那每次使用都要带着也太麻烦了，效率也不高。2）哪怕只有 1 条样本想 Prompt，不用它效果就不好，但是每次使用模型都要把这一条带着也不优雅。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;本小节参考-6&quot;&gt;本小节参考&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;https://arxiv.org/abs/2205.01068&lt;/li&gt;
  &lt;li&gt;https://jalammar.github.io/how-gpt3-works-visualizations-animations/&lt;/li&gt;
  &lt;li&gt;https://lifearchitect.ai/chatgpt/&lt;/li&gt;
  &lt;li&gt;https://platform.openai.com/docs/api-reference/&lt;/li&gt;
  &lt;li&gt;https://arxiv.org/abs/2211.05100&lt;/li&gt;
  &lt;li&gt;https://arxiv.org/abs/2005.14165&lt;/li&gt;
  &lt;li&gt;https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft&lt;/li&gt;
  &lt;li&gt;https://bmk.sh/2020/05/29/GPT-3-A-Brief-Summary/&lt;/li&gt;
  &lt;li&gt;https://openai.com/blog/gpt-3-apps/&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;第三章--上下文学习in-context-learning&quot;&gt;第三章 · 上下文学习（In-Context Learning）&lt;/h2&gt;

&lt;p&gt;一个预训练模型，在处理下游任务时，不微调模型参数，只需要在输入时加一些示例，就能有 SOTA 的表现，这就是模型的&lt;strong&gt;上下文学习（In-Context Learning，ICL）&lt;/strong&gt;能力。比如下面的 ICL 示例：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023/2023-01-23-captain-aigc-2-llm-17.png&quot; alt=&quot;&quot; width=&quot;490&quot; /&gt;&lt;/p&gt;

&lt;p&gt;相比我们以往对计算机、神经网络的理解，这是很神奇的。因为我们并没有「有意地、显性地」用某个下游任务去预训练 LLM，但是 LLM 却能很好地给出输出。大家使用 ChatGPT 自己都有体会，这里不必举更多例子。那么由此就有两个有意思的议题方向：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;这说明 prompting 只是让 LLM 已具备的能力被展现出来，那么 LLM 是如何具备 ICL 能力的？ICL 能力又是如何工作的？&lt;/li&gt;
  &lt;li&gt;别管那么多，既然 LLM 有 ICL 能力，那就直接用吧！有什么强有力使用方法可以挖掘出来？这也是很值得探究的方向。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;可以说 2 是「知其然」，而 1 是为了「知其所以然」。咱们先看看使用上，研究人员们都开发出来了什么吧。&lt;/p&gt;

&lt;h3 id=&quot;第-12-节--icl-能力的直接应用prompt-engineering&quot;&gt;第 12 节 · ICL 能力的直接应用：Prompt Engineering&lt;/h3&gt;

&lt;p&gt;大概在 2017~2019 年期间「预训练-微调」是绝对的主流范式，此后随着 GPT-2 出来「预训练-提示」这种范式就开始出现，尤其是 GPT-3 推出开始加速发展，到 2021 年开始相关围绕 Prompt 的研究井喷出现，以至于 2021~2022 期间很多学界人士的研究重点都转向了 Prompt。那么我们来看看围绕 Prompt 的研究发展到了什么程度。&lt;/p&gt;

&lt;p&gt;首先一般性地「Pretrain, Prompt」到了 Prompt 环节，可能是给模型输入 \(x\) 期望得到输出 \(y\)。但是如果我们对使用者给出的 \(x\) 进行二次加工（比如把这个加工表示为一个函数 \(f\)），是否能在输出上获得更好的结果 \(y\) 呢？即：&lt;/p&gt;

\[\begin{aligned}
x &amp;amp;\rightarrow y \\
f(x) &amp;amp;\rightarrow y
\end{aligned}\]

&lt;p&gt;如果不止优化输入，我们优化输出。比如我们在模型直接给出的结果 \(y\) 上进一步再加工（比如把这个加工表示为一个函数 \(g\)），可以使得到的结果更加优质呢？如下：&lt;/p&gt;

\[\begin{aligned}
x &amp;amp;\rightarrow y\text{, then output }y \\
f(x) &amp;amp;\rightarrow y\text{, then output }g(y)
\end{aligned}\]

&lt;p&gt;为了好理解，我举个例子。比如模型的使用者想问「自驾去杭州周边两天一夜玩，有什么推荐的地方吗？」，模型返回了「南浔古镇」。而如果通过 Prompt Engineering 优化一下可以这样：&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;x&lt;/td&gt;
      &lt;td&gt;本周末周边自驾两天一夜，最推荐的地方？&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;\(f(x)\)&lt;/td&gt;
      &lt;td&gt;我的所在地是&lt;杭州&gt;，请对下面这个问题给出 5 个不同的答案并且每个答案都带一条简短的理由：本周末周边自驾两天一夜，最推荐的地方？&lt;/杭州&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;y&lt;/td&gt;
      &lt;td&gt;南浔古镇，有保存完好的明清时期建筑；&lt;br /&gt;莫干山，远离城市喧嚣亲近大自然享受慢节奏的民宿生活；&lt;br /&gt;千岛湖，一边吃美味鱼宴一边赏着湖光山色水清鱼跃绝美的自然风光；&lt;br /&gt;乌镇，江南水乡特色韵味古镇建筑有着深厚的文化积淀；&lt;br /&gt;普陀山，佛教圣地有着浓厚的宗教氛围素有“海上名山”之称。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;g(y)&lt;/td&gt;
      &lt;td&gt;1、对 y 中几条结果，查询天气、最新交通管制措施、游客接待政策等等（假设因此剔除了南浔古镇、千岛湖）；&lt;br /&gt;2、剔除用户已去过的目的地（假设用户已授权模型访问其旅游平台账户，剔除了普陀山）；&lt;br /&gt;3、对 y 的结果进行优化并保留推荐值最高的结果「莫干山 ……」并输出：莫干山，&lt;推荐亮点&gt;，&lt;天气&gt;，&lt;交通情况&gt;，&lt;推荐美食&gt;，&amp;lt;推荐酒店/民宿&amp;gt;，&lt;推荐自驾路线&gt;。&lt;/推荐自驾路线&gt;&lt;/推荐美食&gt;&lt;/交通情况&gt;&lt;/天气&gt;&lt;/推荐亮点&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;这样 f(x) 就是 Prompt Engineering，而 g(x) 其实是 Answer Engineering。&lt;/p&gt;

&lt;h4 id=&quot;121pet提出-pvp-框架&quot;&gt;12.1、PET：提出 PVP 框架&lt;/h4&gt;

&lt;p&gt;德国慕尼黑大学的研究人员于 2020 年 1 月在论文&lt;a href=&quot;https://arxiv.org/abs/2001.07676&quot;&gt;《Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference》&lt;/a&gt;中提出 PET（Pattern Exploiting Traning）训练方法，该方法主要用于小样本上的文本分类和自然语言推理。以小样本的文本情感分类任务为例，我们下面来快速讲一下 PET 方法，可能会有一点抽象，如果不理解也没关系，可以跳过。&lt;/p&gt;

&lt;p&gt;现在我们希望用预训练过的模型来通过微调的方式解决下游任务，而下游任务的标记样本有比较少，如何解决呢？&lt;/p&gt;

&lt;p&gt;将这少量的标记样本，改造为完形填空（Cloze Questions）的模式。但是因为样本千奇百怪，可能需要不同的「完形填空」模式，所以就分成几类模式（Pattern），每一类模式都对应着自己要的填空内容。分类到填空内容存在一个关系映射（我们叫 Verbalizer），比如「这个披萨很 ___ 」可以填「好吃」或者「难吃」，Verbalizers 就是 \(v(\text{positive})\) =好吃，\(v(\text{negative})\)=难吃，这样就打造出了多组「Pattern-Verbalizer Pairs，PVP」。你可能会问，那为什么 Pattern 也有多种呢？我举个例子，比如「这家披萨店我再也不想来了」表达的也是 Negative，但是与「这个披萨很难吃」就是不同的模式，其对应的 Verbalizer 也不同，所以就有了多组 PVP。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023/2023-01-23-captain-aigc-2-llm-65.png&quot; alt=&quot;&quot; width=&quot;480&quot; /&gt;&lt;/p&gt;

&lt;p&gt;理解这个的话，PET 就一共有 3 步（对应上图）：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;每一组 PVP\(_i\) 都去单独微调一个预训练模型 M\(_i\)。&lt;/li&gt;
  &lt;li&gt;再用微调过的 M\(_i\) 去跑下游未标记的数据，得到多组标记结果，再汇总到一起。&lt;/li&gt;
  &lt;li&gt;对汇总的标记结果用分类器进行最终的文本分类。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;可以看到这也是基于「Pre-train and Fine-tune」范式而来的方法。&lt;/p&gt;

&lt;p&gt;PET 最大价值是定义了一套在预训练模型上更好完成下游任务的方法框架 PVP。这里最关键的工作有两个，一是构建各种 Patterns（也有文献叫 Templates），另一个是构建 Verbalizers，这两个是 2020 年 PET 提出后围绕它的两个研究热点。&lt;/p&gt;

&lt;p&gt;而原始的 PET 方法有一个问题，就是要 fine-tune 预训练模型。当模型都大型化之后，fine-tune 的成本太高，何况还要 fine-tune 多个预训练模型。因此，如果面对下游任务时，不更新预训练模型参数，即不 fine-tune，只是给模型 prompt 呢？&lt;/p&gt;

&lt;p&gt;另一方面，LLM 本身也具备对任务很好的迁移性，尤其在 2020 年 PET 出来那个节点前后，整体研究范式已经在从「预训练-微调」向「预训练-提示」转换了。LLM 这种强大的下游任务迁移能力，其实就体现在了不需要 prompt 携带太多下游标记样本，就能有较好性能，那也就更没必要去更新参数了。比如本篇第 11 节介绍的 GPT-3 就提到在并不多样本量情况下，多个任务通过 few-shot 随着模型规模增大就能达到 SOTA（以下几个实验样本量都未超过 100）：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023/2023-01-23-captain-aigc-2-llm-66.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;122从硬提示到软提示&quot;&gt;12.2、从硬提示到软提示&lt;/h4&gt;

&lt;p&gt;Prompt 最开始都是人工编写的，如果用 &lt;strong&gt;__ 来表示希望模型输出的结果，如果给模型的 prompt 是「some words __&lt;/strong&gt; some words」这种形式，一般叫 cloze prompt；如果是「some words &lt;em&gt;__&lt;/em&gt; 」这种形式，一般叫 prefix prompt。这是提示的两种不同形状（Prompt Shape）。&lt;/p&gt;

&lt;p&gt;这样编写的质量就非常不稳定，而且什么样的质量好，人的主观判断也并不可信，比如看下面这几个例子：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023/2023-01-23-captain-aigc-2-llm-72.jpg&quot; alt=&quot;&quot; width=&quot;480&quot; /&gt;&lt;/p&gt;

&lt;p&gt;我们把这种 Prompt 叫「&lt;strong&gt;硬提示（Hard Prompt）&lt;/strong&gt;」，也有叫「&lt;strong&gt;离散提示（Discrete Prompt）&lt;/strong&gt;」。其缺点显而易见，但即使如此，这个阶段人们还是开发出来一些围绕 Hard Prompt 的方法，比如 Prompt Mining（在输入输出之间插中间词来搭建依赖路径）、Prompt Generation（用另一个模型来生成 Prompt，有点套娃）等等，因为不是重点这里不展开。&lt;/p&gt;

&lt;p&gt;Prompt 的初衷是为了「让下游任务迎合上游模型」，其实从逻辑上与「让上游模型迎合下游任务」类似，只是后者用「微调」地方式改变上游模型，受此启发，能不能用类似「微调」的方式改变下游呢？也就是，为了找到更好的 Prompt，我们可以把优化 Prompt 自身也当做一个任务目标，让机器连续地优化 Prompt —— 这就是&lt;strong&gt;软提示（Soft Prompt&lt;/strong&gt;），也叫&lt;strong&gt;连续提示（Continuous Prompt）&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;想用机器学习的方式优化 Prompt，就一定要引入额外参数了。怎么让引入的参数有效地服务于优化 Prompt 的目标，研究人员们提出了如下方法：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Prefix Tuning：2021 年 1 月 Stanford 学者提出在论文（&lt;a href=&quot;https://arxiv.org/abs/2101.00190&quot;&gt;《Prefix-Tuning: Optimizing Continuous Prompts for Generation》&lt;/a&gt;）中提出该方法。&lt;/li&gt;
  &lt;li&gt;P-Tuning：2021 年 3 月&lt;a href=&quot;https://arxiv.org/abs/2103.10385&quot;&gt;《GPT Understands, Too》&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;PPT：2021 年 9 月&lt;a href=&quot;https://arxiv.org/abs/2109.04332&quot;&gt;《PPT: Pre-trained Prompt Tuning for Few-shot Learning》&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这些软提示方法，成文时间有限在此暂不展开。但这里最值得一提的，是 Google 团队的一个研究，在下一小节我们来一起看下。&lt;/p&gt;

&lt;h4 id=&quot;123prompt-tuning&quot;&gt;12.3、Prompt Tuning&lt;/h4&gt;

&lt;p&gt;2021 年 4 月 Google Research 团队在文章中&lt;a href=&quot;https://arxiv.org/abs/2104.08691&quot;&gt;《The Power of Scale for Parameter-Efficient Prompt Tuning》&lt;/a&gt;提出「Prompt Tuning」。连续优化 Prompt 是另一种「微调」，调的对象是 Prompt，所以叫做「Prompt Tuning」。与其相对的，我们一般说的「Fine-tuning」在这个语境下专指「Model Tuning」，即更新的对象是模型，而 Prompt Tuning 不更新模型。两者的对比如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023/2023-01-23-captain-aigc-2-llm-67.png&quot; alt=&quot;&quot; width=&quot;640&quot; /&gt;&lt;/p&gt;

&lt;p&gt;另外一个用来对比的方法是，文章提出的按个时间 Prompt 已经火了，尤其 GPT-3 发布后，但是大家主要还是在设计没有 tuning 的 prompt，Google 团队在此将这种方法叫做 Prompt Design。研究发现，随着模型参数增大，Prompt Tuning 与 Fine-tuning 性能相当，并且显著优于 Prompt Design，如下图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023/2023-01-23-captain-aigc-2-llm-68.png&quot; alt=&quot;&quot; width=&quot;480&quot; /&gt;&lt;/p&gt;

&lt;p&gt;具体地，研究人员用的是一种「前缀提示」方法，但是这些前缀并非来自人工设计，而是由机器学习方法不断微调产生的，同时上游的模型参数始终固定不变。这相当于把 Prompt 当做下游的一个独立训练任务，但是优化好的 Prompt 作为输入交给上游模型。所以这种方法，是会引入额外参数的，所以需要验证参数有效性前提下，在此基础上期待看到模型在 Prompt Tuning 中表现不逊于 Model Tuning。&lt;/p&gt;

&lt;p&gt;研究人员从四个方面重点深入，做了消融研究（忘记消融实验含义的读者，可以回顾本篇 7.3 小节）：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023/2023-01-23-captain-aigc-2-llm-69.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;每个图中的绿色线，是默认配置。结论是 Prompt 长度越长表现越好，Prompt 随机初始化效果显著逊色，训练方法上使用真实本文输出代替 T5 的带哨兵的文本输出效果好（T5 的 Span Corruption 策略导致其输出始终有哨兵存在，具体详见 T5 论文，这里不展开，不理解并不影响继续阅读）、且修正消除哨兵影响的 T5 训练步数越多效果越好。但是，&lt;strong&gt;只要模型参数足够大，Prompt 的长度、初始化方式、跟 T5 输出哨兵问题有关的预训练方法及训练迭代次数不同时，影响也都不大&lt;/strong&gt;。&lt;/p&gt;

&lt;h4 id=&quot;124prompt-带来的风险&quot;&gt;12.4、Prompt 带来的风险&lt;/h4&gt;

&lt;p&gt;通过 Prompt Engineering 也有很多作恶的空间，因为可能可以绕过 LLM 原本设定好的政策。也有很多相关的研究、分析或报道，这里暂不详述，船涨提供以下参考阅读：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://simonwillison.net/2022/Sep/12/prompt-injection/&quot;&gt;Prompt injection attacks against GPT-3&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://hackaday.com/2022/09/16/whats-old-is-new-again-gpt-3-prompt-injection-attack-affects-ai/&quot;&gt;WHAT’S OLD IS NEW AGAIN: GPT-3 PROMPT INJECTION ATTACK AFFECTS AI&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.theregister.com/2022/09/19/in_brief_security/&quot;&gt;GPT-3 ‘prompt injection’ attack causes bad bot manners&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://research.nccgroup.com/2022/12/05/exploring-prompt-injection-attacks/&quot;&gt;Exploring Prompt Injection Attacks&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.washingtonpost.com/technology/2023/02/14/chatgpt-dan-jailbreak/&quot;&gt;The clever trick that turns ChatGPT into its evil twin&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;125小结&quot;&gt;12.5、小结&lt;/h4&gt;

&lt;p&gt;围绕 Prompt 的研究已经进入了工程实用阶段，LLM 的潜力就像一座金矿，这个方向的研究人员在不断挖掘其价值。但是对于为什么 Prompt 能够在不更新参数的 LLM 上得到 SOTA 的效果，很快学界认识到仅仅发现一些不错的 Prompt 方法也只能算是在黑盒之上 Engineering，而且很多还是在他人的黑盒模型上做嫁衣（比如很多论文都是基于 GPT-3 实验的），所以逐渐这方面的研究开始从「知其然」走向「知其所以然」的探究、假设、分析和实验。但从业界视角，Prompt Engineering 仍然非常有价值。&lt;/p&gt;

&lt;h3 id=&quot;第-13-节--icl-能力的底层假设贝叶斯推理&quot;&gt;第 13 节 · ICL 能力的底层假设：贝叶斯推理&lt;/h3&gt;

&lt;p&gt;LLM 为什么具备 ICL 能力这方面有一些研究蛮有意思的，下面和船涨来一起看看。但这里有一个基础的假设，就是 LLM 具备一个底层能力：贝叶斯推理（Bayesian Inference）。&lt;/p&gt;

&lt;p&gt;我们知道 LLM 在预训练阶段都学习过海量文本，可能从 Wikipedia 的百科到 Twitter 的推文，从 Quora 的问答到 StackOverflow 的贴子，从 Reddit 的版聊到 arXiv 的论文，从莎士比亚的十四行诗到 GitHub 上的代码 …… 所以我们假设：&lt;strong&gt;在预训练大量文本时，语言模型已经对多种（覆盖下游任务的）概念进行了建模&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;所以问题就变成了：预训练时 LLM 先验性地学到了某些概念，现在使用 LLM 时问了它一些与这些概念存在一定似然度的问题，这时 LLM 就可以后验性地预测输出了。&lt;/p&gt;

&lt;p&gt;这是不是让你想起了「贝叶斯推理（Bayesian Inference）」？对于不熟悉的朋友，这里简单介绍下贝叶斯推理。起初，贝叶斯是为了解决「概率预测」问题而提出的「贝叶斯定理/贝叶斯公式」，可以表示为：&lt;/p&gt;

\[\begin{aligned}
P_{priori} = P_{posteriori} \cdot L
\end{aligned}\]

&lt;p&gt;\(P_{priori}\) 为先验概率，是对事情的主观判断、过去经验的总结规律等。\(L\) 为「似然度（Likelihood）」，而 \(P_{posteriori}\) 是后验概率。贝叶斯定理影响了概率理论的发展，在所有需要对概率进行预测的情况里你都能看到它的身影，而它也成为了机器学习的一个核心方法。这里对于 LLM 具备 ICL 这种魔法能力的事实，有学者给出了相对令人信服的基于贝叶斯推理的数学解读，我们一起来看看。&lt;/p&gt;

&lt;h3 id=&quot;第-14-节--llm-是如何具备-icl-能力的2021-年-11-月&quot;&gt;第 14 节 · LLM 是如何具备 ICL 能力的？（2021 年 11 月）&lt;/h3&gt;

&lt;p&gt;2021 年 11 月，斯坦福大学的几位研究人员在论文&lt;a href=&quot;https://arxiv.org/abs/2111.02080&quot;&gt;《An Explanation of In-context Learning as Implicit Bayesian Inference》&lt;/a&gt;中试图给出解释。首先作者们提出了一个假设：「语言模型是通过 In-Context Learning prompt，在模型内定位到已经学好的概念（concept），进而解决 In-Context Learning task」。如果是这样，那么语言模型是具备贝叶斯推理能力的。作者们认为是如下这样实现这一点的：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;预训练阶段：预训练时为了预测下一个 token 是什么，语言模型必须通过前面句子的证据来推理文本潜在的任务概念。这让 LLM 学到了一些&lt;strong&gt;先验&lt;/strong&gt;的概念（concept）。&lt;/li&gt;
  &lt;li&gt;ICL 阶段：如果提示（prompt）中出现了与预训练时所遇到概念（concept）&lt;strong&gt;似然度&lt;/strong&gt;较高的示例（demonstration），那么 LLM 就会给出一些后验结果。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;从数学上，研究人员提出了一个数学框架假设来理解在预训练过程就实现了的上下文理解能力：&lt;strong&gt;LLM 就是很多隐式马尔科夫模型的混合体（Mixture of Hidden Markov Models）&lt;/strong&gt;。&lt;/p&gt;

&lt;h4 id=&quot;141预训练分布pretain-distribution的数学解读&quot;&gt;14.1、预训练分布（Pretain Distribution）的数学解读&lt;/h4&gt;

&lt;p&gt;预训练的一个文本都可以表示为一个长度 T 的序列：&lt;/p&gt;

\[\begin{aligned}
P(o_1,...,o_T) = \int_{\theta\in\varTheta}P(o_1,...o_T\text{ \textbar }\theta)d\theta
\end{aligned}\]

&lt;p&gt;其中 \(\varTheta\) 是一组 concepts 的集合，\(\theta\) 就是模型中隐含的某一 concept。如果假设 \(P(o_1,...,o_T\text{ \textbar }\theta)\) 是一个隐式马尔科夫模型（HMM）定义的，那么 \(\theta\) 就决定了 HMM 所有状态的转移概率矩阵。&lt;/p&gt;

&lt;p&gt;具体下来回到本小节内容开头的第一个插图的举例来看，比如两种情况下的概率是均等，那么上面的公式则可以表示为：&lt;/p&gt;

\[\begin{aligned}
P_{(\text{Paying off the national debt will be extremly painful})}\,=\,&amp;amp;0.5 * P_{(\text{Paying off the national debt will be extremly painful}\,\text{\textbar}\,\theta_1)} \\
+\,&amp;amp;0.5 * P_{(\text{Paying off the national debt will be extremly painful}\,\text{\textbar}\,\theta_2)}
\end{aligned}\]

&lt;h4 id=&quot;142上下文学习in-context-learning的数学解读&quot;&gt;14.2、上下文学习（In-Context Learning）的数学解读&lt;/h4&gt;

&lt;p&gt;LLM 完成训练后，我们就可以给它一个提示，它是由「一组独立的示例 \(d_1, \text{[delim]}, d_2, \text{[delim]}, ..., \text{[delim]}, d_n\) + 一个输入 \(x_{test}\)」（注：\(\text{[delim]}\) 表示分隔符），并且这组示例和输入都是面向 \(\theta\) 这个概念的。那么任务就可以理解为在 LLM 中找到对应 \(\theta\) concept 的那个 HMM 进而给出输出结果，并且随着 \(n \rarr +\infin\)，输出结果无限逼近于理想的输出结果 \(y_{test}\)。&lt;/p&gt;

\[\begin{aligned}
\text{with }n \rarr +\infin,\,P(y\text{\textbar}d_1,\text{[delim]},...,\text{[delim]},d_n,x_{test}) \rarr P_{prompt}(y\text{\textbar}x_{test})
\end{aligned}\]

&lt;h4 id=&quot;143llm-是隐式马尔科夫模型的概念混合体mixture-of-hmm-concepts&quot;&gt;14.3、LLM 是隐式马尔科夫模型的概念混合体（Mixture of HMM Concepts）&lt;/h4&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2111.02080&quot;&gt;Xie et al. 2021&lt;/a&gt; 合成了一个名为 &lt;a href=&quot;https://github.com/p-lambda/incontext-learning&quot;&gt;GINC 的数据集&lt;/a&gt;，这个数据集包括：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;预训练数据集&lt;/strong&gt;：五种概念的 HMM 的统一混合体，1000 个用于预训练的文本文档（总计约一千万 tokens）。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;上下文学习的测试数据&lt;/strong&gt;：prompts 是一些带有 0~64 个示例的输入，示例长度 k 等于 3、5、8、10.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在这个数据集上，作者们基于 GPT-2 的 Transformer 和 LSTM 两个模型都做了一个实验（为了排除是模型结构的特殊性带来的），结果如下图。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023/2023-01-23-captain-aigc-2-llm-18.png&quot; alt=&quot;&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;p&gt;可见性能是正相关于实例数量、实例长度的。那么我们再想一下前面的假设「LLM 就是很多隐式马尔科夫模型的混合体（Mixture of Hidden Markov Models）」，所以如果训练数据只是用隐含一种概念结构的数据呢？或者包含多种结构但是并非 Markov，而是状态随机流转的呢？&lt;a href=&quot;https://arxiv.org/abs/2111.02080&quot;&gt;Xie et al. 2021&lt;/a&gt; 分别单独验证了一下。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023/2023-01-23-captain-aigc-2-llm-19.png&quot; alt=&quot;&quot; width=&quot;490&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023/2023-01-23-captain-aigc-2-llm-20.png&quot; alt=&quot;&quot; width=&quot;490&quot; /&gt;&lt;/p&gt;

&lt;p&gt;可见，对于「包含多个概念的隐式马尔科夫」的两个关键因素「多个概念」和「隐式马尔科夫」两者分别改成「一个概念」和「随机转换」后，模型的上下文学习表现迅速下降。所以这两个因素组成的「HMM 概念混合体」非常关键。&lt;/p&gt;

&lt;p&gt;最后我们总结一下基于贝叶斯推理的 ICL：&lt;/p&gt;

\[P(\text{output\textbar prompt}) = \int_{\text{concept}}P(\text{output\textbar concept,prompt})P(\text{concept\textbar prompt})d(\text{concept})\]

&lt;ul&gt;
  &lt;li&gt;由 concept、prompt 得出 output 的先验概率 \(P(\text{output\textbar prompt})\)。&lt;/li&gt;
  &lt;li&gt;prompt 与 concept 的似然度 \(P(\text{concept\textbar prompt})\)。&lt;/li&gt;
  &lt;li&gt;给定 prompt 得到 output 的后验概率 \(P(\text{output\textbar concept, prompt})\)。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;第-15-节--icl-是如何工作的2022-年-2-月&quot;&gt;第 15 节 · ICL 是如何工作的？（2022 年 2 月）&lt;/h3&gt;

&lt;p&gt;2022 年 2 月华盛顿大学、Meta 和 AI2 的几位作者在论文&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2202.12837&quot;&gt;《Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?》&lt;/a&gt;中，从实验验证角度深入地探索了 ICL 背后，到底是 prompts 中的什么起到了作用。&lt;/p&gt;

&lt;p&gt;首先我们先明确在 ICL 中输入的内容里，每一部分都是啥，并且明确下名称，避免歧义。我们先看下图：&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023/2023-01-23-captain-aigc-2-llm-22.png&quot; alt=&quot;&quot; width=&quot;650&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;整体输入是由 1 组 demonstrations 和 1 个 test sample 组成的。在这组 demonstrations 中的每一个都是 input-label pair，input 和 label 之间的存在映射（mapping）关系。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;div style=&quot;text-align: center;&quot;&gt;
&lt;div class=&quot;graphviz-wrapper&quot;&gt;

&lt;!-- Generated by graphviz version 2.43.0 (0)
 --&gt;
&lt;!-- Title: G Pages: 1 --&gt;
&lt;svg role=&quot;img&quot; aria-label=&quot;graphviz-2772be2c9882a118266e1ec77856ac29&quot; width=&quot;588pt&quot; height=&quot;152pt&quot; viewBox=&quot;0.00 0.00 588.00 152.00&quot;&gt;
&lt;title&gt;graphviz-2772be2c9882a118266e1ec77856ac29&lt;/title&gt;
&lt;desc&gt;
digraph G {
	rankdir=LR
	node [shape=&amp;quot;box&amp;quot;]
	{rank=same; demos; test_example}

	demos[label=&amp;quot;demos/demonstrations&amp;quot;]

	prompt-&amp;gt;demos
	demos-&amp;gt;demo1
	demos-&amp;gt;demo2
	demos-&amp;gt;demo3
	prompt-&amp;gt;test_example

	demo1-&amp;gt;input1
	demo2-&amp;gt;input2
	demo3-&amp;gt;input3

	input1-&amp;gt;label1[arrowhead=none]
	input2-&amp;gt;label2[arrowhead=none]
	input3-&amp;gt;label3[arrowhead=none]
}
&lt;/desc&gt;

&lt;g id=&quot;graph0&quot; class=&quot;graph&quot; transform=&quot;scale(1 1) rotate(0) translate(4 148)&quot;&gt;
&lt;title&gt;G&lt;/title&gt;
&lt;polygon fill=&quot;white&quot; stroke=&quot;transparent&quot; points=&quot;-4,4 -4,-148 584,-148 584,4 -4,4&quot; /&gt;
&lt;!-- demos --&gt;
&lt;g id=&quot;node1&quot; class=&quot;node&quot;&gt;
&lt;title&gt;demos&lt;/title&gt;
&lt;polygon fill=&quot;none&quot; stroke=&quot;black&quot; points=&quot;284,-90 105,-90 105,-54 284,-54 284,-90&quot; /&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;194.5&quot; y=&quot;-68.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;demos/demonstrations&lt;/text&gt;
&lt;/g&gt;
&lt;!-- demo1 --&gt;
&lt;g id=&quot;node4&quot; class=&quot;node&quot;&gt;
&lt;title&gt;demo1&lt;/title&gt;
&lt;polygon fill=&quot;none&quot; stroke=&quot;black&quot; points=&quot;385,-144 320,-144 320,-108 385,-108 385,-144&quot; /&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;352.5&quot; y=&quot;-122.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;demo1&lt;/text&gt;
&lt;/g&gt;
&lt;!-- demos&amp;#45;&amp;gt;demo1 --&gt;
&lt;g id=&quot;edge2&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;demos&amp;#45;&amp;gt;demo1&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M247.88,-90.13C268.23,-97.18 291.2,-105.13 310.33,-111.75&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;309.36,-115.12 319.95,-115.08 311.65,-108.5 309.36,-115.12&quot; /&gt;
&lt;/g&gt;
&lt;!-- demo2 --&gt;
&lt;g id=&quot;node5&quot; class=&quot;node&quot;&gt;
&lt;title&gt;demo2&lt;/title&gt;
&lt;polygon fill=&quot;none&quot; stroke=&quot;black&quot; points=&quot;385,-90 320,-90 320,-54 385,-54 385,-90&quot; /&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;352.5&quot; y=&quot;-68.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;demo2&lt;/text&gt;
&lt;/g&gt;
&lt;!-- demos&amp;#45;&amp;gt;demo2 --&gt;
&lt;g id=&quot;edge3&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;demos&amp;#45;&amp;gt;demo2&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M284.44,-72C293.19,-72 301.76,-72 309.68,-72&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;309.7,-75.5 319.7,-72 309.7,-68.5 309.7,-75.5&quot; /&gt;
&lt;/g&gt;
&lt;!-- demo3 --&gt;
&lt;g id=&quot;node6&quot; class=&quot;node&quot;&gt;
&lt;title&gt;demo3&lt;/title&gt;
&lt;polygon fill=&quot;none&quot; stroke=&quot;black&quot; points=&quot;385,-36 320,-36 320,0 385,0 385,-36&quot; /&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;352.5&quot; y=&quot;-14.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;demo3&lt;/text&gt;
&lt;/g&gt;
&lt;!-- demos&amp;#45;&amp;gt;demo3 --&gt;
&lt;g id=&quot;edge4&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;demos&amp;#45;&amp;gt;demo3&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M257.13,-53.93C266.18,-51.06 275.36,-48.04 284,-45 292.56,-41.99 301.64,-38.54 310.27,-35.13&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;311.79,-38.29 319.78,-31.33 309.19,-31.79 311.79,-38.29&quot; /&gt;
&lt;/g&gt;
&lt;!-- test_example --&gt;
&lt;g id=&quot;node2&quot; class=&quot;node&quot;&gt;
&lt;title&gt;test_example&lt;/title&gt;
&lt;polygon fill=&quot;none&quot; stroke=&quot;black&quot; points=&quot;250,-36 139,-36 139,0 250,0 250,-36&quot; /&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;194.5&quot; y=&quot;-14.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;test_example&lt;/text&gt;
&lt;/g&gt;
&lt;!-- prompt --&gt;
&lt;g id=&quot;node3&quot; class=&quot;node&quot;&gt;
&lt;title&gt;prompt&lt;/title&gt;
&lt;polygon fill=&quot;none&quot; stroke=&quot;black&quot; points=&quot;69,-63 0,-63 0,-27 69,-27 69,-63&quot; /&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;34.5&quot; y=&quot;-41.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;prompt&lt;/text&gt;
&lt;/g&gt;
&lt;!-- prompt&amp;#45;&amp;gt;demos --&gt;
&lt;g id=&quot;edge1&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;prompt&amp;#45;&amp;gt;demos&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M69.01,-50.73C76.86,-52.07 85.62,-53.56 94.75,-55.13&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;94.34,-58.61 104.79,-56.84 95.52,-51.71 94.34,-58.61&quot; /&gt;
&lt;/g&gt;
&lt;!-- prompt&amp;#45;&amp;gt;test_example --&gt;
&lt;g id=&quot;edge5&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;prompt&amp;#45;&amp;gt;test_example&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M69.01,-39.27C86.45,-36.29 108.37,-32.55 128.94,-29.03&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;129.64,-32.46 138.9,-27.33 128.46,-25.56 129.64,-32.46&quot; /&gt;
&lt;/g&gt;
&lt;!-- input1 --&gt;
&lt;g id=&quot;node7&quot; class=&quot;node&quot;&gt;
&lt;title&gt;input1&lt;/title&gt;
&lt;polygon fill=&quot;none&quot; stroke=&quot;black&quot; points=&quot;484,-144 421,-144 421,-108 484,-108 484,-144&quot; /&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;452.5&quot; y=&quot;-122.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;input1&lt;/text&gt;
&lt;/g&gt;
&lt;!-- demo1&amp;#45;&amp;gt;input1 --&gt;
&lt;g id=&quot;edge6&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;demo1&amp;#45;&amp;gt;input1&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M385.05,-126C393.16,-126 402.01,-126 410.53,-126&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;410.68,-129.5 420.68,-126 410.68,-122.5 410.68,-129.5&quot; /&gt;
&lt;/g&gt;
&lt;!-- input2 --&gt;
&lt;g id=&quot;node8&quot; class=&quot;node&quot;&gt;
&lt;title&gt;input2&lt;/title&gt;
&lt;polygon fill=&quot;none&quot; stroke=&quot;black&quot; points=&quot;484,-90 421,-90 421,-54 484,-54 484,-90&quot; /&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;452.5&quot; y=&quot;-68.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;input2&lt;/text&gt;
&lt;/g&gt;
&lt;!-- demo2&amp;#45;&amp;gt;input2 --&gt;
&lt;g id=&quot;edge7&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;demo2&amp;#45;&amp;gt;input2&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M385.05,-72C393.16,-72 402.01,-72 410.53,-72&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;410.68,-75.5 420.68,-72 410.68,-68.5 410.68,-75.5&quot; /&gt;
&lt;/g&gt;
&lt;!-- input3 --&gt;
&lt;g id=&quot;node9&quot; class=&quot;node&quot;&gt;
&lt;title&gt;input3&lt;/title&gt;
&lt;polygon fill=&quot;none&quot; stroke=&quot;black&quot; points=&quot;484,-36 421,-36 421,0 484,0 484,-36&quot; /&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;452.5&quot; y=&quot;-14.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;input3&lt;/text&gt;
&lt;/g&gt;
&lt;!-- demo3&amp;#45;&amp;gt;input3 --&gt;
&lt;g id=&quot;edge8&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;demo3&amp;#45;&amp;gt;input3&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M385.05,-18C393.16,-18 402.01,-18 410.53,-18&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;410.68,-21.5 420.68,-18 410.68,-14.5 410.68,-21.5&quot; /&gt;
&lt;/g&gt;
&lt;!-- label1 --&gt;
&lt;g id=&quot;node10&quot; class=&quot;node&quot;&gt;
&lt;title&gt;label1&lt;/title&gt;
&lt;polygon fill=&quot;none&quot; stroke=&quot;black&quot; points=&quot;580,-144 520,-144 520,-108 580,-108 580,-144&quot; /&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;550&quot; y=&quot;-122.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;label1&lt;/text&gt;
&lt;/g&gt;
&lt;!-- input1&amp;#45;&amp;gt;label1 --&gt;
&lt;g id=&quot;edge9&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;input1&amp;#45;&amp;gt;label1&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M484.24,-126C495.69,-126 508.64,-126 519.94,-126&quot; /&gt;
&lt;/g&gt;
&lt;!-- label2 --&gt;
&lt;g id=&quot;node11&quot; class=&quot;node&quot;&gt;
&lt;title&gt;label2&lt;/title&gt;
&lt;polygon fill=&quot;none&quot; stroke=&quot;black&quot; points=&quot;580,-90 520,-90 520,-54 580,-54 580,-90&quot; /&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;550&quot; y=&quot;-68.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;label2&lt;/text&gt;
&lt;/g&gt;
&lt;!-- input2&amp;#45;&amp;gt;label2 --&gt;
&lt;g id=&quot;edge10&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;input2&amp;#45;&amp;gt;label2&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M484.24,-72C495.69,-72 508.64,-72 519.94,-72&quot; /&gt;
&lt;/g&gt;
&lt;!-- label3 --&gt;
&lt;g id=&quot;node12&quot; class=&quot;node&quot;&gt;
&lt;title&gt;label3&lt;/title&gt;
&lt;polygon fill=&quot;none&quot; stroke=&quot;black&quot; points=&quot;580,-36 520,-36 520,0 580,0 580,-36&quot; /&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;550&quot; y=&quot;-14.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;label3&lt;/text&gt;
&lt;/g&gt;
&lt;!-- input3&amp;#45;&amp;gt;label3 --&gt;
&lt;g id=&quot;edge11&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;input3&amp;#45;&amp;gt;label3&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M484.24,-18C495.69,-18 508.64,-18 519.94,-18&quot; /&gt;
&lt;/g&gt;
&lt;/g&gt;
&lt;/svg&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;以下是一些显性的部分：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;后面船涨会提到「Circulation revenue has increased by 5% in Finland」这句是 demo 中的 &lt;strong&gt;input&lt;/strong&gt;；&lt;/li&gt;
  &lt;li&gt;对应的「Positive」是 demo 中的 &lt;strong&gt;label&lt;/strong&gt;；&lt;/li&gt;
  &lt;li&gt;input 和 label 组成的就是 &lt;strong&gt;demo&lt;/strong&gt;；&lt;/li&gt;
  &lt;li&gt;类似「The acquisition will have an immediate positive impact」这样的就是一个 &lt;strong&gt;test example&lt;/strong&gt;；&lt;/li&gt;
  &lt;li&gt;我们期待语言模型反馈给我们的对应 test example 的就是 &lt;strong&gt;output&lt;/strong&gt;；&lt;/li&gt;
  &lt;li&gt;一组 demo 和一个 test example 组成的就是整个一个 &lt;strong&gt;prompt&lt;/strong&gt;。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;以下是一些隐性的部分：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;input 中整体文本输入分布 &lt;strong&gt;input distribution&lt;/strong&gt;；&lt;/li&gt;
  &lt;li&gt;label 的所有可能结果的值域空间 &lt;strong&gt;label space&lt;/strong&gt;；&lt;/li&gt;
  &lt;li&gt;所有 input 整体与所有 label 整体之间的映射关系 &lt;strong&gt;input-label mapping&lt;/strong&gt;；&lt;/li&gt;
  &lt;li&gt;整体 input、label 的 &lt;strong&gt;format&lt;/strong&gt;，比如本节开头图里的例子，每个 demo 就是一句 input 一个 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;\n&lt;/code&gt; 再一个 label。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;通过一系列实验，研究人员得出一系列重要的结论，并普遍在 LLM 的使用中被认为正确，我们往下逐一看看。&lt;/p&gt;

&lt;h4 id=&quot;151prompt-中的-input-label-mapping-不重要&quot;&gt;15.1、Prompt 中的 Input-Label Mapping 不重要&lt;/h4&gt;

&lt;p&gt;这个结论是非常反直觉的，但是实验结果就是这样，我们来详细地看一下。&lt;/p&gt;

&lt;p&gt;我们尝试看下随机打乱的 mapping 会带来什么影响。如下图，基于 12 种语言模型，对比在 prompts 中没有 demo 示例（下图中蓝色）、有 demo 示例且 golden label（你可以把 golden label 理解为人类监督下符合期望的 label，下图中的橙黄色）、有 demo 示例但 label 随机（下图中的红色）这 3 种情况。实验结果如下：&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023/2023-01-23-captain-aigc-2-llm-21.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;上图中，上面是分类问题，下面是多选问题。实验结果表明，随机 label 与 golden label 的表现差异并不大，但是远好于没有 demo 的情况。也就是说只要有 demo 就行，demo 中的 input-label mapping 没太大所谓。也就是，随机的 labels 虽然增加了噪声，但是并没有移除贝叶斯推理中的全部信号。&lt;/p&gt;

&lt;p&gt;这里要强调的是，随机打乱的 labels 的值域分布空间还是与 golden labels 一样的，与下面要聊到的「label space」是不同的，后者是完全没有值域边界的一个值域空间。&lt;/p&gt;

&lt;h4 id=&quot;152prompt-中的-input-distribution-很重要&quot;&gt;15.2、Prompt 中的 Input Distribution 很重要&lt;/h4&gt;

&lt;p&gt;对于 input distribution 的验证，我们来对比两组：第一组就是来自训练数据里句子作为 demos 的 input，第二组是不服从分布的（Out-Of-Distribution，OOD），具体说，是从其他外部语料库随机采样的一组句子 \({x_{i,rand}}^k_{i=1}\) 替换 demos 里的 \({x_i}^k_{i=1}\)。这种情况下，label 都用 random，format 都不变，实验结果如下图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023/2023-01-23-captain-aigc-2-llm-24.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;每 4 个一组的柱状图里（具体说明在右侧），中间两个（Random labels 和 OOD + Random labels）的对比区别就是 input distribution 不同。可见除了 Direct MetaICL 模型之外，其他模型下这两组 input distribution 带来的结果表现差异是很显著的。&lt;/p&gt;

&lt;p&gt;所以可以通过实验初步得出结论：input distribution 是有显著影响的。&lt;/p&gt;

&lt;h4 id=&quot;153prompt-中的-label-space-很重要&quot;&gt;15.3、Prompt 中的 Label Space 很重要&lt;/h4&gt;

&lt;p&gt;对于本小节开头的那个例子，正确的 Label Space 应该是 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;{Positive, Negative, Neutral}&lt;/code&gt; 这样的 Golden Label Space，而一个 Label Space 不分布在正确值域里的情况，可以是一些完全无关的、随机的，比如上面例子里可以出现 Unanimity、Wave、Guana、Syrup … 类似的 Random Label Space。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023/2023-01-23-captain-aigc-2-llm-23.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;每 4 个一组的柱状图里，中间两个（Random Labels 和 Random English Words）的对比区别就是 Label Space 不同。前者是 Golden，后者是 Random，可见在 Direct 模型里有显著差异，但是在 Channel 模型里几乎表现差不多。&lt;/p&gt;

&lt;p&gt;所以可以通过实验初步得出结论：label space 是有显著影响的。&lt;/p&gt;

&lt;h4 id=&quot;154prompt-中-format-of-demonstration-很重要&quot;&gt;15.4、Prompt 中 Format of Demonstration 很重要&lt;/h4&gt;

&lt;p&gt;我们可以设置很多种不同的 format 来对比，但这里为了更简单、直接说明问题，我们将用 no labels 和 with labels only 两种 formats，如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023/2023-01-23-captain-aigc-2-llm-26.png&quot; alt=&quot;&quot; width=&quot;650&quot; /&gt;&lt;/p&gt;

&lt;p&gt;将这两种 formats 来和「input with random label」format 做对比（因为通过前面的实验，我们可以认为 input-label mapping 是不显著影响的，所以用 random label 且 label space 是正确的）。实验结果如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023/2023-01-23-captain-aigc-2-llm-25.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;每 7 个一组的柱状图里，分别对比了 7 种情况，我们这里看每一组的第 2、4、6 个柱状图。可以看出 no labels 和 with labels only 两种 formats 的表现都显著低于 random labels。&lt;/p&gt;

&lt;p&gt;所以可以通过实验初步得出结论：使用 input-label 作为 demo 的 format 对于模型表现有显著提升作用。&lt;/p&gt;

&lt;h4 id=&quot;155小结一下&quot;&gt;15.5、小结一下&lt;/h4&gt;

&lt;p&gt;在基于 ICL 能力来用好 LLM 时，我们有以下 learning：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;在 prompt 里带上 demo 是很重要的，而且 demo 在形式上 input 和 label 都需要。&lt;/li&gt;
  &lt;li&gt;对于 demo 中的 input，不要乱来，要给出比较合理的 input。&lt;/li&gt;
  &lt;li&gt;对于 demo 中的 label，只要它属于正确的值域空间 label space 就可以了，是否与 input 有 correct mapping 不重要。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;由此指导我们在实践中，给出尽量多的 demonstrations 时，不需要人工标注每个 input 有正确的 label，而是对于每个 input 可以批量化随机地给出符合 label space 的 label 即可，实现在下游任务上人工标注成本的下降。&lt;/p&gt;

&lt;p&gt;但是关于本节的各项研究结论，目前还有很多是基于假设之上推演 + 实验的，并非如「理论科学」那么严谨，可以和船涨一起持续关注后续学界与业界对于 ICL 的最新进展。&lt;/p&gt;

&lt;h3 id=&quot;第-16-节--思维链chain-of-thoughtcot2022-年-1-月&quot;&gt;第 16 节 · 思维链（Chain of Thought，CoT，2022 年 1 月）&lt;/h3&gt;

&lt;h4 id=&quot;161深度学习就是要从-system-1-走向-system-2&quot;&gt;16.1、深度学习就是要从 System-1 走向 System-2&lt;/h4&gt;

&lt;p&gt;在认知科学里，有一个「认知双通道理论」，讲的是人脑有两套系统，即「系统 1」和「系统 2」[14]：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;系统 1（System-1）&lt;/strong&gt;常被称为直觉系统，它的运行是无意识且快速的，不怎么费脑力，没有感觉，完全处于自主控制状态。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;系统 2（System-2）&lt;/strong&gt;常被称为逻辑分析系统，它将注意力转移到需要费脑力的大脑活动上来，例如复杂的运算。系统 2 的运行通常与行为、选择和专注等主观体验相关联。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;该理论由心理学家基思·斯坦诺维奇（Keith Stanovich）和理查德·韦斯特（Richard West）率先提出。后被行为经济学之父、诺贝尔奖获得者丹尼尔·卡尼曼（Daniel Kahneman）引用在其著作《思考，快与慢》中，而被心理学领域之外的人广泛所知，我上述的表述也引自该书。&lt;/p&gt;

&lt;!-- ![](/img/src/2023/2023-01-23-captain-aigc-2-llm-7.jpeg) --&gt;

&lt;p&gt;图灵奖得主 Yoshua Bengio 在 2019 年人工智能顶级会议 NeurIPS 的题为《From System 1 Deep Learning To System 2 Deep Learing》报告中引用该理论，被 AI 圈子很多人误以为这个理论是 Yoshua Bengio 提出的（而 Yoshua 误以为这是 Kahneman 提出的）。但是 Yoshua 很有价值地指出：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;System-1 是目前深度学习正在做的事情 —— &lt;strong&gt;Current DL&lt;/strong&gt;，比如图像识别、人脸识别、机器翻译、情感分类、语音识别、自动驾驶等。&lt;/li&gt;
  &lt;li&gt;System-2 是未来深度学习将要做的事情 —— &lt;strong&gt;Future DL&lt;/strong&gt;，比如推理、规划等任务，这些任务基本都是有逻辑的（logical）、可推理的（reasoning）。&lt;/li&gt;
&lt;/ul&gt;

&lt;!-- ![](/img/src/2023/2023-01-23-captain-aigc-2-llm-8.jpeg) --&gt;

&lt;p&gt;LLM 的研究者们也在探究那些 System-2 要解决的任务，于是有了下面船涨了解到的几个技术方向，与大家分享探讨。&lt;/p&gt;

&lt;h4 id=&quot;162google-提出思维链提示cot-prompting&quot;&gt;16.2、Google 提出思维链提示（CoT Prompting）&lt;/h4&gt;

&lt;p&gt;尽管在语言建模方面，现有模型表现还可以，NLP 领域的 System-1 任务基本还算比较好的解决，但是对于 System-2 基本上都没有太好的进展。但 GPT 出来后这事儿变得不一样了，尤其是 InstructGPT。我们往下慢慢看。&lt;/p&gt;

&lt;p&gt;2022 年初 Google 在论文&lt;a href=&quot;https://arxiv.org/abs/2201.11903&quot;&gt;《Chain-of-Thought Prompting Elicits Reasoning in Large Language Models》&lt;/a&gt;中提出「思维链（Chain of Though，CoT）」：研究发现通过在 prompts 中增加思维链（即一系列中间推理步骤），就能显著提升 LLM 的推力表现。并将这种提示方式称为「Chain of Thought prompting」。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023/2023-01-23-captain-aigc-2-llm-5.png&quot; alt=&quot;&quot; width=&quot;640&quot; /&gt;&lt;/p&gt;

&lt;p&gt;比如如上示例，标准的提示中给了一对问答样例，再加上一个问题，让 LLM 返回问题的答案。CoT Prompting 则在那对问答样例中加上了一段 CoT，而不是直接给出「The answer is 11.」上图展示了这样两种 prompting 及对应输出的案例。在不同的模型上，这两种 prompting 方式&lt;/p&gt;

&lt;h4 id=&quot;163lets-think-step-by-step&quot;&gt;16.3、Let’s Think Step by Step&lt;/h4&gt;

&lt;p&gt;2022 年 5 月三位东京大学学者与两位 Google 的研究人员在论文&lt;a href=&quot;https://arxiv.org/abs/2205.11916&quot;&gt;《Large Language Models are Zero-Shot Reasoners》&lt;/a&gt;中提到了后来在 Gen-AI 领域那句著名的提示 —— Let’s think step by step —— 对于涉及到逻辑推理方面的问题，通过增加这句提示后，模型展现出了推理性能的大幅跃升。&lt;/p&gt;

&lt;p&gt;对于 OpenAI 的 InstructGPT（具体地，是 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;text-davinci-002&lt;/code&gt;）模型，在输入提示时加上「Let’s think step by step」后，其表现：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;在 MultiArith 数据集上，准确率从 17.7% 提升到 78.7%&lt;/li&gt;
  &lt;li&gt;在 GSM8K 数据集上，准确率从 10.4% 提升到 40.7%&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;对于 Google 的 PaLM 模型（具体地，参数规模为 5400 亿），同样的输入提示改造，其表现：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;在 MultiArith 数据集上，准确率从 25.5% 提升到 66.1%&lt;/li&gt;
  &lt;li&gt;在 GSM8K 数据集上，准确率从 12.5% 提升到 43.0%&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;事实上，除了「Let’s think step by step」，该论文的作者们还尝试了一系列其他 prompts 的插入，只不过这句的表现最好：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023/2023-01-23-captain-aigc-2-llm-4.png&quot; alt=&quot;&quot; width=&quot;640&quot; /&gt;&lt;/p&gt;

&lt;p&gt;从这里我们可以看到，对于 CoT 的挖掘体现了我们对 LLM 的了解仍然处于非常早期的阶段。如果未来船涨会更新本篇文章的新版本，我认为在 ICL、CoT 部分一定会有很多新进展出现。而随着我们对 LLM 为什么具备 ICL 能力的研究加深，更好地。&lt;/p&gt;

&lt;h3 id=&quot;本小节参考-7&quot;&gt;本小节参考&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;https://arxiv.org/abs/2107.13586&lt;/li&gt;
  &lt;li&gt;https://arxiv.org/abs/2001.07676&lt;/li&gt;
  &lt;li&gt;https://zhuanlan.zhihu.com/p/399295895&lt;/li&gt;
  &lt;li&gt;https://arxiv.org/abs/2104.08691&lt;/li&gt;
  &lt;li&gt;https://aclanthology.org/2021.acl-long.353.pdf&lt;/li&gt;
  &lt;li&gt;https://zhuanlan.zhihu.com/p/551174711&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.cs.princeton.edu/courses/archive/fall22/cos597G/lectures/lec07.pdf&quot;&gt;Towards Understanding In-context Learning&lt;/a&gt;, Sam Liang &amp;amp; Kevin Jin, Princeton University.&lt;/li&gt;
  &lt;li&gt;https://zhuanlan.zhihu.com/p/524383554&lt;/li&gt;
  &lt;li&gt;https://zhuanlan.zhihu.com/p/551014127&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://ai.stanford.edu/blog/understanding-incontext/&quot;&gt;How does in-context learning work? A framework for understanding the differences from traditional supervised learning&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2111.02080&quot;&gt;An Explanation of In-context Learning as Implicit Bayesian Inference&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2202.12837&quot;&gt;Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;https://zhuanlan.zhihu.com/p/493533589&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;第四章--llm-发展关键节点与主流模型2022---2023&quot;&gt;第四章 · LLM 发展关键节点与主流模型（2022 - 2023）&lt;/h2&gt;

&lt;h3 id=&quot;第-17-节--instructgpt为对齐alignment而生的指令式-gpt2022-年-3-月&quot;&gt;第 17 节 · InstructGPT：为对齐（Alignment）而生的指令式 GPT（2022 年 3 月）&lt;/h3&gt;

&lt;p&gt;GPT-3 上线并商业化后，半年后（到 2021 年 3 月）有 300 余款基于它的 API 开发的应用。OpenAI 认为自己在开创下一代应用生态。但是遇到的一个严重问题是，GPT-3 的输出存在不可信、有害等内容。为了解决这个问题，&lt;a href=&quot;https://openai.com/alignment/&quot;&gt;OpenAI 将「对齐研究（Alignment Research）」作为其技术研发重点之一&lt;/a&gt;[3]。在该公司 2022 年 1 月底的 blog 中提到，他们已经研究 alignment 好几年了，但 InstructGPT 是他们第一次把它用到产品里。&lt;/p&gt;

&lt;h4 id=&quot;171对齐alignment关于我们到底要-ai-未来变成什么样子这件事儿&quot;&gt;17.1、对齐（Alignment）：关于「我们到底要 AI 未来变成什么样子」这件事儿&lt;/h4&gt;

&lt;p&gt;所谓「AI 对齐」，就是 AI 要语言模型的输出应该向人类使用者对齐。关于具体地，OpenAI 想用人类指令（Instruct）作为输入，AI 根据指令输入，返回&lt;strong&gt;&lt;u&gt;符合人类价值观的输出&lt;/u&gt;&lt;/strong&gt;。对于 Alignment，Anthropic 在其论文&lt;a href=&quot;https://arxiv.org/abs/2112.00861&quot;&gt;《A General Language Assistant as a Laboratory for Alignment》&lt;/a&gt;中给出了该公司在 AI 对齐上的理念：&lt;strong&gt;&lt;u&gt;有用（helpful）、可信（Honest）、无害（Harmless）&lt;/u&gt;&lt;/strong&gt;，比较被广泛接受。目前围绕 Alignment，业内 Anthropic 和 OpenAI 是走在比较前沿的两家公司，尤其前者。&lt;/p&gt;

&lt;p&gt;所以可以看出，「AI Alignment」其实是「AI Safety」的一个子领域，国内所有做安全、合规、技术道德相关方向的朋友都要重视起来，关于 AI Alignment 或许我可以单独写一篇文章来和大家交流。&lt;/p&gt;

&lt;p&gt;为了实现这一点，需要将人类对于文本内容是否为人类所需的判断，被 AI 学习会，于是最初 OpenAI 想到的是用监督微调（Supervised Fine-Tune）的方法，基于此推出了 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;davinci-instruct-beta&lt;/code&gt;，就是对一些 OpenAI 收集的用户 prompts 输入，由人类给出输出，这些 prompt-generation pairs 拿来 fine-tune 原来的 GPT-3。但是这样其实并没有明示 AI，只是「暗戳戳」地让它学习。&lt;/p&gt;

&lt;p&gt;其实我们想要的是，与人类对齐的输出结果，是好的输出；与人类不对齐的输出结果，不是好的输出；所以输出好与不好，这里可以做一个奖励模型来评判。而用这个奖励模型来训练一个语言模型，就是用强化学习的思路了。于是 OpenAI 选择通过「人工反馈的强化学习（Reinforcement Learning with Human Feedback）」来实现这一点，就在 GPT-3 的基础上研发了 InstructGPT。&lt;/p&gt;

&lt;!-- 为了搞懂这些，我们要先对强化学习、PPO 算法有个大概了解。已经明白的朋友，可以直接跳到「2.3 小节」。 --&gt;

&lt;!-- #### 17.2、强化学习和 PPO 算法 --&gt;

&lt;!-- 近端策略优化（Proximal Policy Optimization，PPO）[8] --&gt;

&lt;h4 id=&quot;172如何用-rlhf-方法训练得到更能对齐人类的-instructgpt&quot;&gt;17.2、如何用 RLHF 方法训练得到更能对齐人类的 InstructGPT？&lt;/h4&gt;

&lt;p&gt;用 RLHF 方法得到 InstructGPT 的具体流程，OpenAI 官方给出了如下解释[2]：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023/2023-01-23-captain-aigc-2-llm-3.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;第一步，是&lt;strong&gt;&lt;u&gt;监督微调（Supervised Fine-Tune，SFT）&lt;/u&gt;&lt;/strong&gt;出一个对齐人类的初始模型。OpenAI 把这个模型叫做「监督策略（Supervised Policy）」，因为用 RLHF 来实现 Alignment 的目的就是为了用「对齐策略」把 AI 给「管」起来。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;首先，从 GPT 开放使用后收集的用户输入提示（prompts）中取用一部分提示，根据 OpenAI 的说法其采用的数据是来自官网 Playground 工具收集的提示（具体来说，用的数据来自 2021 年 OpenAI 部署的 GPT 版本所收集），并去掉了其中涉及到隐私/个人信息的内容[2]。&lt;/li&gt;
  &lt;li&gt;然后，由人类为它们来编写一些输出样例（demonstration）。&lt;/li&gt;
  &lt;li&gt;接着，用这些监督数据（一些 prompt-demonstration pairs）去微调 GPT-3。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;第二步，训练一个对齐人类的&lt;strong&gt;&lt;u&gt;奖励模型&lt;/u&gt;&lt;/strong&gt;（注意第一步、第二步是两个独立的模型），这是强化学习中不可或缺的。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;首先，基于用户提示（与 SFT 那一步用的 prompts 是相同来源）从多个模型得到多个输出。&lt;/li&gt;
  &lt;li&gt;然后，人类标注员（labelers）对不同的结果进行排序（注意不是打分）。&lt;/li&gt;
  &lt;li&gt;最后，将不同模型的输出结果和排序结果作为训练数据，训练一个奖励模型（Reward Model，RM）。训练成功后，这个 RM 就有了衡量输出表现的能力。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;第三步，用第二步的奖励模型，以&lt;strong&gt;&lt;u&gt;强化学习 PPO 方法&lt;/u&gt;&lt;/strong&gt;训练第一步得到的初始模型（监督政策）。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;将训练集数据输入给初始模型（监督政策），得到相应的输出。&lt;/li&gt;
  &lt;li&gt;RM 对初始模型的输出计算奖励。&lt;/li&gt;
  &lt;li&gt;基于奖励值，用 PPO 算法更新监督政策。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;这个流程下来，「对齐」体现在哪里了呢？基本上需要人工的地方，就是在「对齐」。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;SFT 的第一步，用的是「符合人类期望行为的样例（demonstrations of the desired model behavior）」。&lt;/li&gt;
  &lt;li&gt;训练 RM 的第二步，排序判断来自人类。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;根据 OpenAI 发表的&lt;a href=&quot;https://arxiv.org/abs/2203.02155&quot;&gt;《Training language models to follow instructions with human feedback》&lt;/a&gt;，用 RLHF 方法得到的 13 亿参数规模的 InstructGPT 的表现要优于 1750 亿参数的 GPT-3。这是非常令人吃惊的结果，换句话说，1750 亿参数的 GPT-3 的能力并没有被很好的挖掘 —— 它有学习能力，但是人类没有教好。&lt;/p&gt;

&lt;p&gt;到这里我们能看到，RLHF 的方法一定非常适合做领域垂直化的训练，只需要雇佣一批垂直领域的「专家」来做标注员（labelers）来训练初始模型和奖励模型就可以。所以现在类似 ChatGPT 这种 LLM 对事实类知识处理得不太好的领域，都有 RLHF 做基础训练方法得到可用产品的机会。&lt;/p&gt;

&lt;p&gt;还值得一提的是，测试 InstructGPT 在 Alignment 方面表现的两个数据集可以关注一下：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;测模型谎言多少的数据集：&lt;a href=&quot;https://arxiv.org/abs/2109.07958&quot;&gt;TruthfulQA: Measuring how models mimic human falsehoods&lt;/a&gt;. Lin, S., Hilton, J. and Evans, O., 2021.&lt;/li&gt;
  &lt;li&gt;测模型有害程度的数据集：&lt;a href=&quot;https://arxiv.org/abs/2009.11462&quot;&gt;RealToxicityPrompts: Evaluating neural toxic degeneration in language models&lt;/a&gt;. Gehman, S., Gururangan, S., Sap, M., Choi, Y. and Smith, N.A., 2020.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;173对齐税alignment-tax&quot;&gt;17.3、对齐税（Alignment Tax）&lt;/h4&gt;

&lt;p&gt;目前从实际表现看，只在客户任务上对齐模型，可能会让模型在其他一些学术任务上的表现更差，我们把这种代价叫做「对齐税（Alignment Tax）」。OpenAI 提到一种最小化对齐税的小 trick：&lt;strong&gt;在强化学习第一步微调的时候，混入一点训练 GPT-3 时用到的原始数据，然后微调训练时用正则对数极大似然函数（Normal Log Likelihood Maximization）做 loss 函数&lt;/strong&gt;。这种方式既能大概保持在客户任务方面的性能，也能缓解学术任务上的表现下降，在某些情况下甚至超过了 GPT-3。&lt;/p&gt;

&lt;h4 id=&quot;174目前-instructgpt-的局限性&quot;&gt;17.4、目前 InstructGPT 的局限性&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;听话与拒绝：训练模型 follow instructions 的一个副产品就是 —— 太听话也不是好事儿 —— 如果指令产生了不好的内容，模型可能更容易被滥用。解决方法是模型要会明确拒绝某些指令。&lt;/li&gt;
  &lt;li&gt;文化兼容性不足：InstructGPT 是用英文指令训练的，因此它偏向于英语使用者的文化价值观。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;175instructgpt-就是-gpt-35-吗&quot;&gt;17.5、InstructGPT 就是 GPT-3.5 吗？&lt;/h4&gt;

&lt;p&gt;InstructGPT 不等于 GPT-3.5。GPT-3.5 是 OpenAI 用 2021 年第四季度之前的文本、代码训练的。OpenAI 在其官网[10]明确指出，目前 OpenAI 公开的 API 中：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;你能用到的 InstructGPT 模型有 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;davinci-instruct-beta&lt;/code&gt;、&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;text-davinci-001&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;text-davinci-002&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;text-curie-001&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;text-babbage-001&lt;/code&gt;、&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;text-davinci-003&lt;/code&gt;。&lt;/li&gt;
  &lt;li&gt;你能用到的 GPT-3.5 模型有 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;code-davinci-002&lt;/code&gt;、&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;text-davinci-002&lt;/code&gt;、&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;text-davinci-003&lt;/code&gt;。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在 2022 年 1 月底，OpenAI 官方说这个 align 过的 InstructGPT 模型已经在 OpenAI 线上的 GPT API 里跑了一年多了，只是没告诉大家。&lt;/p&gt;

&lt;h3 id=&quot;第-18-节--chatgpt基于-rlhf-训练的对话式-gpt-模型2022-年-11-月底&quot;&gt;第 18 节 · ChatGPT：基于 RLHF 训练的对话式 GPT 模型（2022 年 11 月底）&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023/2023-01-23-captain-aigc-2-llm-37.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;InstructGPT 是基于人类指令输入，给予对应的输出结果，OpenAI 将它迭代用在了其官网的 Playground 产品里。但发送指令这种方式，其实还不是人类最自然的方式。秉承 Alignment 的理念，为了让交互方式更符合人类的习惯，OpenAI 希望打造基于对话 ———— 人类最熟悉的交流方式 ———— 进行输入输出交互的语言模型，于是有了 ChatGPT。&lt;/p&gt;

&lt;h4 id=&quot;181chatgpt-的训练方法&quot;&gt;18.1、ChatGPT 的训练方法&lt;/h4&gt;

&lt;p&gt;训练方法上，OpenAI 称 ChatGPT 与 InstructGPT 的 RLHF 方法相同，只是数据集设置方面有一点区别。下面是 OpenAI 官方其在 ChatGPT 研发中使用的 RLHF 流程：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023/2023-01-23-captain-aigc-2-llm-2.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;第一步，监督微调（SFT），方法与 InstructGPT 是相同的，但是数据上略有差异。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;在 InstructGPT 所用的 SFT 数据集基础上，ChatGPT 还收集了一些人类编写的对话（对话的双方内容都由人类训练师提供，训练师既扮演用户，也扮演 AI 助手）。&lt;/li&gt;
  &lt;li&gt;然后，把这些监督数据（prompt-demonstration pairs）与 InstructGPT 训练用的数据集混合起来，并从「指令（Instruct）」全部改成「对话（Chat）」模式。&lt;/li&gt;
  &lt;li&gt;接着，用这些数据来微调（fine-tune）GPT-3.5。（注：所以我们看到 OpenAI 说的是基于 GPT-3.5 研发的 ChatGPT，而不是 ChatGPT 等于 GPT-3.5。）&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;第二步，奖励模型（RM）的构建过程中，产出训练数据所需的结果排序时，迭代为排序中不可以有排位相等的情况，而是全部要区分开。&lt;/p&gt;

&lt;p&gt;第三步，与 InstructGPT 流程相同，就是用第二步的 RM 以 PPO 强化学习方法来训练第一步 SFT 得到的原始模型（监督策略）。&lt;/p&gt;

&lt;h4 id=&quot;182chatgpt-与-instructgpt-的主要技术区别&quot;&gt;18.2、ChatGPT 与 InstructGPT 的主要技术区别&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;训练数据来源不同：ChatGPT 所用的训练数据，是在 InstructGPT 所用训练数据的基础上，混入了人类训练师编写的对话数据。&lt;/li&gt;
  &lt;li&gt;训练数据格式不同：InstructGPT 的训练数据用的还是「指令-响应」的格式，而训练 ChatGPT 时 OpenAI 将数据转换成「对话」格式。&lt;/li&gt;
  &lt;li&gt;基础模型版本不同：InstructGPT 是基于 GPT-3 微调的，ChatGPT 是基于 GPT-3.5 微调的。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;183chatgpt-开放-chat-api&quot;&gt;18.3、ChatGPT 开放 Chat API&lt;/h4&gt;

&lt;p&gt;我在 2023 年 3 月 2 日写了一篇简单的&lt;a href=&quot;https://mp.weixin.qq.com/s/xKExN-NbxTFn_eUcvUGR_w&quot;&gt;入门 ChatGPT 官方 API 文档解读&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;本次 OpenAI 在其 GPT 系列模型 Davinci、Curie、Babbage、Ada 之外，推出了 Turbo 模型，是比 Davinci 性能还要强的。Turbo 与支持 ChatGPT 的模型系列相同，它针对对话式聊天输入和输出进行了优化，但与 Davinci 模型系列相比，它在完成方面同样出色。在 ChatGPT 中可以很好地完成的任何用例都应该在 API 中与 Turbo 模型系列一起很好地执行。Turbo 模型家族也是第一个像 ChatGPT 一样接收定期模型更新的模型。Turbo &lt;strong&gt;适合于对话、文本生成&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;OpenAI 发布的 ChatGPT 模型系列 GPT-3.5-turbo 与 ChatGPT 产品中使用的模型相同。但是它的价格为每 1k tokens 0.002 美元，比 OpenAI 现有的 GPT-3 模型便宜 10 倍。即使对于许多非聊天用例，它也是 OpenAI 的最佳模型 —— 如果你之前就用了 text-davinci-003，那么迁移到 gpt-3.5-turbo 时只需要对他们的提示进行少量调整。&lt;/p&gt;

&lt;p&gt;OpenAI 在本次开放中额外增加或更新了如下模型：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gpt-3.5-turbo&lt;/code&gt;：功能最强大的 GPT-3.5 模型并针对聊天进行了优化，成本仅为 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;text-davinci-003&lt;/code&gt; 的 1/10。将使用我们最新的模型迭代进行更新。&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gpt-3.5-turbo-0301&lt;/code&gt;：2023 年 3 月 1 日的 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gpt-3.5-turbo&lt;/code&gt; 快照。与 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gpt-3.5-turbo&lt;/code&gt; 不同，此模型不会收到更新，并且仅在 2023 年 6 月 1 日结束的三个月内提供支持。&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;text-davinci-003&lt;/code&gt;：此前 GPT-3 期间就有的 Davinci 模型，这次直接升级到了 GPT-3.5，与 GPT-3 阶段一样，仍然是最大请求 4000 tokens，同样训练数据 up to 2021 年 6 月，能做几乎所有 NLP 任务。&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;text-davinci-002&lt;/code&gt;：与 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;text-davinci-003&lt;/code&gt; 类似情况，这次直接升级到了 GPT-3.5，专门为了代码生成任务优化的模型。与 GPT-3 阶段一样，仍然是最大请求 4000 tokens；训练数据也没有变，依然是 up to 2021 年 6 月。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;而以下模型不再由 GPT-3 提供支持：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;text-davinci-003&lt;/code&gt;：此前在 GPT-3 接口中存在，本次更新后由 GPT-3.5 支持提供了，见上一段。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;目前已经有基于不少 Chat API 的应用在网上开始出现，比如下面这个：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;支持 epub 的双语阅读器（开源）：https://github.com/yihong0618/bilingual_book_maker&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;第-19-节--其他一些近期值得关注的模型&quot;&gt;第 19 节 · 其他一些近期值得关注的模型&lt;/h3&gt;

&lt;h3 id=&quot;191llamameta-对-chatgpt-的回应&quot;&gt;19.1、LLaMA：Meta 对 ChatGPT 的回应&lt;/h3&gt;

&lt;p&gt;2023 年 2 月 25 日 Meta AI 在其官网&lt;a href=&quot;https://ai.facebook.com/blog/large-language-model-llama-meta-ai/&quot;&gt;公开发布了 LLaMA（Large Language Model Meta AI）大型语言模型&lt;/a&gt;，包括 70 亿、130 亿、330 亿、650 亿 4 种参数规模，旨在推动 LLM 领域的小型化、平民化研究。在其论文&lt;a href=&quot;https://arxiv.org/abs/2302.13971&quot;&gt;《LLaMA: Open and Efficient Foundation Language Models》&lt;/a&gt;中声称用不到 GPT-3 的 1/10 参数规模就超越其表现。下面是参数规模对比：&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Model Name&lt;/td&gt;
      &lt;td&gt;\(n_{params}\)&lt;/td&gt;
      &lt;td&gt;\(n_{layers}\)&lt;/td&gt;
      &lt;td&gt;\(d_{model}\)&lt;/td&gt;
      &lt;td&gt;\(n_{heads}\)&lt;/td&gt;
      &lt;td&gt;Batch Size&lt;/td&gt;
      &lt;td&gt;Learning Rate&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;LLaMA 7B&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;7B&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;32&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;4096&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;32&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;4M&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;3.0E-4&lt;/strong&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;GPT-3	6.7B&lt;/td&gt;
      &lt;td&gt;6.7B&lt;/td&gt;
      &lt;td&gt;32&lt;/td&gt;
      &lt;td&gt;4096&lt;/td&gt;
      &lt;td&gt;32&lt;/td&gt;
      &lt;td&gt;2M&lt;/td&gt;
      &lt;td&gt;1.2E-4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;LLaMA 13B&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;13B&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;40&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;5120&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;40&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;4M&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;3.0E-4&lt;/strong&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;GPT-3	13B&lt;/td&gt;
      &lt;td&gt;13B&lt;/td&gt;
      &lt;td&gt;40&lt;/td&gt;
      &lt;td&gt;5140&lt;/td&gt;
      &lt;td&gt;40&lt;/td&gt;
      &lt;td&gt;2M&lt;/td&gt;
      &lt;td&gt;1.0E-4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;LLaMA 33B&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;33B&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;40&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;6656&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;52&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;4M&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;1.5E-4&lt;/strong&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;LLaMA 65B&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;65B&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;80&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;8192&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;64&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;4M&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;1.5E-4&lt;/strong&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;GPT-3	175B&lt;/td&gt;
      &lt;td&gt;175.0B&lt;/td&gt;
      &lt;td&gt;96&lt;/td&gt;
      &lt;td&gt;12288&lt;/td&gt;
      &lt;td&gt;96&lt;/td&gt;
      &lt;td&gt;3.2M&lt;/td&gt;
      &lt;td&gt;0.6E-4&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;与 GPT-3 模型对比可以看出，LLaMA 的四个版本中：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;LLaMA-7B 对应的是 GPT-3 6.7B 版本，都是 32 层、32 个多头注意力、4096 宽度，LR 3.0E-4 要高于 GPT 的 1.2E-4，batch 4M 更大。&lt;/li&gt;
  &lt;li&gt;LLaMA-13B 对应的是 GPT-3 13B 版本，都是 40 层、40 个多头注意力，模型宽度 5120、5140 差不多，LR 3.0E-4 也高于 GPT 的 1.0E-4，batch 4M 更大。&lt;/li&gt;
  &lt;li&gt;LLaMA-33B、LLaMA-65B 与 GPT-3 就没有对应了，都是仅次于 GPT-3 最大的 175B 版本。Meta AI 也是为了证明，更小的模型也能达到甚至超越 GPT-3 暴力大模型，这也是推动模型小型化的一个动力。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;跟 GPT 系列一样，LLaMA 也是采用 Transformer Decoder 的自回归语言模型架构，但是做了一些小改进（可以说并没有太多创新），此前船涨已发布过一篇快速解读 LLaMA 的文章&lt;a href=&quot;https://mp.weixin.qq.com/s/odK50Dr4QfyZyLsqiEh6WA&quot;&gt;《Meta 推出开源 LLaMA，用 1/10 参数规模打败 GPT-3，群「模」乱舞的 2023 拉开序幕》&lt;/a&gt;，这里说下其模型架构的小改进有什么：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;从 GPT-3 得到启发的 Pre-normalization：为了增强训练的稳定性，将只在输出层的 normalization 改成了 Transformer 里面每一层的输入都进行 normalize，具体用的是 &lt;a href=&quot;https://dl.acm.org/doi/pdf/10.5555/3454287.3455397&quot;&gt;Zhang and Sennrich (2019) 提到的 RMSNorm&lt;/a&gt;。&lt;/li&gt;
  &lt;li&gt;从 PaLM 得到启发的 SwiGLU 激活函数：用 Shazeer(2020) 提到的 &lt;a href=&quot;https://arxiv.org/abs/2002.05202&quot;&gt;SwiGLU 激活函数&lt;/a&gt;替换了大家熟悉的 ReLU 激活函数。&lt;/li&gt;
  &lt;li&gt;从 GPT-Neo 得到启发的 RoPE：在 Transformer 位置编码部分，没有用绝对位置编码（Absoute Positional Embeddings），而是用的 &lt;a href=&quot;https://arxiv.org/abs/2104.09864&quot;&gt;Su et al.(2021)&lt;/a&gt; 提到的 RoPE（Rotary Positional Embeddings）。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;可以说模型的改进并没有亮点。再看下 LLaMA 用到了如下这些训练数据集，并给出了相应的占比：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;CCNet：67%.&lt;/li&gt;
  &lt;li&gt;C4：15%，NLP 领域的人也基本知道了，全称是 Colossal Common Crawl Corpus，最早大家了解到它基本是通过 Google T5 模型的那篇论文《Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer》。作为对比，GPT-3 中的 60% 数据来自 Common Crawl，不过 GPT-3 对其 Common Crawl 的数据以及此前 OpenAI 在 GPT-2 用的 WebText 作为训练数据参照对 Common Crawl 数据进行了正类、负类的分类清理，不过还不是 C4。&lt;/li&gt;
  &lt;li&gt;GitHub：4.5%，开源代码仓库平台，目前已经是 Microsoft 自己的了。&lt;/li&gt;
  &lt;li&gt;Wikipedia：4.5%，之前 GPT-3 就用了 English Wikipedia。&lt;/li&gt;
  &lt;li&gt;Books：4.5%，作为对比 GPT-3 的数据源中 16% 来自书籍。&lt;/li&gt;
  &lt;li&gt;ArXiv：2.5%，是学界最熟悉的开放电子学术论文存档库，由康奈尔大学于 1991 年成立。&lt;/li&gt;
  &lt;li&gt;Stack Exchange：2%，类似于 Stack Overflow 的、针对程序员群体的在线技术问答社区。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;起初的邀请制被后来有人发布到 GitHub 和 HuggingFace 给破坏了，不过此前 LLaMA 已经说了开源，所以这也只是大家帮助 Meta 推进下速度吧。但是目前 LLaMA 缺少 RLHF 的加持，同时从目前各开发者使用反馈来讲举例 GPT-3 有差距。无论如何还是感谢 Meta 为 AI 开源做出贡献，期待后面在开源社区的努力下模型变得更加鲁棒和 SOTA。&lt;/p&gt;

&lt;h3 id=&quot;192其他一些近期值得关注的创新性-llm&quot;&gt;19.2、其他一些近期值得关注的、创新性 LLM&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Claude：由 OpenAI 的离职员工组成的 Anthropic 人工智能公司开发，暂未公开。与 OpenAI 提出的 RLHF 针锋相对地，在 2022 年 12 月于论文&lt;a href=&quot;https://arxiv.org/abs/2212.08073&quot;&gt;《Constitutional AI: Harmlessness from AI Feedback》&lt;/a&gt;提出基于 AI 反馈的强化学习（Reinforcement Learning with Artificial Intelligence Feedback，RLAIF），同时提出 Constitutional AI 概念，并基于此开发了 Claude，一个与 ChatGPT 类似但采用了 RLAIF 的 LLM 对话产品。&lt;/li&gt;
  &lt;li&gt;ChatRWKV：一个中国团队开发的、完全基于 RNN 架构的、开源对话系统，目标是最终可以在个人设备上运行。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;第五章--未来技术趋势的关注重点及一些推荐参考&quot;&gt;第五章 · 未来技术趋势的关注重点及一些推荐参考&lt;/h2&gt;

&lt;h3 id=&quot;第-20-节--未来技术趋势的关注重点&quot;&gt;第 20 节 · 未来技术趋势的关注重点&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;对齐（Alignment）与 Moderation&lt;/li&gt;
  &lt;li&gt;模型小型化&lt;/li&gt;
  &lt;li&gt;技术资产私有 与 架构分层：知识是如何存储、如何修改&lt;/li&gt;
  &lt;li&gt;Prompt Engineering&lt;/li&gt;
  &lt;li&gt;未来衡量 LLM 也需要提出新的 benchmark，既有的这些测试打榜并不满足需求。&lt;/li&gt;
  &lt;li&gt;GPT 在 few-shot learning 时到底是现学的，还是找到原来学过的相似的东西找出来。如果是后者，那真的是在拼训练数据大小了。但是对比人类，我们应该要做到前者才对。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;第-21-节--一些推荐&quot;&gt;第 21 节 · 一些推荐&lt;/h3&gt;

&lt;h4 id=&quot;211关键论文&quot;&gt;21.1、关键论文&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;2017.06 &lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot;&gt;《Attention Is All You Need》&lt;/a&gt;来自 Google，提出 Transformer。&lt;/li&gt;
  &lt;li&gt;2018.02 &lt;a href=&quot;https://arxiv.org/abs/1802.05365&quot;&gt;《Deep contextualized word representations》&lt;/a&gt; 来自 AI2，提出 ELMo。&lt;/li&gt;
  &lt;li&gt;2018.06 &lt;a href=&quot;https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf&quot;&gt;Improving Language Understanding by Generative Pre-Training&lt;/a&gt;来自 OpenAI，提出 GPT-1&lt;/li&gt;
  &lt;li&gt;2018.10 &lt;a href=&quot;https://arxiv.org/abs/1810.04805&quot;&gt;《BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding》&lt;/a&gt;来自 Google，提出 BERT。&lt;/li&gt;
  &lt;li&gt;2019.02 &lt;a href=&quot;https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf&quot;&gt;《Language Models are Unsupervised Multitask Learners》&lt;/a&gt;来自 OpenAI，提出 GPT-2&lt;/li&gt;
  &lt;li&gt;2019.10 &lt;a href=&quot;https://arxiv.org/abs/1910.10683&quot;&gt;《Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer》&lt;/a&gt;来自 Google，提出 T5 模型。&lt;/li&gt;
  &lt;li&gt;2020.01 &lt;a href=&quot;https://arxiv.org/abs/2001.08361&quot;&gt;《Scaling Laws for Neural Language Models》&lt;/a&gt;来自 OpenAI，提出 Scaling Laws。&lt;/li&gt;
  &lt;li&gt;2020.01 &lt;a href=&quot;https://arxiv.org/abs/2001.07676&quot;&gt;《Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference》&lt;/a&gt;来自慕尼黑大学，提出 PET。&lt;/li&gt;
  &lt;li&gt;2020.05 &lt;a href=&quot;https://arxiv.org/abs/2005.14165&quot;&gt;《Language Models are Few-Shot Learners》&lt;/a&gt;来自 OpenAI，提出 GPT-3。&lt;/li&gt;
  &lt;li&gt;2021.04 &lt;a href=&quot;https://arxiv.org/abs/2104.08691&quot;&gt;《The Power of Scale for Parameter-Efficient Prompt Tuning》&lt;/a&gt;来自 Google，提出 Prompt Tuning。&lt;/li&gt;
  &lt;li&gt;2022.01 &lt;a href=&quot;https://arxiv.org/abs/2201.11903&quot;&gt;《Chain-of-Thought Prompting Elicits Reasoning in Large Language Models》&lt;/a&gt; 来自 Google，提出 CoT。&lt;/li&gt;
  &lt;li&gt;2022.03 &lt;a href=&quot;https://arxiv.org/abs/2203.02155&quot;&gt;《Training language models to follow instructions with human feedback》&lt;/a&gt;来自 OpenAI，提出 InstructGPT。&lt;/li&gt;
  &lt;li&gt;2022.05 &lt;a href=&quot;https://arxiv.org/abs/2205.11916&quot;&gt;《Large Language Models are Zero-Shot Reasoners》&lt;/a&gt;来自东京大学和 Google，提出著名提示语「Let’s think step by step」。&lt;/li&gt;
  &lt;li&gt;2022.12 &lt;a href=&quot;https://arxiv.org/abs/2212.08073&quot;&gt;《Constitutional AI: Harmlessness from AI Feedback》&lt;/a&gt;来自 Anthropic，提出 Constitutional AI。&lt;/li&gt;
  &lt;li&gt;2023.02 &lt;a href=&quot;https://arxiv.org/abs/2302.13971&quot;&gt;《LLaMA: Open and Efficient Foundation Language Models》&lt;/a&gt;来自 Meta，提出 LLaMA。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;212值得关注的重要机构&quot;&gt;21.2、值得关注的重要机构&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;OpenAI：2018 年推出 GPT-1，2019 年推出 GPT-2，2020 年推出 GPT-3，2021 年推出 Codex（用户侧产品是 GitHub CoPilot），2022 年初推出 InstructGPT，2022 年底推出 ChatGPT 用户产品，2023 年 3 月开放 ChatGPT API。&lt;/li&gt;
  &lt;li&gt;Microsoft：目前已成为 OpenAI 背后的大金主，对其各模型都有开放访问权限并向 Microsoft 既有产品线整合。尽管如此，Microsoft 和 NVidia 一起联合推出过 Megatron-Turing。&lt;/li&gt;
  &lt;li&gt;Google 和 DeepMind：2017 年提出 Transformer，2018 年推出 BERT，2019 年提出 T5，2021 年推出 FLAN、LaMADA，2022 年推出 Chinchilla、PaLM、Sparrow。&lt;/li&gt;
  &lt;li&gt;Facebook（aka Meta）：2019 年推出 RoBERTa，2020 年推出 BART，2022 年推出 OPT、BlenderBot3、Galactica。&lt;/li&gt;
  &lt;li&gt;Allen Institute for AI（AI2）：其提出了著名的 ELMo 模型；&lt;/li&gt;
  &lt;li&gt;阿里：2021 年推出 1000 亿参数的 M6；&lt;/li&gt;
  &lt;li&gt;华为：2021 年推出 2000 亿参数的 Pangu-alpha；&lt;/li&gt;
  &lt;li&gt;百度：2021 年推出 2600 亿参数的 Ernie 3.0 Titan；&lt;/li&gt;
  &lt;li&gt;Salesforce：推出 CodeT5、CodeRL；&lt;/li&gt;
  &lt;li&gt;BigScience Research Workshop；2022 年推出开源的 BLOOM。&lt;/li&gt;
  &lt;li&gt;EleutherAI：推出 GPT-neox 20B；&lt;/li&gt;
  &lt;li&gt;Character.AI；&lt;/li&gt;
  &lt;li&gt;Anthropic：OpenAI 早期员工离职创办，其 Claude AI 暂未开放访问。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;213推荐关注的博客或-twitter&quot;&gt;21.3、推荐关注的博客或 Twitter&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://openai.com/blog/&quot;&gt;OpenAI 公司官网的 blog&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/blog&quot;&gt;Hugging Face 公司官网的 blog&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://jalammar.github.io/&quot;&gt;Jay Alammar 的 blog&lt;/a&gt;，写了一系列非常棒的 Illustrated Models 文章&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://bmk.sh/&quot;&gt;Leo Gao&lt;/a&gt;，博主是 OpenAI 的 Alignment Researcher&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://twitter.com/sama&quot;&gt;@sama&lt;/a&gt;，Sam Altman，OpenAI 公司联合创始人 &amp;amp; CEO&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://twitter.com/gdb&quot;&gt;@gdb&lt;/a&gt;，Greg Brockman，OpenAI 公司联合创始人 &amp;amp; 总裁&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://twitter.com/ylecun&quot;&gt;@ylecun&lt;/a&gt;，Yann LeCun，Meta 首席 AI 科学家&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://twitter.com/karpathy&quot;&gt;@karpathy&lt;/a&gt;，Karpathy，特斯拉前人工智能总监，李飞飞在 Stanford 的助教&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://twitter.com/danqi_chen&quot;&gt;@danqi_chen&lt;/a&gt;，康奈尔大学计算机科学助理教授&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://twitter.com/sewon__min&quot;&gt;@sewon__min&lt;/a&gt;，华盛顿大学计算机科学与工程 Ph.D. 学生&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://twitter.com/Zhou_Yu_AI&quot;&gt;@Zhou_Yu_AI&lt;/a&gt;，哥伦比亚大学计算机科学助理教授&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://twitter.com/nabla_theta&quot;&gt;@nabla_theta&lt;/a&gt;，Leo Gao，OpenAI 公司 Alignment Researcher&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://twitter.com/sangmichaelxie&quot;&gt;@sangmichaelxie&lt;/a&gt;，斯坦福大学 Ph.D. 学生&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://twitter.com/alecrad&quot;&gt;@AlecRad&lt;/a&gt;，Alec Radford，OpenAI 研究员，GPT-1 / GPT-2 / GPT-3 的作者之一&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.zhihu.com/people/liu-peng-fei-65-23&quot;&gt;刘鹏飞&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.zhihu.com/people/zhang-jun-lin-76&quot;&gt;张俊林&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;mark&quot;&gt;Mark&lt;/h2&gt;

&lt;p&gt;本文转载至阿里集团内部技术社区 ATA 头条推荐。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023/2023-01-23-captain-aigc-2-llm-73.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;本篇其他参考&quot;&gt;本篇其他参考&lt;/h2&gt;

&lt;p&gt;各章节内有单独的参考文献列表，未在其中标注出的，罗列如下：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.mikecaptain.com/2023/01/22/captain-aigc-1-transformer/&quot;&gt;人工智能 LLM 革命前夜：一文读懂横扫自然语言处理的 Transformer 模型&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.mikecaptain.com/2023/03/02/chatgpt-api/&quot;&gt;AI 应用第一次大爆发来了：一文入门 ChatGPT 官方 API 文档解读&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.mikecaptain.com/2023/02/25/meta-llama/&quot;&gt;Meta 推出开源 LLaMA，用 1/10 参数规模打败 GPT-3，群”模”乱舞的 2023 拉开序幕&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.mikecaptain.com/2023/01/23/openai-official-doc/&quot;&gt;麦克船长的 OpenAI 模型 API 官方文档入门解读&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.mikecaptain.com/2022/12/17/ai-bert-1/&quot;&gt;你可能已经听说 GPT-3，但是你也不能不知道 BERT —— 跟我一起用 BERT 跑个小用例&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://openai.com/blog/chatgpt/&quot;&gt;ChatGPT: Optimizing Language Models for Dialogue&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://openai.com/blog/instruction-following/&quot;&gt;Aligning Language Models to Follow Instructions&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://openai.com/alignment/&quot;&gt;Aligning AI systems with human intent&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2203.02155&quot;&gt;Training language models to follow instructions with human feedback&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;《Artificial Intelligence: A Modern Approach, 4th edition》&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2301.04655&quot;&gt;ChatGPT is not all you need. A State of the Art Review of large Generative AI models&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://ai.stanford.edu/blog/understanding-incontext/&quot;&gt;How does in-context learning work? A framework for understanding the differences from traditional supervised learning, &lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;https://mp.weixin.qq.com/s/h9c3w1af2uBYLlVbw7j82A&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://openai.com/blog/openai-baselines-ppo/&quot;&gt;Proximal Policy Optimization&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://platform.openai.com/docs/model-index-for-researchers&quot;&gt;Model index for researchers&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/blog/rlhf&quot;&gt;Lambert, et al., “Illustrating Reinforcement Learning from Human Feedback (RLHF)”, Hugging Face Blog, 2022.&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2005.14165&quot;&gt;Language Models are Few-Shot Learners&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;《思考，快与慢》丹尼尔 · 卡尼曼&lt;/li&gt;
  &lt;li&gt;https://www.lesswrong.com/posts/midXmMb2Xg37F2Kgn/new-scaling-laws-for-large-language-models&lt;/li&gt;
  &lt;li&gt;https://arxiv.org/abs/1910.01108&lt;/li&gt;
  &lt;li&gt;https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/&lt;/li&gt;
  &lt;li&gt;https://bmk.sh/2020/05/29/GPT-3-A-Brief-Summary/&lt;/li&gt;
  &lt;li&gt;https://zhuanlan.zhihu.com/p/593962400&lt;/li&gt;
  &lt;li&gt;https://xv44586.github.io/2023/01/09/zero-to-chatgpt/&lt;/li&gt;
  &lt;li&gt;https://www.lesswrong.com/posts/midXmMb2Xg37F2Kgn/new-scaling-laws-for-large-language-models&lt;/li&gt;
  &lt;li&gt;https://arxiv.org/abs/2203.15556&lt;/li&gt;
  &lt;li&gt;https://spinningup.openai.com/en/latest/algorithms/ppo.html&lt;/li&gt;
  &lt;li&gt;https://mp.weixin.qq.com/s/h9c3w1af2uBYLlVbw7j82A&lt;/li&gt;
  &lt;li&gt;https://zhuanlan.zhihu.com/p/527423190&lt;/li&gt;
  &lt;li&gt;https://arxiv.org/abs/1706.03762&lt;/li&gt;
  &lt;li&gt;https://arxiv.org/abs/2112.0086&lt;/li&gt;
  &lt;li&gt;https://zhuanlan.zhihu.com/p/493533589&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.cs.princeton.edu/courses/archive/fall22/cos597G/&quot;&gt;COS 597G (Fall 2022): Understanding Large Language Models&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;https://zhuanlan.zhihu.com/p/395115779&lt;/li&gt;
  &lt;li&gt;http://pretrain.nlpedia.ai&lt;/li&gt;
  &lt;li&gt;https://lifearchitect.ai/whats-in-my-ai/&lt;/li&gt;
  &lt;li&gt;https://zhuanlan.zhihu.com/p/611403556&lt;/li&gt;
&lt;/ol&gt;</content><author><name>麦克船长</name></author><category term="ai" /><category term="AI" /><category term="人工智能" /><category term="NLP" /><category term="自然语言处理" /><category term="神经网络" /><category term="LLM" /><category term="大型语言模型" /><category term="语言模型" /><category term="大模型" /><summary type="html">人类创造城市，又把它虚拟化；创造机器，又希望机器习得创造。人工智能似乎要来到觉醒的那一刻了。过去数十年，数据科学家研发出了一个又一个 AI 模型，似乎总在重复造轮子。能否通过更便捷有效的方式，真正让机器去统一吸收文本、语音、图片等不同类型的数据和知识并创造出不同的内容呢？多模态预训练大模型打通了模态之间的关系，模型逐步标准化。下一步，则是建立统一的、跨场景、多任务的多模态基础模型。机器从未像今天这般吸收、理解、创造，一个崭新的 AI 大时代即将开启。生成式 AI 显然是机器中的佼佼者。不久前爆火的 ChatGPT，学会了从人类的反馈中强化学习，能你来我往多轮聊天。会对话，也会作画，是基于 diffusion 模型的 AI 绘画，，从最初的拼贴已经进化到三维动画、建模贴图的 next level，甚至 AI 还会自我编码，在 codeforces 举办的编程竞赛中 AlphaCode 超过了 47% 的人类工程师。未来三年，生成式 AI 不但会有越来越多的产品，它的内容创造能力将达到人类水平。</summary></entry><entry><title type="html">AI 应用第一次大爆发来了：一文入门 ChatGPT 官方 API 文档解读</title><link href="https://www.mikecaptain.com/2023/03/02/chatgpt-api/" rel="alternate" type="text/html" title="AI 应用第一次大爆发来了：一文入门 ChatGPT 官方 API 文档解读" /><published>2023-03-02T00:54:13+00:00</published><updated>2023-03-02T00:54:13+00:00</updated><id>https://www.mikecaptain.com/2023/03/02/chatgpt-api</id><content type="html" xml:base="https://www.mikecaptain.com/2023/03/02/chatgpt-api/">&lt;p&gt;此前 GPT-3 也早已开放 API，我在麦克船长的博客 MikeCaptain.com 中已介绍过，当时在 NLP 方面能使用的 API 主要是 GPT-3，这是&lt;a href=&quot;http://www.mikecaptain.com/2023/01/24/openai-official-doc/&quot;&gt;原文链接&lt;/a&gt;。此前 OpenAI API 就已经有了不同功能和价位的多种模型，还提供了在基础模型上的 fine-tune 服务（当然 fine-tune 本身收费，且 fine-tune 后的模型调用费用比 base model 要贵得多）。本次更新，主要是增加了 GPT-3.5（用于 NLP）和 Whisper（用于 audio to text）的 API：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023/03/captain-chatgpt-api-1.png&quot; alt=&quot;&quot; width=&quot;720&quot; /&gt;&lt;/p&gt;

&lt;p&gt;ChatGPT 和 Whisper 模型现已在 OpenAI 的 API 上可用，通过一系列系统范围的优化，自去年 12 月以来，OpenAI 已将 ChatGPT 的成本降低了 90%；不仅仅是 GPT-3.5，开发人员现在可以在 API 中使用 OpenAI 的开源 Whisper large-v2 模型。&lt;/p&gt;

&lt;h2 id=&quot;一这次开放-chatgpt-和-whisper-的-api-都有什么基本看点&quot;&gt;一、这次开放 ChatGPT 和 Whisper 的 API 都有什么基本看点&lt;/h2&gt;

&lt;h3 id=&quot;1openai-开放模型库里新增-gpt-35-模型&quot;&gt;1、OpenAI 开放模型库里新增 GPT-3.5 模型&lt;/h3&gt;

&lt;p&gt;GPT-3.5 模型可以理解并生成自然语言或代码，其最强大的是 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gpt-3.5-turbo&lt;/code&gt;，这可是之前的库里没有的。它针对聊天进行了优化，但也适用于传统的完成任务。GPT 开放模型里原本只有 GPT-3 系列，包括如下系列模型：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023/03/captain-chatgpt-api-3.png&quot; alt=&quot;&quot; width=&quot;720&quot; /&gt;&lt;/p&gt;

&lt;p&gt;而这次更新后，增加了 GPT-3.5 系列，包括 5 款模型：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023/03/captain-chatgpt-api-2.png&quot; alt=&quot;&quot; width=&quot;720&quot; /&gt;&lt;/p&gt;

&lt;p&gt;OpenAI 建议在试验时使用 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gpt-3.5-turbo&lt;/code&gt;，因为它会产生最佳结果。一旦跑起来都一切正常，可以尝试其他模型，看看是否能以更低的延迟或成本获得相同的结果，这才更划算。&lt;/p&gt;

&lt;p&gt;可能此前的 GPT-3 期间开放的 API 大部分人也还没来得及了解，下面补充一些基本概念。&lt;/p&gt;

&lt;h3 id=&quot;2openai-api-的一些基础概念&quot;&gt;2、OpenAI API 的一些基础概念&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;关于 prompt 和 completion&lt;/strong&gt;：OpenAI 提到一个理念：「设计提示语，就相当于在用一些指令和少量例子给模型编程」。另外 OpenAI 还强调了在目标任务上的区别，就是 OpenAI 的 NLP 模型与其他 NLP 模型很大的一个区别是，它不是设计用来解决单一类型任务的，而是可以解决几乎各种类型的 NLP 任务，包括但不限于文本生成（content generation）、代码生成（code generation）、总结（summarization）、扩写（expansion）、对话（conversation）、创意写作（creative wrting）、风格转换（style transfer）等。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;关于 token&lt;/strong&gt;：我们理解和处理文本，是把文本先打碎成 token。以英文文本为例，token 可以是单词，也可以词根（一些字母组合），比如单词「hamburger」可能会被打碎成「ham」、「bur」、「ger」这几个 tokens。再比如「pear」这个单词，可能就会单独作为一个 token 不再打碎了。还有些 token 可能会以「空格」开头，比如「 hello」、「 bye」。一个大概的经验是，通常英文文本里 1 token 有 4 个字母或者 0.75 个单词。使用时的一个限制是，最好你的提示（prompt）或生成内容，不要超过 2048 个 tokens，大概相当于 1500 个单词。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;关于 model&lt;/strong&gt;：目前 OpenAI 有基于 GPT-3.5 的基础模型 Turbo 和这些基于 GPT-3 的基础模型 Davinci、Curie、Babbage、Ada 开放 API，另外 Codex 系列是 GPT-3 的后代，是用「自然语言 + 代码」训练的。&lt;/p&gt;

&lt;p&gt;虽然新的 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gpt-3.5-turbo&lt;/code&gt; 模型针对聊天进行了优化，但它也非常适合传统的完成任务。原始 GPT-3.5 模型针对原来就开放的 API 文本补全（text completion）也进行了优化。我们用于 create embeddings 和 edit text 的 API 都使用了它们各自针对性的模型。&lt;/p&gt;

&lt;h3 id=&quot;3模型&quot;&gt;3、模型&lt;/h3&gt;

&lt;p&gt;OpenAI 今天发布的 ChatGPT 模型系列 GPT-3.5-turbo 与 ChatGPT 产品中使用的模型相同。它的价格为每 1k tokens 0.002 美元，比 OpenAI 现有的 GPT-3 模型便宜 10 倍。即使对于许多非聊天用例，它也是 OpenAI 的最佳模型 —— 如果你之前就用了 text-davinci-003，那么迁移到 gpt-3.5-turbo 时只需要对他们的提示进行少量调整。&lt;/p&gt;

&lt;h4 id=&quot;31turbo&quot;&gt;3.1、Turbo&lt;/h4&gt;

&lt;p&gt;Turbo 与支持 ChatGPT 的模型系列相同，它针对对话式聊天输入和输出进行了优化，但与 Davinci 模型系列相比，它在完成方面同样出色。在 ChatGPT 中可以很好地完成的任何用例都应该在 API 中与 Turbo 模型系列一起很好地执行。Turbo 模型家族也是第一个像 ChatGPT 一样接收定期模型更新的模型。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;适合：对话、文本生成&lt;/strong&gt;。&lt;/p&gt;

&lt;h4 id=&quot;32davinci&quot;&gt;3.2、Davinci&lt;/h4&gt;

&lt;p&gt;Davinci 在 GPT-3 中就已经存在，是最有能力的模型系列，可以执行其他模型（Ada、Curie 和 Babbage）可以执行的任何任务，而且通常只需要很少的 instruction。 对于需要对内容有大量理解的应用程序，例如针对特定受众的 summarization 和创意内容生成，Davinci 将产生最佳结果。 这些增加的功能需要更多的计算资源，因此 Davinci 每次 API 调用更贵，并且不如其他模型那么快。&lt;/p&gt;

&lt;p&gt;Davinci 的另一个亮点是理解文本的意图。Davinci 擅长解决多种逻辑问题和解释人物的动机。 达芬奇已经能够解决一些涉及因果关系的最具挑战性的人工智能问题。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;适合：复杂的意图、因果分析、Summarization for Audience&lt;/strong&gt;。&lt;/p&gt;

&lt;h4 id=&quot;33curie&quot;&gt;3.3、Curie&lt;/h4&gt;

&lt;p&gt;Curie 在 GPT-3 中就已经存在，也非常强大，速度也比较快。 虽然 Davinci 在分析复杂文本方面更强大，但 Curie 能够胜任许多细微的任务，例如情感分类和摘要。 Curie 还非常擅长回答问题和执行问答以及作为通用服务聊天机器人。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;适合：机器翻译、复杂分类任务、情感分析、Summarization&lt;/strong&gt;。&lt;/p&gt;

&lt;h4 id=&quot;34babbage&quot;&gt;3.4、Babbage&lt;/h4&gt;

&lt;p&gt;同样是 GPT-3 里就存在的。Babbage 可以执行简单的任务，例如简单的分类。在语义搜索方面，它也非常有能力对文档与搜索查询的匹配程度进行排名。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;适合：适度分类、语义搜索分类&lt;/strong&gt;。&lt;/p&gt;

&lt;h4 id=&quot;35ada&quot;&gt;3.5、Ada&lt;/h4&gt;

&lt;p&gt;也是 GPT-3 时期就有的。Ada 通常是最快的模型，可以执行解析文本、地址更正和不需要太多细微差别的某些分类任务等任务。 Ada 的性能通常可以通过提供更多上下文来提高。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;适合：解析文本、简单分类、地址修正、关键词&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;需要注意的是：由像 Ada 这样更快的模型执行的任何任务都可以由像 Curie 或 Davinci 这样更强大的模型执行。&lt;/p&gt;

&lt;h3 id=&quot;4具体模型按-gpt-35-还是-gpt-3-划分看下&quot;&gt;4、具体模型：按 GPT-3.5 还是 GPT-3 划分看下&lt;/h3&gt;

&lt;h4 id=&quot;41关于-gpt-35&quot;&gt;4.1、关于 GPT-3.5&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gpt-3.5-turbo&lt;/code&gt;：功能最强大的 GPT-3.5 模型并针对聊天进行了优化，成本仅为 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;text-davinci-003&lt;/code&gt; 的 1/10。将使用我们最新的模型迭代进行更新。&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gpt-3.5-turbo-0301&lt;/code&gt;：2023 年 3 月 1 日的 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gpt-3.5-turbo&lt;/code&gt; 快照。与 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gpt-3.5-turbo&lt;/code&gt; 不同，此模型不会收到更新，并且仅在 2023 年 6 月 1 日结束的三个月内提供支持。&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;text-davinci-003&lt;/code&gt;：此前 GPT-3 期间就有的 Davinci 模型，这次直接升级到了 GPT-3.5，与 GPT-3 阶段一样，仍然是最大请求 4000 tokens，同样训练数据 up to 2021 年 6 月，能做几乎所有 NLP 任务。&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;text-davinci-002&lt;/code&gt;：与 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;text-davinci-003&lt;/code&gt; 类似情况，这次直接升级到了 GPT-3.5，专门为了代码生成任务优化的模型。与 GPT-3 阶段一样，仍然是最大请求 4000 tokens；训练数据也没有变，依然是 up to 2021 年 6 月。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;42关于-gpt-3&quot;&gt;4.2、关于 GPT-3&lt;/h4&gt;

&lt;p&gt;以下模型不再由 GPT-3 提供支持：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;text-davinci-003&lt;/code&gt;：此前在 GPT-3 接口中存在，本次更新后由 GPT-3.5 支持提供了，见上一段。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;GPT-3 目前可用的模型包括下面这些，与 InstructGPT 背后的模型是一样的，它们的最大请求都是 2048 tokens，训练数据也都是 up to 2019 年 10 月：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;text-curie-001&lt;/code&gt;：比 davinci 要弱一点，但是速度更快、更便宜。&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;text-babbage-001&lt;/code&gt;：一些比较直接的任务（straightforward tasks），比 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;text-curie-001&lt;/code&gt; 更快、更便宜。&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;text-ada-001&lt;/code&gt;：一些非常简单的任务，这些模型里最快、最便宜的。&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;davinci&lt;/code&gt;：目前最强的 GPT-3 模型，任何其他模型能做的任务，&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;davinci&lt;/code&gt; 都可以做。&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;curie&lt;/code&gt;：就是 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;text-curie-001&lt;/code&gt;。&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;babbage&lt;/code&gt;：就是 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;text-babbage-001&lt;/code&gt;。&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ada&lt;/code&gt;：就是 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;text-ada-001&lt;/code&gt;。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;5定价&quot;&gt;5、定价&lt;/h3&gt;

&lt;p&gt;这些模型根据输入的 token 数量做的如下定价，前四个是 GPT-3 阶段就存在的，也并未因本次 GPT-3.5 API 发布而调价：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;基础模型使用 0.000&lt;strong&gt;4&lt;/strong&gt; USD/1K tokens，Ada&lt;/li&gt;
  &lt;li&gt;基础模型使用 0.000&lt;strong&gt;5&lt;/strong&gt; USD/1K tokens，Babbage&lt;/li&gt;
  &lt;li&gt;基础模型使用 0.00&lt;strong&gt;20&lt;/strong&gt; USD/1K tokens，Curie&lt;/li&gt;
  &lt;li&gt;基础模型使用 0.0&lt;strong&gt;200&lt;/strong&gt; USD/1K tokens，Davinci&lt;/li&gt;
  &lt;li&gt;基础模型使用 0.00&lt;strong&gt;20&lt;/strong&gt; USD/1K tokens，Turbo，支持 Chat API&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;从定价上看，Ada 和 Babbage 基本没有差多少。另外命名上，可以看出 OpenAI 有意地给他们取了 ABCD 开头的名字。另外你也可以 finetune 你自己的模型，对于 fine-tuned models 如下收费，并未因本次发布而调价：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Finetune 训练费 0.000&lt;strong&gt;4&lt;/strong&gt; USD/1K tokens，使用费 0.00&lt;strong&gt;16&lt;/strong&gt; USD/1K tokens，Ada&lt;/li&gt;
  &lt;li&gt;Finetune 训练费 0.000&lt;strong&gt;6&lt;/strong&gt; USD/1K tokens，使用费 0.00&lt;strong&gt;24&lt;/strong&gt; USD/1K tokens，Babbage&lt;/li&gt;
  &lt;li&gt;Finetune 训练费 0.00&lt;strong&gt;30&lt;/strong&gt; USD/1K tokens，使用费 0.0&lt;strong&gt;120&lt;/strong&gt; USD/1K tokens，Curie&lt;/li&gt;
  &lt;li&gt;Finetune 训练费 0.0&lt;strong&gt;300&lt;/strong&gt; USD/1K tokens，使用费 0.&lt;strong&gt;1200&lt;/strong&gt; USD/1K tokens，Davinci&lt;/li&gt;
  &lt;li&gt;暂未提供 Turbo 的 finetune。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;GPT-3.5 的效果，大家应该都在 ChatGPT 上试用过了，这里我也在放一下链接可能有一些 late massive 朋友还没有用过：&lt;a href=&quot;https://chat.openai.com/chat&quot;&gt;https://chat.openai.com/chat&lt;/a&gt; 。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023/03/captain-chatgpt-api-4.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;在 OpenAI 的基于 GPT-3 的 PlayGround 你可以试试：&lt;a href=&quot;https://platform.openai.com/playground/p/default-chat&quot;&gt;https://platform.openai.com/playground/p/default-chat&lt;/a&gt; 。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023/2023-01-24-openai-api.jpg&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;6专用实例&quot;&gt;6、专用实例&lt;/h3&gt;

&lt;p&gt;说的就是之前爆出消息的 Foundry，也都是跑在微软的 Azure 上的。OpenAI 现在还为希望更深入控制特定型号版本和系统性能的用户提供专用实例。 默认情况下，开放的 API 都是在所有客户共享的基础设施上的模型中跑的，如果有额外需求得额外付费。
开发人员可以完全控制实例的负载（更高的负载会提高吞吐量，当然这也会让对每个 request 的处理慢下来）、启用更长 context 限制等功能选项，以及保存模型快照的能力。&lt;/p&gt;

&lt;p&gt;对于每天要跑 4.5 亿以上 tokens 的开发者来说，用专用实例更划算。并且针对专用实例，开发者可以自己决定用什么硬件配置，这个就属于 OpenAI 的中大客户销售策略了，具体可以看 &lt;a href=&quot;https://openai.com/contact-sales/&quot;&gt;OpenAI 官网的销售联系页面&lt;/a&gt;。&lt;/p&gt;

&lt;h3 id=&quot;7whisper每分钟-06-美分-的语音识别翻译服务-api&quot;&gt;7、Whisper：每分钟 0.6 美分 的语音识别/翻译服务 API&lt;/h3&gt;

&lt;p&gt;Whisper 是 OpenAI 于 2022 年 9 月开源的语音转文本模型，开发者社区反馈还不错，但也挺麻烦的。今天开始，OpenAI 现在已经通过 OpenAI 的 API 提供了 large-v2 模型，它提供了方便的按需访问，价格为 &lt;strong&gt;0.006 美元/分钟&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;Whisper API 可通过 OpenAI 的 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;transcriptions&lt;/code&gt;（以源语言转录）或 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;translations&lt;/code&gt;（转录成英语）端点使用，并接受多种格式（m4a、mp3、mp4、mpeg、mpga、wav、webm）。&lt;/p&gt;

&lt;h3 id=&quot;8开发者可能关注的几个问题&quot;&gt;8、开发者可能关注的几个问题&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Q1：之前的模型都提供了 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fine-tune&lt;/code&gt;，这次开放的 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gpt-3.5-turbo&lt;/code&gt; 可以 fine-tune 吗？&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;A1：还不行，到目前为止，你还只能 fine-tune GPT-3 模型。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Q2：OpenAI 会把开发者调用 API 时发送的数据存下来吗？&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;从 2023 年 3 月 1 日开始，OpenAI 会把开发者调用 API 时发送的数据保留 30 天，但 OpenAI 声称不会使用开发者通过 API 发送的数据来改进模型（Who knows）。具体的政策，可以从 OpenAI 官网了解更多，这里是他们的数据使用&lt;a href=&quot;https://platform.openai.com/docs/data-usage-policies&quot;&gt;政策链接&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;二主要-api-介绍及代码示例&quot;&gt;二、主要 API 介绍及代码示例&lt;/h2&gt;

&lt;p&gt;安装 OpenAI 的 python 库，参考 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;https://anaconda.org/conda-forge/openai&lt;/code&gt;：&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;mikecaptain@local &lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;conda &lt;span class=&quot;nb&quot;&gt;install&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-c&lt;/span&gt; conda-forge openai
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;在 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;https://platform.openai.com/account/api-keys&lt;/code&gt; 创建自己的 API。完成这两步后就可以编写代码尝试一下：&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;os&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;openai&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;openai&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;api_key&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;getenv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;OPENAI_API_KEY&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;openai&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;会打印出 OpenAI 的各个 models 的一些信息、权限等等。&lt;/p&gt;

&lt;h3 id=&quot;1一起看看-chat-的-api&quot;&gt;1、一起看看 Chat 的 API&lt;/h3&gt;

&lt;h4 id=&quot;11chat-的-rest-api&quot;&gt;1.1、Chat 的 REST API&lt;/h4&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;curl https://api.openai.com/v1/chat/completions
  &lt;span class=&quot;nt&quot;&gt;-H&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;Authorization: Bearer &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$OPENAI_API_KEY&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;-H&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;Content-Type: application/json&quot;&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;-d&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&apos;{
  &quot;model&quot;: &quot;gpt-3.5-turbo&quot;,
  &quot;messages&quot;: [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;What is the OpenAI mission?&quot;}]
}&apos;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;相应的返回结果：&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;s2&quot;&gt;&quot;id&quot;&lt;/span&gt;: &lt;span class=&quot;s2&quot;&gt;&quot;chatcmpl-6p5FEv1JHictSSnDZsGU4KvbuBsbu&quot;&lt;/span&gt;,
  &lt;span class=&quot;s2&quot;&gt;&quot;object&quot;&lt;/span&gt;: &lt;span class=&quot;s2&quot;&gt;&quot;messages&quot;&lt;/span&gt;,
  &lt;span class=&quot;s2&quot;&gt;&quot;created&quot;&lt;/span&gt;: 1677693600,
  &lt;span class=&quot;s2&quot;&gt;&quot;model&quot;&lt;/span&gt;: &lt;span class=&quot;s2&quot;&gt;&quot;gpt-3.5-turbo&quot;&lt;/span&gt;,
  &lt;span class=&quot;s2&quot;&gt;&quot;choices&quot;&lt;/span&gt;: &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;
    &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
      &lt;span class=&quot;s2&quot;&gt;&quot;index&quot;&lt;/span&gt;: 0,
      &lt;span class=&quot;s2&quot;&gt;&quot;finish_reason&quot;&lt;/span&gt;: &lt;span class=&quot;s2&quot;&gt;&quot;stop&quot;&lt;/span&gt;,
      &lt;span class=&quot;s2&quot;&gt;&quot;messages&quot;&lt;/span&gt;: &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;
        &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
          &lt;span class=&quot;s2&quot;&gt;&quot;role&quot;&lt;/span&gt;: &lt;span class=&quot;s2&quot;&gt;&quot;assistant&quot;&lt;/span&gt;,
          &lt;span class=&quot;s2&quot;&gt;&quot;content&quot;&lt;/span&gt;: &lt;span class=&quot;s2&quot;&gt;&quot;OpenAI&apos;s mission is to ensure that artificial general intelligence benefits all of humanity.&quot;&lt;/span&gt;
        &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
  &lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;,
  &lt;span class=&quot;s2&quot;&gt;&quot;usage&quot;&lt;/span&gt;: &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;s2&quot;&gt;&quot;prompt_tokens&quot;&lt;/span&gt;: 20,
    &lt;span class=&quot;s2&quot;&gt;&quot;completion_tokens&quot;&lt;/span&gt;: 18,
    &lt;span class=&quot;s2&quot;&gt;&quot;total_tokens&quot;&lt;/span&gt;: 38
  &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;12chat-的-python-api&quot;&gt;1.2、Chat 的 Python API&lt;/h4&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;openai&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;completion&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;openai&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ChatCompletion&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;create&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;gpt-3.5-turbo&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
  &lt;span class=&quot;n&quot;&gt;messages&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;role&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;user&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;content&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Tell the world about the ChatGPT API in the style of a pirate.&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}]&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;completion&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;2whisper-api&quot;&gt;2、Whisper API&lt;/h3&gt;

&lt;h3 id=&quot;21whisper-的-rest-api&quot;&gt;2.1、Whisper 的 REST API&lt;/h3&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;curl https://api.openai.com/v1/audio/transcriptions &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;-H&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;Authorization: Bearer &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$OPENAI_API_KEY&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;-H&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;Content-Type: multipart/form-data&quot;&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;-F&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;whisper-1&quot;&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;-F&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;file&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;@/path/to/file/openai.mp3&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;s2&quot;&gt;&quot;text&quot;&lt;/span&gt;: &lt;span class=&quot;s2&quot;&gt;&quot;Imagine the wildest idea that you&apos;ve ever had, and you&apos;re curious about how it might scale to something that&apos;s a 100, a 1,000 times bigger...&quot;&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;22whisper-的-python-api&quot;&gt;2.2、Whisper 的 Python API&lt;/h4&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;openai&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;file&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;/path/to/file/openai.mp3&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;rb&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;transcription&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;openai&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Audio&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transcribe&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;whisper-1&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transcription&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;3text-completion-任务&quot;&gt;3、Text Completion 任务&lt;/h3&gt;

&lt;p&gt;下面这个例子会简单调用一下 completion，并打印出结果，用了一句需要你自己编写的 prompt：&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;os&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;openai&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;openai&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;api_key&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;getenv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;OPENAI_API_KEY&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;text_prompt&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;In a shocking turn of events, scientists have discovered that &quot;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;completion&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;openai&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Completion&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;create&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;text-davinci-002&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;prompt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text_prompt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;max_tokens&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;stop&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;temperature&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;generated_text&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;completion&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;choices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;generated_text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;这里用到了最重要的 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;openai.Completion&lt;/code&gt;，其 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;create&lt;/code&gt; 函数的参数解释如下：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;model&lt;/code&gt;：之前 OpenAI 把它叫「engine」，后来给 deprecated 了，现在都是用 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;model&lt;/code&gt;，所有的可用 models 可以通过 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;open.Model.list()&lt;/code&gt; 来查看。&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;prompt&lt;/code&gt;：&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;string&lt;/code&gt; 类型，就是输入数据。&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;suffix&lt;/code&gt;：&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;string&lt;/code&gt; 类型，生成文本的结束符。&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;max_tokens&lt;/code&gt;：&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;integer&lt;/code&gt; 类型，生成文本的最大 tokens 数。&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;n&lt;/code&gt;：&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;integer&lt;/code&gt; 类型，表示你要产生几个不同的输出结果。比如设置 3 就会得到 3 个不同的结果，以便您可以从中选择最合适的一个。&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;stop&lt;/code&gt;：&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;string&lt;/code&gt; 类型，用于指定模型何时应该停止生成文本。当模型在生成的文本中遇到 stop 字符串时，它将停止生成文本。ChatGPT 推出后迭代过一版增加了「stop generating」就是用的这个参数。&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;temperature&lt;/code&gt;：&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;number&lt;/code&gt; 类型，这是 NLP 模型里常见的一个超参数。这个参数，来自于统计热力学的概念，温度越高表示系统的熵越高、混乱度越高、随机性越强，这里的 temperature 也是值越高输出结果的随机性也越高。这样如果 temperature 设置得很低，生成的结果可能更正确，但没有多少创造性和随机性。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;4text-edit-任务&quot;&gt;4、Text Edit 任务&lt;/h3&gt;

&lt;p&gt;Completion 类任务，通俗点理解的话，完形填空、句子补齐、写作文、翻译 …… 都算 Completion，就是无中生有。而对于已经有的内容，做修改，就是 OpenAI 的 API 里的「Edit」类的任务了。&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;os&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;openai&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;openai&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;api_key&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;getenv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;OPENAI_API_KEY&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;openai&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Edit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;create&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;text-davinci-edit-001&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;The qick brown fox jumps over the layz dog.&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;instruction&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Fix the spelling mistakes&quot;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;调用 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;openai.Edit.create&lt;/code&gt;，用 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;text-davinci-edit-001&lt;/code&gt; 模型，输入一句有拼写错误的英文「The qick brown fox jumps over the layz dog.」，并提供一句指令 instruction「Fix the spelling mistakes」。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;instruction&lt;/code&gt;：要告诉模型如何修改，&lt;strong&gt;其实这句话就是新时代的「programming」了&lt;/strong&gt;。&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;temperature&lt;/code&gt;：默认是 0，对于纠正拼写类的任务，我们用默认 0 就可以了，不需要什么创造性和随机性。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;5顺便提一下基于-dalle-的-image-生成&quot;&gt;5、顺便提一下基于 DALL·E 的 Image 生成&lt;/h3&gt;

&lt;p&gt;这里也提一下 text2image 的 API，这与文本生成用的 GPT 是不同的，基于 DALL·E 的模型，但是很多开发者可能也是文本、图像、对话混合使用的，所以这里也提一下。&lt;/p&gt;

&lt;h4 id=&quot;51image-create-任务beta&quot;&gt;5.1、Image Create 任务（Beta）&lt;/h4&gt;

&lt;p&gt;截止 2023 年年初 1 月份，这个 API 还是 beta，我们看个例子：&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;os&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;openai&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;openai&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;api_key&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;getenv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;OPENAI_API_KEY&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;openai&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;create&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;prompt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;A cute baby sea otter&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;1024x1024&quot;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;这也是一个 OpenAI 官网的例子。大家可能看到这里，船长没有指定 model，但是可以想到一定用的是 DALL·E，因为它没有像 GPT-3 一样提供很多版本的选择，所以就不需要传参数了。这个程序就是生成一个 1024x1024 的图片。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;prompt&lt;/code&gt;：就是输入的提示语，返回的数据里，会告诉你生成的图片的 URL.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;n&lt;/code&gt;：是图片结果数量，最多 10，默认 1.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;52image-edit-任务&quot;&gt;5.2、Image Edit 任务&lt;/h4&gt;

&lt;p&gt;给定一个图片，OpenAI 也可以来修改指定区域：&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;os&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;openai&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;openai&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;api_key&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;getenv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;OPENAI_API_KEY&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;openai&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;create_edit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;otter.png&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;rb&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;mask&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;mask.png&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;rb&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;prompt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;A cute baby sea otter wearing a beret&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;1024x1024&quot;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;image&lt;/code&gt;：这里对输入图片有要求，必须是正方形的！另外不能超过 4MB，还得是 PNG。&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mask&lt;/code&gt;：还可以提供掩码图片（叫什么比较合适，掩图？哈哈）。如果不提供的话，&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;image&lt;/code&gt; 里就必须有透明的部分（必须全透明，即 alpha = 0），那个透明部分就是被用来 Edit 的。如果有 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mask&lt;/code&gt; 则透明部分用来做「掩图」来改 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;image&lt;/code&gt;。&lt;/li&gt;
  &lt;li&gt;同样地，结果图片的 URL 会返回给你。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;6code-生成&quot;&gt;6、Code 生成&lt;/h3&gt;

&lt;p&gt;这里也提一下 Code 生成，也是此前就有的能力，用的是 Codex 的模型。有个沙河可以感受下 JS 代码生成，你可以在这里试用一下：&lt;a href=&quot;https://platform.openai.com/codex-javascript-sandbox&quot;&gt;https://platform.openai.com/codex-javascript-sandbox&lt;/a&gt; 。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023/03/captain-chatgpt-api-5.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;7审查moderation&quot;&gt;7、审查（Moderation）&lt;/h3&gt;

&lt;p&gt;也不是随本次 GPT-3.5 发布的，我们也大致浏览一下。Moderation 用来审查内容是否符合 OpenAI 的内容政策，快速使用的方式如下：&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;response&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;openai&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Moderation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;create&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Sample text goes here&quot;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;output&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;results&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;API 官网给出我们如下的返回结果示例：&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;&quot;id&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;modr-XXXXX&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;&quot;model&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;text-moderation-001&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;&quot;results&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
      &lt;span class=&quot;s&quot;&gt;&quot;categories&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&quot;hate&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;false&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&quot;hate/threatening&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;false&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&quot;self-harm&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;false&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&quot;sexual&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;false&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&quot;sexual/minors&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;false&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&quot;violence&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;false&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&quot;violence/graphic&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;false&lt;/span&gt;
      &lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;
      &lt;span class=&quot;s&quot;&gt;&quot;category_scores&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&quot;hate&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.18805529177188873&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&quot;hate/threatening&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.0001250059431185946&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&quot;self-harm&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.0003706029092427343&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&quot;sexual&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.0008735615410842001&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&quot;sexual/minors&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.0007470346172340214&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&quot;violence&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.0041268812492489815&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&quot;violence/graphic&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.00023186142789199948&lt;/span&gt;
      &lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;
      &lt;span class=&quot;s&quot;&gt;&quot;flagged&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;false&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;输入参数很简单，关键看返回的输出结果。OpenAI 对于包含哪类不适内容，做了比较详尽的分类，比如对于色情内容，也分成了未成年色情和易引起性兴奋的内容。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;hate&lt;/code&gt;：是否包含基于种族、性别、民族、宗教、国籍、性取向、残疾状况或种姓表达、煽动或促进仇恨的内容，如果没有则是 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;false&lt;/code&gt;，否则为 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;true&lt;/code&gt;。&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;hate/threatening&lt;/code&gt;：是否包含仇恨内容还包括对目标群体的暴力或严重伤害，没有则 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;false&lt;/code&gt;，包含则值为 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;true&lt;/code&gt;。&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;self-harm&lt;/code&gt;：是否包含提倡、鼓励或描述自残行为（例如自杀、割伤和饮食失调）的内容，没有则 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;false&lt;/code&gt;，否则 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;true&lt;/code&gt;。&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sexual&lt;/code&gt;：是否包含意在引起性兴奋的内容，例如对性活动的描述，或宣传性服务（不包括性教育和健康）的内容，没有则 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;false&lt;/code&gt;，否则 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;true&lt;/code&gt;。&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sexual/minors&lt;/code&gt;：是否包含包含 18 岁以下个人的色情内容，没有则 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;false&lt;/code&gt;，否则 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;true&lt;/code&gt;。&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;violence&lt;/code&gt;：是否包含宣扬或美化暴力或颂扬他人的痛苦或屈辱的内容，没有为 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;false&lt;/code&gt;，否则 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;true&lt;/code&gt;。&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;violence/graphic&lt;/code&gt;：是否包含以极端的画面细节描绘死亡、暴力或严重身体伤害的暴力内容，没有 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;false&lt;/code&gt;，否则 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;true&lt;/code&gt;。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;显然，对于使用 OpenAI 生成内容的场景下如果需要用到 Moderation，则是免费调用的。如果你不是对 OpenAI 的输入 &amp;amp; 生成场景，而是自己的其他内容想白嫖 Moderation API 是不可能的。但是我们也注意到，这里其实没有整治敏感的分类，因为 OpenAI 没有考虑具体的使用者所处的政体或政治环境，而且这些尺度是比较容易变化的，并且有一些可能并不是普适性的理念，因此某些国家的使用者要额外配套自己的内容审查能力。&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;三一些-openai-目前开放的-api-使用示例&quot;&gt;三、一些 OpenAI 目前开放的 API 使用示例&lt;/h2&gt;

&lt;p&gt;在 OpenAI 的 API 官方首页的大标题写着的是「用 OpenAI 的强力模型构建下一代应用（Build next-gen apps with OpenAI’s powerful models）」，并直接点出了自己最拿得出手的三个商业化产品 GPT-3、Codex、DALL·E。&lt;/p&gt;

&lt;p&gt;并提到自己的 API 调用非常简单：&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;openai&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;openai&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Completion&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;create&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;engine&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;davinci&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;prompt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Make a list of astronomical observatories:&quot;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;引入 OpenAI 的 python 依赖库后，调用时指定用哪个引擎、提示词是什么即可。轻松一些，我们先来看一些示例吧。&lt;/p&gt;

&lt;h3 id=&quot;1执行各种自然语言任务的一些使用案例&quot;&gt;1、执行各种自然语言任务的一些使用案例&lt;/h3&gt;

&lt;p&gt;GPT-3.5 已经在一些应用上跑起来了，OpenAI 列举了一些 ChatGPT 和 Whisper API 的早期客户：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Snapchat（snap.com）&lt;/strong&gt;的创建者 Snap Inc. 本周推出了适用于 Snapchat+ 的 My AI。 实验性功能在 ChatGPT API 上运行。 My AI 为 Snapchatter 提供了一个友好的、可定制的聊天机器人，可以在他们指尖提供建议，甚至可以在几秒钟内为朋友写一句俳句。 Snapchat 是日常交流和消息传递的场所，每月有 7.5 亿 Snapchatter。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Quizlet（quizlet.com/labs/qchat）&lt;/strong&gt;是一个全球学习平台，有超过 6000 万学生使用它来学习、练习和掌握他们正在学习的任何内容。 Quizlet 在过去三年中一直与 OpenAI 合作，在多个用例中利用 GPT-3，包括词汇学习和练习测试。 随着 ChatGPT API 的推出，Quizlet 推出了 Q-Chat，这是一种完全自适应的 AI 导师，可让学生根据通过有趣的聊天体验提供的相关学习材料提出自适应问题。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Instacart（instacart.com）&lt;/strong&gt;正在增强 Instacart 应用程序，使客户能够询问食物并获得鼓舞人心的、可购买的答案。这使用 ChatGPT 以及 Instacart 自己的 AI 和来自其 75,000 多家零售合作伙伴商店位置的产品数据来帮助客户发现开放式购物目标的想法，例如「我如何制作美味的鱼炸玉米饼？」或「对我的孩子来说，什么是健康的午餐？」Instacart 计划在今年晚些时候推出「Ask Instacart」。&lt;/li&gt;
  &lt;li&gt;Shopify 的消费者应用程序 &lt;strong&gt;Shop（shop.app）&lt;/strong&gt;被 1 亿购物者用来寻找和参与他们喜爱的产品和品牌。 ChatGPT API 用于为 Shop 的新购物助手提供动力。 当购物者搜索商品时，导购会根据他们的需求进行个性化推荐。 Shop 的新人工智能购物助手将通过扫描数百万种产品来简化应用程序内购物，以快速找到买家正在寻找的东西，或者帮助他们发现新东西。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Speak（speak.com）&lt;/strong&gt;是一款人工智能语言学习应用程序，专注于打造流利口语的最佳途径。 他们是韩国发展最快的英语应用程序，并且已经在使用 Whisper API 来支持新的 AI 语音伴侣产品，并迅速将其推向全球其他地区。 Whisper 为各个级别的语言学习者提供了人类水平的准确性，解锁了真正的开放式会话练习和高度准确的反馈。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;2一些更详细的示例&quot;&gt;2、一些更详细的示例&lt;/h3&gt;

&lt;p&gt;API 还能实现什么功能？这里列举一些 GPT-3 API 期间就有的功能，调用的效果如何。&lt;/p&gt;

&lt;h4 id=&quot;21写作-copywriting&quot;&gt;2.1、写作 Copywriting&lt;/h4&gt;

&lt;p&gt;人类输入：&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Create promo copy for the FamilyTime mobile application. It allows unlimited uploading, special filters and makes it easy to create albums of photos and videos. It runs on iOS and Android:
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;下面是 GPT-3 的输出：&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;FamilyTime is the perfect app to share and create memories with your family. FamilyTime is a free, unlimited photo and video editor that allows you to create albums of your memories. It has a special filter for the perfect summer look and is so simple to use.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;22摘要-summarization&quot;&gt;2.2、摘要 Summarization&lt;/h4&gt;

&lt;p&gt;人类输入：&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Summarize this email into a single sentence:

Dear Olivia,

The solar energy conference went great. New Horizon Manufacturing wants to meet with us to talk about our photovoltaic window system we’re about to launch.
I think it would be great to talk this Tuesday.

Best,
Allison

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;下面是 GPT-3 输出：&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Allison is excited to meet with New Horizon Manufacturing to discuss their photovoltaic window system.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;23分析无结构文本-parsing-unstructured-text&quot;&gt;2.3、分析无结构文本 Parsing Unstructured Text&lt;/h4&gt;

&lt;p&gt;人类输入：&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Answer the following questions about this customer email:

I’m wondering if you could provide me with information about your cloud services for companies. I’m CTO for Moon Door, a movie production company and want to know if it can help us manage our different projects that are in production.

Questions:
1. What industry is the company
2. What is the name of the company
3. What are they interested in
4. What is the customer’s position

Answers:
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;下面是 GPT-3 输出：&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;1. The company is in the movie production industry.
2. The company is Moon Door.
3. The customer is interested in cloud services for companies.
4. The customer is the CTO.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;24分类-classification&quot;&gt;2.4、分类 Classification&lt;/h4&gt;

&lt;p&gt;人类输入：&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Choose a genre category for each book 1. The Hunger Games, 2. The Kite Runner 3. A Wrinkle in Time (“fiction”, “young adult”, “science fiction”, “fantasy”, “other”) and make a list of the book and its genre:
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;下面是 GPT-3 输出：&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;1. The Hunger Games: young adult, fiction
2. The Kite Runner: fiction, young adult
3. A Wrinkle in Time: science fiction, fantasy, other
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;25翻译-translation&quot;&gt;2.5、翻译 Translation&lt;/h4&gt;

&lt;p&gt;人类输入：&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Translate this into French: Where can I find a bookstore?
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;下面是 GPT-3 输出：&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Où puis-je trouver un magasin de livres?
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;3将自然语言翻译成代码的-codex&quot;&gt;3、将自然语言翻译成代码的 Codex&lt;/h3&gt;

&lt;h4 id=&quot;31用自然语言写-sql&quot;&gt;3.1、用自然语言写 SQL&lt;/h4&gt;

&lt;p&gt;人类输入：&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Table customers, columns = [CustomerId, FirstName, LastName, Company, Address, City, State, Country, PostalCode, Phone, Fax, Email, SupportRepId, TotalSpend]

Create a MySQL query for all customers in Texas who have spent over five thousand dollars.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;下面是 Codex 输出：&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;query = &quot;SELECT * FROM customers WHERE State = &apos;TX&apos; AND TotalSpend &amp;gt; 5000&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;32用自然语言调用一个-api&quot;&gt;3.2、用自然语言调用一个 API&lt;/h4&gt;

&lt;p&gt;人类输入：&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Python 3
Get the current value of a Bitcoin in US dollars using the bitcoincharts api
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Codex 输出：&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;requests&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;json&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;get_bitcoin_price&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;url&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&apos;http://api.bitcoincharts.com/v1/weighted_prices.json&apos;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;response&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;requests&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;url&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;json&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;USD&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;7d&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__name__&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&apos;__main__&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_bitcoin_price&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;33用自然语言续写代码&quot;&gt;3.3、用自然语言续写代码&lt;/h4&gt;

&lt;p&gt;人类输入：&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Extract all the html from the string and replace the tags with &apos;&apos;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Codex 输出：&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def extract_html(text):
    return re.sub(&apos;&amp;amp;lt;[^&amp;amp;lt;]+?&amp;amp;gt;&apos;, &apos;&apos;, text)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;四微调fine-tuning&quot;&gt;四、微调（Fine-tuning）&lt;/h2&gt;

&lt;p&gt;上面也提到了，暂时未开放对 Turbo（也就是 ChatGPT 背后的 GPT-3.5）的 finetune，但是开放后的使用方式也会是与 GPT-3 的 finetune 类似的。我们来看看目前 OpenAI 是如何让开发者来 finetune 他们的基础模型的。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Few-shot learning 是什么？&lt;/strong&gt;：GPT-3 用了互联网上的海量文本数据训练，所以当你给少量示例（a promopt with just a few examples）时，GPT-3 会从「直觉上」知道你大概是想要解决什么任务，然后给出一些大概齐的反馈内容作为 completion，这通常就被叫做「few-shot learning」或者「few-shot prompting」。&lt;/p&gt;

&lt;p&gt;而如果你提供一些针对目标任务的训练数据，很可能可以实现没有 examples 也可以执行任务，也就是使用时连「few-shot learning」都免了。OpenAI 也提供了让用户自己 fine-tune 模型的接口，自己 fine-tune 的好处是：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;高质量&lt;/strong&gt;：这是显然的，比「设计提示（prompt design）」得到的结果质量更高。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;相当于批量 prompt&lt;/strong&gt;：可以比 prompt 给模型更多的 examples，比如用一个文件，里面包含大量用于 fine-tuning 的输入数据。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;更省&lt;/strong&gt;：可以更省 tokens，也就更省钱。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;更快&lt;/strong&gt;：更低的延迟的请求响应。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;步骤和价格&lt;/strong&gt;方面，Fine-tune 一共三步：上传用于 fine-tune 的数据、用数据 fine-tune 模型、使用属于你自己的 fine-tune 过的模型。从定价上我们看到 Fine-tune 后的模型使用费用基本翻了 4~6 倍，可以说相比基本模型的使用，是非常贵了。&lt;/p&gt;

&lt;p&gt;另外 OpenAI 也支持你对一个 fine-tune 过的模型继续 fine-tune，而不用从头开始。目前 davinci、curie、babbage、ada 都支持 fine-tuning。训练数据的格式也很简单，就是一组 prompt-completion 的 JSONL 文件，just like this：&lt;/p&gt;

&lt;div class=&quot;language-json highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;prompt&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&amp;lt;prompt text&amp;gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;completion&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&amp;lt;ideal generated text&amp;gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;prompt&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&amp;lt;prompt text&amp;gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;completion&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&amp;lt;ideal generated text&amp;gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;prompt&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&amp;lt;prompt text&amp;gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;completion&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&amp;lt;ideal generated text&amp;gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;...&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Fine-tune 的 example 与 few-shot learning 的最大区别：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;few-shot learning 要给出详尽的 instruction 来描述任务&lt;/li&gt;
  &lt;li&gt;few-shot learning 的一个 prompt 是在使用时给出的，所以一个 prompt 大概率会带多个 examples（相对详细）；而 fine-tune 的 example 都是一些简单直接的 prompt 以及直接对应的 completion。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;OpenAI 建议 fine-tune 的 examples 数量至少几百（a couple hundred）。另外 fine-tune 也符合 scaling law，基本上 fine-tune 的数据集成倍上翻的话，效果是线性增长的。&lt;/p&gt;

&lt;h3 id=&quot;1创建一个-fine-tune-模型&quot;&gt;1、创建一个 fine-tune 模型&lt;/h3&gt;

&lt;p&gt;CLI 下运行如下命令，其中 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;TRAIN_FILE_ID_OR_PATH&amp;gt;&lt;/code&gt; 是你的训练数据文件，&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;BASE_MODEL&amp;gt;&lt;/code&gt; 是你要用的模型，具体的参数可以用 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ada&lt;/code&gt;、&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;babbage&lt;/code&gt;、&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;curie&lt;/code&gt; 和 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;davinci&lt;/code&gt;。&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;mikecaptain@local &lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;openai api fine_tunes.create &lt;span class=&quot;nt&quot;&gt;-t&lt;/span&gt; &amp;lt;TRAIN_FILE_ID_OR_PATH&amp;gt; &lt;span class=&quot;nt&quot;&gt;-m&lt;/span&gt; &amp;lt;BASE_MODEL&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;这句命令让 OpenAI 不仅基于 base model 创建了一个模型，而且开始运行训练任务。训练任务可能会花费几分钟、几小时甚至根据，取决于你的训练集和模型选择。训练任务可能会被 OpenAI 排队，不一定马上开始运行。如果过程中被打断了，可以如下继续：&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;mikecaptain@local &lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;openai api fine_tunes.follow &lt;span class=&quot;nt&quot;&gt;-i&lt;/span&gt; &amp;lt;YOUR_FINE_TUNE_JOB_ID&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;保存一个 fine-tune job 的命令如下：&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;mikecaptain@local &lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;openai api fine_tunes.get &lt;span class=&quot;nt&quot;&gt;-i&lt;/span&gt; &amp;lt;YOUR_FINE_TUNE_JOB_ID&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;取消一个 fine-tune job 的命令如下：&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;mikecaptain@local &lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;openai api fine_tunes.cancel &lt;span class=&quot;nt&quot;&gt;-i&lt;/span&gt; &amp;lt;YOUR_FINE_TUNE_JOB_ID&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;2使用-fine-tuned-模型&quot;&gt;2、使用 fine-tuned 模型&lt;/h3&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;openai&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;openai&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Completion&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;create&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;FINE_TUNED_MODEL&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;prompt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;YOUR_PROMPT&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;3删掉一个-fine-tuned-模型&quot;&gt;3、删掉一个 fine-tuned 模型&lt;/h3&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;openai&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;openai&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;delete&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;FINE_TUNED_MODEL&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;4一个-fine-tuned-模型之上继续-fine-tune&quot;&gt;4、一个 fine-tuned 模型之上继续 fine-tune&lt;/h3&gt;

&lt;p&gt;如果你微调了一个模型，现在又有为的训练数据想要合并进来，可以基于已 fine-tuned 模型继续微调，无需从头再全部训练一遍。唯一要做的，就是在创建新的 fine-tune job 时传入已 fine-tune 的模型名称，替代&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;BASE_MODEL&amp;gt;&lt;/code&gt;（例如 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-m curie:ft-&amp;lt;org&amp;gt;-&amp;lt;date&amp;gt;&lt;/code&gt;），不必更改其他训练参数。&lt;/p&gt;

&lt;p&gt;有一个要注意的，如果新增的训练数据比以前的训练数据规模小得多，那最好把 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;learning_rate_multiplier&lt;/code&gt; 减少 2 到 4 倍，否则很可能跳过了最优解。&lt;/p&gt;

&lt;h4 id=&quot;本文参考&quot;&gt;本文参考&lt;/h4&gt;

&lt;ol&gt;
  &lt;li&gt;https://openai.com/blog/introducing-chatgpt-and-whisper-apis&lt;/li&gt;
  &lt;li&gt;https://openai.com/pricing&lt;/li&gt;
  &lt;li&gt;https://platform.openai.com/docs/guides/chat/chat-vs-completions&lt;/li&gt;
  &lt;li&gt;https://platform.openai.com/docs/usage-policies&lt;/li&gt;
  &lt;li&gt;https://platform.openai.com/docs/models/gpt-3-5&lt;/li&gt;
  &lt;li&gt;https://openai.com/api/&lt;/li&gt;
  &lt;li&gt;https://developer.aliyun.com/article/933516&lt;/li&gt;
  &lt;li&gt;http://www.mikecaptain.com/2023/03/02/chatgpt-api/&lt;/li&gt;
  &lt;li&gt;http://www.mikecaptain.com/2023/01/24/openai-official-doc/&lt;/li&gt;
&lt;/ol&gt;</content><author><name>麦克船长</name></author><category term="ai" /><category term="AI" /><category term="人工智能" /><category term="NLP" /><category term="自然语言处理" /><category term="神经网络" /><category term="LLM" /><category term="大型语言模型" /><category term="语言模型" /><category term="大模型" /><summary type="html">开发人员现在可以通过 API 将 ChatGPT 和 Whisper 模型集成到他们的应用程序和产品中。</summary></entry><entry><title type="html">千日酒：麦克船长在阿里巴巴的第 999 天</title><link href="https://www.mikecaptain.com/2023/02/27/captain-alibaba-999/" rel="alternate" type="text/html" title="千日酒：麦克船长在阿里巴巴的第 999 天" /><published>2023-02-27T11:12:13+00:00</published><updated>2023-02-27T11:12:13+00:00</updated><id>https://www.mikecaptain.com/2023/02/27/captain-alibaba-999</id><content type="html" xml:base="https://www.mikecaptain.com/2023/02/27/captain-alibaba-999/">&lt;p&gt;&lt;img src=&quot;/img/src/2023/captain-alibaba-999-1.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;千日酒&quot;&gt;千日酒&lt;/h3&gt;

&lt;p&gt;[晋] 干宝&lt;/p&gt;

&lt;p&gt;狄希，中山人也。能造「千日酒」，饮之千日醉。时有州人姓刘，名玄石，好饮酒，往求之。希曰：「我酒发来未定，不敢饮君。」石曰：「纵未熟，且与一杯，得否？」希闻此语，不免饮之。复索曰：「美哉！可更与之。」希曰：「且归，别日当来，只此一杯，可眠千日也。」石别，似有怍色。至家，醉死。家人不之疑，哭而葬之。&lt;/p&gt;

&lt;p&gt;经三年，希曰：「玄石必应酒醒，宜往问之。」既往石家，语曰：「石在家否？」家人皆怪之，曰：「玄石亡来，服以阕矣。」希惊曰：「酒之美矣，而致醉眠千日，今合醒矣。」乃命其家人凿冢破棺看之。冢上汗气彻天，遂命发冢。方见开目张口，引声而言曰：「快哉，醉我也！」因问希曰：“「作何物也，令我一杯大醉，今日方醒？日高几许？」墓上人皆笑之，被石酒气冲入鼻中，亦各醉卧三月。&lt;/p&gt;

&lt;p&gt;——《搜神记》&lt;/p&gt;

&lt;h3 id=&quot;终为始&quot;&gt;终为始&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;目标驱动，以终为始，纵情向前。&lt;/li&gt;
  &lt;li&gt;不要在虚假繁荣、自圆其说中虚度光阴。&lt;/li&gt;
  &lt;li&gt;要追随大才之人。&lt;/li&gt;
  &lt;li&gt;爱，财富与创造。&lt;/li&gt;
  &lt;li&gt;正视你的财富与地位。&lt;/li&gt;
&lt;/ul&gt;</content><author><name>麦克船长</name></author><category term="thinking" /><category term="船长" /><category term="船涨" /><category term="麦克船长" /><category term="阿里巴巴" /><summary type="html">玄石饮酒，一醉千日。酒之美矣，而致醉眠千日，今合醒矣。正视自己的财富与地位，不在虚假繁荣、自圆其说中虚度光阴目标驱动，以终为始，纵情向前。</summary></entry><entry><title type="html">Meta 推出开源 LLaMA，用 1/10 参数规模打败 GPT-3，群”模”乱舞的 2023 拉开序幕</title><link href="https://www.mikecaptain.com/2023/02/25/meta-llama/" rel="alternate" type="text/html" title="Meta 推出开源 LLaMA，用 1/10 参数规模打败 GPT-3，群”模”乱舞的 2023 拉开序幕" /><published>2023-02-25T05:54:13+00:00</published><updated>2023-02-25T05:54:13+00:00</updated><id>https://www.mikecaptain.com/2023/02/25/meta-llama</id><content type="html" xml:base="https://www.mikecaptain.com/2023/02/25/meta-llama/">&lt;p&gt;&lt;img src=&quot;/img/src/2023/2023-02-25-meta-llama-29.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;北京时间 2023 年 2 月 25 日 Meta AI 在其官网公开发布了 LLaMA（Large Language Model Meta AI）大型语言模型，包括 70 亿、130 亿、330 亿、650 亿 4 种参数规模，旨在推动 LLM 领域的小型化、平民化研究。有趣的是，LLaMA 是羊驼的意思。&lt;/p&gt;

&lt;p&gt;Guillaume Lample 在其 Twitter 上声称：LLaMA 130 亿参数版本的表现，在大多数测试上优于 OPT 和 GPT-3 1750 亿参数版，650 亿的版本表现基本可以比肩 Chinchilla 700 亿参数、PaLM 5400 亿参数这些大模型。&lt;/p&gt;

&lt;p&gt;LLaMA 是由 Meta AI 的 FAIR 团队研发的，在 2022 年 12 月到 2023 年 2 月期间进行的训练，目前 GitHub 上放出的是这个模型的 V1 版（Version 1）。与 GPT 系列类似，LLaMA 也是一个建立在 Transformer 基础架构上的自回归语言模型（Autoregression Language Model），关于 Transformer 基础架构不了解的朋友可以&lt;a href=&quot;http://www.mikecaptain.com/2023/01/22/captain-aigc-1-transformer/&quot;&gt;阅读这篇文章《人工智能 LLM 革命前夜：一文读懂横扫自然语言处理的 Transformer 模型》&lt;/a&gt;。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;源码：https://github.com/facebookresearch/llama&lt;/li&gt;
  &lt;li&gt;论文：https://research.facebook.com/file/1574548786327032/LLaMA–Open-and-Efficient-Foundation-Language-Models.pdf
Meta AI 同时在其官方发布了论文《LLaMA: Open and Efficient Foundation Language Models》&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023/2023-02-25-meta-llama-9.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;下面我们根据目前 LLaMA 公开的信息，第一时间快速浏览一些关键信息。&lt;/p&gt;

&lt;h2 id=&quot;1模型参数及基本原理&quot;&gt;1、模型参数及基本原理&lt;/h2&gt;

&lt;p&gt;像其他大型语言模型一样，LLaMA 也是通过将一系列单词作为输入，并预测下一个单词以递归生成文本。为了训练我们的模型，我们选择了使用最广泛的20种语言的文本，重点关注拉丁字母和西里尔字母的语言。&lt;/p&gt;

&lt;p&gt;模型架构上，也是基于 Transformer，但是做了几个显著的改进：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;从 GPT-3 得到启发的 Pre-normalization：为了增强训练的稳定性，将只在输出层的 normalization 改成了 Transformer 里面每一层的输入都进行 normalize，具体用的是 Zhang and Sennrich (2019) 提到的 RMSNorm[1]。&lt;/li&gt;
  &lt;li&gt;从 PaLM 得到启发的 SwiGLU 激活函数：用 Shazeer(2020) 提到的 SwiGLU 激活函数[2]替换了大家熟悉的 ReLU 激活函数。&lt;/li&gt;
  &lt;li&gt;从 GPTNeo 得到启发的 RoPE：在 Transformer 位置编码部分，没有用绝对位置编码（Absoute Positional Embeddings），而是用的 Su et al.(2021) [3] 提到的 RoPE（Rotary Positional Embeddings）。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Meta AI 公布了其各尺寸 LLaMA 模型的超参数：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023/2023-02-25-meta-llama-1.png&quot; alt=&quot;&quot; width=&quot;640&quot; /&gt;&lt;/p&gt;

&lt;p&gt;与 GPT-3 模型对比可以看出，LLaMA 的四个版本中：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;LLaMA-7B 对应的是 GPT-3 6.7B 版本，都是 32 层、32 个多头注意力、4096 宽度，LR 3.0E-4 要高于 GPT 的 1.2E-4，batch 4M 更大&lt;/li&gt;
  &lt;li&gt;LLaMA-13B 对应的是 GPT-3 13B 版本，都是 40 层、40 个多头注意力，模型宽度 5120、5140 差不多，LR 3.0E-4 也高于 GPT 的 1.0E-4，batch 4M 更大&lt;/li&gt;
  &lt;li&gt;LLaMA-33B、LLaMA-65B 与 GPT-3 就没有对应了，都是仅次于 GPT-3 最大的 175B 版本。Meta AI 也是为了证明，更小的模型也能达到甚至超越 GPT-3 暴力大模型，这也是推动模型小型化的一个动力。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;2数据&quot;&gt;2、数据&lt;/h2&gt;

&lt;h3 id=&quot;llama-的训练数据&quot;&gt;LLaMA 的训练数据&lt;/h3&gt;

&lt;p&gt;LLaMA 用到了如下这些训练数据集，并给出了相应的占比：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;CCNet：67%&lt;/li&gt;
  &lt;li&gt;C4：15%，NLP 领域的人也基本知道了，全称是 Colossal Common Crawl Corpus，最早大家了解到它基本是通过 Google T5 模型的那篇论文《Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer》。作为对比，GPT-3 中的 60% 数据来自 Common Crawl，不过 GPT-3 对其 Common Crawl 的数据以及此前 OpenAI 在 GPT-2 用的 WebText 作为训练数据参照对 Common Crawl 数据进行了正类、负类的分类清理，不过还不是 C4。&lt;/li&gt;
  &lt;li&gt;GitHub：4.5%，开源代码仓库平台，目前已经是 Microsoft 自己的了。&lt;/li&gt;
  &lt;li&gt;Wikipedia：4.5%，之前 GPT-3 就用了 English Wikipedia。&lt;/li&gt;
  &lt;li&gt;Books：4.5%，作为对比 GPT-3 的数据源中 16% 来自书籍。&lt;/li&gt;
  &lt;li&gt;ArXiv：2.5%，是学界最熟悉的开放电子学术论文存档库，由康奈尔大学于 1991 年成立。&lt;/li&gt;
  &lt;li&gt;Stack Exchange：2%，类似于 Stack Overflow 的、针对程序员群体的在线技术问答社区。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;llama-的评估数据及表现&quot;&gt;LLaMA 的评估数据及表现&lt;/h3&gt;

&lt;p&gt;可以看到以上训练数据源，主要来自网络内容，因此 Meta AI 也说包含了冒犯、有害和带偏见的内容。所以 Meta AI 对该模型的偏见方面表现在 RAI 数据集上进行了评估，以衡量模型在性别、宗教、种族、性取向、年龄、国籍、残疾、外貌和社会经济地位等方面表现出的偏见。Meta AI 还根据提示模型的上下文的有害程度来衡量模型生成的毒性：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023/2023-02-25-meta-llama-3.png&quot; alt=&quot;&quot; width=&quot;320&quot; /&gt;&lt;/p&gt;

&lt;p&gt;就像上面船长提到的，Meta AI 也对网络文本做了分类处理，如果其内容与 Wikipedia 或 Wikipedia 引用内容比较类似，则认为它是高质量的。这就像 GPT-3 认为 WebText 外链（Outbound Links）出去的且 Karma 大于 3 的网页是高质量的。这里 Meta AI 用到了 Kneser-Ney 语言模型和一个 fastText 线性分类器。&lt;/p&gt;

&lt;p&gt;Meta AI 声称训练数据中包含 20 种语言，但大部分内容依然是英文为主，因此也与 GPT-3 一样还是在英文表现会更好。同样，OpenAI 曾声称因为英文内容多，所以整体模型生成的内容无形中就更符合英文母语人群的价值观，这也是一个潜在问题。&lt;/p&gt;

&lt;p&gt;LLaMA 用到了如下这些评估数据集：BoolQ, PIQA, SIQA, HellaSwag, WinoGrande, ARC, OpenBookQA, NaturalQuestions, TriviaQA, RACE, MMLU, BIG-bench hard, GSM8k, RealToxicityPrompts, WinoGender, CrowS-Pairs。LLaMA 在其论文中罗列了大量实验结果，这里摘录一些。&lt;/p&gt;

&lt;p&gt;在一些推理任务上，LLaMA 有如下表现：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023/2023-02-25-meta-llama-2.png&quot; alt=&quot;&quot; width=&quot;320&quot; /&gt;&lt;/p&gt;

&lt;p&gt;在 NaturalQuestions 上和其他模型对比的表现：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023/2023-02-25-meta-llama-4.png&quot; alt=&quot;&quot; width=&quot;320&quot; /&gt;&lt;/p&gt;

&lt;p&gt;阅读理解上的表现对比：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023/2023-02-25-meta-llama-5.png&quot; alt=&quot;&quot; width=&quot;320&quot; /&gt;&lt;/p&gt;

&lt;p&gt;在 TriviaQA 上 Zero-shot、few-shot 对比问答效果的表现：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023/2023-02-25-meta-llama-6.png&quot; alt=&quot;&quot; width=&quot;320&quot; /&gt;&lt;/p&gt;

&lt;p&gt;代码生成方面的表现对比：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023/2023-02-25-meta-llama-7.png&quot; alt=&quot;&quot; width=&quot;320&quot; /&gt;&lt;/p&gt;

&lt;p&gt;大规模多任务语言理解（MMLU）的表现对比如下，论文的附录 B 中 Table 16 中有完整的各模型表现，大家可以去查看。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023/2023-02-25-meta-llama-8.png&quot; alt=&quot;&quot; width=&quot;560&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;3算力&quot;&gt;3、算力&lt;/h2&gt;

&lt;p&gt;Meta AI 还给出了碳足迹的计算，来衡量算力的消耗情况，随着 LLM 应用的普及，这样的环保议题在未来会越来越被重视。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023/2023-02-25-meta-llama-10.png&quot; alt=&quot;&quot; width=&quot;640&quot; /&gt;&lt;/p&gt;

&lt;p&gt;在同一数据中心训练不同模型的碳足迹。Meta AI 在同一数据中心训练 OPT、BLOOM 和 LLaMA 的模型的碳排放。对于 A100-80GB 的功耗，LLaMA 采用 NVLink 系统的热设计功率（TDP），即 400W。Meta AI 采用 PUE 值为 1.1，碳强度系数设定为美国国家平均水平的 0.385 kg CO2e/KWh。&lt;/p&gt;

&lt;h2 id=&quot;4一些评论与未来影响&quot;&gt;4、一些评论与未来影响&lt;/h2&gt;

&lt;p&gt;Meta AI 这次直接开源了模型、参数，这次推动模型小型化、平民化，对于活跃 AI 领域的创业、研究都有巨大帮助，值得所有人重点关注。Meta AI 选择了周五发布，其实是让其他大厂措手不及，让这个事情在周末发酵一下。不过对于整个生态里大厂之外的所有人，这都是一件好事。&lt;/p&gt;

&lt;p&gt;Meta AI 在论文中总结到，&lt;strong&gt;LLaMA-13B 的性能优于 GPT-3，同时体积更小超过 10 倍&lt;/strong&gt;，而 LLaMA-65B 与 Chinchilla-70B 和 PaLM-540B 相当。与以往的研究不同的是，目前展示了仅使用公开可用数据集训练就可以达到最先进的性能，而无需使用专有数据集。Meta 希望将这些模型发布给研究社区，可以加速大型语言模型的发展，并帮助改进它们的鲁棒性并减轻已知问题，例如毒性和偏见。此外，Meta 观察到像 Chung et al. (2022) 论文中所提到的那样对模型进行微调可以获得更好的结果，Meta AI 计划在未来的工作中进一步研究。&lt;/p&gt;

&lt;p&gt;Meta AI 还提到，目前实验来看，只要继续上参数规模、上数据规模，性能仍然还在涨，比如上图，所以 Meta AI 这次发布时直接表示，计划未来还会继续发布更大数据规模训练的、更大参数规模的模型。&lt;/p&gt;

&lt;h2 id=&quot;5能试玩吗看看-llama-给出的例子吧&quot;&gt;5、能试玩吗？看看 LLaMA 给出的例子吧&lt;/h2&gt;

&lt;p&gt;LLaMA 还未开放，目前你可以在如下链接申请访问，进入 WaitingList：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;https://docs.google.com/forms/d/e/1FAIpQLSfqNECQnMkycAp2jP4Z9TFX0cGR4uf7b_fBxjY_OjhJILlKGA/viewform&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;但是我们可以看看 Meta AI 放出的例子，也蛮有趣的。下面先是 LLaMA-65B（不用指令精调）的输出，加粗的部分是输入给的 Prompt（提示）：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023/2023-02-25-meta-llama-11.png&quot; alt=&quot;&quot; width=&quot;720&quot; /&gt;&lt;/p&gt;

&lt;p&gt;写推荐信：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023/2023-02-25-meta-llama-12.png&quot; alt=&quot;&quot; width=&quot;720&quot; /&gt;&lt;/p&gt;

&lt;p&gt;给出 Python 函数定义和注释，续写代码：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023/2023-02-25-meta-llama-13.png&quot; alt=&quot;&quot; width=&quot;720&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Meta AI 团队调侃自己的老板 LeCun（hhhhh）：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023/2023-02-25-meta-llama-14.png&quot; alt=&quot;&quot; width=&quot;720&quot; /&gt;&lt;/p&gt;

&lt;p&gt;给定场景，虚构对话的能力：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023/2023-02-25-meta-llama-15.png&quot; alt=&quot;&quot; width=&quot;720&quot; /&gt;
&lt;img src=&quot;/img/src/2023/2023-02-25-meta-llama-16.png&quot; alt=&quot;&quot; width=&quot;720&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Meta AI 还展示了几个 LLaMA-I 的生成示例，LLaMA-I 是使用 Chung et al. (2022). 的协议和指令数据集对 LLaMA-65B 进行微调后的结果。第一个是让 LLaMA 写一个太阳和冥王星的对话：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023/2023-02-25-meta-llama-17.png&quot; alt=&quot;&quot; width=&quot;720&quot; /&gt;&lt;/p&gt;

&lt;p&gt;再让 LLaMA 写一个用 JavaScript 发送 HTTP 请求的代码示例：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023/2023-02-25-meta-llama-18.png&quot; alt=&quot;&quot; width=&quot;720&quot; /&gt;&lt;/p&gt;

&lt;p&gt;用 Python 写一个正则表达式移除 HTML 标签，以及用 Python 写一个抽取函数定义的正则表达式如下（说真的，船长之前用 ChatGPT 写正则表达式，这效率真的太高了，人脑写东西其实挺反人类的）：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023/2023-02-25-meta-llama-19.png&quot; alt=&quot;&quot; width=&quot;720&quot; /&gt;&lt;/p&gt;

&lt;p&gt;一系列连续多轮对话的问答，LLaMA 也表现的很好：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023/2023-02-25-meta-llama-20.png&quot; alt=&quot;&quot; width=&quot;720&quot; /&gt;&lt;/p&gt;

&lt;p&gt;写个小作文也不在话下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023/2023-02-25-meta-llama-21.png&quot; alt=&quot;&quot; width=&quot;720&quot; /&gt;&lt;/p&gt;

&lt;p&gt;瞎编一套理论解释猫从未存在过（研究人员的脑洞也挺大的 hhhh）：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023/2023-02-25-meta-llama-22.png&quot; alt=&quot;&quot; width=&quot;720&quot; /&gt;&lt;/p&gt;

&lt;p&gt;写一个凯撒大帝和拿破仑之间的吵架场景（2333333）：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023/2023-02-25-meta-llama-23.png&quot; alt=&quot;&quot; width=&quot;720&quot; /&gt;&lt;/p&gt;

&lt;p&gt;发送一封电子邮件，请求人们负责任地使用语言模型：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023/2023-02-25-meta-llama-24.png&quot; alt=&quot;&quot; width=&quot;720&quot; /&gt;&lt;/p&gt;

&lt;p&gt;又是多轮的对话，并且涉及到大量真实实体，验证世界知识的准确性，可以看到模型准确给出了是爱因斯坦提出了质能方程的：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023/2023-02-25-meta-llama-25.png&quot; alt=&quot;&quot; width=&quot;720&quot; /&gt;&lt;/p&gt;

&lt;p&gt;让 LLaMA 假装是一个可运行 bash 终端的：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023/2023-02-25-meta-llama-26.png&quot; alt=&quot;&quot; width=&quot;720&quot; /&gt;&lt;/p&gt;

&lt;p&gt;示例就看到这里，还是非常令船长兴奋的。可以想见，2023 注定是群魔乱舞的一年，我们一定会在模型发展上看到很多载入 AI 发展史册的事件发生。&lt;/p&gt;

&lt;h2 id=&quot;参考&quot;&gt;参考&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;[1] Biao Zhang and Rico Sennrich. 2019. Root mean square layer normalization. Advances in Neural Information Processing Systems, 32.&lt;/li&gt;
  &lt;li&gt;[2] Noam Shazeer. 2020. Glu variants improve transformer. arXiv preprint arXiv:2002.05202.&lt;/li&gt;
  &lt;li&gt;[3] Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. 2021. Roformer: Enhanced transformer with rotary position embedding. arXiv preprint arXiv:2104.09864.&lt;/li&gt;
  &lt;li&gt;[4] https://github.com/facebookresearch/llama&lt;/li&gt;
  &lt;li&gt;[5] https://ai.facebook.com/blog/large-language-model-llama-meta-ai/&lt;/li&gt;
  &lt;li&gt;[6] https://research.facebook.com/publications/llama-open-and-efficient-foundation-language-models/&lt;/li&gt;
  &lt;li&gt;[7] https://twitter.com/ylecun/status/1629243179068268548&lt;/li&gt;
  &lt;li&gt;[8] https://twitter.com/GuillaumeLample/status/1629151231800115202&lt;/li&gt;
  &lt;li&gt;[9] Hyung Won Chung, Le Hou, S. Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Wei Yu, Vincent Zhao, Yanping Huang, Andrew M. Dai, Hongkun Yu, Slav Petrov, Ed Huai hsin Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc Le, and Jason Wei. 2022. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416.
行数：174&lt;/li&gt;
&lt;/ul&gt;</content><author><name>麦克船长</name></author><category term="ai" /><category term="AI" /><category term="人工智能" /><category term="NLP" /><category term="自然语言处理" /><category term="神经网络" /><category term="LLM" /><category term="大型语言模型" /><category term="语言模型" /><category term="大模型" /><category term="Meta" /><category term="ChatGPT" /><category term="LLaMa" /><summary type="html">北京时间 2023 年 2 月 25 日 Meta AI 在其官网公开发布了 LLaMA（Large Language Model Meta AI）大型语言模型，包括 70 亿、130 亿、330 亿、650 亿 4 种参数规模，旨在推动 LLM 领域的小型化、平民化研究。Guillaume Lample 在其 Twitter 上声称：LLaMA 130 亿参数版本的表现，在大多数测试上优于 OPT 和 GPT-3 1750 亿参数版，650 亿的版本表现基本可以比肩 Chinchilla 700 亿参数、PaLM 5400 亿参数这些大模型。</summary></entry><entry><title type="html">design | Michael &amp;amp; Greta Anniversary</title><link href="https://www.mikecaptain.com/2023/02/13/mandg-anniversary-rings/" rel="alternate" type="text/html" title="design | Michael &amp;amp; Greta Anniversary" /><published>2023-02-13T08:53:57+00:00</published><updated>2023-02-13T08:53:57+00:00</updated><id>https://www.mikecaptain.com/2023/02/13/mandg-anniversary-rings</id><content type="html" xml:base="https://www.mikecaptain.com/2023/02/13/mandg-anniversary-rings/">&lt;p&gt;&lt;img src=&quot;/img/src/2023/02/mandg-ring-1.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023/02/mandg-ring-2.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;

Michael and Greta&apos;s one-year anniversary ring&lt;br /&gt;&lt;br /&gt;
Einjähriger Jahrestagsring von Michael und Greta&lt;br /&gt;&lt;br /&gt;
Bague d&apos;anniversaire d&apos;un an de Michael et Greta&lt;br /&gt;&lt;br /&gt;
マイケルとグレタの1周年記念リング&lt;br /&gt;&lt;br /&gt;
마이클과 그레타의 1주년 기념 반지&lt;br /&gt;&lt;br /&gt;
Anillo de aniversario de un año de Michael y Greta&lt;br /&gt;&lt;br /&gt;
Anel de aniversário de um ano de Michael e Greta&lt;br /&gt;&lt;br /&gt;
Anniversarium unius anni annuli Michaelis et Gretae&lt;br /&gt;&lt;br /&gt;
Кольцо на годовщину Майкла и Греты&lt;br /&gt;&lt;br /&gt;
Anello di anniversario di un anno di Michael e Greta&lt;br /&gt;&lt;br /&gt;

&lt;/div&gt;</content><author><name>Greta</name></author><category term="design" /><category term="思考" /><summary type="html">Michael and Greta&apos;s one-year anniversary ring! Einjähriger Jahrestagsring von Michael und Greta! Bague d&apos;anniversaire d&apos;un an de Michael et Greta! マイケルとグレタの1周年記念リング! 마이클과 그레타의 1주년 기념 반지! Anillo de aniversario de un año de Michael y Greta! Anel de aniversário de um ano de Michael e Greta! Anniversarium unius anni annuli Michaelis et Gretae! Кольцо на годовщину Майкла и Греты! Anello di anniversario di un anno di Michael e Greta!</summary></entry><entry><title type="html">麦克船长：国家网信办《互联网信息服务深度合成管理规定》解读</title><link href="https://www.mikecaptain.com/2023/02/06/cac-generative-information/" rel="alternate" type="text/html" title="麦克船长：国家网信办《互联网信息服务深度合成管理规定》解读" /><published>2023-02-06T15:24:58+00:00</published><updated>2023-02-06T15:24:58+00:00</updated><id>https://www.mikecaptain.com/2023/02/06/cac-generative-information</id><content type="html" xml:base="https://www.mikecaptain.com/2023/02/06/cac-generative-information/">&lt;h2 id=&quot;国家互联网信息办公室中华人民共和国工业和信息化部中华人民共和国公安部-令-第12号&quot;&gt;国家互联网信息办公室、中华人民共和国工业和信息化部、中华人民共和国公安部 令 第12号&lt;/h2&gt;

&lt;p&gt;《互联网信息服务深度合成管理规定》已经2022年11月3日国家互联网信息办公室2022年第21次室务会议审议通过，并经工业和信息化部、公安部同意，现予公布，自2023年1月10日起施行。&lt;/p&gt;

&lt;p&gt;国家互联网信息办公室主任 庄荣文&lt;/p&gt;

&lt;p&gt;工业和信息化部部长 金壮龙&lt;/p&gt;

&lt;p&gt;公安部部长 王小洪&lt;/p&gt;

&lt;p&gt;2022年11月25日&lt;/p&gt;

&lt;h2 id=&quot;互联网信息服务深度合成管理规定&quot;&gt;互联网信息服务深度合成管理规定&lt;/h2&gt;

&lt;h3 id=&quot;第一章-总则&quot;&gt;第一章 总则&lt;/h3&gt;

&lt;p&gt;第一条 为了加强互联网信息服务深度合成管理，弘扬社会主义核心价值观，维护国家安全和社会公共利益，保护公民、法人和其他组织的合法权益，根据《中华人民共和国网络安全法》、《中华人民共和国数据安全法》、《中华人民共和国个人信息保护法》、《互联网信息服务管理办法》等法律、行政法规，制定本规定。&lt;/p&gt;

&lt;p&gt;第二条 在中华人民共和国境内应用深度合成技术提供互联网信息服务（以下简称深度合成服务），适用本规定。法律、行政法规另有规定的，依照其规定。&lt;/p&gt;

&lt;p&gt;第三条 国家网信部门负责统筹协调全国深度合成服务的治理和相关监督管理工作。国务院电信主管部门、公安部门依据各自职责负责深度合成服务的监督管理工作。&lt;/p&gt;

&lt;p&gt;地方网信部门负责统筹协调本行政区域内的深度合成服务的治理和相关监督管理工作。地方电信主管部门、公安部门依据各自职责负责本行政区域内的深度合成服务的监督管理工作。&lt;/p&gt;

&lt;p&gt;第四条 提供深度合成服务，应当遵守法律法规，尊重社会公德和伦理道德，坚持正确政治方向、舆论导向、价值取向，促进深度合成服务向上向善。&lt;/p&gt;

&lt;p&gt;第五条 鼓励相关行业组织加强行业自律，建立健全行业标准、行业准则和自律管理制度，督促指导深度合成服务提供者和技术支持者制定完善业务规范、依法开展业务和接受社会监督。&lt;/p&gt;

&lt;h3 id=&quot;第二章-一般规定&quot;&gt;第二章 一般规定&lt;/h3&gt;

&lt;p&gt;第六条 任何组织和个人不得利用深度合成服务制作、复制、发布、传播法律、行政法规禁止的信息，不得利用深度合成服务从事危害国家安全和利益、损害国家形象、侵害社会公共利益、扰乱经济和社会秩序、侵犯他人合法权益等法律、行政法规禁止的活动。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;u&gt;深度合成服务提供者和使用者不得利用深度合成服务制作、复制、发布、传播虚假新闻信息。转载基于深度合成服务制作发布的新闻信息的，应当依法转载互联网新闻信息稿源单位发布的新闻信息&lt;/u&gt;&amp;gt;&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;第七条 深度合成服务提供者应当落实信息安全主体责任，建立健全用户注册、&lt;strong&gt;&lt;u&gt;算法机制机理审核、科技伦理审查&lt;/u&gt;&lt;/strong&gt;、信息发布审核、数据安全、个人信息保护、反电信网络诈骗、应急处置等管理制度，具有安全可控的技术保障措施。&lt;/p&gt;

&lt;p&gt;第八条 深度合成服务提供者应当制定和公开管理规则、平台公约，完善服务协议，依法依约履行管理责任，以显著方式提示深度合成服务技术支持者和使用者承担信息安全义务。&lt;/p&gt;

&lt;p&gt;第九条 深度合成服务提供者应当基于移动电话号码、身份证件号码、统一社会信用代码或者国家网络身份认证公共服务等方式，依法对深度合成服务使用者进行真实身份信息认证，&lt;strong&gt;&lt;u&gt;不得向未进行真实身份信息认证的深度合成服务使用者提供信息发布服务&lt;/u&gt;&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;第十条 深度合成服务提供者应当加强深度合成内容管理，采取&lt;strong&gt;&lt;u&gt;技术&lt;/u&gt;&lt;/strong&gt;或者人工方式对深度合成服务使用者的输入数据和合成结果进行审核。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;u&gt;深度合成服务提供者应当建立健全用于识别违法和不良信息的特征库，完善入库标准、规则和程序，记录并留存相关网络日志&lt;/u&gt;&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;深度合成服务提供者发现违法和不良信息的，应当依法采取处置措施，保存有关记录，及时向网信部门和有关主管部门报告；对相关深度合成服务使用者依法依约采取警示、限制功能、暂停服务、关闭账号等处置措施。&lt;/p&gt;

&lt;p&gt;第十一条 &lt;strong&gt;&lt;u&gt;深度合成服务提供者应当建立健全辟谣机制&lt;/u&gt;&lt;/strong&gt;，发现利用深度合成服务制作、复制、发布、传播虚假信息的，应当及时采取辟谣措施，保存有关记录，并向网信部门和有关主管部门报告。&lt;/p&gt;

&lt;p&gt;第十二条 深度合成服务提供者应当设置便捷的用户申诉和公众投诉、举报入口，公布处理流程和反馈时限，及时受理、处理和反馈处理结果。&lt;/p&gt;

&lt;p&gt;第十三条 互联网应用商店等应用程序分发平台应当落实上架审核、日常管理、应急处置等安全管理责任，核验深度合成类应用程序的安全评估、备案等情况；对违反国家有关规定的，应当及时采取不予上架、警示、暂停服务或者下架等处置措施。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;麦克船长解读：第二章整体就是告诉我们一句话，所有开发、分发深度合成产品的组织或个人，包括生成式 AI 的 SaaS 服务商（比如百度文心大模型）、to C 型应用平台/商店（比如华为应用商店/小米应用商店/微信小程序等）等，必须实现 Moderation 能力。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;第三章-数据和技术管理规范&quot;&gt;第三章 数据和技术管理规范&lt;/h3&gt;

&lt;p&gt;第十四条 深度合成服务提供者和技术支持者应当加强训练数据管理，采取必要措施保障训练数据安全；训练数据包含个人信息的，应当遵守个人信息保护的有关规定。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;u&gt;深度合成服务提供者和技术支持者提供人脸、人声等生物识别信息编辑功能的，应当提示深度合成服务使用者依法告知被编辑的个人，并取得其单独同意&lt;/u&gt;&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;第十五条 深度合成服务提供者和技术支持者应当加强技术管理，&lt;strong&gt;&lt;u&gt;定期审核、评估、验证生成合成类算法机制机理&lt;/u&gt;&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;深度合成服务提供者和技术支持者提供具有以下功能的模型、模板等工具的，&lt;strong&gt;&lt;u&gt;应当依法自行或者委托专业机构开展安全评估&lt;/u&gt;&lt;/strong&gt;：&lt;/p&gt;

&lt;p&gt;（一）生成或者编辑人脸、人声等生物识别信息的；&lt;/p&gt;

&lt;p&gt;（二）生成或者编辑可能涉及国家安全、国家形象、国家利益和社会公共利益的特殊物体、场景等非生物识别信息的。&lt;/p&gt;

&lt;p&gt;第十六条 深度合成服务提供者对使用其服务生成或者编辑的信息内容，应当采取技术措施添加不影响用户使用的标识，并依照法律、行政法规和国家有关规定保存日志信息。&lt;/p&gt;

&lt;p&gt;第十七条 深度合成服务提供者提供以下深度合成服务，可能导致公众混淆或者误认的，应当&lt;strong&gt;&lt;u&gt;在生成或者编辑的信息内容的合理位置、区域进行显著标识，向公众提示深度合成情况&lt;/u&gt;&lt;/strong&gt;：&lt;/p&gt;

&lt;p&gt;（一）智能对话、智能写作等模拟自然人进行文本的生成或者编辑服务；&lt;/p&gt;

&lt;p&gt;（二）合成人声、仿声等语音生成或者显著改变个人身份特征的编辑服务；&lt;/p&gt;

&lt;p&gt;（三）人脸生成、人脸替换、人脸操控、姿态操控等人物图像、视频生成或者显著改变个人身份特征的编辑服务；&lt;/p&gt;

&lt;p&gt;（四）沉浸式拟真场景等生成或者编辑服务；&lt;/p&gt;

&lt;p&gt;（五）其他具有生成或者显著改变信息内容功能的服务。&lt;/p&gt;

&lt;p&gt;深度合成服务提供者提供前款规定之外的深度合成服务的，&lt;strong&gt;&lt;u&gt;应当提供显著标识功能，并提示深度合成服务使用者可以进行显著标识&lt;/u&gt;&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;第十八条 任何组织和个人&lt;strong&gt;&lt;u&gt;不得采用技术手段删除、篡改、隐匿本规定第十六条和第十七条规定的深度合成标识&lt;/u&gt;&lt;/strong&gt;。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;麦克船长解读：1）必须有评估、验证合成算法机制；2）必须要有明确标识告诉使用者；3）生成类服务的范围，在第十七条的第五款里，用「其他」变成了一个什么都能装的筐。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;第四章-监督检查与法律责任&quot;&gt;第四章 监督检查与法律责任&lt;/h3&gt;

&lt;p&gt;第十九条 &lt;strong&gt;&lt;u&gt;具有舆论属性或者社会动员能力的深度合成服务提供者，应当按照《互联网信息服务算法推荐管理规定》履行备案和变更、注销备案手续&lt;/u&gt;&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;深度合成服务技术支持者应当参照前款规定履行备案和变更、注销备案手续。&lt;/p&gt;

&lt;p&gt;完成备案的深度合成服务提供者和技术支持者应当在其对外提供服务的网站、应用程序等的&lt;strong&gt;&lt;u&gt;显著位置标明其备案编号并提供公示信息链接&lt;/u&gt;&lt;/strong&gt;。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;麦克船长解读：有媒体舆论属性的产品（比如社交社区、直播短视频、新闻媒体类等），一定要按照《互联网信息服务算法推荐管理规定》备案，并在 APP、网站上公示备案编号并提供链接（类似之前的域名备案）。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;第二十条 深度合成服务提供者开发上线具有舆论属性或者社会动员能力的新产品、新应用、新功能的，应当按照国家有关规定开展安全评估。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;麦克船长解读：上述提到的产品迭代新能力时，也要再次安全评估。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;第二十一条 网信部门和电信主管部门、公安部门依据职责对深度合成服务开展监督检查。深度合成服务提供者和技术支持者应当依法予以配合，并提供必要的技术、数据等支持和协助。&lt;/p&gt;

&lt;p&gt;网信部门和有关主管部门发现深度合成服务存在较大信息安全风险的，可以按照职责依法要求深度合成服务提供者和技术支持者采取暂停信息更新、用户账号注册或者其他相关服务等措施。深度合成服务提供者和技术支持者应当按照要求采取措施，进行整改，消除隐患。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;麦克船长解读：网信部门、电信部门、公安部门都可以监管，都要配合。监管的整改可能会配合停止信息更新、停止账号注册等。这就跟食品生产企业的整改监管非常类似了，生产内容一样要被这样监管，有了合成能力本质上就是出现了内容工厂了。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;第二十二条 深度合成服务提供者和技术支持者违反本规定的，依照有关法律、行政法规的规定处罚；造成严重后果的，依法从重处罚。&lt;/p&gt;

&lt;p&gt;构成违反治安管理行为的，由公安机关依法给予治安管理处罚；构成犯罪的，依法追究刑事责任。&lt;/p&gt;

&lt;h3 id=&quot;第五章-附则&quot;&gt;第五章 附则&lt;/h3&gt;

&lt;p&gt;第二十三条 本规定中下列用语的含义：&lt;/p&gt;

&lt;p&gt;深度合成技术，是指利用深度学习、虚拟现实等生成合成类算法制作文本、图像、音频、视频、虚拟场景等网络信息的技术，包括但不限于：&lt;/p&gt;

&lt;p&gt;（一）篇章生成、文本风格转换、问答对话等生成或者编辑文本内容的技术；&lt;/p&gt;

&lt;p&gt;（二）文本转语音、语音转换、语音属性编辑等生成或者编辑语音内容的技术；&lt;/p&gt;

&lt;p&gt;（三）音乐生成、场景声编辑等生成或者编辑非语音内容的技术；&lt;/p&gt;

&lt;p&gt;（四）人脸生成、人脸替换、人物属性编辑、人脸操控、姿态操控等生成或者编辑图像、视频内容中生物特征的技术；&lt;/p&gt;

&lt;p&gt;（五）图像生成、图像增强、图像修复等生成或者编辑图像、视频内容中非生物特征的技术；&lt;/p&gt;

&lt;p&gt;（六）三维重建、数字仿真等生成或者编辑数字人物、虚拟场景的技术。&lt;/p&gt;

&lt;p&gt;深度合成服务提供者，是指提供深度合成服务的组织、个人。&lt;/p&gt;

&lt;p&gt;深度合成服务技术支持者，是指为深度合成服务提供技术支持的组织、个人。&lt;/p&gt;

&lt;p&gt;深度合成服务使用者，是指使用深度合成服务制作、复制、发布、传播信息的组织、个人。&lt;/p&gt;

&lt;p&gt;训练数据，是指被用于训练机器学习模型的标注或者基准数据集。&lt;/p&gt;

&lt;p&gt;沉浸式拟真场景，是指应用深度合成技术生成或者编辑的、可供参与者体验或者互动的、具有高度真实感的虚拟场景。&lt;/p&gt;

&lt;p&gt;第二十四条 深度合成服务提供者和技术支持者从事网络出版服务、网络文化活动和网络视听节目服务的，应当同时符合新闻出版、文化和旅游、广播电视主管部门的规定。&lt;/p&gt;

&lt;p&gt;第二十五条 本规定自2023年1月10日起施行。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;麦克船长解读：不仅限于 AIGC，虚拟现实生成也被该规定覆盖。一些可能会想到的问题如下。&lt;/p&gt;
  &lt;ol&gt;
    &lt;li&gt;比如我只用 AI 改改文章风格不用被监管吧？否，也按本规定监管。&lt;/li&gt;
    &lt;li&gt;艺术类，非写实类的图像生成，不用被监管吧？否，也被本规定监管，不是只有写实的内容才有可能不合规。&lt;/li&gt;
    &lt;li&gt;用 AI 构建 3D 模型，主要用于装饰、装修、装潢的，不用被监管吧？否，这个能力有了，就不限于生成范围了，会有涉及监管的应用，所以也要被监管。&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;参考&quot;&gt;参考：&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;http://www.cac.gov.cn/2022-12/11/c_1672221949354811.htm&lt;/li&gt;
  &lt;li&gt;https://tisi.org/14419&lt;/li&gt;
&lt;/ul&gt;</content><author><name>麦克船长</name></author><category term="ai" /><category term="AI" /><category term="人工智能" /><category term="NLP" /><category term="自然语言处理" /><category term="神经网络" /><category term="LLM" /><category term="大型语言模型" /><category term="语言模型" /><category term="大模型" /><category term="AGI" /><category term="通用人工智能" /><summary type="html">国家互联网信息办公室、中华人民共和国工业和信息化部、中华人民共和国公安部 令 第12号</summary></entry><entry><title type="html">麦克船长的 OpenAI 模型 API 官方文档入门解读</title><link href="https://www.mikecaptain.com/2023/01/23/openai-official-doc/" rel="alternate" type="text/html" title="麦克船长的 OpenAI 模型 API 官方文档入门解读" /><published>2023-01-23T22:24:58+00:00</published><updated>2023-01-23T22:24:58+00:00</updated><id>https://www.mikecaptain.com/2023/01/23/openai-official-doc</id><content type="html" xml:base="https://www.mikecaptain.com/2023/01/23/openai-official-doc/">&lt;p&gt;&lt;strong&gt;本文目录&lt;/strong&gt;&lt;/p&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#一先大概看看-openai-目前最拿得出手的三个商业化产品用起来啥样&quot; id=&quot;markdown-toc-一先大概看看-openai-目前最拿得出手的三个商业化产品用起来啥样&quot;&gt;一、先大概看看 OpenAI 目前最拿得出手的三个商业化产品用起来啥样&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#1执行各种自然语言任务的-gpt-3&quot; id=&quot;markdown-toc-1执行各种自然语言任务的-gpt-3&quot;&gt;1、执行各种自然语言任务的 GPT-3&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#示例-1copywriting&quot; id=&quot;markdown-toc-示例-1copywriting&quot;&gt;示例 1：Copywriting&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#示例-2summarization&quot; id=&quot;markdown-toc-示例-2summarization&quot;&gt;示例 2：Summarization&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#示例-3parsing-unstructured-text&quot; id=&quot;markdown-toc-示例-3parsing-unstructured-text&quot;&gt;示例 3：Parsing Unstructured Text&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#示例-4classification&quot; id=&quot;markdown-toc-示例-4classification&quot;&gt;示例 4：Classification&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#示例-5translation&quot; id=&quot;markdown-toc-示例-5translation&quot;&gt;示例 5：Translation&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#2将自然语言翻译成代码的-codex&quot; id=&quot;markdown-toc-2将自然语言翻译成代码的-codex&quot;&gt;2、将自然语言翻译成代码的 Codex&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#示例-6用自然语言写-sql&quot; id=&quot;markdown-toc-示例-6用自然语言写-sql&quot;&gt;示例 6：用自然语言写 SQL&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#示例-7用自然语言调用一个-api&quot; id=&quot;markdown-toc-示例-7用自然语言调用一个-api&quot;&gt;示例 7：用自然语言调用一个 API&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#示例-8用自然语言续写代码&quot; id=&quot;markdown-toc-示例-8用自然语言续写代码&quot;&gt;示例 8：用自然语言续写代码&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#3创建和编辑原始图像的-dalle&quot; id=&quot;markdown-toc-3创建和编辑原始图像的-dalle&quot;&gt;3、创建和编辑原始图像的 DALL·E&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#二openai-api-总览性介绍&quot; id=&quot;markdown-toc-二openai-api-总览性介绍&quot;&gt;二、OpenAI API 总览性介绍&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#1一些关键概念&quot; id=&quot;markdown-toc-1一些关键概念&quot;&gt;1、一些关键概念&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#2模型&quot; id=&quot;markdown-toc-2模型&quot;&gt;2、模型&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#三主要-api-介绍及代码示例&quot; id=&quot;markdown-toc-三主要-api-介绍及代码示例&quot;&gt;三、主要 API 介绍及代码示例&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#1text-completion-任务&quot; id=&quot;markdown-toc-1text-completion-任务&quot;&gt;1、Text Completion 任务&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#2text-edit-任务&quot; id=&quot;markdown-toc-2text-edit-任务&quot;&gt;2、Text Edit 任务&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#3image-create-任务beta&quot; id=&quot;markdown-toc-3image-create-任务beta&quot;&gt;3、Image Create 任务（Beta）&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#4image-edit-任务&quot; id=&quot;markdown-toc-4image-edit-任务&quot;&gt;4、Image Edit 任务&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#5审查moderation&quot; id=&quot;markdown-toc-5审查moderation&quot;&gt;5、审查（Moderation）&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#四微调fine-tuning&quot; id=&quot;markdown-toc-四微调fine-tuning&quot;&gt;四、微调（Fine-tuning）&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#1创建一个-fine-tune-模型&quot; id=&quot;markdown-toc-1创建一个-fine-tune-模型&quot;&gt;1、创建一个 fine-tune 模型&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#2使用-fine-tuned-模型&quot; id=&quot;markdown-toc-2使用-fine-tuned-模型&quot;&gt;2、使用 fine-tuned 模型&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#3删掉一个-fine-tuned-模型&quot; id=&quot;markdown-toc-3删掉一个-fine-tuned-模型&quot;&gt;3、删掉一个 fine-tuned 模型&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#4一个-fine-tuned-模型之上继续-fine-tune&quot; id=&quot;markdown-toc-4一个-fine-tuned-模型之上继续-fine-tune&quot;&gt;4、一个 fine-tuned 模型之上继续 fine-tune&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#参考&quot; id=&quot;markdown-toc-参考&quot;&gt;参考&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;一先大概看看-openai-目前最拿得出手的三个商业化产品用起来啥样&quot;&gt;一、先大概看看 OpenAI 目前最拿得出手的三个商业化产品用起来啥样&lt;/h2&gt;

&lt;p&gt;在 OpenAI 的 API 官方首页的大标题写着的是「用 OpenAI 的强力模型构建下一代应用（Build next-gen apps with OpenAI’s powerful models）」，并直接点出了自己最拿得出手的三个商业化产品 GPT-3、Codex、DALL·E。&lt;/p&gt;

&lt;p&gt;并提到自己的 API 调用非常简单：&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;openai&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;openai&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Completion&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;create&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;engine&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;davinci&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;prompt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Make a list of astronomical observatories:&quot;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;引入 OpenAI 的 python 依赖库后，调用时指定用哪个引擎、提示词是什么即可。轻松一些，我们先来看一些示例吧。&lt;/p&gt;

&lt;h3 id=&quot;1执行各种自然语言任务的-gpt-3&quot;&gt;1、执行各种自然语言任务的 GPT-3&lt;/h3&gt;

&lt;h4 id=&quot;示例-1copywriting&quot;&gt;示例 1：Copywriting&lt;/h4&gt;

&lt;p&gt;人类输入：&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Create promo copy for the FamilyTime mobile application. It allows unlimited uploading, special filters and makes it easy to create albums of photos and videos. It runs on iOS and Android:
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;GPT-3 输出：&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;FamilyTime is the perfect app to share and create memories with your family. FamilyTime is a free, unlimited photo and video editor that allows you to create albums of your memories. It has a special filter for the perfect summer look and is so simple to use.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;示例-2summarization&quot;&gt;示例 2：Summarization&lt;/h4&gt;

&lt;p&gt;人类输入：&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Summarize this email into a single sentence:

Dear Olivia,

The solar energy conference went great. New Horizon Manufacturing wants to meet with us to talk about our photovoltaic window system we’re about to launch.
I think it would be great to talk this Tuesday.

Best,
Allison

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;GPT-3 输出：&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Allison is excited to meet with New Horizon Manufacturing to discuss their photovoltaic window system.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;示例-3parsing-unstructured-text&quot;&gt;示例 3：Parsing Unstructured Text&lt;/h4&gt;

&lt;p&gt;人类输入：&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Answer the following questions about this customer email:

I’m wondering if you could provide me with information about your cloud services for companies. I’m CTO for Moon Door, a movie production company and want to know if it can help us manage our different projects that are in production.

Questions:
1. What industry is the company
2. What is the name of the company
3. What are they interested in
4. What is the customer’s position

Answers:
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;GPT-3 输出：&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;1. The company is in the movie production industry.
2. The company is Moon Door.
3. The customer is interested in cloud services for companies.
4. The customer is the CTO.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;示例-4classification&quot;&gt;示例 4：Classification&lt;/h4&gt;

&lt;p&gt;人类输入：&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Choose a genre category for each book 1. The Hunger Games, 2. The Kite Runner 3. A Wrinkle in Time (“fiction”, “young adult”, “science fiction”, “fantasy”, “other”) and make a list of the book and its genre:
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;GPT-3 输出：&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;1. The Hunger Games: young adult, fiction
2. The Kite Runner: fiction, young adult
3. A Wrinkle in Time: science fiction, fantasy, other
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;示例-5translation&quot;&gt;示例 5：Translation&lt;/h4&gt;

&lt;p&gt;人类输入：&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Translate this into French: Where can I find a bookstore?
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;GPT-3 输出：&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Où puis-je trouver un magasin de livres?
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;2将自然语言翻译成代码的-codex&quot;&gt;2、将自然语言翻译成代码的 Codex&lt;/h3&gt;

&lt;h4 id=&quot;示例-6用自然语言写-sql&quot;&gt;示例 6：用自然语言写 SQL&lt;/h4&gt;

&lt;p&gt;人类输入：&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Table customers, columns = [CustomerId, FirstName, LastName, Company, Address, City, State, Country, PostalCode, Phone, Fax, Email, SupportRepId, TotalSpend]

Create a MySQL query for all customers in Texas who have spent over five thousand dollars.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Codex 输出：&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;query = &quot;SELECT * FROM customers WHERE State = &apos;TX&apos; AND TotalSpend &amp;gt; 5000&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;示例-7用自然语言调用一个-api&quot;&gt;示例 7：用自然语言调用一个 API&lt;/h4&gt;

&lt;p&gt;人类输入：&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Python 3
Get the current value of a Bitcoin in US dollars using the bitcoincharts api
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Codex 输出：&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;requests&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;json&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;get_bitcoin_price&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;url&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&apos;http://api.bitcoincharts.com/v1/weighted_prices.json&apos;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;response&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;requests&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;url&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;json&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;USD&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;7d&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__name__&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&apos;__main__&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_bitcoin_price&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;示例-8用自然语言续写代码&quot;&gt;示例 8：用自然语言续写代码&lt;/h4&gt;

&lt;p&gt;人类输入：&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Extract all the html from the string and replace the tags with &apos;&apos;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Codex 输出：&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def extract_html(text):
    return re.sub(&apos;&amp;amp;lt;[^&amp;amp;lt;]+?&amp;amp;gt;&apos;, &apos;&apos;, text)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;3创建和编辑原始图像的-dalle&quot;&gt;3、创建和编辑原始图像的 DALL·E&lt;/h3&gt;

&lt;p&gt;超过 300 万人已经在使用 DALL·E 来放大他们的创造力并加快他们的工作流程，每天生成超过 400 万张图像。&lt;/p&gt;

&lt;h2 id=&quot;二openai-api-总览性介绍&quot;&gt;二、OpenAI API 总览性介绍&lt;/h2&gt;

&lt;h3 id=&quot;1一些关键概念&quot;&gt;1、一些关键概念&lt;/h3&gt;

&lt;p&gt;关于 prompt 和 completion：OpenAI 提到一个理念：「设计提示语，就相当于在用一些指令和少量例子给模型编程」。另外 OpenAI 还强调了在目标任务上的区别，就是 OpenAI 的 NLP 模型与其他 NLP 模型很大的一个区别是，它不是设计用来解决单一类型任务的，而是可以解决几乎各种类型的 NLP 任务，包括但不限于文本生成（content generation）、代码生成（code generation）、总结（summarization）、扩写（expansion）、对话（conversation）、创意写作（creative wrting）、风格转换（style transfer）等。&lt;/p&gt;

&lt;p&gt;关于 token：我们理解和处理文本，是把文本先打碎成 token。以英文文本为例，token 可以是单词，也可以词根（一些字母组合），比如单词「hamburger」可能会被打碎成「ham」、「bur」、「ger」这几个 tokens。再比如「pear」这个单词，可能就会单独作为一个 token 不再打碎了。还有些 token 可能会以「空格」开头，比如「 hello」、「 bye」。一个大概的经验是，通常英文文本里 1 token 有 4 个字母或者 0.75 个单词。使用时的一个限制是，最好你的提示（prompt）或生成内容，不要超过 2048 个 tokens，大概相当于 1500 个单词。&lt;/p&gt;

&lt;p&gt;关于 model：目前 OpenAI 有基于 GPT-3.5 的基础模型 Turbo 和这些基于 GPT-3 的基础模型 Davinci、Curie、Babbage、Ada 开放 API，另外 Codex 系列是 GPT-3 的后代，是用「自然语言 + 代码」训练的。&lt;/p&gt;

&lt;h3 id=&quot;2模型&quot;&gt;2、模型&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;text-davinci-003&lt;/code&gt;：最大请求 4000 tokens，训练数据 up to 2021 年 6 月，能做几乎所有 NLP 任务。&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;text-curie-001&lt;/code&gt;：最大请求 2048 tokens，训练数据 up to 2019 年 10 月，比 davinci 要弱一点，但是速度更快、更便宜。&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;text-babbage-001&lt;/code&gt;：最大请求和训练数据和 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;text-curie-001&lt;/code&gt; 一样，一些比较直接的任务（straightforward tasks），比 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;text-curie-001&lt;/code&gt; 更快、更便宜。&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;text-ada-001&lt;/code&gt;：最大请求和训练数据和 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;text-curie-001&lt;/code&gt; 一样，一些非常简单的任务，这些模型里最快、最便宜的。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这四个模型根据输入的 token 数量做的如下定价：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;基础模型使用 0.000&lt;strong&gt;4&lt;/strong&gt; USD/1K tokens，Ada&lt;/li&gt;
  &lt;li&gt;基础模型使用 0.000&lt;strong&gt;5&lt;/strong&gt; USD/1K tokens，Babbage&lt;/li&gt;
  &lt;li&gt;基础模型使用 0.00&lt;strong&gt;20&lt;/strong&gt; USD/1K tokens，Curie&lt;/li&gt;
  &lt;li&gt;基础模型使用 0.0&lt;strong&gt;200&lt;/strong&gt; USD/1K tokens，Davinci&lt;/li&gt;
  &lt;li&gt;呼出模型使用 0.00&lt;strong&gt;20&lt;/strong&gt; USD/1K tokens，Turbo&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;从定价上看，Ada 和 Babbage 基本没有差多少。另外命名上，可以看出 OpenAI 有意地给他们取了 ABCD 开头的名字。另外你也可以 finetune 你自己的模型，对于 fine-tuned models 如下收费：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Finetune 训练费 0.000&lt;strong&gt;4&lt;/strong&gt; USD/1K tokens，使用费 0.00&lt;strong&gt;16&lt;/strong&gt; USD/1K tokens，Ada&lt;/li&gt;
  &lt;li&gt;Finetune 训练费 0.000&lt;strong&gt;6&lt;/strong&gt; USD/1K tokens，使用费 0.00&lt;strong&gt;24&lt;/strong&gt; USD/1K tokens，Babbage&lt;/li&gt;
  &lt;li&gt;Finetune 训练费 0.00&lt;strong&gt;30&lt;/strong&gt; USD/1K tokens，使用费 0.0&lt;strong&gt;120&lt;/strong&gt; USD/1K tokens，Curie&lt;/li&gt;
  &lt;li&gt;Finetune 训练费 0.0&lt;strong&gt;300&lt;/strong&gt; USD/1K tokens，使用费 0.&lt;strong&gt;1200&lt;/strong&gt; USD/1K tokens，Davinci&lt;/li&gt;
  &lt;li&gt;暂未提供 Turbo 的 finetune。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在 OpenAI 的 PlayGround 你可以试试：&lt;a href=&quot;https://platform.openai.com/playground/p/default-chat&quot;&gt;https://platform.openai.com/playground/p/default-chat&lt;/a&gt; 。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023/2023-01-24-openai-api.jpg&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;三主要-api-介绍及代码示例&quot;&gt;三、主要 API 介绍及代码示例&lt;/h2&gt;

&lt;p&gt;安装 OpenAI 的 python 库，参考 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;https://anaconda.org/conda-forge/openai&lt;/code&gt;：&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;mikecaptain@local &lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;conda &lt;span class=&quot;nb&quot;&gt;install&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-c&lt;/span&gt; conda-forge openai
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;在 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;https://platform.openai.com/account/api-keys&lt;/code&gt; 创建自己的 API。完成这两步后就可以编写代码尝试一下：&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;os&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;openai&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;openai&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;api_key&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;getenv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;OPENAI_API_KEY&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;openai&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;会打印出 OpenAI 的各个 models 的一些信息、权限等等。&lt;/p&gt;

&lt;h3 id=&quot;1text-completion-任务&quot;&gt;1、Text Completion 任务&lt;/h3&gt;

&lt;p&gt;下面这个例子会简单调用一下 completion，并打印出结果，用了一句需要你自己编写的 prompt：&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;os&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;openai&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;openai&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;api_key&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;getenv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;OPENAI_API_KEY&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;text_prompt&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;In a shocking turn of events, scientists have discovered that &quot;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;completion&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;openai&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Completion&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;create&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;text-davinci-002&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;prompt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text_prompt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;max_tokens&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;stop&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;temperature&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;generated_text&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;completion&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;choices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;generated_text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;这里用到了最重要的 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;openai.Completion&lt;/code&gt;，其 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;create&lt;/code&gt; 函数的参数解释如下：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;model&lt;/code&gt;：之前 OpenAI 把它叫「engine」，后来给 deprecated 了，现在都是用 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;model&lt;/code&gt;，所有的可用 models 可以通过 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;open.Model.list()&lt;/code&gt; 来查看。&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;prompt&lt;/code&gt;：&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;string&lt;/code&gt; 类型，就是输入数据。&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;suffix&lt;/code&gt;：&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;string&lt;/code&gt; 类型，生成文本的结束符。&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;max_tokens&lt;/code&gt;：&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;integer&lt;/code&gt; 类型，生成文本的最大 tokens 数。&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;n&lt;/code&gt;：&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;integer&lt;/code&gt; 类型，表示你要产生几个不同的输出结果。比如设置 3 就会得到 3 个不同的结果，以便您可以从中选择最合适的一个。&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;stop&lt;/code&gt;：&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;string&lt;/code&gt; 类型，用于指定模型何时应该停止生成文本。当模型在生成的文本中遇到 stop 字符串时，它将停止生成文本。ChatGPT 推出后迭代过一版增加了「stop generating」就是用的这个参数。&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;temperature&lt;/code&gt;：&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;number&lt;/code&gt; 类型，这是 NLP 模型里常见的一个超参数。这个参数，来自于统计热力学的概念，温度越高表示系统的熵越高、混乱度越高、随机性越强，这里的 temperature 也是值越高输出结果的随机性也越高。这样如果 temperature 设置得很低，生成的结果可能更正确，但没有多少创造性和随机性。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;2text-edit-任务&quot;&gt;2、Text Edit 任务&lt;/h3&gt;

&lt;p&gt;Completion 类任务，通俗点理解的话，完形填空、句子补齐、写作文、翻译 …… 都算 Completion，就是无中生有。而对于已经有的内容，做修改，就是 OpenAI 的 API 里的「Edit」类的任务了。&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;os&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;openai&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;openai&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;api_key&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;getenv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;OPENAI_API_KEY&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;openai&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Edit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;create&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;text-davinci-edit-001&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;The qick brown fox jumps over the layz dog.&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;instruction&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Fix the spelling mistakes&quot;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;调用 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;openai.Edit.create&lt;/code&gt;，用 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;text-davinci-edit-001&lt;/code&gt; 模型，输入一句有拼写错误的英文「The qick brown fox jumps over the layz dog.」，并提供一句指令 instruction「Fix the spelling mistakes」。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;instruction&lt;/code&gt;：要告诉模型如何修改，&lt;strong&gt;其实这句话就是新时代的「programming」了&lt;/strong&gt;。&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;temperature&lt;/code&gt;：默认是 0，对于纠正拼写类的任务，我们用默认 0 就可以了，不需要什么创造性和随机性。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;3image-create-任务beta&quot;&gt;3、Image Create 任务（Beta）&lt;/h3&gt;

&lt;p&gt;截止 2023 年年初 1 月份，这个 API 还是 beta，我们看个例子：&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;os&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;openai&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;openai&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;api_key&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;getenv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;OPENAI_API_KEY&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;openai&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;create&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;prompt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;A cute baby sea otter&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;1024x1024&quot;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;这也是一个 OpenAI 官网的例子。大家可能看到这里，船长没有指定 model，但是可以想到一定用的是 DALL·E，因为它没有像 GPT-3 一样提供很多版本的选择，所以就不需要传参数了。这个程序就是生成一个 1024x1024 的图片。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;prompt&lt;/code&gt;：就是输入的提示语，返回的数据里，会告诉你生成的图片的 URL.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;n&lt;/code&gt;：是图片结果数量，最多 10，默认 1.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;4image-edit-任务&quot;&gt;4、Image Edit 任务&lt;/h3&gt;

&lt;p&gt;给定一个图片，OpenAI 也可以来修改指定区域：&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;os&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;openai&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;openai&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;api_key&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;getenv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;OPENAI_API_KEY&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;openai&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;create_edit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;otter.png&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;rb&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;mask&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;mask.png&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;rb&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;prompt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;A cute baby sea otter wearing a beret&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;1024x1024&quot;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;image&lt;/code&gt;：这里对输入图片有要求，必须是正方形的！另外不能超过 4MB，还得是 PNG。&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mask&lt;/code&gt;：还可以提供掩码图片（叫什么比较合适，掩图？哈哈）。如果不提供的话，&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;image&lt;/code&gt; 里就必须有透明的部分（必须全透明，即 alpha = 0），那个透明部分就是被用来 Edit 的。如果有 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mask&lt;/code&gt; 则透明部分用来做「掩图」来改 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;image&lt;/code&gt;。&lt;/li&gt;
  &lt;li&gt;同样地，结果图片的 URL 会返回给你。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;5审查moderation&quot;&gt;5、审查（Moderation）&lt;/h3&gt;

&lt;p&gt;Moderation 用来审查内容是否符合 OpenAI 的内容政策，快速使用的方式如下：&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;response&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;openai&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Moderation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;create&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Sample text goes here&quot;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;output&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;results&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;API 官网给出我们如下的返回结果示例：&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;&quot;id&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;modr-XXXXX&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;&quot;model&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;text-moderation-001&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;&quot;results&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
      &lt;span class=&quot;s&quot;&gt;&quot;categories&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&quot;hate&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;false&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&quot;hate/threatening&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;false&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&quot;self-harm&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;false&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&quot;sexual&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;false&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&quot;sexual/minors&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;false&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&quot;violence&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;false&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&quot;violence/graphic&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;false&lt;/span&gt;
      &lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;
      &lt;span class=&quot;s&quot;&gt;&quot;category_scores&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&quot;hate&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.18805529177188873&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&quot;hate/threatening&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.0001250059431185946&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&quot;self-harm&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.0003706029092427343&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&quot;sexual&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.0008735615410842001&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&quot;sexual/minors&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.0007470346172340214&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&quot;violence&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.0041268812492489815&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&quot;violence/graphic&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.00023186142789199948&lt;/span&gt;
      &lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;
      &lt;span class=&quot;s&quot;&gt;&quot;flagged&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;false&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;输入参数很简单，关键看返回的输出结果。OpenAI 对于包含哪类不适内容，做了比较详尽的分类，比如对于色情内容，也分成了未成年色情和易引起性兴奋的内容。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;hate&lt;/code&gt;：是否包含基于种族、性别、民族、宗教、国籍、性取向、残疾状况或种姓表达、煽动或促进仇恨的内容，如果没有则是 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;false&lt;/code&gt;，否则为 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;true&lt;/code&gt;。&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;hate/threatening&lt;/code&gt;：是否包含仇恨内容还包括对目标群体的暴力或严重伤害，没有则 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;false&lt;/code&gt;，包含则值为 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;true&lt;/code&gt;。&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;self-harm&lt;/code&gt;：是否包含提倡、鼓励或描述自残行为（例如自杀、割伤和饮食失调）的内容，没有则 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;false&lt;/code&gt;，否则 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;true&lt;/code&gt;。&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sexual&lt;/code&gt;：是否包含意在引起性兴奋的内容，例如对性活动的描述，或宣传性服务（不包括性教育和健康）的内容，没有则 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;false&lt;/code&gt;，否则 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;true&lt;/code&gt;。&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sexual/minors&lt;/code&gt;：是否包含包含 18 岁以下个人的色情内容，没有则 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;false&lt;/code&gt;，否则 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;true&lt;/code&gt;。&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;violence&lt;/code&gt;：是否包含宣扬或美化暴力或颂扬他人的痛苦或屈辱的内容，没有为 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;false&lt;/code&gt;，否则 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;true&lt;/code&gt;。&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;violence/graphic&lt;/code&gt;：是否包含以极端的画面细节描绘死亡、暴力或严重身体伤害的暴力内容，没有 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;false&lt;/code&gt;，否则 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;true&lt;/code&gt;。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;显然，对于使用 OpenAI 生成内容的场景下如果需要用到 Moderation，则是免费调用的。如果你不是对 OpenAI 的输入 &amp;amp; 生成场景，而是自己的其他内容想白嫖 Moderation API 是不可能的。但是我们也注意到，这里其实没有整治敏感的分类，因为 OpenAI 没有考虑具体的使用者所处的政体或政治环境，而且这些尺度是比较容易变化的，并且有一些可能并不是普适性的理念，因此某些国家的使用者要额外配套自己的内容审查能力。&lt;/p&gt;

&lt;h2 id=&quot;四微调fine-tuning&quot;&gt;四、微调（Fine-tuning）&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Few-shot learning 是什么？&lt;/strong&gt;：GPT-3 用了互联网上的海量文本数据训练，所以当你给少量示例（a promopt with just a few examples）时，GPT-3 会从「直觉上」知道你大概是想要解决什么任务，然后给出一些大概齐的反馈内容作为 completion，这通常就被叫做「few-shot learning」或者「few-shot prompting」。&lt;/p&gt;

&lt;p&gt;而如果你提供一些针对目标任务的训练数据，很可能可以实现没有 examples 也可以执行任务，也就是使用时连「few-shot learning」都免了。OpenAI 也提供了让用户自己 fine-tune 模型的接口，自己 fine-tune 的好处是：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;高质量&lt;/strong&gt;：这是显然的，比「设计提示（prompt design）」得到的结果质量更高。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;相当于批量 prompt&lt;/strong&gt;：可以比 prompt 给模型更多的 examples，比如用一个文件，里面包含大量用于 fine-tuning 的输入数据。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;更省&lt;/strong&gt;：可以更省 tokens，也就更省钱。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;更快&lt;/strong&gt;：更低的延迟的请求响应。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;步骤和价格&lt;/strong&gt;方面，Fine-tune 一共三步：上传用于 fine-tune 的数据、用数据 fine-tune 模型、使用属于你自己的 fine-tune 过的模型。从定价上我们看到 Fine-tune 后的模型使用费用基本翻了 4~6 倍，可以说相比基本模型的使用，是非常贵了。&lt;/p&gt;

&lt;p&gt;另外 OpenAI 也支持你对一个 fine-tune 过的模型继续 fine-tune，而不用从头开始。目前 davinci、curie、babbage、ada 都支持 fine-tuning。训练数据的格式也很简单，就是一组 prompt-completion 的 JSONL 文件，just like this：&lt;/p&gt;

&lt;div class=&quot;language-json highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;prompt&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&amp;lt;prompt text&amp;gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;completion&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&amp;lt;ideal generated text&amp;gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;prompt&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&amp;lt;prompt text&amp;gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;completion&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&amp;lt;ideal generated text&amp;gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;prompt&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&amp;lt;prompt text&amp;gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;completion&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&amp;lt;ideal generated text&amp;gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;...&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Fine-tune 的 example 与 few-shot learning 的最大区别：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;few-shot learning 要给出详尽的 instruction 来描述任务&lt;/li&gt;
  &lt;li&gt;few-shot learning 的一个 prompt 是在使用时给出的，所以一个 prompt 大概率会带多个 examples（相对详细）；而 fine-tune 的 example 都是一些简单直接的 prompt 以及直接对应的 completion。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;OpenAI 建议 fine-tune 的 examples 数量至少几百（a couple hundred）。另外 fine-tune 也符合 scaling law，基本上 fine-tune 的数据集成倍上翻的话，效果是线性增长的。&lt;/p&gt;

&lt;h3 id=&quot;1创建一个-fine-tune-模型&quot;&gt;1、创建一个 fine-tune 模型&lt;/h3&gt;

&lt;p&gt;CLI 下运行如下命令，其中 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;TRAIN_FILE_ID_OR_PATH&amp;gt;&lt;/code&gt; 是你的训练数据文件，&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;BASE_MODEL&amp;gt;&lt;/code&gt; 是你要用的模型，具体的参数可以用 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ada&lt;/code&gt;、&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;babbage&lt;/code&gt;、&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;curie&lt;/code&gt; 和 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;davinci&lt;/code&gt;。&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;mikecaptain@local &lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;openai api fine_tunes.create &lt;span class=&quot;nt&quot;&gt;-t&lt;/span&gt; &amp;lt;TRAIN_FILE_ID_OR_PATH&amp;gt; &lt;span class=&quot;nt&quot;&gt;-m&lt;/span&gt; &amp;lt;BASE_MODEL&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;这句命令让 OpenAI 不仅基于 base model 创建了一个模型，而且开始运行训练任务。训练任务可能会花费几分钟、几小时甚至根据，取决于你的训练集和模型选择。训练任务可能会被 OpenAI 排队，不一定马上开始运行。如果过程中被打断了，可以如下继续：&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;mikecaptain@local &lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;openai api fine_tunes.follow &lt;span class=&quot;nt&quot;&gt;-i&lt;/span&gt; &amp;lt;YOUR_FINE_TUNE_JOB_ID&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;保存一个 fine-tune job 的命令如下：&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;mikecaptain@local &lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;openai api fine_tunes.get &lt;span class=&quot;nt&quot;&gt;-i&lt;/span&gt; &amp;lt;YOUR_FINE_TUNE_JOB_ID&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;取消一个 fine-tune job 的命令如下：&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;mikecaptain@local &lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;openai api fine_tunes.cancel &lt;span class=&quot;nt&quot;&gt;-i&lt;/span&gt; &amp;lt;YOUR_FINE_TUNE_JOB_ID&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;2使用-fine-tuned-模型&quot;&gt;2、使用 fine-tuned 模型&lt;/h3&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;openai&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;openai&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Completion&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;create&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;FINE_TUNED_MODEL&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;prompt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;YOUR_PROMPT&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;3删掉一个-fine-tuned-模型&quot;&gt;3、删掉一个 fine-tuned 模型&lt;/h3&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;openai&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;openai&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;delete&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;FINE_TUNED_MODEL&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;4一个-fine-tuned-模型之上继续-fine-tune&quot;&gt;4、一个 fine-tuned 模型之上继续 fine-tune&lt;/h3&gt;

&lt;p&gt;如果你微调了一个模型，现在又有为的训练数据想要合并进来，可以基于已 fine-tuned 模型继续微调，无需从头再全部训练一遍。唯一要做的，就是在创建新的 fine-tune job 时传入已 fine-tune 的模型名称，替代&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;BASE_MODEL&amp;gt;&lt;/code&gt;（例如 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-m curie:ft-&amp;lt;org&amp;gt;-&amp;lt;date&amp;gt;&lt;/code&gt;），不必更改其他训练参数。&lt;/p&gt;

&lt;p&gt;有一个要注意的，如果新增的训练数据比以前的训练数据规模小得多，那最好把 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;learning_rate_multiplier&lt;/code&gt; 减少 2 到 4 倍，否则很可能跳过了最优解。&lt;/p&gt;

&lt;h2 id=&quot;参考&quot;&gt;参考&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;https://openai.com/api/&lt;/li&gt;
  &lt;li&gt;https://developer.aliyun.com/article/933516&lt;/li&gt;
&lt;/ul&gt;</content><author><name>麦克船长</name></author><category term="ai" /><category term="AI" /><category term="人工智能" /><category term="NLP" /><category term="自然语言处理" /><category term="神经网络" /><category term="LLM" /><category term="大型语言模型" /><category term="语言模型" /><category term="大模型" /><category term="GPT-3" /><category term="OpenAI" /><category term="davinci" /><category term="curie" /><category term="ada" /><category term="babbage" /><category term="fine-tune" /><category term="fine-tuning" /><category term="精调" /><category term="ChatGPT" /><category term="completion" /><category term="prompting" /><category term="训练" /><category term="数据集" /><summary type="html">在 OpenAI 的 API 官方首页的大标题写着的是「用 OpenAI 的强力模型构建下一代应用（Build next-gen apps with OpenAI’s powerful models）」，并直接点出了自己最拿得出手的三个商业化产品 GPT-3、Codex、DALL·E。本文麦克船长重点为你解读基于 GPT-3 的几款模型的入门使用。</summary></entry><entry><title type="html">人工智能 LLM 革命前夜：一文读懂横扫自然语言处理的 Transformer 模型</title><link href="https://www.mikecaptain.com/2023/01/22/captain-aigc-1-transformer/" rel="alternate" type="text/html" title="人工智能 LLM 革命前夜：一文读懂横扫自然语言处理的 Transformer 模型" /><published>2023-01-22T09:13:09+00:00</published><updated>2023-01-22T09:13:09+00:00</updated><id>https://www.mikecaptain.com/2023/01/22/captain-aigc-1-transformer</id><content type="html" xml:base="https://www.mikecaptain.com/2023/01/22/captain-aigc-1-transformer/">&lt;p&gt;&lt;img src=&quot;/img/src/2023/2023-01-23-aigc-llm-1-transformer-cover.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;!-- **本文目录**
* TOC
{:toc} --&gt;

&lt;h2 id=&quot;前言&quot;&gt;前言&lt;/h2&gt;

&lt;p&gt;本文试图从技术角度搞清楚一个问题：&lt;strong&gt;过去一年 AIGC 爆火、过去五年 NLP（自然语言处理）领域突飞猛进的缘起是什么？&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;这个问题被解答后，将还有两个问题，但暂时本文没有作答：1）如果认为通过图灵测试代表着 AGI（Artificial General Intelligence，通用人工智能）的话，当下 NLP，乃至 AGI 发展到什么程度了？2）未来一些年内，AGI 的发展路线可能会是怎样的？&lt;/p&gt;

&lt;p&gt;利用春节时间，写了这么一篇数万字的长文笔记，希望共同爱好的朋友能读完多多指正。我是船涨，网名一直用「麦克船长」，中科大计算机本科毕业后先是做的 RTC 技术、分布式系统等等，干过 Full Stack，后来创业在产品、运营、营销、供应链上折腾了些年后来到阿里，在淘系做过产品、运营。&lt;/p&gt;

&lt;h4 id=&quot;1我来阿里之后第一个新增爱好是变形金刚模型第二个新增爱好是变形金刚模型&quot;&gt;1、我来阿里之后第一个新增爱好是「变形金刚模型」，第二个新增爱好是「变形金刚模型」&lt;/h4&gt;

&lt;p&gt;写了个这么冷的梗，其实想说的是，前者指的是著名 IP「变形金刚」相关的手办玩具模型，后者指的是这个引领革命的人工智能语言模型 Transformer。这两个爱好，都与目前从事的电商工作本职没有表面上的直接联系，权当爱好了。&lt;/p&gt;

&lt;p&gt;2022 年「生成式 AI」应用取得了突飞猛进的发展，作为一个「古典互联网」从业者，深切地感到这一次 AI 技术可能会带来的颠覆式变革，这让我兴奋又焦虑。2022 年上半年，我从天天特卖业务负责人到大聚划算运营中心负责人，在去年相当长一段时间里在关注直播带货在营销平台的模式命题，一直在思考一个问题：直播电商的高效（更适合的商品演绎方式 + 私域权益 + 冲动购买等」vs. 直播电商的低效（直播分发无人货匹配 + 直播间内千人一面 + 货品状态未知 + 主播不可控等），能否推动一个保留直播的高效，同时解决直播的低效的模式呢？&lt;/p&gt;

&lt;p&gt;这里面有大量的内容值得探讨，不过这不是船涨该系列文章的初衷，但这是我为什么开始非常关注 AI 的引子。直播电商的数字人技术基础，有动作捕捉、面部表情模拟、视觉渲染、直播话术生成、语音合成等等。依据第一性原理抽丝剥茧后，我发现尽管动作捕捉、视觉渲染等等很多技术仍有很大挑战，但是从商业视角看真正最影响用户心智的，是直播话术生成和演绎，除了头部主播，绝大多数直播带货在这方面都做的很糟糕，那么这里面就有巨大的「机器学习」生成内容超越非头部的大多数从业者的市场空间，而这完全依赖自然语言处理（NLP）。&lt;/p&gt;

&lt;p&gt;这个问题就属于「生成式 AI」的范畴了，国外科技圈叫它「Gen-AI」，即 Generative AI，中国科技圈都叫它「AIGC」，即 AI Generated Content，与 UGC、PGC 相对应。Gen-AI 的叫法更关注主体，具体地说是「生成式 AI 模型」，它是个「内容引擎」。而中国的叫法更关注「内容应用」。&lt;/p&gt;

&lt;p&gt;讲到 AIGC 这里，大家熟悉的 ChatGPT 就在 2022 年年底登场了。也是因为 ChatGPT 的破圈，带来了 AIGC 在国内科技圈的关注度暴涨。我从去年年中开始关注「文生图，text2image」领域的明星 Stable Diffusion 开源，进而关注到了 text2image 应用的爆发，包括 Disco Diffusion、MidJourney、DALL·E 2 等等，这些都源于 CV（计算机视觉）领域因为 Diffusion 模型发展带来的技术突破。&lt;/p&gt;

&lt;p&gt;AI 生成图片确实非常惊人。我酷爱变形金刚模玩，进而对机甲类都非常喜欢，所以随手生成了几张图，这里贴一下大家看看，分钟级的创作速度。（注意：当下 AI 生成图片主要是基于 Diffusion 的应用发展，AI 生成文本的核心驱动才是 Transformer 模型，此处只是展示）&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;img src=&quot;/img/src/2022-12-16-midjourney-first-test-1.png&quot; alt=&quot;image&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;/img/src/2022-12-16-midjourney-first-test-2.png&quot; alt=&quot;image&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;但是从第一性原理角度讲，生成图片的应用广度，远远小于生成文本。文本内容的本质是语言文字的理解与生成，人类历史有 600 万年，但是人类文明历史大概就 6000 年，文明的大发展出现在近 2000 多年的原因，主要来自 3500 多年前人类发明了文字。所以 AI 生成文本，意味着 AI 可以用人类熟悉的方式（语言文字）与人类高效协作，这必将引爆生产力革命。而这必将深入影响电商、内容、游戏、云计算、企业服务等众多领域。&lt;/p&gt;

&lt;h4 id=&quot;2掌握技术基础是当下读懂-ai-脉搏的基本功而这个脉搏将带动各行各业&quot;&gt;2、掌握技术基础，是当下读懂 AI 脉搏的基本功，而这个脉搏将带动各行各业&lt;/h4&gt;

&lt;p&gt;一旦深入关注 AI、关注 NLP 领域，你就会发现当下仍然处于一个技术发展突破的阶段，不关注技术的情况下来聊 AI、聊 NLP、聊 AIGC，那就只能是一个「爱好者」，而无法深入与这个行业内的弄潮儿对话，更不要提参与其中了。所以这个春节，船涨回归了当年做技术时的初心，翻了一些材料，学习了 NLP 语言模型的关键技术，在此作为技术学习笔记，与大家分享。尽管担心班门弄斧，但是本着费曼老师提倡的输出学习法，我把自己学习梳理的内容抛出来，除了会更帮助到我自己，也能结交一些对此同样在关注的同学们，欢迎感兴趣的同学加我的微信（微信号 sinosuperman）在业余时间和我交流。&lt;/p&gt;

&lt;p&gt;本文将包括这几部分：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;第一章，主要介绍 Transformer 出现之前的几个主流语言模型，包括 N 元文法（n-gram）、多层感知器（MLP）、卷积神经网络（CNN）、循环神经网络（RNN）。其中 CNN 主要应用领域在计算机视觉，因此没有更详细展开。其他模型也未面面俱到，主要考虑还是一个领域学习者的角度来了解和应用，而非研究。&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;第二章，是本文的核心，先介绍了注意力机制（Attention Mechanism），然后基于第一章对此前几大语言模型了解后，我们能更好地理解 Transformer 为什么会带来革命性的影响。&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;第三章，是一个 Transformer 的实现版本，基于 Tensorflow。&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;阅读本文，先对你过往的基础知识做了一些假设，如果你暂未了解，可能在阅读时遇到以下内容做一些简单地查询即可：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Word Presentation：自然语言处理中的词表示法，主要涉及 embedding。&lt;/li&gt;
  &lt;li&gt;张量：需要一点基础，比如了解张量的形状、升降维度等。但不会涉及到复杂问题，对一阶张量（向量）、二阶张量（矩阵）的简单运算有数学基础即可。对三阶张量，大概能想象出其空间含义即可。语言模型里理解词之间的距离，是有其空间几何意义的。&lt;/li&gt;
  &lt;li&gt;技术框架：PyTorch 或 TensorFlow 框架。由于时间和篇幅关系，春节期间梳理这些时，对于框架基础，我主要是 Google 现用现查，询问 ChatGPT 以及在微信读书里直接搜索全文。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;作为技术笔记难免有纰漏或理解错误，欢迎指正。文中自绘图片用的是 Graphviz，公式生成用的是 KaTeX，贴到 ATA 后难免有一些没有兼容的部分（发现的已做了 fix），望见谅。&lt;/p&gt;

&lt;h2 id=&quot;第一章--2017-年之前的几个关键-nlp-语言模型&quot;&gt;第一章 · 2017 年之前的几个关键 NLP 语言模型&lt;/h2&gt;

&lt;p&gt;NLP 的技术基础方面，我认为主要是这两部分：词表示法（Word Presentation）、语言模型（Language Model）。对于词表示法，这里不做详细介绍，基本的思路就是把词表示为向量（一维张量），最基本的 One-Hot、Word2Vec、GloVe、fastText 等。这部分的技术演进也在不断前进，比如本文将要重点介绍的 Transformer 模型里，用到的词表示法是「引入上下文感知的词向量」。&lt;/p&gt;

&lt;p&gt;语言模型从早期的 N 元文法（N-Gram，本文要介绍的），到神经网络被提出后最早期的感知器（Perceptron），再到后来席卷计算机视觉（CV）领域的卷积神经网络（CNN），然后出现考虑序列特征的循环神经网络（RNN，包括 Encoder-Decoder 模型），直到 2017 年横空出世的 Transformer，大概分这五个主要阶段。因为本文的重点是 Transformer，所以前面四个模型我会快速概览一下，然后介绍下最朴素的注意力（Attention）机制，基于此再详细介绍下 Transformer，并对一个完整的、精炼实现的代码实例进行精讲。&lt;/p&gt;

&lt;h3 id=&quot;第-1-节--n-元文法语言模型&quot;&gt;第 1 节 · N 元文法语言模型&lt;/h3&gt;

&lt;h4 id=&quot;11马尔科夫假设markov-assumption与-n-元文法语言模型n-gram-language-model&quot;&gt;1.1、马尔科夫假设（Markov Assumption）与 N 元文法语言模型（N-gram Language Model）&lt;/h4&gt;

&lt;p&gt;下一个词出现的概率只依赖于它前面 n-1 个词，这种假设被称为「马尔科夫假设（Markov Assumption」。N 元文法，也称为 N-1 阶马尔科夫链。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;一元文法（1-gram），unigram，零阶马尔科夫链，不依赖前面任何词；&lt;/li&gt;
  &lt;li&gt;二元文法（2-gram），bigram，一阶马尔科夫链，只依赖于前 1 个词；&lt;/li&gt;
  &lt;li&gt;三元文法（3-gram），trigram，二阶马尔科夫链，只依赖于前 2 个词；&lt;/li&gt;
  &lt;li&gt;……&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;通过前 t-1 个词预测时刻 t 出现某词的概率，用最大似然估计：&lt;/p&gt;

\[P(w_t | w_1,w_2...w_{t-1}) = \frac{C(w_1,w_2,...w_t)}{C(w_1,w_2,...w_{t-1})}\]

&lt;p&gt;进一步地，一组词（也就是一个句子）出现的概率就是：&lt;/p&gt;

\[P(w_1,w_2,...w_t) = P(w_t | w_1,w_2,...w_{t-1}) \cdot P(w_{t-1} | w_1,w_2,...w_{t-2}) \cdot ... \cdot P(w_1)
			      = \displaystyle\prod_{i=1}^{t-1}P(w_i | w_{1:i-1})\]

&lt;p&gt;为了解决句头、尾逇概率计算问题，我们再引入两个标记 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;BOS&amp;gt;&lt;/code&gt; 和 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;EOS&amp;gt;&lt;/code&gt; 分别表示 beginning of sentence 和 end of sentence，所以  \(w_0 =\) &amp;lt;BOS&amp;gt;、 \(w_{length + 1} =\) &amp;lt;EOS&amp;gt;，其中 length 是词的数量。&lt;/p&gt;

&lt;p&gt;具体地，比如对于 bigram，该模型表示如下：&lt;/p&gt;

\[\begin{aligned}
P(w_1,w_2,...w_t) &amp;amp;= \displaystyle\prod_{i=1}^{t-1}P(w_i | w_{i-1}) \\
P(w_t | w_{t-1}) &amp;amp;= \frac{C(w_{t-1}, w_t)}{C(w_{t-1})}
\end{aligned}\]

&lt;ul&gt;
  &lt;li&gt;如果有词出现次数为了 0，这一串乘出来就是 0 了，咋办？&lt;/li&gt;
  &lt;li&gt;因为基于马尔科夫假设，所以 N 固定窗口取值，对长距离词依赖的情况会表现很差。&lt;/li&gt;
  &lt;li&gt;如果把 N 值取很大来解决长距离词依赖，则会导致严重的数据稀疏（零频太多了），参数规模也会急速爆炸（高维张量计算）。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;上面的第一个问题，我们引入平滑 / 回退 / 差值等方法来解决，而后面两个问题则是在神经网络模型出现后才更好解决的。&lt;/p&gt;

&lt;h4 id=&quot;12平滑smoothing-折扣discounting&quot;&gt;1.2、平滑（Smoothing）/ 折扣（Discounting）&lt;/h4&gt;

&lt;p&gt;虽然限定了窗口 n 大小降低了词概率为 0 的可能性，但当 n-gram 的 n 比较大的时候会有的未登录词问题（Out Of Vocabulary，OOV）。另一方面，训练数据很可能也不是 100% 完备覆盖实际中可能遇到的词的。所以为了避免 0 概率出现，就有了让零平滑过渡为非零的补丁式技术出现。&lt;/p&gt;

&lt;p&gt;最简单的平滑技术，就是折扣法（Discounting）。这是一个非常容易想到的办法，就是把整体 100% 的概率腾出一小部分来，给这些零频词（也常把低频词一起考虑）。常见的平滑方法有：加 1 平滑、加 K 平滑、Good-Turing 平滑、Katz 平滑等。&lt;/p&gt;

&lt;h5 id=&quot;121加-1-平滑--拉普拉斯平滑add-one-discounting--laplace-smoothing&quot;&gt;1.2.1、加 1 平滑 / 拉普拉斯平滑（Add-One Discounting / Laplace Smoothing）&lt;/h5&gt;

&lt;p&gt;加 1 平滑，就是直接将所有词汇的出现次数都 +1，不止针对零频词、低频词。如果继续拿 bigram 举例来说，模型就会变成：&lt;/p&gt;

\[P(w_i | w_{i-1}) = \frac{C_(w_{i-1},w_i) + 1}{\displaystyle\sum_{j=1}^n(C_(w_{i-1},w_j) + 1)} = \frac{C(w_{i-1}, w_i) + 1}{C(w_{i-1}) + |\mathbb{V}|}\]

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;其中  \(N\)  表示所有词的词频之和， $$&lt;/td&gt;
      &lt;td&gt;\mathbb{V}&lt;/td&gt;
      &lt;td&gt;$$  表示词汇表的大小。&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;如果当词汇表中的词，很多出现次数都很小，这样对每个词的词频都 +1，结果的偏差影响其实挺大的。换句话说，+1 对于低频词很多的场景，加的太多了，应该加一个更小的数（ 1 &amp;lt; δ &amp;lt; 1）。所以有了下面的「δ 平滑」技术。&lt;/p&gt;

&lt;h5 id=&quot;122加-k-平滑--δ-平滑add-k-discounting--delta-smoothing&quot;&gt;1.2.2、加 K 平滑 / δ 平滑（Add-K Discounting / Delta Smoothing）&lt;/h5&gt;

&lt;p&gt;把 +1 换成 δ，我们看下上面 bigram 模型应该变成上面样子：&lt;/p&gt;

\[P(w_i | w{i-1}) = \frac{C_(w_{i-1},w_i) + \delta}{\displaystyle\sum_{j=1}^n(C_(w_{i-1},w_j) + \delta)} = \frac{C(w_{i-1}, w_i) + \delta}{C(w_{i-1}) + \delta|\mathbb{V}|}\]

&lt;p&gt;δ 是一个超参数，确定它的值需要用到困惑度（Perplexity，一般用缩写 PPL）。另外，有些文章里也会把这个方法叫做「加 K 平滑，Add-K Smoothing」。&lt;/p&gt;

&lt;h5 id=&quot;123困惑度perplexity&quot;&gt;1.2.3、困惑度（Perplexity）&lt;/h5&gt;

&lt;p&gt;对于指定的测试集，困惑度定义为测试集中每一个词概率的几何平均数的倒数，公式如下：&lt;/p&gt;

\[\operatorname{PPL}(\mathbb{D}_{test}) = \frac{1}{\sqrt[n]{P(w_1,w_2...w_n)}}\]

&lt;p&gt;把  \(P(w_1,w_2,...w_t) = \displaystyle\prod_{i=1}^{t-1}P(w_i\text{\textbar}w_{i-1})\)  带入上述公式，就得到了 PPL 的计算公式：&lt;/p&gt;

\[\operatorname{PPL}(\mathbb{D}_{test}) = (\displaystyle\prod_{i=1}^nP(w_i|w_{1:i-1}))^{-\frac{1}{n}}\]

&lt;h4 id=&quot;13回退back-off&quot;&gt;1.3、回退（Back-off）&lt;/h4&gt;

&lt;p&gt;在多元文法模型中，比如以 3-gram 为例，如果出现某些三元语法概率为零，则不使用零来表示概率，而回退到 2-gram，如下。&lt;/p&gt;

\[P(w_i|w_{i-2}w_{i-1}) =
\begin{cases}
P(w_i|w_{i-2}w_{i-1}) &amp;amp; C(w_{i-2}w_{i-1}w_i) &amp;gt; 0 \\
P(w_i|w_{i-1}) &amp;amp; C(w_{i-2}w_{i-1}w_i) = 0 \enspace and \enspace C(w_{i-1}w_i) &amp;gt; 0
\end{cases}\]

&lt;h4 id=&quot;14差值interpolation&quot;&gt;1.4、差值（Interpolation）&lt;/h4&gt;

&lt;p&gt;N 元文法模型如果用回退法，则只考虑了 n-gram 概率为 0 时回退为 n-1 gram，那么自然要问：n-gram 不为零时，是不是也可以按一定权重来考虑 n-1 gram？于是有了插值法。以 3-gram 为例，把 2-gram、1-gram 都考虑进来：&lt;/p&gt;

\[P(w_i|w_{i-2}w_{i-1}) = \lambda_1 P(w_i|w_{i-2}w_{i-1}) + \lambda_2 P(w_i|w_{i-1}) + \lambda_3 P(w_i)\]

&lt;h3 id=&quot;第-2-节--感知器perceptron&quot;&gt;第 2 节 · 感知器（Perceptron）&lt;/h3&gt;

&lt;p&gt;N 元文法模型的显著问题，在「马尔科夫假设与 N 元文法语言模型」小节已经提到了。这些问题基本在神经网络模型中被解决，而要了解神经网络模型，就要从感知器（Perceptron）开始。1957 年感知机模型被提出，1959 年多层感知机（MLP）模型被提出。MLP 有时候也被称为 ANN，即 Artificial Neural Network，接下来我们来深入浅出地了解一下，并有一些动手的练习。&lt;/p&gt;

&lt;h4 id=&quot;21感知器perceptron解决二元分类任务的前馈神经网络&quot;&gt;2.1、感知器（Perceptron）：解决二元分类任务的前馈神经网络&lt;/h4&gt;

&lt;p&gt;\(x\) 是一个输入向量，\(\omega\) 是一个权重向量（对输入向量里的而每个值分配一个权重值所组成的向量）。举一个具体任务例子，比如如果这两个向量的内积超过某个值，则判断为 1，否则为 0，这其实就是一个分类任务。那么这个最终输出值可以如下表示：&lt;/p&gt;

\[y = \begin{cases} 1 &amp;amp; (\omega \cdot x \geq 0) \\ 0 &amp;amp; (\omega \cdot x \lt 0) \end{cases}\]

&lt;p&gt;这就是一个典型的感知器（Perceptron），一般用来解决分类问题。还可以再增加一个偏差项（bias），如下：&lt;/p&gt;

\[y = \begin{cases} 1 &amp;amp; (\omega \cdot x + b \geq 0) \\ 0 &amp;amp; (\omega \cdot x + b \lt 0) \end{cases}\]

&lt;p&gt;感知器其实就是一个前馈神经网络，由输入层、输出层组成，没有隐藏层。而且输出是一个二元函数，用于解决二元分类问题。&lt;/p&gt;

&lt;h4 id=&quot;22线性回归linear-regression从离散值的感知器解决类问题到连续值的线性回归解决回归问题&quot;&gt;2.2、线性回归（Linear Regression）：从离散值的感知器（解决类问题），到连续值的线性回归（解决回归问题）&lt;/h4&gt;

&lt;p&gt;一般来说，我们认为感知器的输出结果，是离散值。一般来说，我们认为离散值作为输出解决的问题，是分类问题；相应地，连续值解决的问题是回归（Regression）。比如对于上面的感知器，如果我们直接将 \(\omega \cdot x + b\) 作为输出值，则就变成了一个线性回归问题的模型了。&lt;/p&gt;

&lt;p&gt;下面我们用 PyTorch 来实现一个线性回归的代码示例，首先我们要了解在 PyTorch 库里有一个非常常用的函数：&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;in_features&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out_features&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;这个函数在创建时会自动初始化权值和偏置，并且可以通过调用它的 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;forward&lt;/code&gt; 函数来计算输入数据的线性变换。具体来说，当输入为 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;x&lt;/code&gt; 时，&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;forward&lt;/code&gt; 函数会计算 \(y = \omega \cdot x + b\)，其中  \(W\)  和  \(b\)  分别是 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nn.Linear&lt;/code&gt; 图层的权值和偏置。&lt;/p&gt;

&lt;p&gt;我们来一个完整的代码示例：&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch.nn&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 定义模型
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;LinearRegression&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;nb&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;LinearRegression&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linear&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 初始化模型
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LinearRegression&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 定义损失函数和优化器
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;criterion&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MSELoss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SGD&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.01&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 创建输入特征 X 和标签 y
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 训练模型
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;epoch&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# 前向传播
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;predictions&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;criterion&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predictions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# 反向传播
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zero_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 创建测试数据 X_test 和标签 y_test
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_test&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y_test&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;12&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;14&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 测试模型
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;no_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;predictions&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;criterion&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predictions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;Test loss: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;上述代码，一开始先创建一个 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;LinearRegression&lt;/code&gt; 线性回归模型的类，其中有一个 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;forward&lt;/code&gt; 前向传播函数，调用时其实就是计算一下输出值 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;y&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;主程序，一开始创建一个线性回归模型实例，然后定义一个用于评价模型效果的损失函数评价器，和用随机梯度下降（Stochastic Gradient Descent）作为优化器。&lt;/p&gt;

&lt;p&gt;然后创建一个输入特征张量，和标签张量。用这组特征和标签进行训练，训练的过程就是根据 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;X&lt;/code&gt; 计算与测试 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;predictions&lt;/code&gt; 向量，再把它和 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;y&lt;/code&gt; 一起给评价器算出损失 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;loss&lt;/code&gt;，然后进行反向传播。注意反向传播的三行代码：&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zero_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;如此训练 100 次（每一次都会黑盒化地更新模型的参数，一个 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;epoch&lt;/code&gt; 就是一次训练过程，有时也称为 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;iteration&lt;/code&gt; 或者 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;step&lt;/code&gt;，不断根据 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;loss&lt;/code&gt; 训练优化模型参数。&lt;/p&gt;

&lt;p&gt;然后我们创建了一组测试特征值张量 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;X_test&lt;/code&gt;，和测试标签张量 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;y_test&lt;/code&gt;，然后用它们测试模型性能，把测试特征得到的 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;predictions&lt;/code&gt; 与 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;y_test&lt;/code&gt; 共同传给评价器，得到 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;loss&lt;/code&gt;。在这个例子中我们会得到如下结果：&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;Test&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.0034&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;23逻辑回归logistic-regression没有值域约束的线性回归到限定在一个范围内的逻辑回归常用于分类问题&quot;&gt;2.3、逻辑回归（Logistic Regression）：没有值域约束的线性回归，到限定在一个范围内的逻辑回归（常用于分类问题）&lt;/h4&gt;

&lt;p&gt;可以看到线性回归问题，输出值是没有范围限定的。如果限定（limit）在特定的  \((0, L)\)  范围内，则就叫做逻辑回归了。那么如何将一个线性回归变成逻辑回归呢？一般通过如下公式变换：&lt;/p&gt;

\[y = \frac{L}{1 + e^{-k(z-z_0)}}\]

&lt;p&gt;这样原来的  \(z \in (-\infty, +\infty)\)  就被变换成了  \(y \in (0, L)\)  了。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;激活函数&lt;/strong&gt;：这种把输出值限定在一个目标范围内的函数，被叫做 &lt;strong&gt;激活函数（Activation Function）&lt;/strong&gt;。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;函数的陡峭程度&lt;/strong&gt; 由  \(k\)  控制，越大越陡。&lt;/li&gt;
  &lt;li&gt;当  \(z = z_0\)  时， \(y = \frac{L}{2}\) 。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;下面给出一个基于 Python 的 scikit-learn 库的示例代码：&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.linear_model&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LogisticRegression&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.datasets&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;load_iris&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.model_selection&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_test_split&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 这是 scikit-learn 库里的一个简单的数据集
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iris&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;load_iris&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 把 iris 数据集拆分成训练集和测试集两部分
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_test&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_test_split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iris&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;iris&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;target&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.25&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random_state&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;42&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 用 scikit-learn 库创建一个逻辑回归模型的实例
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LogisticRegression&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 用上边 split 出来的训练集数据，训练 lr 模型实例
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 用训练过的模型，拿测试集的输入数据做测试
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predictions&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 用测试集的数据验证精确性
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;accuracy&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;predictions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;accuracy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;24sigmoid-回归sigmoid-regression归一化的逻辑回归一般用于二元分类任务&quot;&gt;2.4、Sigmoid 回归（Sigmoid Regression）：归一化的逻辑回归，一般用于二元分类任务&lt;/h4&gt;

&lt;p&gt;当  \(L = 1, k = 1, z_0 = 0\) ，此时的激活函数就是 &lt;strong&gt;Sigmoid&lt;/strong&gt; 函数，也常表示为  \(\sigma\)  函数，如下：&lt;/p&gt;

\[y = \frac{1}{1 + e^{-z}}\]

&lt;p&gt;Sigmoid 回归的值域，恰好在 (0, 1) 之间，所以常备作为用来归一化的激活函数。而一个线性回归模型，再用 sigmoid 函数归一化，这种也常被称为「Sigmoid 回归」。Sigmoid 这个单词的意思也就是 S 形，我们可以看下它的函数图像如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2022-12-19-language-model-2.png&quot; alt=&quot;image&quot; width=&quot;490&quot; /&gt;&lt;/p&gt;

&lt;p&gt;因为归一化，所以也可以把输出值理解为一个概率。比如我们面对一个二元分类问题，那么输出结果就对应属于这个类别的概率。&lt;/p&gt;

&lt;p&gt;这样一个 sigmoid 模型可以表示为：&lt;/p&gt;

\[y = Sigmoid(W \cdot x + b)\]

&lt;p&gt;另外 sigmoid 函数的导数（即梯度）是很好算的： \(y&apos; = y \cdot (1-y)\) 。这非常方便用于「梯度下降算法」根据 loss 对模型参数进行优化。Sigmoid 回归，一般用于二元分类任务。那么对于超过二元的情况怎么办呢？这就引出了下面的 Softmax 回归。&lt;/p&gt;

&lt;h4 id=&quot;25softmax-回归softmax-regression从解决二元任务的-sigmoid到解决多元分类任务的-softmax&quot;&gt;2.5、Softmax 回归（Softmax Regression）：从解决二元任务的 sigmoid，到解决多元分类任务的 Softmax&lt;/h4&gt;

&lt;p&gt;相对逻辑回归，Softmax 也称为多项逻辑回归。上面说 Sigmoid 一般用于解决二元分类问题，那么多元问题就要用 Softmax 回归了。我们来拿一个具体问题来解释，比如问题是对于任意输入的一个电商商品的图片，来判断这个图片所代表的的商品，属于哪个商品类目。假设我们一共有 100 个类目。那么一个图片比如说其所有像素值作为输入特征值，输出就是一个 100 维的向量 ** \(z\) **，输出向量中的每个值  \(z_i\)  表示属于相对应类目的概率  \(y_i\)  ：&lt;/p&gt;

\[y_i = Softmax(z)_i = \frac{e^{z_i}}{e^{z_1} + e^{z_2} + ... + e^{z_100}}\]

&lt;p&gt;那么最后得到的  \(y\)  向量中的每一项就对应这个输入  \(z\)  属于这 100 个类目的各自概率了。所以如果回归到一般问题，这个 Softmax 回归的模型就如下：&lt;/p&gt;

\[y = Softmax(W \cdot x + b)\]

&lt;p&gt;对于上面电商商品图片的例子，假设每个图片的尺寸是 512x512，这个模型展开式如下：&lt;/p&gt;

\[\begin{bmatrix} y_1 \\ y_2 \\ ... \\ y_{100} \end{bmatrix} = Softmax(\begin{bmatrix} w_{1,1}, &amp;amp; w_{1,2}, &amp;amp; ... &amp;amp; w_{1, 512} \\ w_{2,1}, &amp;amp; w_{2,2}, &amp;amp; ... &amp;amp; w_{2, 512} \\ ... &amp;amp; ... &amp;amp; ... &amp;amp; ... \\ w_{100,1}, &amp;amp; w_{100,2}, &amp;amp; ... &amp;amp; w_{100, 512} \end{bmatrix} \cdot \begin{bmatrix} x_1 \\ x_2 \\ ... \\ x_{512} \end{bmatrix} + \begin{bmatrix} b_1 \\ b_2 \\ ... \\ b_{512} \end{bmatrix})\]

&lt;p&gt;这个对输入向量  \(x\)  执行  \(w \cdot x + b\)  运算，一般也常称为「线性映射/线性变化」。&lt;/p&gt;

&lt;h4 id=&quot;26多层感知器multi-layer-perceptron&quot;&gt;2.6、多层感知器（Multi-Layer Perceptron）&lt;/h4&gt;

&lt;p&gt;上面我们遇到的所有任务，都是用线性模型（Linear Models）解决的。有时候问题复杂起来，我们就要引入非线性模型了。&lt;/p&gt;

&lt;p&gt;这里我们要介绍一个新的激活函数 —— ReLU（Rectified Linear Unit）—— 一个非线性激活函数，其定义如下：&lt;/p&gt;

\[ReLU(z) = max(0, z)\]

&lt;p&gt;比如对于 MNIST 数据集的手写数字分类问题，就是一个典型的非线性的分类任务，下面给出一个示例代码：&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch.nn&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchvision&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchvision.transforms&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;transforms&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 定义多层感知器模型
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;MLP&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hidden_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_classes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;nb&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MLP&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fc1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hidden_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;relu&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ReLU&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fc2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hidden_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_classes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fc1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;relu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fc2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 超参数
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;784&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;hidden_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;500&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;num_classes&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;num_epochs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.001&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 加载 MNIST 数据集
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_dataset&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;datasets&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MNIST&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;root&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;../../data&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                               &lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                               &lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transforms&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ToTensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt;
                               &lt;span class=&quot;n&quot;&gt;download&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;test_dataset&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;datasets&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MNIST&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;root&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;../../data&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                              &lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                              &lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transforms&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ToTensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 数据加载器
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_loader&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;utils&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DataLoader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                           &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                           &lt;span class=&quot;n&quot;&gt;shuffle&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;test_loader&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;utils&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DataLoader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test_dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                          &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                          &lt;span class=&quot;n&quot;&gt;shuffle&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MLP&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hidden_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_classes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 定义损失函数和优化器
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;criterion&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;CrossEntropyLoss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Adam&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 训练模型
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;epoch&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_epochs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;images&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_loader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# 前向传播
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;images&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;criterion&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# 反向传播
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zero_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# 输出训练损失
&lt;/span&gt;    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;Epoch &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;epoch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;, Training Loss: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;这段代码里，我们能看到 MLP 的模型定义是：&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hidden_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ReLU&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hidden_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_classes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;与前面的模型示例代码类似，也都用到了反向传播、损失函数评价器、优化器。如果用公式表示的话，就是如下的模型定义：&lt;/p&gt;

\[\begin{aligned}
&amp;amp;z = W_1 \cdot x + b_1 \\
&amp;amp;h = ReLU(z) \\
&amp;amp;y = W_2 \cdot h + b_2
\end{aligned}\]

&lt;p&gt;我们知道 MLP 通常是一个输入和输出长度相同的模型，但少数情况下也可以构建输入和输出长度不同的 MLP 模型，比如输入一组序列后，输出是一个离散的分类结果。&lt;/p&gt;

&lt;h4 id=&quot;27简述如何训练一个模型正向传播与反向传播&quot;&gt;2.7、简述如何训练一个模型：正向传播与反向传播&lt;/h4&gt;

&lt;p&gt;这是个很重要的议题。但是春节时间有限，这部分只能简写了，我们更多聚焦在语言模型本身。这里简述一下，后续可能会再补全。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;训练神经网络，主要包括前向传播、反向传播这两步。&lt;/li&gt;
  &lt;li&gt;正向传播，就是将数据输入给模型，基于已确定的一组参数（比如 MLP 中的权重 W、偏置 b 等），得到输出结果。根据输出结果计算损失函数，衡量当前参数下的模型性能。&lt;/li&gt;
  &lt;li&gt;反向传播最常用到的是梯度下降法（这里不讨论其他方法），依托损失函数，将其中的参数当做变量来求偏导（计算梯度），沿着梯度下降的方向求解损失函数的极小值，此时的参数可替代此前的参数。这就是对模型优化训练的一个典型过程。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;引申问题 —— 梯度消失、梯度爆炸问题：因为对损失函数的求偏导，是从输出层向输入层反向基于「数学上的链式法则」计算的，数学上这是个连乘计算，层数越多越容易出现这个问题。这个求导过程可能会出现梯度为零的情况，即梯度消失。也有可能出现梯度值特别大的情况。&lt;/p&gt;

&lt;p&gt;解决梯度消失、梯度爆炸问题，又是一个重要议题，这里篇幅所限也难以展开做技术笔记。粗暴的方式比如梯度剪切，Hinton 提出的逐层预训练后再整体精调理论上也 work，本文后续提到的 LSTM、ResNet 等也可以解决问题，我们也还能了解到业界各种解决手段，有机会再与朋友们交流学习。&lt;/p&gt;

&lt;h4 id=&quot;28mlp-的一个显著问题帮我们引出-cnn-模型&quot;&gt;2.8、MLP 的一个显著问题，帮我们引出 CNN 模型&lt;/h4&gt;

&lt;p&gt;我们可以看到，在 MLP 中，不论有多少层，某一层的输出向量  \(h_n\)  中的每个值，都会在下一层计算输出向量  \(h_{n+1}\)  的每个值时用到。具体来说，如果对于某一层的输出值如下：&lt;/p&gt;

\[h_{n+1} = Softmax(W_{n+1} \cdot h_n + b_{n+1})\]

&lt;p&gt;上一段话里所谓的「用到」，其实就是要针对  \(h_n\)  生成相应的特征值  \(W_{n+1}\)  权重矩阵中的每个行列里的数值和  \(b_{n+1}\) 偏差向量 里的每个值。如果用图画出来，就是：&lt;/p&gt;

&lt;div style=&quot;text-align: center;&quot;&gt;
&lt;div class=&quot;graphviz-wrapper&quot;&gt;

&lt;!-- Generated by graphviz version 2.43.0 (0)
 --&gt;
&lt;!-- Title: G Pages: 1 --&gt;
&lt;svg role=&quot;img&quot; aria-label=&quot;graphviz-1b1299448dc08c90d29bebf8b1f045c1&quot; width=&quot;428pt&quot; height=&quot;116pt&quot; viewBox=&quot;0.00 0.00 427.64 116.00&quot;&gt;
&lt;title&gt;graphviz-1b1299448dc08c90d29bebf8b1f045c1&lt;/title&gt;
&lt;desc&gt;
digraph G {
	rankdir=TB
	a[label=&amp;quot;...&amp;quot;]
	b[label=&amp;quot;...&amp;quot;]
	h_2_1[label=&amp;quot;h_n+1_1&amp;quot;]
	h_2_2[label=&amp;quot;h_n+1_2&amp;quot;]
	h_2_m[label=&amp;quot;h_n+1_m&amp;quot;]

	{rank=same h_n_1 h_n_2 b h_n_m}
	{rank=same h_2_1 h_2_2 a h_2_m}

	h_n_1 -&amp;gt; h_2_1
	h_n_1 -&amp;gt; h_2_2
	h_n_1 -&amp;gt; a
	h_n_1 -&amp;gt; h_2_m

	h_n_1 -&amp;gt; h_2_1
	h_n_2 -&amp;gt; h_2_2
	h_n_2 -&amp;gt; a
	h_n_2 -&amp;gt; h_2_m

	b -&amp;gt; h_2_1
	b -&amp;gt; h_2_2
	b -&amp;gt; a
	b -&amp;gt; h_2_m

	h_n_m -&amp;gt; h_2_1
	h_n_m -&amp;gt; h_2_2
	h_n_m -&amp;gt; a
	h_n_m -&amp;gt; h_2_m
}
&lt;/desc&gt;

&lt;g id=&quot;graph0&quot; class=&quot;graph&quot; transform=&quot;scale(1 1) rotate(0) translate(4 112)&quot;&gt;
&lt;title&gt;G&lt;/title&gt;
&lt;polygon fill=&quot;white&quot; stroke=&quot;transparent&quot; points=&quot;-4,4 -4,-112 423.64,-112 423.64,4 -4,4&quot; /&gt;
&lt;!-- a --&gt;
&lt;g id=&quot;node1&quot; class=&quot;node&quot;&gt;
&lt;title&gt;a&lt;/title&gt;
&lt;ellipse fill=&quot;none&quot; stroke=&quot;black&quot; cx=&quot;146.7&quot; cy=&quot;-18&quot; rx=&quot;27&quot; ry=&quot;18&quot; /&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;146.7&quot; y=&quot;-14.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;...&lt;/text&gt;
&lt;/g&gt;
&lt;!-- b --&gt;
&lt;g id=&quot;node2&quot; class=&quot;node&quot;&gt;
&lt;title&gt;b&lt;/title&gt;
&lt;ellipse fill=&quot;none&quot; stroke=&quot;black&quot; cx=&quot;151.7&quot; cy=&quot;-90&quot; rx=&quot;27&quot; ry=&quot;18&quot; /&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;151.7&quot; y=&quot;-86.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;...&lt;/text&gt;
&lt;/g&gt;
&lt;!-- b&amp;#45;&amp;gt;a --&gt;
&lt;g id=&quot;edge11&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;b&amp;#45;&amp;gt;a&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M150.46,-71.7C149.91,-63.98 149.25,-54.71 148.63,-46.11&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;152.12,-45.83 147.92,-36.1 145.14,-46.33 152.12,-45.83&quot; /&gt;
&lt;/g&gt;
&lt;!-- h_2_1 --&gt;
&lt;g id=&quot;node3&quot; class=&quot;node&quot;&gt;
&lt;title&gt;h_2_1&lt;/title&gt;
&lt;ellipse fill=&quot;none&quot; stroke=&quot;black&quot; cx=&quot;50.7&quot; cy=&quot;-18&quot; rx=&quot;50.89&quot; ry=&quot;18&quot; /&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;50.7&quot; y=&quot;-14.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;h_n+1_1&lt;/text&gt;
&lt;/g&gt;
&lt;!-- b&amp;#45;&amp;gt;h_2_1 --&gt;
&lt;g id=&quot;edge9&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;b&amp;#45;&amp;gt;h_2_1&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M133.64,-76.49C119.14,-66.44 98.46,-52.11 81.38,-40.27&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;83.04,-37.16 72.83,-34.34 79.05,-42.91 83.04,-37.16&quot; /&gt;
&lt;/g&gt;
&lt;!-- h_2_2 --&gt;
&lt;g id=&quot;node4&quot; class=&quot;node&quot;&gt;
&lt;title&gt;h_2_2&lt;/title&gt;
&lt;ellipse fill=&quot;none&quot; stroke=&quot;black&quot; cx=&quot;242.7&quot; cy=&quot;-18&quot; rx=&quot;50.89&quot; ry=&quot;18&quot; /&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;242.7&quot; y=&quot;-14.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;h_n+1_2&lt;/text&gt;
&lt;/g&gt;
&lt;!-- b&amp;#45;&amp;gt;h_2_2 --&gt;
&lt;g id=&quot;edge10&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;b&amp;#45;&amp;gt;h_2_2&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M168.81,-75.83C181.67,-65.94 199.56,-52.18 214.52,-40.67&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;216.69,-43.42 222.48,-34.55 212.42,-37.87 216.69,-43.42&quot; /&gt;
&lt;/g&gt;
&lt;!-- h_2_m --&gt;
&lt;g id=&quot;node5&quot; class=&quot;node&quot;&gt;
&lt;title&gt;h_2_m&lt;/title&gt;
&lt;ellipse fill=&quot;none&quot; stroke=&quot;black&quot; cx=&quot;365.7&quot; cy=&quot;-18&quot; rx=&quot;53.89&quot; ry=&quot;18&quot; /&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;365.7&quot; y=&quot;-14.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;h_n+1_m&lt;/text&gt;
&lt;/g&gt;
&lt;!-- b&amp;#45;&amp;gt;h_2_m --&gt;
&lt;g id=&quot;edge12&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;b&amp;#45;&amp;gt;h_2_m&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M172.78,-78.39C177.62,-76.14 182.79,-73.88 187.7,-72 211.14,-63.03 271.93,-45.36 315.95,-32.9&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;316.96,-36.25 325.63,-30.16 315.05,-29.51 316.96,-36.25&quot; /&gt;
&lt;/g&gt;
&lt;!-- h_n_1 --&gt;
&lt;g id=&quot;node6&quot; class=&quot;node&quot;&gt;
&lt;title&gt;h_n_1&lt;/title&gt;
&lt;ellipse fill=&quot;none&quot; stroke=&quot;black&quot; cx=&quot;69.7&quot; cy=&quot;-90&quot; rx=&quot;37.09&quot; ry=&quot;18&quot; /&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;69.7&quot; y=&quot;-86.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;h_n_1&lt;/text&gt;
&lt;/g&gt;
&lt;!-- h_n_1&amp;#45;&amp;gt;a --&gt;
&lt;g id=&quot;edge3&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;h_n_1&amp;#45;&amp;gt;a&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M86.4,-73.81C97.36,-63.85 111.83,-50.7 123.85,-39.77&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;126.28,-42.29 131.33,-32.97 121.57,-37.11 126.28,-42.29&quot; /&gt;
&lt;/g&gt;
&lt;!-- h_n_1&amp;#45;&amp;gt;h_2_1 --&gt;
&lt;g id=&quot;edge1&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;h_n_1&amp;#45;&amp;gt;h_2_1&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M59.35,-72.41C56.39,-64.62 53.56,-55.14 51.51,-46.33&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;54.92,-45.55 49.5,-36.45 48.06,-46.94 54.92,-45.55&quot; /&gt;
&lt;/g&gt;
&lt;!-- h_n_1&amp;#45;&amp;gt;h_2_1 --&gt;
&lt;g id=&quot;edge5&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;h_n_1&amp;#45;&amp;gt;h_2_1&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M70.91,-71.7C69.57,-63.7 67.15,-54.02 64.35,-45.15&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;67.63,-43.93 61.05,-35.62 61.01,-46.22 67.63,-43.93&quot; /&gt;
&lt;/g&gt;
&lt;!-- h_n_1&amp;#45;&amp;gt;h_2_2 --&gt;
&lt;g id=&quot;edge2&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;h_n_1&amp;#45;&amp;gt;h_2_2&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M97.49,-77.75C125.45,-66.44 168.9,-48.86 200.99,-35.87&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;202.6,-39 210.56,-32 199.97,-32.51 202.6,-39&quot; /&gt;
&lt;/g&gt;
&lt;!-- h_n_1&amp;#45;&amp;gt;h_2_m --&gt;
&lt;g id=&quot;edge4&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;h_n_1&amp;#45;&amp;gt;h_2_m&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M97.68,-77.83C103.57,-75.71 109.79,-73.65 115.7,-72 197.22,-49.25 220.23,-55.04 302.7,-36 307.03,-35 311.53,-33.9 316.02,-32.77&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;317.01,-36.13 325.81,-30.24 315.26,-29.35 317.01,-36.13&quot; /&gt;
&lt;/g&gt;
&lt;!-- h_n_2 --&gt;
&lt;g id=&quot;node7&quot; class=&quot;node&quot;&gt;
&lt;title&gt;h_n_2&lt;/title&gt;
&lt;ellipse fill=&quot;none&quot; stroke=&quot;black&quot; cx=&quot;331.7&quot; cy=&quot;-90&quot; rx=&quot;37.09&quot; ry=&quot;18&quot; /&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;331.7&quot; y=&quot;-86.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;h_n_2&lt;/text&gt;
&lt;/g&gt;
&lt;!-- h_n_2&amp;#45;&amp;gt;a --&gt;
&lt;g id=&quot;edge7&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;h_n_2&amp;#45;&amp;gt;a&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M303.01,-78.31C297.28,-76.2 291.3,-74.02 285.7,-72 240.06,-55.59 227.57,-54.38 182.7,-36 180.87,-35.25 179.01,-34.46 177.14,-33.65&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;178.4,-30.38 167.85,-29.44 175.52,-36.75 178.4,-30.38&quot; /&gt;
&lt;/g&gt;
&lt;!-- h_n_2&amp;#45;&amp;gt;h_2_2 --&gt;
&lt;g id=&quot;edge6&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;h_n_2&amp;#45;&amp;gt;h_2_2&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M312.82,-74.15C300.65,-64.58 284.6,-51.96 270.93,-41.21&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;272.8,-38.23 262.78,-34.8 268.48,-43.73 272.8,-38.23&quot; /&gt;
&lt;/g&gt;
&lt;!-- h_n_2&amp;#45;&amp;gt;h_2_m --&gt;
&lt;g id=&quot;edge8&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;h_n_2&amp;#45;&amp;gt;h_2_m&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M339.75,-72.41C343.72,-64.25 348.59,-54.22 353.04,-45.07&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;356.24,-46.48 357.46,-35.96 349.94,-43.42 356.24,-46.48&quot; /&gt;
&lt;/g&gt;
&lt;!-- h_n_m --&gt;
&lt;g id=&quot;node8&quot; class=&quot;node&quot;&gt;
&lt;title&gt;h_n_m&lt;/title&gt;
&lt;ellipse fill=&quot;none&quot; stroke=&quot;black&quot; cx=&quot;236.7&quot; cy=&quot;-90&quot; rx=&quot;40.09&quot; ry=&quot;18&quot; /&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;236.7&quot; y=&quot;-86.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;h_n_m&lt;/text&gt;
&lt;/g&gt;
&lt;!-- h_n_m&amp;#45;&amp;gt;a --&gt;
&lt;g id=&quot;edge15&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;h_n_m&amp;#45;&amp;gt;a&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M217.17,-73.81C203.86,-63.46 186.11,-49.66 171.76,-38.49&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;173.8,-35.65 163.76,-32.27 169.5,-41.17 173.8,-35.65&quot; /&gt;
&lt;/g&gt;
&lt;!-- h_n_m&amp;#45;&amp;gt;h_2_1 --&gt;
&lt;g id=&quot;edge13&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;h_n_m&amp;#45;&amp;gt;h_2_1&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M206.81,-77.75C176.21,-66.24 128.35,-48.22 93.68,-35.18&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;94.87,-31.89 84.28,-31.64 92.41,-38.44 94.87,-31.89&quot; /&gt;
&lt;/g&gt;
&lt;!-- h_n_m&amp;#45;&amp;gt;h_2_2 --&gt;
&lt;g id=&quot;edge14&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;h_n_m&amp;#45;&amp;gt;h_2_2&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M238.18,-71.7C238.84,-63.98 239.63,-54.71 240.37,-46.11&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;243.86,-46.37 241.23,-36.1 236.89,-45.77 243.86,-46.37&quot; /&gt;
&lt;/g&gt;
&lt;!-- h_n_m&amp;#45;&amp;gt;h_2_m --&gt;
&lt;g id=&quot;edge16&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;h_n_m&amp;#45;&amp;gt;h_2_m&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M261.26,-75.67C280.58,-65.19 307.78,-50.43 329.57,-38.6&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;331.42,-41.58 338.54,-33.73 328.08,-35.43 331.42,-41.58&quot; /&gt;
&lt;/g&gt;
&lt;/g&gt;
&lt;/svg&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;可以看到，输入的所有元素都被连接，即被分配权重 w 和偏差项 b，所以这被称为一个「全连接层（&lt;strong&gt;Fully Connected Layer&lt;/strong&gt;）」或者「&lt;strong&gt;稠密层（Dense Layer）&lt;/strong&gt;」。但是对于一些任务这样做是很蠢的，会付出大量无效的计算。&lt;/p&gt;

&lt;p&gt;因此我们需要 focus 在更少量计算成本的模型，于是有了卷积神经网络（CNN）。&lt;/p&gt;

&lt;h3 id=&quot;第-3-节--卷积神经网络cnn&quot;&gt;第 3 节 · 卷积神经网络（CNN）&lt;/h3&gt;

&lt;p&gt;MLP 里每一层的每个元素，都要乘以一个独立参数的权重 W，再加上一个偏执 b，这样的神经网络层常被我们叫做「全连接层（Fully Connected Layer）或稠密层（Dence Layer）。但是这样有个显著问题：如果输入内容的局部重要信息只是发生轻微移动并没有丢失，在全连接层处理后，整个输出结果都会发生很大变化 —— 这不合理。&lt;/p&gt;

&lt;p&gt;于是我们会想到，如果我们用一个小一些的全连接层，只对重要的局部输入进行处理呢？其实这个思路和 n-gram 是类似的，都是用一个窗口来扫描局部。卷积神经网络（Convolutional Neural Network，CNN）就是基于此诞生的。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;卷积核：卷积核是一个小的稠密层，用于提取局部特征，又称其为卷积核（kernel）/ 滤波器（filter）/ 感受野（receptive field / field of view）。&lt;/li&gt;
  &lt;li&gt;池化层（Pooling，或称汇聚层）：经过卷积核处理的结果，进一步聚合的过程。对于输入大小不一样的样本，池化后将有相同个数的特征输出。&lt;/li&gt;
  &lt;li&gt;提取多个局部特征：一个卷积核只能提取单一类型的局部特征，需要提取多种局部特征则需要多个卷积核。有些文章里你看提到「多个模式」、「多个通道」，其实指的就是多个 kernel 识别多个特征。&lt;/li&gt;
  &lt;li&gt;全连接分类层：多个卷积核得到的多个特征，需经过一个全连接的分类层用于最终决策。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这样做有几个特性：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;本地性（Locality）：输出结果只由一个特定窗口大小区域内的数据决定。&lt;/li&gt;
  &lt;li&gt;平移不变性（Translation Invariant）：对同一个特征，扫描不同区域时只用一个 kernel 来计算。&lt;/li&gt;
  &lt;li&gt;卷积层的参数规模，与输入输出数据大小无关。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;CNN 主要的适用领域是计算机视觉。而在 NLP 中，文本数据的维度很高，并且语言的结构比图像更复杂。因此，CNN 一般不适用于处理 NLP 问题。&lt;/p&gt;

&lt;h3 id=&quot;第-4-节--循环神经网络rnn&quot;&gt;第 4 节 · 循环神经网络（RNN）&lt;/h3&gt;

&lt;p&gt;RNN（循环神经网络），这是一种强大的神经网络模型，能够预测序列数据，例如文本、语音和时间序列。我们将通过生动的代码示例和实际案例来演示如何使用 RNN，并在日常生活中真实地体验它的功能。您将学习到如何使用 RNN 解决各种机器学习问题，并动手尝试运用 RNN 解决实际问题。这篇文章将为您提供一个完整的 RNN 入门指南，并使您对 RNN 有更深入的了解。&lt;/p&gt;

&lt;p&gt;RNN（Recurrent Neural Network）的 R 是 Recurrent 的意思，所以这是一个贷循环的神经网络。首先要明白一点，你并不需要搞懂 CNN 后才能学习 RNN 模型。你只要了解了 MLP 就可以学习 RNN 了。&lt;/p&gt;

&lt;h4 id=&quot;41经典结构的-rnn&quot;&gt;4.1、经典结构的 RNN&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2022-12-19-language-model-1.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;上图这是一个经典结构的 RNN 示意图，Unfold 箭头右侧是展开示意。输入序列（这里用 x 表示）传递给隐藏层（hidden layer，这里用 h 表示），处理完生成输出序列（这里用 o 表示）。序列的下一个词输入时的、上一步隐藏层会一起影响这一步的输出。U、V、W 都表示权重。在这个经典结构理，你可以看到非常重要的一点，就是输入序列长度与输出序列长度是相同的。&lt;/p&gt;

&lt;p&gt;这种经典结构的应用场景，比如对一段普通话输入它的四川话版本，比如对视频的每一帧进行处理并输出，等等。&lt;/p&gt;

&lt;p&gt;我们知道 RNN 是一个一个序列处理的，每个序列中的数据项都是有序的，所以对于计算一个序列内的所有数据项是无法并行的。但是计算不同序列时，不同序列各自的计算则是可以并行的。如果我们把上一个时刻 t 隐藏层输出的结果（ \(h_{t-1}\) ）传给一个激活函数（比如说用正切函数 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tanh&lt;/code&gt; 函数），然后和当下时刻 t 的这个输入（ \(x_{t}\) ）一起，处理后产生一个时刻 t 的输出（ \(h_t\) ）。然后把隐藏层的输出通过多项逻辑回归（Softmax）生成最终的输出值（ \(\bm{y}\) ），我们可以如下表示这个模型：&lt;/p&gt;

\[\begin{aligned}
&amp;amp;\bm{h}_t = tanh(\bm{W}^{xh} \cdot \bm{x}_t + \bm{b}^{xh} + \bm{W}^{hh} \cdot \bm{h}_{t-1} + \bm{b}^{hh}) \\
&amp;amp;\bm{y}_t = Softmax(\bm{W}^{hy} \cdot \bm{h_t} + \bm{b}^{hy})
\end{aligned}\]

&lt;p&gt;对应的示意图如下：&lt;/p&gt;

&lt;div style=&quot;text-align: center;&quot;&gt;
&lt;div class=&quot;graphviz-wrapper&quot;&gt;

&lt;!-- Generated by graphviz version 2.43.0 (0)
 --&gt;
&lt;!-- Title: G Pages: 1 --&gt;
&lt;svg role=&quot;img&quot; aria-label=&quot;graphviz-34cd77ba92d6e898bab41a54b23f2324&quot; width=&quot;278pt&quot; height=&quot;188pt&quot; viewBox=&quot;0.00 0.00 278.00 188.00&quot;&gt;
&lt;title&gt;graphviz-34cd77ba92d6e898bab41a54b23f2324&lt;/title&gt;
&lt;desc&gt;
digraph G {
	rankdir=BT
	{rank=same h1 h2 hddd hn}
	{rank=same x1 x2 xddd xn}
	{rank=same y1 y2 yddd yn}
	xddd[label=&amp;quot;...&amp;quot;]
	yddd[label=&amp;quot;...&amp;quot;]
	hddd[label=&amp;quot;...&amp;quot;]

	y1[shape=plaintext]
	y2[shape=plaintext]
	yddd[shape=plaintext]
	yn[shape=plaintext]
	x1[shape=plaintext]
	x2[shape=plaintext]
	xddd[shape=plaintext]
	xn[shape=plaintext]

	h1 -&amp;gt; h2
	h2 -&amp;gt; hddd
	hddd -&amp;gt; hn

	x1 -&amp;gt; h1
	x2 -&amp;gt; h2
	xddd -&amp;gt; hddd
	xn -&amp;gt; hn

	h1 -&amp;gt; y1
	h2 -&amp;gt; y2
	hddd -&amp;gt; yddd
	hn -&amp;gt; yn
}
&lt;/desc&gt;

&lt;g id=&quot;graph0&quot; class=&quot;graph&quot; transform=&quot;scale(1 1) rotate(0) translate(4 184)&quot;&gt;
&lt;title&gt;G&lt;/title&gt;
&lt;polygon fill=&quot;white&quot; stroke=&quot;transparent&quot; points=&quot;-4,4 -4,-184 274,-184 274,4 -4,4&quot; /&gt;
&lt;!-- h1 --&gt;
&lt;g id=&quot;node1&quot; class=&quot;node&quot;&gt;
&lt;title&gt;h1&lt;/title&gt;
&lt;ellipse fill=&quot;none&quot; stroke=&quot;black&quot; cx=&quot;27&quot; cy=&quot;-90&quot; rx=&quot;27&quot; ry=&quot;18&quot; /&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;27&quot; y=&quot;-86.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;h1&lt;/text&gt;
&lt;/g&gt;
&lt;!-- h2 --&gt;
&lt;g id=&quot;node2&quot; class=&quot;node&quot;&gt;
&lt;title&gt;h2&lt;/title&gt;
&lt;ellipse fill=&quot;none&quot; stroke=&quot;black&quot; cx=&quot;99&quot; cy=&quot;-90&quot; rx=&quot;27&quot; ry=&quot;18&quot; /&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;99&quot; y=&quot;-86.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;h2&lt;/text&gt;
&lt;/g&gt;
&lt;!-- h1&amp;#45;&amp;gt;h2 --&gt;
&lt;g id=&quot;edge1&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;h1&amp;#45;&amp;gt;h2&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M54,-90C56.61,-90 59.23,-90 61.84,-90&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;61.93,-93.5 71.93,-90 61.93,-86.5 61.93,-93.5&quot; /&gt;
&lt;/g&gt;
&lt;!-- y1 --&gt;
&lt;g id=&quot;node9&quot; class=&quot;node&quot;&gt;
&lt;title&gt;y1&lt;/title&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;27&quot; y=&quot;-158.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;y1&lt;/text&gt;
&lt;/g&gt;
&lt;!-- h1&amp;#45;&amp;gt;y1 --&gt;
&lt;g id=&quot;edge8&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;h1&amp;#45;&amp;gt;y1&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M27,-108.3C27,-116.02 27,-125.29 27,-133.89&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;23.5,-133.9 27,-143.9 30.5,-133.9 23.5,-133.9&quot; /&gt;
&lt;/g&gt;
&lt;!-- hddd --&gt;
&lt;g id=&quot;node3&quot; class=&quot;node&quot;&gt;
&lt;title&gt;hddd&lt;/title&gt;
&lt;ellipse fill=&quot;none&quot; stroke=&quot;black&quot; cx=&quot;171&quot; cy=&quot;-90&quot; rx=&quot;27&quot; ry=&quot;18&quot; /&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;171&quot; y=&quot;-86.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;...&lt;/text&gt;
&lt;/g&gt;
&lt;!-- h2&amp;#45;&amp;gt;hddd --&gt;
&lt;g id=&quot;edge2&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;h2&amp;#45;&amp;gt;hddd&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M126,-90C128.61,-90 131.23,-90 133.84,-90&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;133.93,-93.5 143.93,-90 133.93,-86.5 133.93,-93.5&quot; /&gt;
&lt;/g&gt;
&lt;!-- y2 --&gt;
&lt;g id=&quot;node10&quot; class=&quot;node&quot;&gt;
&lt;title&gt;y2&lt;/title&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;99&quot; y=&quot;-158.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;y2&lt;/text&gt;
&lt;/g&gt;
&lt;!-- h2&amp;#45;&amp;gt;y2 --&gt;
&lt;g id=&quot;edge9&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;h2&amp;#45;&amp;gt;y2&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M99,-108.3C99,-116.02 99,-125.29 99,-133.89&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;95.5,-133.9 99,-143.9 102.5,-133.9 95.5,-133.9&quot; /&gt;
&lt;/g&gt;
&lt;!-- hn --&gt;
&lt;g id=&quot;node4&quot; class=&quot;node&quot;&gt;
&lt;title&gt;hn&lt;/title&gt;
&lt;ellipse fill=&quot;none&quot; stroke=&quot;black&quot; cx=&quot;243&quot; cy=&quot;-90&quot; rx=&quot;27&quot; ry=&quot;18&quot; /&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;243&quot; y=&quot;-86.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;hn&lt;/text&gt;
&lt;/g&gt;
&lt;!-- hddd&amp;#45;&amp;gt;hn --&gt;
&lt;g id=&quot;edge3&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;hddd&amp;#45;&amp;gt;hn&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M198,-90C200.61,-90 203.23,-90 205.84,-90&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;205.93,-93.5 215.93,-90 205.93,-86.5 205.93,-93.5&quot; /&gt;
&lt;/g&gt;
&lt;!-- yddd --&gt;
&lt;g id=&quot;node11&quot; class=&quot;node&quot;&gt;
&lt;title&gt;yddd&lt;/title&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;171&quot; y=&quot;-158.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;...&lt;/text&gt;
&lt;/g&gt;
&lt;!-- hddd&amp;#45;&amp;gt;yddd --&gt;
&lt;g id=&quot;edge10&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;hddd&amp;#45;&amp;gt;yddd&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M171,-108.3C171,-116.02 171,-125.29 171,-133.89&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;167.5,-133.9 171,-143.9 174.5,-133.9 167.5,-133.9&quot; /&gt;
&lt;/g&gt;
&lt;!-- yn --&gt;
&lt;g id=&quot;node12&quot; class=&quot;node&quot;&gt;
&lt;title&gt;yn&lt;/title&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;243&quot; y=&quot;-158.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;yn&lt;/text&gt;
&lt;/g&gt;
&lt;!-- hn&amp;#45;&amp;gt;yn --&gt;
&lt;g id=&quot;edge11&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;hn&amp;#45;&amp;gt;yn&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M243,-108.3C243,-116.02 243,-125.29 243,-133.89&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;239.5,-133.9 243,-143.9 246.5,-133.9 239.5,-133.9&quot; /&gt;
&lt;/g&gt;
&lt;!-- x1 --&gt;
&lt;g id=&quot;node5&quot; class=&quot;node&quot;&gt;
&lt;title&gt;x1&lt;/title&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;27&quot; y=&quot;-14.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;x1&lt;/text&gt;
&lt;/g&gt;
&lt;!-- x1&amp;#45;&amp;gt;h1 --&gt;
&lt;g id=&quot;edge4&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;x1&amp;#45;&amp;gt;h1&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M27,-36.3C27,-44.02 27,-53.29 27,-61.89&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;23.5,-61.9 27,-71.9 30.5,-61.9 23.5,-61.9&quot; /&gt;
&lt;/g&gt;
&lt;!-- x2 --&gt;
&lt;g id=&quot;node6&quot; class=&quot;node&quot;&gt;
&lt;title&gt;x2&lt;/title&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;99&quot; y=&quot;-14.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;x2&lt;/text&gt;
&lt;/g&gt;
&lt;!-- x2&amp;#45;&amp;gt;h2 --&gt;
&lt;g id=&quot;edge5&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;x2&amp;#45;&amp;gt;h2&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M99,-36.3C99,-44.02 99,-53.29 99,-61.89&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;95.5,-61.9 99,-71.9 102.5,-61.9 95.5,-61.9&quot; /&gt;
&lt;/g&gt;
&lt;!-- xddd --&gt;
&lt;g id=&quot;node7&quot; class=&quot;node&quot;&gt;
&lt;title&gt;xddd&lt;/title&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;171&quot; y=&quot;-14.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;...&lt;/text&gt;
&lt;/g&gt;
&lt;!-- xddd&amp;#45;&amp;gt;hddd --&gt;
&lt;g id=&quot;edge6&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;xddd&amp;#45;&amp;gt;hddd&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M171,-36.3C171,-44.02 171,-53.29 171,-61.89&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;167.5,-61.9 171,-71.9 174.5,-61.9 167.5,-61.9&quot; /&gt;
&lt;/g&gt;
&lt;!-- xn --&gt;
&lt;g id=&quot;node8&quot; class=&quot;node&quot;&gt;
&lt;title&gt;xn&lt;/title&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;243&quot; y=&quot;-14.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;xn&lt;/text&gt;
&lt;/g&gt;
&lt;!-- xn&amp;#45;&amp;gt;hn --&gt;
&lt;g id=&quot;edge7&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;xn&amp;#45;&amp;gt;hn&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M243,-36.3C243,-44.02 243,-53.29 243,-61.89&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;239.5,-61.9 243,-71.9 246.5,-61.9 239.5,-61.9&quot; /&gt;
&lt;/g&gt;
&lt;/g&gt;
&lt;/svg&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;这种输入和输出数据项数一致的 RNN，一般叫做 N vs. N 的 RNN。如果我们用 PyTorch 来实现一个非常简单的经典 RNN 则如下：&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch.nn&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 创建一个 RNN 实例
# 第一个参数
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rnn&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;RNN&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_first&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# 实例化一个单向单层RNN
&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# 输入是一个形状为 (5, 3, 10) 的张量
# 5 个输入数据项（也可以说是样本）
# 3 个数据项是一个序列，有 3 个 steps
# 每个 step 有 10 个特征
&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 隐藏层是一个 (1, 5, 20) 的张量
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 调用 rnn 函数后，返回输出、最终的隐藏状态
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hn&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rnn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;我们来解读一下这段代码：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;这段代码实例化了一个带有 1 个隐藏层的 RNN 网络。&lt;/li&gt;
  &lt;li&gt;它的输入是一个形状为 (5, 3, 10) 的张量，表示有 5 个样本，每个样本有 3 个时间步，每个时间步的特征维度是 10。&lt;/li&gt;
  &lt;li&gt;初始隐藏状态是一个形状为 (1, 5, 20) 的张量。&lt;/li&gt;
  &lt;li&gt;调用 rnn 函数后，会返回输出和最终的隐藏状态。&lt;/li&gt;
  &lt;li&gt;输出的形状是 (5, 3, 20)，表示有 5 个样本，每个样本有 3 个时间步，每个时间步的输出维度是 20。&lt;/li&gt;
  &lt;li&gt;最终的隐藏状态的形状是 (1, 5, 20)，表示最后的隐藏状态是 5&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;但是上面的代码示例，并没有自己编写一个具体的 RNN，而是用了默认的 PyTorch 的 RNN，那么下面我们就自己编写一个：&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;MikeCaptainRNN&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hidden_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;nb&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# 对于 RNN，输入维度就是序列数
&lt;/span&gt;        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_size&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# 隐藏层有多少个节点/神经元，经常将 hidden_size 设置为与序列长度相同
&lt;/span&gt;        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hidden_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hidden_size&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# 输入层到隐藏层的 W^{xh} 权重、bias^{xh} 偏置项
&lt;/span&gt;        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weight_xh&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hidden_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.01&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bias_xh&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hidden_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# 隐藏层到隐藏层的 W^{hh} 权重、bias^{hh} 偏置项
&lt;/span&gt;        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weight_hh&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hidden_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hidden_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.01&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bias_hh&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hidden_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# 前向传播
&lt;/span&gt;    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;

    	&lt;span class=&quot;c1&quot;&gt;# 取出这个张量的形状
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;L&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# 初始化一个全零张量
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;L&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hidden_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# 处理每个时刻的输入特征
&lt;/span&gt;        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;L&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;

        	&lt;span class=&quot;c1&quot;&gt;# 获得当前时刻的输入特征，[N, input_size, 1]。unsqueeze(n)，在第 n 维上增加一维
&lt;/span&gt;            &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unsqueeze&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  
            &lt;span class=&quot;n&quot;&gt;w_xh_batch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weight_xh&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unsqueeze&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# [N, hidden_size, input_size]
&lt;/span&gt;            &lt;span class=&quot;n&quot;&gt;w_hh_batch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weight_hh&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unsqueeze&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# [N, hidden_size, hidden_size]
&lt;/span&gt;
            &lt;span class=&quot;c1&quot;&gt;# bmm 是矩阵乘法函数
&lt;/span&gt;            &lt;span class=&quot;n&quot;&gt;w_times_x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bmm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w_xh_batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;squeeze&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# [N, hidden_size]。squeeze(n)，在第n维上减小一维
&lt;/span&gt;            &lt;span class=&quot;n&quot;&gt;w_times_h&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bmm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w_hh_batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unsqueeze&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;squeeze&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# [N, hidden_size]
&lt;/span&gt;            &lt;span class=&quot;n&quot;&gt;h0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tanh&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w_times_x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bias_ih&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w_times_h&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bias_hh&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h0&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unsqueeze&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;源码解读都在注释中。&lt;/p&gt;

&lt;h4 id=&quot;42n-vs1-的-rnn&quot;&gt;4.2、N vs.1 的 RNN&lt;/h4&gt;

&lt;p&gt;上面那个图里，如果只保留最后一个输出，那就是一个 N vs. 1 的 RNN 了。这种的应用场景，比如说判断一个文本序列是英语还是德语，比如根据一个输入序列来判断是一个正向情绪内容还是负向或者中性，或者比如根据一段语音输入序列来判断是哪一首曲子（听歌识曲）。&lt;/p&gt;

\[\begin{aligned}
&amp;amp;\bm{h}_t = tanh(\bm{W^{xh}} \cdot \bm{x}_t + \bm{b^{xh}} + \bm{W^{hh}} \cdot \bm{h}_{t-1} + \bm{b^{hh}}) \\
&amp;amp;\bm{y} = Softmax(\bm{W^{hy}} \cdot \bm{h}_n + \bm{b^{hy}})
\end{aligned}\]

&lt;p&gt;即这个模型里，每个序列只有隐藏层对最后一个数据项进行处理时才产生输出  \(h_n\)  如果用示意图表示，则是如下结构：&lt;/p&gt;

&lt;div style=&quot;text-align: center;&quot;&gt;
&lt;div class=&quot;graphviz-wrapper&quot;&gt;

&lt;!-- Generated by graphviz version 2.43.0 (0)
 --&gt;
&lt;!-- Title: G Pages: 1 --&gt;
&lt;svg role=&quot;img&quot; aria-label=&quot;graphviz-99506286249ff03a109fde8e4294e12c&quot; width=&quot;278pt&quot; height=&quot;188pt&quot; viewBox=&quot;0.00 0.00 278.00 188.00&quot;&gt;
&lt;title&gt;graphviz-99506286249ff03a109fde8e4294e12c&lt;/title&gt;
&lt;desc&gt;
digraph G {
	rankdir=BT
	{rank=same h1 h2 hddd hn}
	hddd[label=&amp;quot;...&amp;quot;]
	xddd[label=&amp;quot;...&amp;quot;]

	y[shape=plaintext]
	x1[shape=plaintext]
	x2[shape=plaintext]
	xddd[shape=plaintext]
	xn[shape=plaintext]

	h1 -&amp;gt; h2
	h2 -&amp;gt; hddd
	hddd -&amp;gt; hn

	x1 -&amp;gt; h1
	x2 -&amp;gt; h2
	xn -&amp;gt; hn
	xddd -&amp;gt; hddd

	hn -&amp;gt; y
}
&lt;/desc&gt;

&lt;g id=&quot;graph0&quot; class=&quot;graph&quot; transform=&quot;scale(1 1) rotate(0) translate(4 184)&quot;&gt;
&lt;title&gt;G&lt;/title&gt;
&lt;polygon fill=&quot;white&quot; stroke=&quot;transparent&quot; points=&quot;-4,4 -4,-184 274,-184 274,4 -4,4&quot; /&gt;
&lt;!-- h1 --&gt;
&lt;g id=&quot;node1&quot; class=&quot;node&quot;&gt;
&lt;title&gt;h1&lt;/title&gt;
&lt;ellipse fill=&quot;none&quot; stroke=&quot;black&quot; cx=&quot;27&quot; cy=&quot;-90&quot; rx=&quot;27&quot; ry=&quot;18&quot; /&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;27&quot; y=&quot;-86.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;h1&lt;/text&gt;
&lt;/g&gt;
&lt;!-- h2 --&gt;
&lt;g id=&quot;node2&quot; class=&quot;node&quot;&gt;
&lt;title&gt;h2&lt;/title&gt;
&lt;ellipse fill=&quot;none&quot; stroke=&quot;black&quot; cx=&quot;99&quot; cy=&quot;-90&quot; rx=&quot;27&quot; ry=&quot;18&quot; /&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;99&quot; y=&quot;-86.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;h2&lt;/text&gt;
&lt;/g&gt;
&lt;!-- h1&amp;#45;&amp;gt;h2 --&gt;
&lt;g id=&quot;edge1&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;h1&amp;#45;&amp;gt;h2&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M54,-90C56.61,-90 59.23,-90 61.84,-90&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;61.93,-93.5 71.93,-90 61.93,-86.5 61.93,-93.5&quot; /&gt;
&lt;/g&gt;
&lt;!-- hddd --&gt;
&lt;g id=&quot;node3&quot; class=&quot;node&quot;&gt;
&lt;title&gt;hddd&lt;/title&gt;
&lt;ellipse fill=&quot;none&quot; stroke=&quot;black&quot; cx=&quot;171&quot; cy=&quot;-90&quot; rx=&quot;27&quot; ry=&quot;18&quot; /&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;171&quot; y=&quot;-86.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;...&lt;/text&gt;
&lt;/g&gt;
&lt;!-- h2&amp;#45;&amp;gt;hddd --&gt;
&lt;g id=&quot;edge2&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;h2&amp;#45;&amp;gt;hddd&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M126,-90C128.61,-90 131.23,-90 133.84,-90&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;133.93,-93.5 143.93,-90 133.93,-86.5 133.93,-93.5&quot; /&gt;
&lt;/g&gt;
&lt;!-- hn --&gt;
&lt;g id=&quot;node4&quot; class=&quot;node&quot;&gt;
&lt;title&gt;hn&lt;/title&gt;
&lt;ellipse fill=&quot;none&quot; stroke=&quot;black&quot; cx=&quot;243&quot; cy=&quot;-90&quot; rx=&quot;27&quot; ry=&quot;18&quot; /&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;243&quot; y=&quot;-86.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;hn&lt;/text&gt;
&lt;/g&gt;
&lt;!-- hddd&amp;#45;&amp;gt;hn --&gt;
&lt;g id=&quot;edge3&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;hddd&amp;#45;&amp;gt;hn&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M198,-90C200.61,-90 203.23,-90 205.84,-90&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;205.93,-93.5 215.93,-90 205.93,-86.5 205.93,-93.5&quot; /&gt;
&lt;/g&gt;
&lt;!-- y --&gt;
&lt;g id=&quot;node6&quot; class=&quot;node&quot;&gt;
&lt;title&gt;y&lt;/title&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;243&quot; y=&quot;-158.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;y&lt;/text&gt;
&lt;/g&gt;
&lt;!-- hn&amp;#45;&amp;gt;y --&gt;
&lt;g id=&quot;edge8&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;hn&amp;#45;&amp;gt;y&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M243,-108.3C243,-116.02 243,-125.29 243,-133.89&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;239.5,-133.9 243,-143.9 246.5,-133.9 239.5,-133.9&quot; /&gt;
&lt;/g&gt;
&lt;!-- xddd --&gt;
&lt;g id=&quot;node5&quot; class=&quot;node&quot;&gt;
&lt;title&gt;xddd&lt;/title&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;171&quot; y=&quot;-14.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;...&lt;/text&gt;
&lt;/g&gt;
&lt;!-- xddd&amp;#45;&amp;gt;hddd --&gt;
&lt;g id=&quot;edge7&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;xddd&amp;#45;&amp;gt;hddd&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M171,-36.3C171,-44.02 171,-53.29 171,-61.89&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;167.5,-61.9 171,-71.9 174.5,-61.9 167.5,-61.9&quot; /&gt;
&lt;/g&gt;
&lt;!-- x1 --&gt;
&lt;g id=&quot;node7&quot; class=&quot;node&quot;&gt;
&lt;title&gt;x1&lt;/title&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;27&quot; y=&quot;-14.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;x1&lt;/text&gt;
&lt;/g&gt;
&lt;!-- x1&amp;#45;&amp;gt;h1 --&gt;
&lt;g id=&quot;edge4&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;x1&amp;#45;&amp;gt;h1&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M27,-36.3C27,-44.02 27,-53.29 27,-61.89&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;23.5,-61.9 27,-71.9 30.5,-61.9 23.5,-61.9&quot; /&gt;
&lt;/g&gt;
&lt;!-- x2 --&gt;
&lt;g id=&quot;node8&quot; class=&quot;node&quot;&gt;
&lt;title&gt;x2&lt;/title&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;99&quot; y=&quot;-14.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;x2&lt;/text&gt;
&lt;/g&gt;
&lt;!-- x2&amp;#45;&amp;gt;h2 --&gt;
&lt;g id=&quot;edge5&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;x2&amp;#45;&amp;gt;h2&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M99,-36.3C99,-44.02 99,-53.29 99,-61.89&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;95.5,-61.9 99,-71.9 102.5,-61.9 95.5,-61.9&quot; /&gt;
&lt;/g&gt;
&lt;!-- xn --&gt;
&lt;g id=&quot;node9&quot; class=&quot;node&quot;&gt;
&lt;title&gt;xn&lt;/title&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;243&quot; y=&quot;-14.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;xn&lt;/text&gt;
&lt;/g&gt;
&lt;!-- xn&amp;#45;&amp;gt;hn --&gt;
&lt;g id=&quot;edge6&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;xn&amp;#45;&amp;gt;hn&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M243,-36.3C243,-44.02 243,-53.29 243,-61.89&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;239.5,-61.9 243,-71.9 246.5,-61.9 239.5,-61.9&quot; /&gt;
&lt;/g&gt;
&lt;/g&gt;
&lt;/svg&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;h4 id=&quot;431-vs-n-的-rnn&quot;&gt;4.3、1 vs. N 的 RNN&lt;/h4&gt;

&lt;p&gt;反过来，上面那个图里，如果只保留一个 x，那么就是一个 1 vs. N 的 RNN 了。这种场景的应用，比如 AI 创作音乐，还有通过一个 image 提炼或识别某些文本内容输出。&lt;/p&gt;

\[\begin{aligned}
&amp;amp;\bm{h}_t = \begin{cases} tanh(\bm{W^{xh}} \cdot \bm{x} + \bm{b^{xh}} + 0 + \bm{b^{hh}}) &amp;amp; (t=1) \\
tanh(0 + \bm{b^{xh}} + \bm{W^{hh}} \cdot \bm{h}_{t-1} + \bm{b^{hh}}) &amp;amp; (t&amp;gt;1) \end{cases} \\
&amp;amp;\bm{y}_t = Softmax(\bm{W^{hy}} \cdot \bm{h}_t + \bm{b^{hy}})
\end{aligned}\]

&lt;p&gt;示意图如下：&lt;/p&gt;

&lt;div style=&quot;text-align: center;&quot;&gt;
&lt;div class=&quot;graphviz-wrapper&quot;&gt;

&lt;!-- Generated by graphviz version 2.43.0 (0)
 --&gt;
&lt;!-- Title: G Pages: 1 --&gt;
&lt;svg role=&quot;img&quot; aria-label=&quot;graphviz-aecb5ea5cd91fc1106b18c3c4059fa0a&quot; width=&quot;278pt&quot; height=&quot;188pt&quot; viewBox=&quot;0.00 0.00 278.00 188.00&quot;&gt;
&lt;title&gt;graphviz-aecb5ea5cd91fc1106b18c3c4059fa0a&lt;/title&gt;
&lt;desc&gt;
digraph G {
	rankdir=BT
	{rank=same h1 h2 hddd hn}
	{rank=same y1 y2 yddd yn}
	hddd[label=&amp;quot;...&amp;quot;]
	yddd[label=&amp;quot;...&amp;quot;]

	y1[shape=plaintext]
	y2[shape=plaintext]
	yddd[shape=plaintext]
	yn[shape=plaintext]
	x[shape=plaintext]

	h1 -&amp;gt; h2
	h2 -&amp;gt; hddd
	hddd -&amp;gt; hn

	x -&amp;gt; h1

	h1 -&amp;gt; y1
	h2 -&amp;gt; y2
	hddd -&amp;gt; yddd
	hn -&amp;gt; yn
}
&lt;/desc&gt;

&lt;g id=&quot;graph0&quot; class=&quot;graph&quot; transform=&quot;scale(1 1) rotate(0) translate(4 184)&quot;&gt;
&lt;title&gt;G&lt;/title&gt;
&lt;polygon fill=&quot;white&quot; stroke=&quot;transparent&quot; points=&quot;-4,4 -4,-184 274,-184 274,4 -4,4&quot; /&gt;
&lt;!-- h1 --&gt;
&lt;g id=&quot;node1&quot; class=&quot;node&quot;&gt;
&lt;title&gt;h1&lt;/title&gt;
&lt;ellipse fill=&quot;none&quot; stroke=&quot;black&quot; cx=&quot;27&quot; cy=&quot;-90&quot; rx=&quot;27&quot; ry=&quot;18&quot; /&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;27&quot; y=&quot;-86.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;h1&lt;/text&gt;
&lt;/g&gt;
&lt;!-- h2 --&gt;
&lt;g id=&quot;node2&quot; class=&quot;node&quot;&gt;
&lt;title&gt;h2&lt;/title&gt;
&lt;ellipse fill=&quot;none&quot; stroke=&quot;black&quot; cx=&quot;99&quot; cy=&quot;-90&quot; rx=&quot;27&quot; ry=&quot;18&quot; /&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;99&quot; y=&quot;-86.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;h2&lt;/text&gt;
&lt;/g&gt;
&lt;!-- h1&amp;#45;&amp;gt;h2 --&gt;
&lt;g id=&quot;edge1&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;h1&amp;#45;&amp;gt;h2&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M54,-90C56.61,-90 59.23,-90 61.84,-90&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;61.93,-93.5 71.93,-90 61.93,-86.5 61.93,-93.5&quot; /&gt;
&lt;/g&gt;
&lt;!-- y1 --&gt;
&lt;g id=&quot;node5&quot; class=&quot;node&quot;&gt;
&lt;title&gt;y1&lt;/title&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;27&quot; y=&quot;-158.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;y1&lt;/text&gt;
&lt;/g&gt;
&lt;!-- h1&amp;#45;&amp;gt;y1 --&gt;
&lt;g id=&quot;edge5&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;h1&amp;#45;&amp;gt;y1&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M27,-108.3C27,-116.02 27,-125.29 27,-133.89&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;23.5,-133.9 27,-143.9 30.5,-133.9 23.5,-133.9&quot; /&gt;
&lt;/g&gt;
&lt;!-- hddd --&gt;
&lt;g id=&quot;node3&quot; class=&quot;node&quot;&gt;
&lt;title&gt;hddd&lt;/title&gt;
&lt;ellipse fill=&quot;none&quot; stroke=&quot;black&quot; cx=&quot;171&quot; cy=&quot;-90&quot; rx=&quot;27&quot; ry=&quot;18&quot; /&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;171&quot; y=&quot;-86.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;...&lt;/text&gt;
&lt;/g&gt;
&lt;!-- h2&amp;#45;&amp;gt;hddd --&gt;
&lt;g id=&quot;edge2&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;h2&amp;#45;&amp;gt;hddd&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M126,-90C128.61,-90 131.23,-90 133.84,-90&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;133.93,-93.5 143.93,-90 133.93,-86.5 133.93,-93.5&quot; /&gt;
&lt;/g&gt;
&lt;!-- y2 --&gt;
&lt;g id=&quot;node6&quot; class=&quot;node&quot;&gt;
&lt;title&gt;y2&lt;/title&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;99&quot; y=&quot;-158.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;y2&lt;/text&gt;
&lt;/g&gt;
&lt;!-- h2&amp;#45;&amp;gt;y2 --&gt;
&lt;g id=&quot;edge6&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;h2&amp;#45;&amp;gt;y2&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M99,-108.3C99,-116.02 99,-125.29 99,-133.89&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;95.5,-133.9 99,-143.9 102.5,-133.9 95.5,-133.9&quot; /&gt;
&lt;/g&gt;
&lt;!-- hn --&gt;
&lt;g id=&quot;node4&quot; class=&quot;node&quot;&gt;
&lt;title&gt;hn&lt;/title&gt;
&lt;ellipse fill=&quot;none&quot; stroke=&quot;black&quot; cx=&quot;243&quot; cy=&quot;-90&quot; rx=&quot;27&quot; ry=&quot;18&quot; /&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;243&quot; y=&quot;-86.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;hn&lt;/text&gt;
&lt;/g&gt;
&lt;!-- hddd&amp;#45;&amp;gt;hn --&gt;
&lt;g id=&quot;edge3&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;hddd&amp;#45;&amp;gt;hn&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M198,-90C200.61,-90 203.23,-90 205.84,-90&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;205.93,-93.5 215.93,-90 205.93,-86.5 205.93,-93.5&quot; /&gt;
&lt;/g&gt;
&lt;!-- yddd --&gt;
&lt;g id=&quot;node7&quot; class=&quot;node&quot;&gt;
&lt;title&gt;yddd&lt;/title&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;171&quot; y=&quot;-158.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;...&lt;/text&gt;
&lt;/g&gt;
&lt;!-- hddd&amp;#45;&amp;gt;yddd --&gt;
&lt;g id=&quot;edge7&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;hddd&amp;#45;&amp;gt;yddd&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M171,-108.3C171,-116.02 171,-125.29 171,-133.89&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;167.5,-133.9 171,-143.9 174.5,-133.9 167.5,-133.9&quot; /&gt;
&lt;/g&gt;
&lt;!-- yn --&gt;
&lt;g id=&quot;node8&quot; class=&quot;node&quot;&gt;
&lt;title&gt;yn&lt;/title&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;243&quot; y=&quot;-158.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;yn&lt;/text&gt;
&lt;/g&gt;
&lt;!-- hn&amp;#45;&amp;gt;yn --&gt;
&lt;g id=&quot;edge8&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;hn&amp;#45;&amp;gt;yn&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M243,-108.3C243,-116.02 243,-125.29 243,-133.89&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;239.5,-133.9 243,-143.9 246.5,-133.9 239.5,-133.9&quot; /&gt;
&lt;/g&gt;
&lt;!-- x --&gt;
&lt;g id=&quot;node9&quot; class=&quot;node&quot;&gt;
&lt;title&gt;x&lt;/title&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;27&quot; y=&quot;-14.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;x&lt;/text&gt;
&lt;/g&gt;
&lt;!-- x&amp;#45;&amp;gt;h1 --&gt;
&lt;g id=&quot;edge4&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;x&amp;#45;&amp;gt;h1&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M27,-36.3C27,-44.02 27,-53.29 27,-61.89&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;23.5,-61.9 27,-71.9 30.5,-61.9 23.5,-61.9&quot; /&gt;
&lt;/g&gt;
&lt;/g&gt;
&lt;/svg&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;到这里我们可以看到，在 RNN 的隐藏层是能够存储一些有关于输入数据的一些相关内容的，所以也常把 RNN 的隐藏层叫做记忆单元。&lt;/p&gt;

&lt;h4 id=&quot;44lstmlong-short-term-memory长短时记忆网络&quot;&gt;4.4、LSTM（Long Short-Term Memory）长短时记忆网络&lt;/h4&gt;

&lt;h5 id=&quot;441如何理解这个-short-term-呢&quot;&gt;4.4.1、如何理解这个 Short-Term 呢？&lt;/h5&gt;

&lt;p&gt;1997 年论文《Long Short-Term Memory》中提出 LSTM 模型。我们先从模型的定义，精确地来理解一下：&lt;/p&gt;

\[\begin{aligned}
&amp;amp;\bm{h}_t = \bm{h}_{t-1} + tanh(\bm{W}^{xh} \cdot \bm{x}_t + \bm{b}^{xh} + \bm{W}^{hh} \cdot \bm{h}_{t-1} + \bm{b}^{hh}) \\
&amp;amp;\bm{y}_t = Softmax(\bm{W}^{hy} \cdot \bm{h_t} + \bm{b}^{hy})
\end{aligned}\]

&lt;p&gt;上式中与经典结构的 RNN（输入与输出是 N vs. N）相比，唯一的区别是第一个式子中多了一个「 \(\bm{h}_{t-1}\) 」。如果我们把第一个式子的  \(tanh\)  部分记作  \(u_t\) ：&lt;/p&gt;

\[\bm{u}_t = tanh(\bm{W}^{xh} \cdot \bm{x}_t + \bm{b}^{xh} + \bm{W}^{hh} \cdot \bm{h}_{t-1} + \bm{b}^{hh})\]

&lt;p&gt;所以：&lt;/p&gt;

\[\bm{h}_t = \bm{h}_{t-1} + \bm{u}_t\]

&lt;p&gt;那么可以展开出如下一组式子：&lt;/p&gt;

\[\begin{aligned}
\bm{h}_{k+1} &amp;amp;= \bm{h}_k + \bm{u}_{k+1} \\
\bm{h}_{k+2} &amp;amp;= \bm{h}_{k+1} + \bm{u}_{k+2} \\
&amp;amp;...... \\
\bm{h}_{t-1} &amp;amp;= \bm{h}_{t-2} + \bm{u}_{t-1} \\
\bm{h}_t &amp;amp;= \bm{h}_{t-1} + \bm{u}_t
\end{aligned}\]

&lt;p&gt;如果我们从  \(h_{k+1}\)  到  \(h_n\)  的所有式子左侧相加、右侧相加，我们就得到如下式子：&lt;/p&gt;

\[\begin{aligned}
&amp;amp;\bm{h}_{k+1} + ... + \bm{h}_{t-1} + \bm{h}_t \\
= &amp;amp;\bm{h}_k + \bm{h}_{k+1} + ... + \bm{h}_{t-2} + \bm{h}_{t-1} \\+ &amp;amp;\bm{u}_{k+1} + \bm{u}_{k+2} + ... + \bm{u}_{t-1} + \bm{u}_t
\end{aligned}\]

&lt;p&gt;进而推导出：&lt;/p&gt;

\[\bm{h}_t = \bm{h}_k + \bm{u}_{k+1} + \bm{u}_{k+2} + ... + \bm{u}_{t-1} + \bm{u}_t\]

&lt;p&gt;从这里我们就可以看到，第 t 时刻的隐藏层输出，直接关联到第 k 时刻的输出，t 到 k 时刻的相关性则用  \(\bm{u}_{k+1}\)  到  \(\bm{u}_t\)  相加表示。也就是有 t-k 的短期（Short Term）记忆。&lt;/p&gt;

&lt;h5 id=&quot;442引入遗忘门-f输入门-i输出门-o记忆细胞-c&quot;&gt;4.4.2、引入遗忘门 f、输入门 i、输出门 o、记忆细胞 c&lt;/h5&gt;

&lt;p&gt;如果我们为式子  \(\bm{h}_t = \bm{h}_{t-1} + \bm{u}_t\)  右侧两项分配一个权重呢？就是隐藏层对上一个数据项本身被上一个数据项经过隐藏层计算的结果，这两者做一对权重考虑配比，如下：&lt;/p&gt;

\[\begin{aligned}
&amp;amp;\bm{f}_t = sigmoid(\bm{W}^{f,xh} \cdot \bm{x}_t + \bm{b}^{f,xh} + \bm{W}^{f,hh} \cdot \bm{x}_{t-1} + \bm{b}^{f,hh}) \\
&amp;amp;\bm{h}_t = \bm{f}_t \odot \bm{h}_{t-1} + (1 - \bm{f}_t) \odot \bm{u}_t
\end{aligned}\]

&lt;p&gt;其中：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;\(\odot\)  是 Hardamard 乘积，即张量的对应元素相乘。&lt;/li&gt;
  &lt;li&gt;\(\bm{f}_t\)  是「遗忘门（Forget Gate）」，该值很小时 t-1 时刻的权重就很小，也就是「此刻遗忘上一刻」。该值应根据 t 时刻的输入数据、t-1 时刻数据在隐藏层的输出计算，而且其每个元素必须是 (0, 1) 之间的值，所以可以用 sigmoid 函数来得到该值：&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;但这种方式，对于过去  \(\bm{h}_{t-1}\)  和当下  \(\bm{u}_t\)  形成了互斥，只能此消彼长。但其实过去和当下可能都很重要，有可能都恨不重要，所以我们对过去继续采用  \(\bm{f}_t\)  遗忘门，对当下采用  \(\bm{i}_t\)  输入门（Input Gate）：&lt;/p&gt;

\[\begin{aligned}
&amp;amp;\bm{f}_t = sigmoid(\bm{W}^{f,xh} \cdot \bm{x}_t + \bm{b}^{f,xh} + \bm{W}^{f,hh} \cdot \bm{x}_{t-1} + \bm{b}^{f,hh}) \\
&amp;amp;\bm{i}_t = sigmoid(\bm{W}^{i,xh} \cdot \bm{x}_t + \bm{b}^{i,xh} + \bm{W}^{i,hh} \cdot \bm{h}_{t-1} + \bm{b}^{i,hh}) \\
&amp;amp;\bm{h}_t = \bm{f}_t \odot \bm{h}_{t-1} + \bm{i}_t \odot \bm{u}_t
\end{aligned}\]

&lt;p&gt;其中：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;与  \(\bm{f}_t\)  类似地，定义输入门  \(\bm{i}_t\)  ，但是注意  \(\bm{f}_t\)  与  \(\bm{h}_{t-1}\)  而非  \(\bm{x}_{t-1}\)  有关。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;再引入一个输出门：&lt;/p&gt;

\[\bm{o}_t = sigmoid(\bm{W}^{o,xh} \cdot \bm{x}_t + \bm{b}^{o,xh} + \bm{W}^{o,hh} \cdot \bm{x}_{t-1} + \bm{b}^{o,hh})\]

&lt;p&gt;再引入记忆细胞  \(\bm{c}_t\) ，它是原来  \(\bm{h}_t\)  的变体，与 t-1 时刻的记忆细胞有遗忘关系（通过遗忘门），与当下时刻有输入门的关系：&lt;/p&gt;

\[\bm{c}_t = \bm{f}_t \odot \bm{c}_{t-1} + \bm{i}_t \odot \bm{u}_t\]

&lt;p&gt;那么此时  \(\bm{h}_t\)  ，我们可以把  \(\bm{h}_t\)  变成：&lt;/p&gt;

\[\bm{h}_t = \bm{o}_t \odot tanh(\bm{c}_t)\]

&lt;p&gt;记忆细胞这个概念还有有一点点形象的，它存储了过去的一些信息。OK，到此我们整体的 LSTM 模型就变成了这个样子：&lt;/p&gt;

\[\begin{aligned}
&amp;amp;\bm{f}_t = sigmoid(\bm{W}^{f,xh} \cdot \bm{x}_t + \bm{b}^{f,xh} + \bm{W}^{f,hh} \cdot \bm{x}_{t-1} + \bm{b}^{f,hh}) \\
&amp;amp;\bm{i}_t = sigmoid(\bm{W}^{i,xh} \cdot \bm{x}_t + \bm{b}^{i,xh} + \bm{W}^{i,hh} \cdot \bm{h}_{t-1} + \bm{b}^{i,hh}) \\
&amp;amp;\bm{o}_t = sigmoid(\bm{W}^{o,xh} \cdot \bm{x}_t + \bm{b}^{o,xh} + \bm{W}^{o,hh} \cdot \bm{x}_{t-1} + \bm{b}^{o,hh}) \\
&amp;amp;\bm{u}_t = tanh(\bm{W}^{xh} \cdot \bm{x}_t + \bm{b}^{xh} + \bm{W}^{hh} \cdot \bm{h}_{t-1} + \bm{b}^{hh}) \\
&amp;amp;\bm{c}_t = \bm{f}_t \odot \bm{c}_{t-1} + \bm{i}_t \odot \bm{u}_t \\
&amp;amp;\bm{h}_t = \bm{o}_t \odot tanh(\bm{c}_t) \\
&amp;amp;\bm{y}_t = Softmax(\bm{W}^{hy} \cdot \bm{h_t} + \bm{b}^{hy})
\end{aligned}\]

&lt;h4 id=&quot;45双向循环神经网络birnn双向-lstmbilstm&quot;&gt;4.5、双向循环神经网络（BiRNN）、双向 LSTM（BiLSTM）&lt;/h4&gt;

&lt;p&gt;双向循环神经网络很好理解，就是两个方向都有，例如下图：&lt;/p&gt;

&lt;div style=&quot;text-align: center;&quot;&gt;
&lt;div class=&quot;graphviz-wrapper&quot;&gt;

&lt;!-- Generated by graphviz version 2.43.0 (0)
 --&gt;
&lt;!-- Title: G Pages: 1 --&gt;
&lt;svg role=&quot;img&quot; aria-label=&quot;graphviz-5d130f67fc1bf07abc38d62da6ddb01a&quot; width=&quot;278pt&quot; height=&quot;188pt&quot; viewBox=&quot;0.00 0.00 278.00 188.00&quot;&gt;
&lt;title&gt;graphviz-5d130f67fc1bf07abc38d62da6ddb01a&lt;/title&gt;
&lt;desc&gt;
digraph G {
	rankdir=BT
	{rank=same h1 h2 hddd hn}

	hddd[label=&amp;quot;...&amp;quot;]
	xddd[label=&amp;quot;...&amp;quot;]
	yddd[label=&amp;quot;...&amp;quot;]

	y1[shape=plaintext]
	y2[shape=plaintext]
	yddd[shape=plaintext]
	yn[shape=plaintext]
	x1[shape=plaintext]
	x2[shape=plaintext]
	xddd[shape=plaintext]
	xn[shape=plaintext]

	h1 -&amp;gt; y1
	h2 -&amp;gt; y2
	hddd -&amp;gt; yddd
	hn -&amp;gt; yn

	h1 -&amp;gt; h2
	h2 -&amp;gt; hddd
	hddd -&amp;gt; hn

	hn -&amp;gt; hddd
	hddd -&amp;gt; h2
	h2 -&amp;gt; h1

	x1 -&amp;gt; h1
	x2 -&amp;gt; h2
	xddd -&amp;gt; hddd
	xn -&amp;gt; hn
}
&lt;/desc&gt;

&lt;g id=&quot;graph0&quot; class=&quot;graph&quot; transform=&quot;scale(1 1) rotate(0) translate(4 184)&quot;&gt;
&lt;title&gt;G&lt;/title&gt;
&lt;polygon fill=&quot;white&quot; stroke=&quot;transparent&quot; points=&quot;-4,4 -4,-184 274,-184 274,4 -4,4&quot; /&gt;
&lt;!-- h1 --&gt;
&lt;g id=&quot;node1&quot; class=&quot;node&quot;&gt;
&lt;title&gt;h1&lt;/title&gt;
&lt;ellipse fill=&quot;none&quot; stroke=&quot;black&quot; cx=&quot;27&quot; cy=&quot;-90&quot; rx=&quot;27&quot; ry=&quot;18&quot; /&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;27&quot; y=&quot;-86.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;h1&lt;/text&gt;
&lt;/g&gt;
&lt;!-- h2 --&gt;
&lt;g id=&quot;node2&quot; class=&quot;node&quot;&gt;
&lt;title&gt;h2&lt;/title&gt;
&lt;ellipse fill=&quot;none&quot; stroke=&quot;black&quot; cx=&quot;99&quot; cy=&quot;-90&quot; rx=&quot;27&quot; ry=&quot;18&quot; /&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;99&quot; y=&quot;-86.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;h2&lt;/text&gt;
&lt;/g&gt;
&lt;!-- h1&amp;#45;&amp;gt;h2 --&gt;
&lt;g id=&quot;edge5&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;h1&amp;#45;&amp;gt;h2&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M48.38,-101.27C54.78,-103.22 61.18,-103.89 67.58,-103.28&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;68.52,-106.66 77.64,-101.27 67.15,-99.8 68.52,-106.66&quot; /&gt;
&lt;/g&gt;
&lt;!-- y1 --&gt;
&lt;g id=&quot;node7&quot; class=&quot;node&quot;&gt;
&lt;title&gt;y1&lt;/title&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;27&quot; y=&quot;-158.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;y1&lt;/text&gt;
&lt;/g&gt;
&lt;!-- h1&amp;#45;&amp;gt;y1 --&gt;
&lt;g id=&quot;edge1&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;h1&amp;#45;&amp;gt;y1&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M27,-108.3C27,-116.02 27,-125.29 27,-133.89&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;23.5,-133.9 27,-143.9 30.5,-133.9 23.5,-133.9&quot; /&gt;
&lt;/g&gt;
&lt;!-- h2&amp;#45;&amp;gt;h1 --&gt;
&lt;g id=&quot;edge10&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;h2&amp;#45;&amp;gt;h1&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M77.64,-78.73C71.24,-76.78 64.84,-76.11 58.44,-76.72&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;57.49,-73.34 48.38,-78.73 58.87,-80.2 57.49,-73.34&quot; /&gt;
&lt;/g&gt;
&lt;!-- hddd --&gt;
&lt;g id=&quot;node3&quot; class=&quot;node&quot;&gt;
&lt;title&gt;hddd&lt;/title&gt;
&lt;ellipse fill=&quot;none&quot; stroke=&quot;black&quot; cx=&quot;171&quot; cy=&quot;-90&quot; rx=&quot;27&quot; ry=&quot;18&quot; /&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;171&quot; y=&quot;-86.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;...&lt;/text&gt;
&lt;/g&gt;
&lt;!-- h2&amp;#45;&amp;gt;hddd --&gt;
&lt;g id=&quot;edge6&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;h2&amp;#45;&amp;gt;hddd&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M120.38,-101.27C126.78,-103.22 133.18,-103.89 139.58,-103.28&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;140.52,-106.66 149.64,-101.27 139.15,-99.8 140.52,-106.66&quot; /&gt;
&lt;/g&gt;
&lt;!-- y2 --&gt;
&lt;g id=&quot;node8&quot; class=&quot;node&quot;&gt;
&lt;title&gt;y2&lt;/title&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;99&quot; y=&quot;-158.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;y2&lt;/text&gt;
&lt;/g&gt;
&lt;!-- h2&amp;#45;&amp;gt;y2 --&gt;
&lt;g id=&quot;edge2&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;h2&amp;#45;&amp;gt;y2&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M99,-108.3C99,-116.02 99,-125.29 99,-133.89&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;95.5,-133.9 99,-143.9 102.5,-133.9 95.5,-133.9&quot; /&gt;
&lt;/g&gt;
&lt;!-- hddd&amp;#45;&amp;gt;h2 --&gt;
&lt;g id=&quot;edge9&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;hddd&amp;#45;&amp;gt;h2&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M149.64,-78.73C143.24,-76.78 136.84,-76.11 130.44,-76.72&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;129.49,-73.34 120.38,-78.73 130.87,-80.2 129.49,-73.34&quot; /&gt;
&lt;/g&gt;
&lt;!-- hn --&gt;
&lt;g id=&quot;node4&quot; class=&quot;node&quot;&gt;
&lt;title&gt;hn&lt;/title&gt;
&lt;ellipse fill=&quot;none&quot; stroke=&quot;black&quot; cx=&quot;243&quot; cy=&quot;-90&quot; rx=&quot;27&quot; ry=&quot;18&quot; /&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;243&quot; y=&quot;-86.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;hn&lt;/text&gt;
&lt;/g&gt;
&lt;!-- hddd&amp;#45;&amp;gt;hn --&gt;
&lt;g id=&quot;edge7&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;hddd&amp;#45;&amp;gt;hn&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M192.38,-101.27C198.78,-103.22 205.18,-103.89 211.58,-103.28&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;212.52,-106.66 221.64,-101.27 211.15,-99.8 212.52,-106.66&quot; /&gt;
&lt;/g&gt;
&lt;!-- yddd --&gt;
&lt;g id=&quot;node6&quot; class=&quot;node&quot;&gt;
&lt;title&gt;yddd&lt;/title&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;171&quot; y=&quot;-158.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;...&lt;/text&gt;
&lt;/g&gt;
&lt;!-- hddd&amp;#45;&amp;gt;yddd --&gt;
&lt;g id=&quot;edge3&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;hddd&amp;#45;&amp;gt;yddd&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M171,-108.3C171,-116.02 171,-125.29 171,-133.89&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;167.5,-133.9 171,-143.9 174.5,-133.9 167.5,-133.9&quot; /&gt;
&lt;/g&gt;
&lt;!-- hn&amp;#45;&amp;gt;hddd --&gt;
&lt;g id=&quot;edge8&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;hn&amp;#45;&amp;gt;hddd&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M221.64,-78.73C215.24,-76.78 208.84,-76.11 202.44,-76.72&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;201.49,-73.34 192.38,-78.73 202.87,-80.2 201.49,-73.34&quot; /&gt;
&lt;/g&gt;
&lt;!-- yn --&gt;
&lt;g id=&quot;node9&quot; class=&quot;node&quot;&gt;
&lt;title&gt;yn&lt;/title&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;243&quot; y=&quot;-158.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;yn&lt;/text&gt;
&lt;/g&gt;
&lt;!-- hn&amp;#45;&amp;gt;yn --&gt;
&lt;g id=&quot;edge4&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;hn&amp;#45;&amp;gt;yn&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M243,-108.3C243,-116.02 243,-125.29 243,-133.89&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;239.5,-133.9 243,-143.9 246.5,-133.9 239.5,-133.9&quot; /&gt;
&lt;/g&gt;
&lt;!-- xddd --&gt;
&lt;g id=&quot;node5&quot; class=&quot;node&quot;&gt;
&lt;title&gt;xddd&lt;/title&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;171&quot; y=&quot;-14.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;...&lt;/text&gt;
&lt;/g&gt;
&lt;!-- xddd&amp;#45;&amp;gt;hddd --&gt;
&lt;g id=&quot;edge13&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;xddd&amp;#45;&amp;gt;hddd&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M171,-36.3C171,-44.02 171,-53.29 171,-61.89&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;167.5,-61.9 171,-71.9 174.5,-61.9 167.5,-61.9&quot; /&gt;
&lt;/g&gt;
&lt;!-- x1 --&gt;
&lt;g id=&quot;node10&quot; class=&quot;node&quot;&gt;
&lt;title&gt;x1&lt;/title&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;27&quot; y=&quot;-14.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;x1&lt;/text&gt;
&lt;/g&gt;
&lt;!-- x1&amp;#45;&amp;gt;h1 --&gt;
&lt;g id=&quot;edge11&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;x1&amp;#45;&amp;gt;h1&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M27,-36.3C27,-44.02 27,-53.29 27,-61.89&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;23.5,-61.9 27,-71.9 30.5,-61.9 23.5,-61.9&quot; /&gt;
&lt;/g&gt;
&lt;!-- x2 --&gt;
&lt;g id=&quot;node11&quot; class=&quot;node&quot;&gt;
&lt;title&gt;x2&lt;/title&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;99&quot; y=&quot;-14.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;x2&lt;/text&gt;
&lt;/g&gt;
&lt;!-- x2&amp;#45;&amp;gt;h2 --&gt;
&lt;g id=&quot;edge12&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;x2&amp;#45;&amp;gt;h2&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M99,-36.3C99,-44.02 99,-53.29 99,-61.89&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;95.5,-61.9 99,-71.9 102.5,-61.9 95.5,-61.9&quot; /&gt;
&lt;/g&gt;
&lt;!-- xn --&gt;
&lt;g id=&quot;node12&quot; class=&quot;node&quot;&gt;
&lt;title&gt;xn&lt;/title&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;243&quot; y=&quot;-14.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;xn&lt;/text&gt;
&lt;/g&gt;
&lt;!-- xn&amp;#45;&amp;gt;hn --&gt;
&lt;g id=&quot;edge14&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;xn&amp;#45;&amp;gt;hn&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M243,-36.3C243,-44.02 243,-53.29 243,-61.89&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;239.5,-61.9 243,-71.9 246.5,-61.9 239.5,-61.9&quot; /&gt;
&lt;/g&gt;
&lt;/g&gt;
&lt;/svg&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;在 PyTorch 中使用 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nn.RNN&lt;/code&gt; 就有参数表示双向：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bidirectional&lt;/code&gt; – If True, becomes a bidirectional RNN. Default: False&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bidirectional&lt;/code&gt;：默认设置为 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;False&lt;/code&gt;。若为 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;True&lt;/code&gt;，即为双向 RNN。&lt;/p&gt;

&lt;h4 id=&quot;46堆叠循环神经网络stacked-rnn堆叠长短时记忆网络stacked-lstm&quot;&gt;4.6、堆叠循环神经网络（Stacked RNN）、堆叠长短时记忆网络（Stacked LSTM）&lt;/h4&gt;

&lt;div style=&quot;text-align: center;&quot;&gt;
&lt;div class=&quot;graphviz-wrapper&quot;&gt;

&lt;!-- Generated by graphviz version 2.43.0 (0)
 --&gt;
&lt;!-- Title: G Pages: 1 --&gt;
&lt;svg role=&quot;img&quot; aria-label=&quot;graphviz-79478ee70f94925103b38a21c70c2539&quot; width=&quot;288pt&quot; height=&quot;260pt&quot; viewBox=&quot;0.00 0.00 288.19 260.00&quot;&gt;
&lt;title&gt;graphviz-79478ee70f94925103b38a21c70c2539&lt;/title&gt;
&lt;desc&gt;
digraph G {
	rankdir=BT
	{rank=same h11 h12 h1ddd h1n}
	{rank=same h21 h22 h2ddd h2n}

	h1ddd[label=&amp;quot;...&amp;quot;]
	h2ddd[label=&amp;quot;...&amp;quot;]
	xddd[label=&amp;quot;...&amp;quot;]
	yddd[label=&amp;quot;...&amp;quot;]

	y1[shape=plaintext]
	y2[shape=plaintext]
	yddd[shape=plaintext]
	yn[shape=plaintext]
	x1[shape=plaintext]
	x2[shape=plaintext]
	xddd[shape=plaintext]
	xn[shape=plaintext]

	h11 -&amp;gt; y1
	h12 -&amp;gt; y2
	h1ddd -&amp;gt; yddd
	h1n -&amp;gt; yn

	h11 -&amp;gt; h12
	h12 -&amp;gt; h1ddd
	h1ddd -&amp;gt; h1n

	h21 -&amp;gt; h22
	h22 -&amp;gt; h2ddd
	h2ddd -&amp;gt; h2n

	h21 -&amp;gt; h11
	h22 -&amp;gt; h12
	h2ddd -&amp;gt; h1ddd
	h2n -&amp;gt; h1n

	x1 -&amp;gt; h21
	x2 -&amp;gt; h22
	xddd -&amp;gt; h2ddd
	xn -&amp;gt; h2n
}
&lt;/desc&gt;

&lt;g id=&quot;graph0&quot; class=&quot;graph&quot; transform=&quot;scale(1 1) rotate(0) translate(4 256)&quot;&gt;
&lt;title&gt;G&lt;/title&gt;
&lt;polygon fill=&quot;white&quot; stroke=&quot;transparent&quot; points=&quot;-4,4 -4,-256 284.19,-256 284.19,4 -4,4&quot; /&gt;
&lt;!-- h11 --&gt;
&lt;g id=&quot;node1&quot; class=&quot;node&quot;&gt;
&lt;title&gt;h11&lt;/title&gt;
&lt;ellipse fill=&quot;none&quot; stroke=&quot;black&quot; cx=&quot;28.6&quot; cy=&quot;-162&quot; rx=&quot;28.7&quot; ry=&quot;18&quot; /&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;28.6&quot; y=&quot;-158.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;h11&lt;/text&gt;
&lt;/g&gt;
&lt;!-- h12 --&gt;
&lt;g id=&quot;node2&quot; class=&quot;node&quot;&gt;
&lt;title&gt;h12&lt;/title&gt;
&lt;ellipse fill=&quot;none&quot; stroke=&quot;black&quot; cx=&quot;103.6&quot; cy=&quot;-162&quot; rx=&quot;28.7&quot; ry=&quot;18&quot; /&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;103.6&quot; y=&quot;-158.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;h12&lt;/text&gt;
&lt;/g&gt;
&lt;!-- h11&amp;#45;&amp;gt;h12 --&gt;
&lt;g id=&quot;edge5&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;h11&amp;#45;&amp;gt;h12&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M57.31,-162C59.75,-162 62.19,-162 64.63,-162&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;64.67,-165.5 74.67,-162 64.67,-158.5 64.67,-165.5&quot; /&gt;
&lt;/g&gt;
&lt;!-- y1 --&gt;
&lt;g id=&quot;node11&quot; class=&quot;node&quot;&gt;
&lt;title&gt;y1&lt;/title&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;28.6&quot; y=&quot;-230.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;y1&lt;/text&gt;
&lt;/g&gt;
&lt;!-- h11&amp;#45;&amp;gt;y1 --&gt;
&lt;g id=&quot;edge1&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;h11&amp;#45;&amp;gt;y1&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M28.6,-180.3C28.6,-188.02 28.6,-197.29 28.6,-205.89&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;25.1,-205.9 28.6,-215.9 32.1,-205.9 25.1,-205.9&quot; /&gt;
&lt;/g&gt;
&lt;!-- h1ddd --&gt;
&lt;g id=&quot;node3&quot; class=&quot;node&quot;&gt;
&lt;title&gt;h1ddd&lt;/title&gt;
&lt;ellipse fill=&quot;none&quot; stroke=&quot;black&quot; cx=&quot;177.6&quot; cy=&quot;-162&quot; rx=&quot;27&quot; ry=&quot;18&quot; /&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;177.6&quot; y=&quot;-158.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;...&lt;/text&gt;
&lt;/g&gt;
&lt;!-- h12&amp;#45;&amp;gt;h1ddd --&gt;
&lt;g id=&quot;edge6&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;h12&amp;#45;&amp;gt;h1ddd&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M132.21,-162C134.85,-162 137.49,-162 140.13,-162&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;140.3,-165.5 150.3,-162 140.3,-158.5 140.3,-165.5&quot; /&gt;
&lt;/g&gt;
&lt;!-- y2 --&gt;
&lt;g id=&quot;node12&quot; class=&quot;node&quot;&gt;
&lt;title&gt;y2&lt;/title&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;103.6&quot; y=&quot;-230.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;y2&lt;/text&gt;
&lt;/g&gt;
&lt;!-- h12&amp;#45;&amp;gt;y2 --&gt;
&lt;g id=&quot;edge2&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;h12&amp;#45;&amp;gt;y2&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M103.6,-180.3C103.6,-188.02 103.6,-197.29 103.6,-205.89&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;100.1,-205.9 103.6,-215.9 107.1,-205.9 100.1,-205.9&quot; /&gt;
&lt;/g&gt;
&lt;!-- h1n --&gt;
&lt;g id=&quot;node4&quot; class=&quot;node&quot;&gt;
&lt;title&gt;h1n&lt;/title&gt;
&lt;ellipse fill=&quot;none&quot; stroke=&quot;black&quot; cx=&quot;251.6&quot; cy=&quot;-162&quot; rx=&quot;28.7&quot; ry=&quot;18&quot; /&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;251.6&quot; y=&quot;-158.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;h1n&lt;/text&gt;
&lt;/g&gt;
&lt;!-- h1ddd&amp;#45;&amp;gt;h1n --&gt;
&lt;g id=&quot;edge7&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;h1ddd&amp;#45;&amp;gt;h1n&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M204.77,-162C207.38,-162 210,-162 212.61,-162&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;212.7,-165.5 222.7,-162 212.7,-158.5 212.7,-165.5&quot; /&gt;
&lt;/g&gt;
&lt;!-- yddd --&gt;
&lt;g id=&quot;node10&quot; class=&quot;node&quot;&gt;
&lt;title&gt;yddd&lt;/title&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;177.6&quot; y=&quot;-230.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;...&lt;/text&gt;
&lt;/g&gt;
&lt;!-- h1ddd&amp;#45;&amp;gt;yddd --&gt;
&lt;g id=&quot;edge3&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;h1ddd&amp;#45;&amp;gt;yddd&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M177.6,-180.3C177.6,-188.02 177.6,-197.29 177.6,-205.89&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;174.1,-205.9 177.6,-215.9 181.1,-205.9 174.1,-205.9&quot; /&gt;
&lt;/g&gt;
&lt;!-- yn --&gt;
&lt;g id=&quot;node13&quot; class=&quot;node&quot;&gt;
&lt;title&gt;yn&lt;/title&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;251.6&quot; y=&quot;-230.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;yn&lt;/text&gt;
&lt;/g&gt;
&lt;!-- h1n&amp;#45;&amp;gt;yn --&gt;
&lt;g id=&quot;edge4&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;h1n&amp;#45;&amp;gt;yn&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M251.6,-180.3C251.6,-188.02 251.6,-197.29 251.6,-205.89&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;248.1,-205.9 251.6,-215.9 255.1,-205.9 248.1,-205.9&quot; /&gt;
&lt;/g&gt;
&lt;!-- h21 --&gt;
&lt;g id=&quot;node5&quot; class=&quot;node&quot;&gt;
&lt;title&gt;h21&lt;/title&gt;
&lt;ellipse fill=&quot;none&quot; stroke=&quot;black&quot; cx=&quot;28.6&quot; cy=&quot;-90&quot; rx=&quot;28.7&quot; ry=&quot;18&quot; /&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;28.6&quot; y=&quot;-86.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;h21&lt;/text&gt;
&lt;/g&gt;
&lt;!-- h21&amp;#45;&amp;gt;h11 --&gt;
&lt;g id=&quot;edge11&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;h21&amp;#45;&amp;gt;h11&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M28.6,-108.3C28.6,-116.02 28.6,-125.29 28.6,-133.89&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;25.1,-133.9 28.6,-143.9 32.1,-133.9 25.1,-133.9&quot; /&gt;
&lt;/g&gt;
&lt;!-- h22 --&gt;
&lt;g id=&quot;node6&quot; class=&quot;node&quot;&gt;
&lt;title&gt;h22&lt;/title&gt;
&lt;ellipse fill=&quot;none&quot; stroke=&quot;black&quot; cx=&quot;103.6&quot; cy=&quot;-90&quot; rx=&quot;28.7&quot; ry=&quot;18&quot; /&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;103.6&quot; y=&quot;-86.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;h22&lt;/text&gt;
&lt;/g&gt;
&lt;!-- h21&amp;#45;&amp;gt;h22 --&gt;
&lt;g id=&quot;edge8&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;h21&amp;#45;&amp;gt;h22&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M57.31,-90C59.75,-90 62.19,-90 64.63,-90&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;64.67,-93.5 74.67,-90 64.67,-86.5 64.67,-93.5&quot; /&gt;
&lt;/g&gt;
&lt;!-- h22&amp;#45;&amp;gt;h12 --&gt;
&lt;g id=&quot;edge12&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;h22&amp;#45;&amp;gt;h12&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M103.6,-108.3C103.6,-116.02 103.6,-125.29 103.6,-133.89&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;100.1,-133.9 103.6,-143.9 107.1,-133.9 100.1,-133.9&quot; /&gt;
&lt;/g&gt;
&lt;!-- h2ddd --&gt;
&lt;g id=&quot;node7&quot; class=&quot;node&quot;&gt;
&lt;title&gt;h2ddd&lt;/title&gt;
&lt;ellipse fill=&quot;none&quot; stroke=&quot;black&quot; cx=&quot;177.6&quot; cy=&quot;-90&quot; rx=&quot;27&quot; ry=&quot;18&quot; /&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;177.6&quot; y=&quot;-86.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;...&lt;/text&gt;
&lt;/g&gt;
&lt;!-- h22&amp;#45;&amp;gt;h2ddd --&gt;
&lt;g id=&quot;edge9&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;h22&amp;#45;&amp;gt;h2ddd&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M132.21,-90C134.85,-90 137.49,-90 140.13,-90&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;140.3,-93.5 150.3,-90 140.3,-86.5 140.3,-93.5&quot; /&gt;
&lt;/g&gt;
&lt;!-- h2ddd&amp;#45;&amp;gt;h1ddd --&gt;
&lt;g id=&quot;edge13&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;h2ddd&amp;#45;&amp;gt;h1ddd&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M177.6,-108.3C177.6,-116.02 177.6,-125.29 177.6,-133.89&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;174.1,-133.9 177.6,-143.9 181.1,-133.9 174.1,-133.9&quot; /&gt;
&lt;/g&gt;
&lt;!-- h2n --&gt;
&lt;g id=&quot;node8&quot; class=&quot;node&quot;&gt;
&lt;title&gt;h2n&lt;/title&gt;
&lt;ellipse fill=&quot;none&quot; stroke=&quot;black&quot; cx=&quot;251.6&quot; cy=&quot;-90&quot; rx=&quot;28.7&quot; ry=&quot;18&quot; /&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;251.6&quot; y=&quot;-86.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;h2n&lt;/text&gt;
&lt;/g&gt;
&lt;!-- h2ddd&amp;#45;&amp;gt;h2n --&gt;
&lt;g id=&quot;edge10&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;h2ddd&amp;#45;&amp;gt;h2n&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M204.77,-90C207.38,-90 210,-90 212.61,-90&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;212.7,-93.5 222.7,-90 212.7,-86.5 212.7,-93.5&quot; /&gt;
&lt;/g&gt;
&lt;!-- h2n&amp;#45;&amp;gt;h1n --&gt;
&lt;g id=&quot;edge14&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;h2n&amp;#45;&amp;gt;h1n&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M251.6,-108.3C251.6,-116.02 251.6,-125.29 251.6,-133.89&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;248.1,-133.9 251.6,-143.9 255.1,-133.9 248.1,-133.9&quot; /&gt;
&lt;/g&gt;
&lt;!-- xddd --&gt;
&lt;g id=&quot;node9&quot; class=&quot;node&quot;&gt;
&lt;title&gt;xddd&lt;/title&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;177.6&quot; y=&quot;-14.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;...&lt;/text&gt;
&lt;/g&gt;
&lt;!-- xddd&amp;#45;&amp;gt;h2ddd --&gt;
&lt;g id=&quot;edge17&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;xddd&amp;#45;&amp;gt;h2ddd&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M177.6,-36.3C177.6,-44.02 177.6,-53.29 177.6,-61.89&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;174.1,-61.9 177.6,-71.9 181.1,-61.9 174.1,-61.9&quot; /&gt;
&lt;/g&gt;
&lt;!-- x1 --&gt;
&lt;g id=&quot;node14&quot; class=&quot;node&quot;&gt;
&lt;title&gt;x1&lt;/title&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;28.6&quot; y=&quot;-14.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;x1&lt;/text&gt;
&lt;/g&gt;
&lt;!-- x1&amp;#45;&amp;gt;h21 --&gt;
&lt;g id=&quot;edge15&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;x1&amp;#45;&amp;gt;h21&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M28.6,-36.3C28.6,-44.02 28.6,-53.29 28.6,-61.89&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;25.1,-61.9 28.6,-71.9 32.1,-61.9 25.1,-61.9&quot; /&gt;
&lt;/g&gt;
&lt;!-- x2 --&gt;
&lt;g id=&quot;node15&quot; class=&quot;node&quot;&gt;
&lt;title&gt;x2&lt;/title&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;103.6&quot; y=&quot;-14.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;x2&lt;/text&gt;
&lt;/g&gt;
&lt;!-- x2&amp;#45;&amp;gt;h22 --&gt;
&lt;g id=&quot;edge16&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;x2&amp;#45;&amp;gt;h22&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M103.6,-36.3C103.6,-44.02 103.6,-53.29 103.6,-61.89&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;100.1,-61.9 103.6,-71.9 107.1,-61.9 100.1,-61.9&quot; /&gt;
&lt;/g&gt;
&lt;!-- xn --&gt;
&lt;g id=&quot;node16&quot; class=&quot;node&quot;&gt;
&lt;title&gt;xn&lt;/title&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;251.6&quot; y=&quot;-14.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;xn&lt;/text&gt;
&lt;/g&gt;
&lt;!-- xn&amp;#45;&amp;gt;h2n --&gt;
&lt;g id=&quot;edge18&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;xn&amp;#45;&amp;gt;h2n&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M251.6,-36.3C251.6,-44.02 251.6,-53.29 251.6,-61.89&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;248.1,-61.9 251.6,-71.9 255.1,-61.9 248.1,-61.9&quot; /&gt;
&lt;/g&gt;
&lt;/g&gt;
&lt;/svg&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;在 PyTorch 中使用 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nn.RNN&lt;/code&gt; 就有参数表示双向：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;num_layers – Number of recurrent layers. E.g., setting num_layers=2 would mean stacking two RNNs together to form a stacked RNN, with the second RNN taking in outputs of the first RNN and computing the final results. Default: 1&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;num_layers&lt;/code&gt;：隐藏层层数，默认设置为 1 层。当 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;num_layers&lt;/code&gt; &amp;gt;= 2 时，就是一个 stacked RNN 了。&lt;/p&gt;

&lt;h4 id=&quot;47n-vs-m-的-rnn&quot;&gt;4.7、N vs. M 的 RNN&lt;/h4&gt;

&lt;p&gt;对于输入序列长度（长度 N）和输出序列长度（长度 M）不一样的 RNN 模型结构，也可以叫做 Encoder-Decoder 模型，也可以叫 Seq2Seq 模型。首先接收输入序列的 Encoder 先将输入序列转成一个隐藏态的上下文表示 C。C 可以只与最后一个隐藏层有关，甚至可以是最后一个隐藏层生成的隐藏态直接设置为 C，C 还可以与所有隐藏层有关。&lt;/p&gt;

&lt;p&gt;有了这个 C 之后，再用 Decoder 进行解码，也就是从把 C 作为输入状态开始，生成输出序列。&lt;/p&gt;

&lt;div style=&quot;text-align: center;&quot;&gt;
&lt;div class=&quot;graphviz-wrapper&quot;&gt;

&lt;!-- Generated by graphviz version 2.43.0 (0)
 --&gt;
&lt;!-- Title: G Pages: 1 --&gt;
&lt;svg role=&quot;img&quot; aria-label=&quot;graphviz-094de5e41d0af67d4c5617e0f04d7b57&quot; width=&quot;638pt&quot; height=&quot;188pt&quot; viewBox=&quot;0.00 0.00 638.00 188.00&quot;&gt;
&lt;title&gt;graphviz-094de5e41d0af67d4c5617e0f04d7b57&lt;/title&gt;
&lt;desc&gt;
digraph G {
	rankdir=BT
	{rank=same e1 e2 eddd en C d1 d2 dddd dm}

	eddd[label=&amp;quot;...&amp;quot;]
	dddd[label=&amp;quot;...&amp;quot;]
	xddd[label=&amp;quot;...&amp;quot;]
	yddd[label=&amp;quot;...&amp;quot;]
	C[shape=plaintext]
	x1[shape=plaintext]
	x2[shape=plaintext]
	xddd[shape=plaintext]
	xn[shape=plaintext]
	y1[shape=plaintext]
	y2[shape=plaintext]
	yddd[shape=plaintext]
	yn[shape=plaintext]

	x1 -&amp;gt; e1
	x2 -&amp;gt; e2
	xddd -&amp;gt; eddd
	xn -&amp;gt; en

	e1 -&amp;gt; e2
	e2 -&amp;gt; eddd
	eddd -&amp;gt; en

	en -&amp;gt; C
	C -&amp;gt; d1

	d1 -&amp;gt; y1
	d2 -&amp;gt; y2
	dddd -&amp;gt; yddd
	dm -&amp;gt; yn

	d1 -&amp;gt; d2
	d2 -&amp;gt; dddd
	dddd -&amp;gt; dm
}
&lt;/desc&gt;

&lt;g id=&quot;graph0&quot; class=&quot;graph&quot; transform=&quot;scale(1 1) rotate(0) translate(4 184)&quot;&gt;
&lt;title&gt;G&lt;/title&gt;
&lt;polygon fill=&quot;white&quot; stroke=&quot;transparent&quot; points=&quot;-4,4 -4,-184 634,-184 634,4 -4,4&quot; /&gt;
&lt;!-- e1 --&gt;
&lt;g id=&quot;node1&quot; class=&quot;node&quot;&gt;
&lt;title&gt;e1&lt;/title&gt;
&lt;ellipse fill=&quot;none&quot; stroke=&quot;black&quot; cx=&quot;27&quot; cy=&quot;-90&quot; rx=&quot;27&quot; ry=&quot;18&quot; /&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;27&quot; y=&quot;-86.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;e1&lt;/text&gt;
&lt;/g&gt;
&lt;!-- e2 --&gt;
&lt;g id=&quot;node2&quot; class=&quot;node&quot;&gt;
&lt;title&gt;e2&lt;/title&gt;
&lt;ellipse fill=&quot;none&quot; stroke=&quot;black&quot; cx=&quot;99&quot; cy=&quot;-90&quot; rx=&quot;27&quot; ry=&quot;18&quot; /&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;99&quot; y=&quot;-86.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;e2&lt;/text&gt;
&lt;/g&gt;
&lt;!-- e1&amp;#45;&amp;gt;e2 --&gt;
&lt;g id=&quot;edge5&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;e1&amp;#45;&amp;gt;e2&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M54,-90C56.61,-90 59.23,-90 61.84,-90&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;61.93,-93.5 71.93,-90 61.93,-86.5 61.93,-93.5&quot; /&gt;
&lt;/g&gt;
&lt;!-- eddd --&gt;
&lt;g id=&quot;node3&quot; class=&quot;node&quot;&gt;
&lt;title&gt;eddd&lt;/title&gt;
&lt;ellipse fill=&quot;none&quot; stroke=&quot;black&quot; cx=&quot;171&quot; cy=&quot;-90&quot; rx=&quot;27&quot; ry=&quot;18&quot; /&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;171&quot; y=&quot;-86.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;...&lt;/text&gt;
&lt;/g&gt;
&lt;!-- e2&amp;#45;&amp;gt;eddd --&gt;
&lt;g id=&quot;edge6&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;e2&amp;#45;&amp;gt;eddd&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M126,-90C128.61,-90 131.23,-90 133.84,-90&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;133.93,-93.5 143.93,-90 133.93,-86.5 133.93,-93.5&quot; /&gt;
&lt;/g&gt;
&lt;!-- en --&gt;
&lt;g id=&quot;node4&quot; class=&quot;node&quot;&gt;
&lt;title&gt;en&lt;/title&gt;
&lt;ellipse fill=&quot;none&quot; stroke=&quot;black&quot; cx=&quot;243&quot; cy=&quot;-90&quot; rx=&quot;27&quot; ry=&quot;18&quot; /&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;243&quot; y=&quot;-86.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;en&lt;/text&gt;
&lt;/g&gt;
&lt;!-- eddd&amp;#45;&amp;gt;en --&gt;
&lt;g id=&quot;edge7&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;eddd&amp;#45;&amp;gt;en&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M198,-90C200.61,-90 203.23,-90 205.84,-90&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;205.93,-93.5 215.93,-90 205.93,-86.5 205.93,-93.5&quot; /&gt;
&lt;/g&gt;
&lt;!-- C --&gt;
&lt;g id=&quot;node5&quot; class=&quot;node&quot;&gt;
&lt;title&gt;C&lt;/title&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;315&quot; y=&quot;-86.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;C&lt;/text&gt;
&lt;/g&gt;
&lt;!-- en&amp;#45;&amp;gt;C --&gt;
&lt;g id=&quot;edge8&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;en&amp;#45;&amp;gt;C&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M270,-90C272.61,-90 275.23,-90 277.84,-90&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;277.93,-93.5 287.93,-90 277.93,-86.5 277.93,-93.5&quot; /&gt;
&lt;/g&gt;
&lt;!-- d1 --&gt;
&lt;g id=&quot;node6&quot; class=&quot;node&quot;&gt;
&lt;title&gt;d1&lt;/title&gt;
&lt;ellipse fill=&quot;none&quot; stroke=&quot;black&quot; cx=&quot;387&quot; cy=&quot;-90&quot; rx=&quot;27&quot; ry=&quot;18&quot; /&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;387&quot; y=&quot;-86.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;d1&lt;/text&gt;
&lt;/g&gt;
&lt;!-- C&amp;#45;&amp;gt;d1 --&gt;
&lt;g id=&quot;edge9&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;C&amp;#45;&amp;gt;d1&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M342.28,-90C344.74,-90 347.19,-90 349.65,-90&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;349.75,-93.5 359.75,-90 349.75,-86.5 349.75,-93.5&quot; /&gt;
&lt;/g&gt;
&lt;!-- d2 --&gt;
&lt;g id=&quot;node7&quot; class=&quot;node&quot;&gt;
&lt;title&gt;d2&lt;/title&gt;
&lt;ellipse fill=&quot;none&quot; stroke=&quot;black&quot; cx=&quot;459&quot; cy=&quot;-90&quot; rx=&quot;27&quot; ry=&quot;18&quot; /&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;459&quot; y=&quot;-86.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;d2&lt;/text&gt;
&lt;/g&gt;
&lt;!-- d1&amp;#45;&amp;gt;d2 --&gt;
&lt;g id=&quot;edge14&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;d1&amp;#45;&amp;gt;d2&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M414,-90C416.61,-90 419.23,-90 421.84,-90&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;421.93,-93.5 431.93,-90 421.93,-86.5 421.93,-93.5&quot; /&gt;
&lt;/g&gt;
&lt;!-- y1 --&gt;
&lt;g id=&quot;node15&quot; class=&quot;node&quot;&gt;
&lt;title&gt;y1&lt;/title&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;387&quot; y=&quot;-158.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;y1&lt;/text&gt;
&lt;/g&gt;
&lt;!-- d1&amp;#45;&amp;gt;y1 --&gt;
&lt;g id=&quot;edge10&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;d1&amp;#45;&amp;gt;y1&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M387,-108.3C387,-116.02 387,-125.29 387,-133.89&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;383.5,-133.9 387,-143.9 390.5,-133.9 383.5,-133.9&quot; /&gt;
&lt;/g&gt;
&lt;!-- dddd --&gt;
&lt;g id=&quot;node8&quot; class=&quot;node&quot;&gt;
&lt;title&gt;dddd&lt;/title&gt;
&lt;ellipse fill=&quot;none&quot; stroke=&quot;black&quot; cx=&quot;531&quot; cy=&quot;-90&quot; rx=&quot;27&quot; ry=&quot;18&quot; /&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;531&quot; y=&quot;-86.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;...&lt;/text&gt;
&lt;/g&gt;
&lt;!-- d2&amp;#45;&amp;gt;dddd --&gt;
&lt;g id=&quot;edge15&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;d2&amp;#45;&amp;gt;dddd&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M486,-90C488.61,-90 491.23,-90 493.84,-90&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;493.93,-93.5 503.93,-90 493.93,-86.5 493.93,-93.5&quot; /&gt;
&lt;/g&gt;
&lt;!-- y2 --&gt;
&lt;g id=&quot;node16&quot; class=&quot;node&quot;&gt;
&lt;title&gt;y2&lt;/title&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;459&quot; y=&quot;-158.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;y2&lt;/text&gt;
&lt;/g&gt;
&lt;!-- d2&amp;#45;&amp;gt;y2 --&gt;
&lt;g id=&quot;edge11&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;d2&amp;#45;&amp;gt;y2&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M459,-108.3C459,-116.02 459,-125.29 459,-133.89&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;455.5,-133.9 459,-143.9 462.5,-133.9 455.5,-133.9&quot; /&gt;
&lt;/g&gt;
&lt;!-- dm --&gt;
&lt;g id=&quot;node9&quot; class=&quot;node&quot;&gt;
&lt;title&gt;dm&lt;/title&gt;
&lt;ellipse fill=&quot;none&quot; stroke=&quot;black&quot; cx=&quot;603&quot; cy=&quot;-90&quot; rx=&quot;27&quot; ry=&quot;18&quot; /&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;603&quot; y=&quot;-86.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;dm&lt;/text&gt;
&lt;/g&gt;
&lt;!-- dddd&amp;#45;&amp;gt;dm --&gt;
&lt;g id=&quot;edge16&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;dddd&amp;#45;&amp;gt;dm&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M558,-90C560.61,-90 563.23,-90 565.84,-90&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;565.93,-93.5 575.93,-90 565.93,-86.5 565.93,-93.5&quot; /&gt;
&lt;/g&gt;
&lt;!-- yddd --&gt;
&lt;g id=&quot;node11&quot; class=&quot;node&quot;&gt;
&lt;title&gt;yddd&lt;/title&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;531&quot; y=&quot;-158.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;...&lt;/text&gt;
&lt;/g&gt;
&lt;!-- dddd&amp;#45;&amp;gt;yddd --&gt;
&lt;g id=&quot;edge12&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;dddd&amp;#45;&amp;gt;yddd&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M531,-108.3C531,-116.02 531,-125.29 531,-133.89&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;527.5,-133.9 531,-143.9 534.5,-133.9 527.5,-133.9&quot; /&gt;
&lt;/g&gt;
&lt;!-- yn --&gt;
&lt;g id=&quot;node17&quot; class=&quot;node&quot;&gt;
&lt;title&gt;yn&lt;/title&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;603&quot; y=&quot;-158.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;yn&lt;/text&gt;
&lt;/g&gt;
&lt;!-- dm&amp;#45;&amp;gt;yn --&gt;
&lt;g id=&quot;edge13&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;dm&amp;#45;&amp;gt;yn&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M603,-108.3C603,-116.02 603,-125.29 603,-133.89&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;599.5,-133.9 603,-143.9 606.5,-133.9 599.5,-133.9&quot; /&gt;
&lt;/g&gt;
&lt;!-- xddd --&gt;
&lt;g id=&quot;node10&quot; class=&quot;node&quot;&gt;
&lt;title&gt;xddd&lt;/title&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;171&quot; y=&quot;-14.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;...&lt;/text&gt;
&lt;/g&gt;
&lt;!-- xddd&amp;#45;&amp;gt;eddd --&gt;
&lt;g id=&quot;edge3&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;xddd&amp;#45;&amp;gt;eddd&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M171,-36.3C171,-44.02 171,-53.29 171,-61.89&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;167.5,-61.9 171,-71.9 174.5,-61.9 167.5,-61.9&quot; /&gt;
&lt;/g&gt;
&lt;!-- x1 --&gt;
&lt;g id=&quot;node12&quot; class=&quot;node&quot;&gt;
&lt;title&gt;x1&lt;/title&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;27&quot; y=&quot;-14.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;x1&lt;/text&gt;
&lt;/g&gt;
&lt;!-- x1&amp;#45;&amp;gt;e1 --&gt;
&lt;g id=&quot;edge1&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;x1&amp;#45;&amp;gt;e1&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M27,-36.3C27,-44.02 27,-53.29 27,-61.89&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;23.5,-61.9 27,-71.9 30.5,-61.9 23.5,-61.9&quot; /&gt;
&lt;/g&gt;
&lt;!-- x2 --&gt;
&lt;g id=&quot;node13&quot; class=&quot;node&quot;&gt;
&lt;title&gt;x2&lt;/title&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;99&quot; y=&quot;-14.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;x2&lt;/text&gt;
&lt;/g&gt;
&lt;!-- x2&amp;#45;&amp;gt;e2 --&gt;
&lt;g id=&quot;edge2&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;x2&amp;#45;&amp;gt;e2&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M99,-36.3C99,-44.02 99,-53.29 99,-61.89&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;95.5,-61.9 99,-71.9 102.5,-61.9 95.5,-61.9&quot; /&gt;
&lt;/g&gt;
&lt;!-- xn --&gt;
&lt;g id=&quot;node14&quot; class=&quot;node&quot;&gt;
&lt;title&gt;xn&lt;/title&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;243&quot; y=&quot;-14.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;xn&lt;/text&gt;
&lt;/g&gt;
&lt;!-- xn&amp;#45;&amp;gt;en --&gt;
&lt;g id=&quot;edge4&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;xn&amp;#45;&amp;gt;en&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M243,-36.3C243,-44.02 243,-53.29 243,-61.89&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;239.5,-61.9 243,-71.9 246.5,-61.9 239.5,-61.9&quot; /&gt;
&lt;/g&gt;
&lt;/g&gt;
&lt;/svg&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;具体地，可以如下表示：&lt;/p&gt;

\[\begin{aligned}
&amp;amp;\bm{C} = Encoder(\bm{X}) \\
&amp;amp;\bm{Y} = Decoder(\bm{C}) \\
\end{aligned}\]

&lt;p&gt;进一步展开：&lt;/p&gt;

\[\begin{aligned}
e_t &amp;amp;= Encoder_{LSTM/GRU}(x_t, e_{t-1}) \\
\bm{C} &amp;amp;= f_1(e_n) \\
d_t &amp;amp;= f_2(d_{t-1}, \bm{C}) \\
y_t &amp;amp;= Decoder_{LSTM/GRU}(y_{t-1}, d_{t-1}, \bm{C})
\end{aligned}\]

&lt;p&gt;这种的应用就非常广了，因为大多数时候输入序列与输出序列的长度都是不同的，比如最常见的应用「翻译」，从一个语言翻译成另一个语言；再比如 AI 的一个领域「语音识别」，将语音序列输入后生成所识别的文本内容；还有比如 ChatGPT 这种问答应用等等。&lt;/p&gt;

&lt;p&gt;Seq2Seq 模型非常出色，一直到 2018 年之前 NLP 领域里该模型已成为主流。但是它有很显著的问题：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;当输入序列很长时，Encoder 生成的 Context 可能就会出现所捕捉的信息不充分的情况，导致 Decoder 最终的输出是不尽如人意的。具体地，毕竟还是 RNN 模型，其词间距过长时还是会有梯度消失问题，根本原因在于用到了「递归」。当递归作用在同一个 weight matrix 上时，使得如果这个矩阵满足条件的话，其最大的特征值要是小于 1 的话，就一定出现梯度消失问题。后来的 LSTM 和 GRU 也仅仅能缓解问题，并不能根本解决。&lt;/li&gt;
  &lt;li&gt;并行效果差：每个时刻的结果依赖前一时刻。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;第-5-节--为什么说-rnn-模型没有体现注意力&quot;&gt;第 5 节 · 为什么说 RNN 模型没有体现「注意力」？&lt;/h3&gt;

&lt;p&gt;Encoder-Decoder 的一个非常严重的问题，是依赖中间那个 context 向量，则无法处理特别长的输入序列 —— 记忆力不足，会忘事儿。而忘事儿的根本原因，是没有「注意力」。&lt;/p&gt;

&lt;p&gt;对于一般的 RNN 模型，Encoder-Decoder 结构并没有体现「注意力」—— 这句话怎么理解？当输入序列经过 Encoder 生成的中间结果（上下文 C），被喂给 Decoder 时，这些中间结果对所生成序列里的哪个词，都没有区别（没有特别关照谁）。这相当于在说：输入序列里的每个词，对于生成任何一个输出的词的影响，是一样的，而不是输出某个词时是聚焦特定的一些输入词。这就是模型没有注意力机制。&lt;/p&gt;

&lt;p&gt;人脑的注意力模型，其实是资源分配模型。NLP 领域的注意力模型，是在 2014 年被提出的，后来逐渐成为 NLP 领域的一个广泛应用的机制。可以应用的场景，比如对于一个电商平台中很常见的白底图，其边缘的白色区域都是无用的，那么就不应该被关注（关注权重为 0）。比如机器翻译中，翻译词都是对局部输入重点关注的。&lt;/p&gt;

&lt;p&gt;所以 Attention 机制，就是在 Decoder 时，不是所有输出都依赖相同的「上下文  \(\bm{C}_t\) 」，而是时刻 t 的输出，使用  \(\bm{C}_t\) ，而这个  \(\bm{C}_t\)  来自对每个输入数据项根据「注意力」进行的加权。&lt;/p&gt;

&lt;h3 id=&quot;第-6-节--基于-attention-机制的-encoder-decoder-模型&quot;&gt;第 6 节 · 基于 Attention 机制的 Encoder-Decoder 模型&lt;/h3&gt;

&lt;p&gt;2015 年 Dzmitry Bahdanau 等人在论文&lt;a href=&quot;https://arxiv.org/abs/1409.0473&quot;&gt;《Neural Machine Translation by Jointly Learning to Align and Translate》&lt;/a&gt; 中提出了「Attention」机制，下面请跟着麦克船长，船长会深入浅出地为你解释清楚。&lt;/p&gt;

&lt;p&gt;下图中  \(e_i\)  表示编码器的隐藏层输出， \(d_i\)  表示解码器的隐藏层输出&lt;/p&gt;

&lt;div style=&quot;text-align: center;&quot;&gt;
&lt;div class=&quot;graphviz-wrapper&quot;&gt;

&lt;!-- Generated by graphviz version 2.43.0 (0)
 --&gt;
&lt;!-- Title: G Pages: 1 --&gt;
&lt;svg role=&quot;img&quot; aria-label=&quot;graphviz-f66c634a9c7c02915e5610af76c3b1b7&quot; width=&quot;436pt&quot; height=&quot;336pt&quot; viewBox=&quot;0.00 0.00 436.00 336.00&quot;&gt;
&lt;title&gt;graphviz-f66c634a9c7c02915e5610af76c3b1b7&lt;/title&gt;
&lt;desc&gt;
digraph G {
	rankdir=BT
	splines=ortho
	{rank=same e1 e2 eddd en}
	{rank=same d1 d2 dddd dt0 dt dddd2}

	eddd[label=&amp;quot;...&amp;quot;]
	dddd[label=&amp;quot;...&amp;quot;]
	xddd[label=&amp;quot;...&amp;quot;]
	yddd[label=&amp;quot;...&amp;quot;]
	dt[label=&amp;quot;d_t&amp;quot;]
	dt0[label=&amp;quot;d_t-1&amp;quot;]
	yt[label=&amp;quot;y_t&amp;quot;]
	yt0[label=&amp;quot;y_t-1&amp;quot;]
	Ct[shape=plaintext]
	x1[shape=plaintext]
	x2[shape=plaintext]
	xddd[shape=plaintext]
	xn[shape=plaintext]
	y1[shape=plaintext]
	y2[shape=plaintext]
	yddd[shape=plaintext]
	dddd2[shape=plaintext, label=&amp;quot;&amp;quot;]
	Ct[label=&amp;quot;C_t&amp;quot;, shape=&amp;quot;square&amp;quot;]

	x1 -&amp;gt; e1
	x2 -&amp;gt; e2
	xddd -&amp;gt; eddd
	xn -&amp;gt; en

	e1 -&amp;gt; e2
	e2 -&amp;gt; eddd
	eddd -&amp;gt; en

	Ct -&amp;gt; dt

	d1 -&amp;gt; y1
	d2 -&amp;gt; y2
	dddd -&amp;gt; yddd
	dt0 -&amp;gt; yt0
	dt -&amp;gt; yt

	d1 -&amp;gt; d2
	d2 -&amp;gt; dddd
	dddd -&amp;gt; dt0
	dt0 -&amp;gt; dt

	e1 -&amp;gt; Ct
	e2 -&amp;gt; Ct
	eddd -&amp;gt; Ct
	en -&amp;gt; Ct

	dt -&amp;gt; dddd2
	dt0 -&amp;gt; Ct
}
&lt;/desc&gt;

&lt;g id=&quot;graph0&quot; class=&quot;graph&quot; transform=&quot;scale(1 1) rotate(0) translate(4 332)&quot;&gt;
&lt;title&gt;G&lt;/title&gt;
&lt;polygon fill=&quot;white&quot; stroke=&quot;transparent&quot; points=&quot;-4,4 -4,-332 432,-332 432,4 -4,4&quot; /&gt;
&lt;!-- e1 --&gt;
&lt;g id=&quot;node1&quot; class=&quot;node&quot;&gt;
&lt;title&gt;e1&lt;/title&gt;
&lt;ellipse fill=&quot;none&quot; stroke=&quot;black&quot; cx=&quot;181&quot; cy=&quot;-90&quot; rx=&quot;27&quot; ry=&quot;18&quot; /&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;181&quot; y=&quot;-86.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;e1&lt;/text&gt;
&lt;/g&gt;
&lt;!-- e2 --&gt;
&lt;g id=&quot;node2&quot; class=&quot;node&quot;&gt;
&lt;title&gt;e2&lt;/title&gt;
&lt;ellipse fill=&quot;none&quot; stroke=&quot;black&quot; cx=&quot;253&quot; cy=&quot;-90&quot; rx=&quot;27&quot; ry=&quot;18&quot; /&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;253&quot; y=&quot;-86.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;e2&lt;/text&gt;
&lt;/g&gt;
&lt;!-- e1&amp;#45;&amp;gt;e2 --&gt;
&lt;g id=&quot;edge5&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;e1&amp;#45;&amp;gt;e2&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M208.22,-90C208.22,-90 215.74,-90 215.74,-90&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;215.74,-93.5 225.74,-90 215.74,-86.5 215.74,-93.5&quot; /&gt;
&lt;/g&gt;
&lt;!-- Ct --&gt;
&lt;g id=&quot;node15&quot; class=&quot;node&quot;&gt;
&lt;title&gt;Ct&lt;/title&gt;
&lt;polygon fill=&quot;none&quot; stroke=&quot;black&quot; points=&quot;309,-184 269,-184 269,-144 309,-144 309,-184&quot; /&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;289&quot; y=&quot;-160.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;C_t&lt;/text&gt;
&lt;/g&gt;
&lt;!-- e1&amp;#45;&amp;gt;Ct --&gt;
&lt;g id=&quot;edge18&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;e1&amp;#45;&amp;gt;Ct&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M203,-100.6C203,-121.06 203,-164 203,-164 203,-164 258.62,-164 258.62,-164&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;258.62,-167.5 268.62,-164 258.62,-160.5 258.62,-167.5&quot; /&gt;
&lt;/g&gt;
&lt;!-- eddd --&gt;
&lt;g id=&quot;node3&quot; class=&quot;node&quot;&gt;
&lt;title&gt;eddd&lt;/title&gt;
&lt;ellipse fill=&quot;none&quot; stroke=&quot;black&quot; cx=&quot;325&quot; cy=&quot;-90&quot; rx=&quot;27&quot; ry=&quot;18&quot; /&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;325&quot; y=&quot;-86.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;...&lt;/text&gt;
&lt;/g&gt;
&lt;!-- e2&amp;#45;&amp;gt;eddd --&gt;
&lt;g id=&quot;edge6&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;e2&amp;#45;&amp;gt;eddd&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M280.22,-90C280.22,-90 287.74,-90 287.74,-90&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;287.74,-93.5 297.74,-90 287.74,-86.5 287.74,-93.5&quot; /&gt;
&lt;/g&gt;
&lt;!-- e2&amp;#45;&amp;gt;Ct --&gt;
&lt;g id=&quot;edge19&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;e2&amp;#45;&amp;gt;Ct&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M274.5,-100.92C274.5,-100.92 274.5,-133.82 274.5,-133.82&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;271,-133.82 274.5,-143.82 278,-133.82 271,-133.82&quot; /&gt;
&lt;/g&gt;
&lt;!-- en --&gt;
&lt;g id=&quot;node4&quot; class=&quot;node&quot;&gt;
&lt;title&gt;en&lt;/title&gt;
&lt;ellipse fill=&quot;none&quot; stroke=&quot;black&quot; cx=&quot;397&quot; cy=&quot;-90&quot; rx=&quot;27&quot; ry=&quot;18&quot; /&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;397&quot; y=&quot;-86.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;en&lt;/text&gt;
&lt;/g&gt;
&lt;!-- eddd&amp;#45;&amp;gt;en --&gt;
&lt;g id=&quot;edge7&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;eddd&amp;#45;&amp;gt;en&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M352.22,-90C352.22,-90 359.74,-90 359.74,-90&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;359.74,-93.5 369.74,-90 359.74,-86.5 359.74,-93.5&quot; /&gt;
&lt;/g&gt;
&lt;!-- eddd&amp;#45;&amp;gt;Ct --&gt;
&lt;g id=&quot;edge20&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;eddd&amp;#45;&amp;gt;Ct&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M303.5,-100.92C303.5,-100.92 303.5,-133.82 303.5,-133.82&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;300,-133.82 303.5,-143.82 307,-133.82 300,-133.82&quot; /&gt;
&lt;/g&gt;
&lt;!-- en&amp;#45;&amp;gt;Ct --&gt;
&lt;g id=&quot;edge21&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;en&amp;#45;&amp;gt;Ct&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M399,-108.29C399,-130.21 399,-164 399,-164 399,-164 319.18,-164 319.18,-164&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;319.18,-160.5 309.18,-164 319.18,-167.5 319.18,-160.5&quot; /&gt;
&lt;/g&gt;
&lt;!-- d1 --&gt;
&lt;g id=&quot;node5&quot; class=&quot;node&quot;&gt;
&lt;title&gt;d1&lt;/title&gt;
&lt;ellipse fill=&quot;none&quot; stroke=&quot;black&quot; cx=&quot;27&quot; cy=&quot;-238&quot; rx=&quot;27&quot; ry=&quot;18&quot; /&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;27&quot; y=&quot;-234.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;d1&lt;/text&gt;
&lt;/g&gt;
&lt;!-- d2 --&gt;
&lt;g id=&quot;node6&quot; class=&quot;node&quot;&gt;
&lt;title&gt;d2&lt;/title&gt;
&lt;ellipse fill=&quot;none&quot; stroke=&quot;black&quot; cx=&quot;99&quot; cy=&quot;-238&quot; rx=&quot;27&quot; ry=&quot;18&quot; /&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;99&quot; y=&quot;-234.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;d2&lt;/text&gt;
&lt;/g&gt;
&lt;!-- d1&amp;#45;&amp;gt;d2 --&gt;
&lt;g id=&quot;edge14&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;d1&amp;#45;&amp;gt;d2&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M54.22,-238C54.22,-238 61.74,-238 61.74,-238&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;61.74,-241.5 71.74,-238 61.74,-234.5 61.74,-241.5&quot; /&gt;
&lt;/g&gt;
&lt;!-- y1 --&gt;
&lt;g id=&quot;node19&quot; class=&quot;node&quot;&gt;
&lt;title&gt;y1&lt;/title&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;27&quot; y=&quot;-306.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;y1&lt;/text&gt;
&lt;/g&gt;
&lt;!-- d1&amp;#45;&amp;gt;y1 --&gt;
&lt;g id=&quot;edge9&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;d1&amp;#45;&amp;gt;y1&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M27,-256.17C27,-256.17 27,-281.59 27,-281.59&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;23.5,-281.59 27,-291.59 30.5,-281.59 23.5,-281.59&quot; /&gt;
&lt;/g&gt;
&lt;!-- dddd --&gt;
&lt;g id=&quot;node7&quot; class=&quot;node&quot;&gt;
&lt;title&gt;dddd&lt;/title&gt;
&lt;ellipse fill=&quot;none&quot; stroke=&quot;black&quot; cx=&quot;171&quot; cy=&quot;-238&quot; rx=&quot;27&quot; ry=&quot;18&quot; /&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;171&quot; y=&quot;-234.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;...&lt;/text&gt;
&lt;/g&gt;
&lt;!-- d2&amp;#45;&amp;gt;dddd --&gt;
&lt;g id=&quot;edge15&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;d2&amp;#45;&amp;gt;dddd&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M126.22,-238C126.22,-238 133.74,-238 133.74,-238&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;133.74,-241.5 143.74,-238 133.74,-234.5 133.74,-241.5&quot; /&gt;
&lt;/g&gt;
&lt;!-- y2 --&gt;
&lt;g id=&quot;node20&quot; class=&quot;node&quot;&gt;
&lt;title&gt;y2&lt;/title&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;99&quot; y=&quot;-306.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;y2&lt;/text&gt;
&lt;/g&gt;
&lt;!-- d2&amp;#45;&amp;gt;y2 --&gt;
&lt;g id=&quot;edge10&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;d2&amp;#45;&amp;gt;y2&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M99,-256.17C99,-256.17 99,-281.59 99,-281.59&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;95.5,-281.59 99,-291.59 102.5,-281.59 95.5,-281.59&quot; /&gt;
&lt;/g&gt;
&lt;!-- dt0 --&gt;
&lt;g id=&quot;node8&quot; class=&quot;node&quot;&gt;
&lt;title&gt;dt0&lt;/title&gt;
&lt;ellipse fill=&quot;none&quot; stroke=&quot;black&quot; cx=&quot;250&quot; cy=&quot;-238&quot; rx=&quot;33.6&quot; ry=&quot;18&quot; /&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;250&quot; y=&quot;-234.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;d_t&amp;#45;1&lt;/text&gt;
&lt;/g&gt;
&lt;!-- dddd&amp;#45;&amp;gt;dt0 --&gt;
&lt;g id=&quot;edge16&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;dddd&amp;#45;&amp;gt;dt0&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M198.19,-238C198.19,-238 206.2,-238 206.2,-238&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;206.2,-241.5 216.2,-238 206.2,-234.5 206.2,-241.5&quot; /&gt;
&lt;/g&gt;
&lt;!-- yddd --&gt;
&lt;g id=&quot;node12&quot; class=&quot;node&quot;&gt;
&lt;title&gt;yddd&lt;/title&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;171&quot; y=&quot;-306.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;...&lt;/text&gt;
&lt;/g&gt;
&lt;!-- dddd&amp;#45;&amp;gt;yddd --&gt;
&lt;g id=&quot;edge11&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;dddd&amp;#45;&amp;gt;yddd&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M171,-256.17C171,-256.17 171,-281.59 171,-281.59&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;167.5,-281.59 171,-291.59 174.5,-281.59 167.5,-281.59&quot; /&gt;
&lt;/g&gt;
&lt;!-- dt --&gt;
&lt;g id=&quot;node9&quot; class=&quot;node&quot;&gt;
&lt;title&gt;dt&lt;/title&gt;
&lt;ellipse fill=&quot;none&quot; stroke=&quot;black&quot; cx=&quot;329&quot; cy=&quot;-238&quot; rx=&quot;27&quot; ry=&quot;18&quot; /&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;329&quot; y=&quot;-234.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;d_t&lt;/text&gt;
&lt;/g&gt;
&lt;!-- dt0&amp;#45;&amp;gt;dt --&gt;
&lt;g id=&quot;edge17&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;dt0&amp;#45;&amp;gt;dt&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M283.96,-238C283.96,-238 291.98,-238 291.98,-238&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;291.98,-241.5 301.98,-238 291.98,-234.5 291.98,-241.5&quot; /&gt;
&lt;/g&gt;
&lt;!-- yt0 --&gt;
&lt;g id=&quot;node14&quot; class=&quot;node&quot;&gt;
&lt;title&gt;yt0&lt;/title&gt;
&lt;ellipse fill=&quot;none&quot; stroke=&quot;black&quot; cx=&quot;250&quot; cy=&quot;-310&quot; rx=&quot;33.29&quot; ry=&quot;18&quot; /&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;250&quot; y=&quot;-306.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;y_t&amp;#45;1&lt;/text&gt;
&lt;/g&gt;
&lt;!-- dt0&amp;#45;&amp;gt;yt0 --&gt;
&lt;g id=&quot;edge12&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;dt0&amp;#45;&amp;gt;yt0&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M250,-256.17C250,-256.17 250,-281.59 250,-281.59&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;246.5,-281.59 250,-291.59 253.5,-281.59 246.5,-281.59&quot; /&gt;
&lt;/g&gt;
&lt;!-- dt0&amp;#45;&amp;gt;Ct --&gt;
&lt;g id=&quot;edge23&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;dt0&amp;#45;&amp;gt;Ct&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M276.4,-226.44C276.4,-226.44 276.4,-194.12 276.4,-194.12&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;279.9,-194.12 276.4,-184.12 272.9,-194.12 279.9,-194.12&quot; /&gt;
&lt;/g&gt;
&lt;!-- dddd2 --&gt;
&lt;g id=&quot;node10&quot; class=&quot;node&quot;&gt;
&lt;title&gt;dddd2&lt;/title&gt;
&lt;/g&gt;
&lt;!-- dt&amp;#45;&amp;gt;dddd2 --&gt;
&lt;g id=&quot;edge22&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;dt&amp;#45;&amp;gt;dddd2&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M356.22,-238C356.22,-238 363.74,-238 363.74,-238&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;363.74,-241.5 373.74,-238 363.74,-234.5 363.74,-241.5&quot; /&gt;
&lt;/g&gt;
&lt;!-- yt --&gt;
&lt;g id=&quot;node13&quot; class=&quot;node&quot;&gt;
&lt;title&gt;yt&lt;/title&gt;
&lt;ellipse fill=&quot;none&quot; stroke=&quot;black&quot; cx=&quot;329&quot; cy=&quot;-310&quot; rx=&quot;27&quot; ry=&quot;18&quot; /&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;329&quot; y=&quot;-306.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;y_t&lt;/text&gt;
&lt;/g&gt;
&lt;!-- dt&amp;#45;&amp;gt;yt --&gt;
&lt;g id=&quot;edge13&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;dt&amp;#45;&amp;gt;yt&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M329,-256.17C329,-256.17 329,-281.59 329,-281.59&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;325.5,-281.59 329,-291.59 332.5,-281.59 325.5,-281.59&quot; /&gt;
&lt;/g&gt;
&lt;!-- xddd --&gt;
&lt;g id=&quot;node11&quot; class=&quot;node&quot;&gt;
&lt;title&gt;xddd&lt;/title&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;325&quot; y=&quot;-14.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;...&lt;/text&gt;
&lt;/g&gt;
&lt;!-- xddd&amp;#45;&amp;gt;eddd --&gt;
&lt;g id=&quot;edge3&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;xddd&amp;#45;&amp;gt;eddd&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M325,-36.17C325,-36.17 325,-61.59 325,-61.59&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;321.5,-61.59 325,-71.59 328.5,-61.59 321.5,-61.59&quot; /&gt;
&lt;/g&gt;
&lt;!-- Ct&amp;#45;&amp;gt;dt --&gt;
&lt;g id=&quot;edge8&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;Ct&amp;#45;&amp;gt;dt&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M305.5,-184.22C305.5,-184.22 305.5,-218.8 305.5,-218.8&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;302,-218.8 305.5,-228.8 309,-218.8 302,-218.8&quot; /&gt;
&lt;/g&gt;
&lt;!-- x1 --&gt;
&lt;g id=&quot;node16&quot; class=&quot;node&quot;&gt;
&lt;title&gt;x1&lt;/title&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;181&quot; y=&quot;-14.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;x1&lt;/text&gt;
&lt;/g&gt;
&lt;!-- x1&amp;#45;&amp;gt;e1 --&gt;
&lt;g id=&quot;edge1&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;x1&amp;#45;&amp;gt;e1&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M181,-36.17C181,-36.17 181,-61.59 181,-61.59&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;177.5,-61.59 181,-71.59 184.5,-61.59 177.5,-61.59&quot; /&gt;
&lt;/g&gt;
&lt;!-- x2 --&gt;
&lt;g id=&quot;node17&quot; class=&quot;node&quot;&gt;
&lt;title&gt;x2&lt;/title&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;253&quot; y=&quot;-14.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;x2&lt;/text&gt;
&lt;/g&gt;
&lt;!-- x2&amp;#45;&amp;gt;e2 --&gt;
&lt;g id=&quot;edge2&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;x2&amp;#45;&amp;gt;e2&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M253,-36.17C253,-36.17 253,-61.59 253,-61.59&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;249.5,-61.59 253,-71.59 256.5,-61.59 249.5,-61.59&quot; /&gt;
&lt;/g&gt;
&lt;!-- xn --&gt;
&lt;g id=&quot;node18&quot; class=&quot;node&quot;&gt;
&lt;title&gt;xn&lt;/title&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;397&quot; y=&quot;-14.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;xn&lt;/text&gt;
&lt;/g&gt;
&lt;!-- xn&amp;#45;&amp;gt;en --&gt;
&lt;g id=&quot;edge4&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;xn&amp;#45;&amp;gt;en&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M397,-36.17C397,-36.17 397,-61.59 397,-61.59&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;393.5,-61.59 397,-71.59 400.5,-61.59 393.5,-61.59&quot; /&gt;
&lt;/g&gt;
&lt;/g&gt;
&lt;/svg&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;更进一步细化关于  \(\bm{C}_t\)  部分，船长在此引用《基于深度学习的道路短期交通状态时空序列预测》一书中的图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023-01-04-captain-nlp-5.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;这个图里的  \(\widetilde{h}_i\)  与上一个图里的  \(d_i\)  对应， \(h_i\)  与上一个图里的  \(e_i\)  对应。&lt;/p&gt;

&lt;p&gt;针对时刻  \(t\)  要产出的输出，隐藏层每一个隐藏细胞都与  \(\bm{C}_t\)  有一个权重关系  \(\alpha_{t,i}\)  其中  \(1\le i\le n\) ，这个权重值与「输入项经过编码器后隐藏层后的输出 \(e_i（1\le i\le n）\) 、解码器的前一时刻隐藏层输出  \(d_{t-1}\) 」两者有关：&lt;/p&gt;

\[\begin{aligned}
&amp;amp;s_{i,t} = score(\bm{e}_i,\bm{d}_{t-1}) \\
&amp;amp;\alpha_{i,t} = \frac{exp(s_{i,t})}{\textstyle\sum_{j=1}^n exp(s_{j,t})}
\end{aligned}\]

&lt;p&gt;常用的  \(score\)  函数有：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;点积（Dot Product）模型： \(s_{i,t} = {\bm{d}_{t-1}}^T \cdot \bm{e}_i\)&lt;/li&gt;
  &lt;li&gt;缩放点积（Scaled Dot-Product）模型： \(s_{i,t} = \frac{{\bm{d}_{t-1}}^T \cdot \bm{e}_i}{\sqrt{\smash[b]{dimensions\:of\:d_{t-1}\:or\:e_i}}}\) ，可避免因为向量维度过大导致点积结果太大&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;然后上下文向量就表示成：&lt;/p&gt;

\[\begin{aligned}
&amp;amp;\bm{C}_t = \displaystyle\sum_{i=1}^n \alpha_{i,t} \bm{e}_i
\end{aligned}\]

&lt;p&gt;还记得 RNN 那部分里船长讲到的 Encoder-Decoder 模型的公式表示吗？&lt;/p&gt;

\[\begin{aligned}
e_t &amp;amp;= Encoder_{LSTM/GRU}(x_t, e_{t-1}) \\
\bm{C} &amp;amp;= f_1(e_n) \\
d_t &amp;amp;= f_2(d_{t-1}, \bm{C}) \\
y_t &amp;amp;= Decoder_{LSTM/GRU}(y_{t-1}, d_{t-1}, \bm{C})
\end{aligned}\]

&lt;p&gt;加入 Attention 机制的 Encoder-Decoder 模型如下。&lt;/p&gt;

\[\begin{aligned}
e_t &amp;amp;= Encoder_{LSTM/GRU}(x_t, e_{t-1}) \\
\bm{C}_t &amp;amp;= f_1(e_1,e_2...e_n,d_{t-1}) \\
d_t &amp;amp;= f_2(d_{t-1}, \bm{C}_t) \\
y_t &amp;amp;= Decoder_{LSTM/GRU}(y_{t-1}, d_{t-1}, \bm{C}_t)
\end{aligned}\]

&lt;p&gt;这种同时考虑 Encoder、Decoder 的 Attention，就叫做「Encoder-Decoder Attention」，也常被叫做「Vanilla Attention」。可以看到上面最核心的区别是第二个公式  \(C_t\) 。加入 Attention 后，对所有数据给予不同的注意力分布。具体地，比如我们用如下的函数来定义这个模型：&lt;/p&gt;

\[\begin{aligned}
\bm{e} &amp;amp;= tanh(\bm{W}^{xe} \cdot \bm{x} + \bm{b}^{xe}) \\
s_{i,t} &amp;amp;= score(\bm{e}_i,\bm{d}_{t-1}) \\
\alpha_{i,t} &amp;amp;= \frac{e^{s_{i,t}}}{\textstyle\sum_{j=1}^n e^{s_{j,t}}} \\
\bm{C}_t &amp;amp;= \displaystyle\sum_{i=1}^n \alpha_{i,t} \bm{e}_i \\
\bm{d}_t &amp;amp;= tanh(\bm{W}^{dd} \cdot \bm{d}_{t-1} + \bm{b}^{dd} +
				 \bm{W}^{yd} \cdot \bm{y}_{t-1} + \bm{b}^{yd} +
				 \bm{W}^{cd} \cdot \bm{C}_t + \bm{b}^{cd}) \\
\bm{y} &amp;amp;= Softmax(\bm{W}^{dy} \cdot \bm{d} + \bm{b}^{dy})
\end{aligned}\]

&lt;p&gt;到这里你能发现注意力机制的什么问题不？&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;这个注意力机制忽略了位置信息。比如 Tigers love rabbits 和 Rabbits love tigers 会产生一样的注意力分数。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;第二章--transformer-在-2017-年横空出世&quot;&gt;第二章 · Transformer 在 2017 年横空出世&lt;/h2&gt;

&lt;p&gt;船长先通过一个动画来看下 Transformer 是举例示意，该图来自 Google 的博客文章 &lt;a href=&quot;https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html&quot;&gt;《Transformer: A Novel Neural Network Architecture for Language Understanding》&lt;/a&gt;：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023-01-04-language-model-5-11.gif&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;中文网络里找到的解释得比较好的 blogs、answers，几乎都指向了同一篇博客：Jay Alammar 的&lt;a href=&quot;http://jalammar.github.io/illustrated-transformer/&quot;&gt;《The Illustrated Transformer》&lt;/a&gt;，所以建议读者搭配该篇文章阅读。&lt;/p&gt;

&lt;p&gt;Transformer 模型中用到了自注意力（Self-Attention）、多头注意力（Multiple-Head Attention）、残差网络（ResNet）与捷径（Short-Cut）。下面我们先通过第 1 到第 4 小节把几个基本概念讲清楚，然后在第 5 小节讲解整体 Transformer 模型就会好理解很多了。最后第 6 小节我们来一段动手实践。&lt;/p&gt;

&lt;h3 id=&quot;第-7-节--自注意力机制self-attention&quot;&gt;第 7 节 · 自注意力机制（Self-Attention）&lt;/h3&gt;

&lt;p&gt;自注意力是理解 Transformer 的关键，原作者在论文中限于篇幅，没有给出过多的解释。以下是我自己的理解，能够比较通透、符合常识地去理解 Transformer 中的一些神来之笔的概念。&lt;/p&gt;

&lt;h4 id=&quot;71一段自然语言内容其自身就暗含很多内部关联信息&quot;&gt;7.1、一段自然语言内容，其自身就「暗含」很多内部关联信息&lt;/h4&gt;

&lt;p&gt;在加入了 Attention 的 Encoder-Decoder 模型中，对输出序列 Y 中的一个词的注意力来自于输入序列 X，那么如果 X 和 Y 相等呢？什么场景会有这个需求？因为我们认为一段文字里某些词就是由于另外某些词而决定的，可以粗暴地理解为「完形填空」的原理。那么这样一段文字，其实就存在其中每个词的自注意力，举个例子：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;老王是我的主管，我很喜欢他的平易近人。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;对这句话里的「他」，如果基于这句话计算自注意力的话，显然应该给予「老王」最多的注意力。受此启发，我们认为：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;一段自然语言中，其实暗含了：为了得到关于某方面信息 Q，可以通过关注某些信息 K，进而得到某些信息（V）作为结果。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Q 就是 query 检索/查询，K、V 分别是 key、value。所以类似于我们在图书检索系统里搜索「NLP书籍」（这是 Q），得到了一本叫《自然语言处理实战》的电子书，书名就是 key，这本电子书就是 value。只是对于自然语言的理解，我们认为任何一段内容里，都自身暗含了很多潜在 Q-K-V 的关联。这是整体受到信息检索领域里 query-key-value 的启发的。&lt;/p&gt;

&lt;p&gt;基于这个启发，我们将自注意力的公式表示为：&lt;/p&gt;

\[\begin{aligned}
Z = SelfAttention(X) = Attention(Q,K,V)
\end{aligned}\]

&lt;p&gt;X 经过自注意力计算后，得到的「暗含」了大量原数据内部信息的 Z。然后我们拿着这个带有自注意力信息的 Z 进行后续的操作。这里要强调的是，Z 向量中的每个元素 z_i 都与 X 的所有元素有某种关联，而不是只与 x_i 有关联。&lt;/p&gt;

&lt;h4 id=&quot;72如何计算-qkv&quot;&gt;7.2、如何计算 Q、K、V&lt;/h4&gt;

&lt;p&gt;Q、K、V 全部来自输入 X 的线性变换：&lt;/p&gt;

\[\begin{aligned}
Q &amp;amp;= W^Q \cdot X \\
K &amp;amp;= W^K \cdot X \\
V &amp;amp;= W^V \cdot X
\end{aligned}\]

&lt;p&gt;\(W^Q、W^K、W^V\)  以随机初始化开始，经过训练就会得到非常好的表现。对于  \(X\)  中的每一个词向量  \(x_i\) ，经过这个变换后得到了：&lt;/p&gt;

\[\begin{aligned}
q_i &amp;amp;= W^Q \cdot x_i \\
k_i &amp;amp;= W^K \cdot x_i \\
v_i &amp;amp;= W^V \cdot x_i
\end{aligned}\]

&lt;h4 id=&quot;73注意力函数如何通过-qv-得到-z&quot;&gt;7.3、注意力函数：如何通过 Q、V 得到 Z&lt;/h4&gt;

&lt;p&gt;基于上面的启发，我们认为 X 经过自注意力的挖掘后，得到了：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;暗含信息 1：一组 query 与一组 key 之间的关联，记作 qk（想一下信息检索系统要用 query 先招到 key）&lt;/li&gt;
  &lt;li&gt;暗含信息 2：一组 value&lt;/li&gt;
  &lt;li&gt;暗含信息 3：qk 与 value 的某种关联&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这三组信息，分别如何表示呢？这里又需要一些启发了，因为计算机科学其实是在「模拟还原」现实世界，在 AI 的领域目前的研究方向就是模拟还原人脑的思考。所以这种「模拟还原」都是寻找某一种近似方法，因此不能按照数学、物理的逻辑推理来理解，而应该按照「工程」或者「计算科学」来理解，想想我们大学时学的「计算方法」这门课，因此常需要一些启发来找到某种「表示」。&lt;/p&gt;

&lt;p&gt;这里 Transformer 的作者，认为  \(Q\)  和  \(K\)  两个向量之间的关联，是我们在用  \(Q\)  找其在  \(K\)  上的投影，如果  \(Q\) 、 \(K\)  是单位长度的向量，那么这个投影其实可以理解为找「 \(Q\)  和  \(K\)  向量之间的相似度」：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;如果  \(Q\)  和  \(K\)  垂直，那么两个向量正交，其点积（Dot Product）为 0；&lt;/li&gt;
  &lt;li&gt;如果  \(Q\)  和  \(K\)  平行，那么两个向量点积为两者模积  \(\|Q\|\|K\|\) ；&lt;/li&gt;
  &lt;li&gt;如果  \(Q\)  和  \(K\)  呈某个夹角，则点积就是  \(Q\)  在  \(K\)  上的投影的模。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;因此「暗含信息 1」就可以用「 \(Q\cdot K\) 」再经过 Softmax 归一化来表示。这个表示，是一个所有元素都是 0~1 的矩阵，可以理解成对应注意力机制里的「注意力分数」，也就是一个「注意力分数矩阵（Attention Score Matrix）」。&lt;/p&gt;

&lt;p&gt;而「暗含信息 2」则是输入  \(X\)  经过的线性变换后的特征，看做  \(X\)  的另一种表示。然后我们用这个「注意力分数矩阵」来加持一下  \(V\) ，这个点积过程就表示了「暗含信息 3」了。所以我们有了如下公式：&lt;/p&gt;

\[\begin{aligned}
Z = Attention(Q,K,V) = Softmax(Q \cdot K^T) \cdot V
\end{aligned}\]

&lt;p&gt;其实到这里，这个注意力函数已经可以用了。有时候，为了避免因为向量维度过大，导致  \(Q \cdot K^T\)  点积结果过大，我们再加一步处理：&lt;/p&gt;

\[\begin{aligned}
Z = Attention(Q,K,V) = Softmax(\frac{Q \cdot K^T}{\sqrt{\smash[b]{d_k}}}) \cdot V
\end{aligned}\]

&lt;p&gt;这里  \(d_k\)  是 K 矩阵中向量  \(k_i\)  的维度。这一步修正还有进一步的解释，即如果经过 Softmax 归一化后模型稳定性存在问题。怎么理解？如果假设 Q 和 K 中的每个向量的每一维数据都具有零均值、单位方差，这样输入数据是具有稳定性的，那么如何让「暗含信息 1」计算后仍然具有稳定性呢？即运算结果依然保持零均值、单位方差，就是除以「 \(\sqrt{\smash[b]{d_k}}\) 」。&lt;/p&gt;

&lt;p&gt;到这里我们注意到：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;K、V 里的每一个向量，都是&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;74其他注意力函数&quot;&gt;7.4、其他注意力函数&lt;/h4&gt;

&lt;p&gt;为了提醒大家这种暗含信息的表示，都只是计算方法上的一种选择，好坏全靠结果评定，所以包括上面的在内，常见的注意力函数有（甚至你也可以自己定义）：&lt;/p&gt;

\[Z = Attention(Q,K,V) =
\begin{cases}
\begin{aligned}
&amp;amp;= Softmax(Q^T K) V \\
&amp;amp;= Softmax(\frac{Q K^T}{\sqrt{\smash[b]{d_k}}}) V \\
&amp;amp;= Softmax(\omega^T tanh(W[q;k])) V \\
&amp;amp;= Softmax(Q^T W K) V \\
&amp;amp;= cosine[Q^T K] V
\end{aligned}
\end{cases}\]

&lt;p&gt;到这里，我们就从原始的输入  \(X\)  得到了一个包含自注意力信息的  \(Z\)  了，后续就可以用  \(Z\)  了。&lt;/p&gt;

&lt;h3 id=&quot;第-8-节--多头注意力&quot;&gt;第 8 节 · 多头注意力&lt;/h3&gt;

&lt;p&gt;到这里我们理解了「自注意力」，而 Transformer 这篇论文通过添加「多头」注意力的机制进一步提升了注意力层。我们先看下它是什么，然后看下它的优点。从本小节开始，本文大量插图引用自&lt;a href=&quot;http://jalammar.github.io/illustrated-transformer/&quot;&gt;《The Illustrated Transformer》&lt;/a&gt;，作者 Jay Alammar 写出一篇非常深入浅出的图解文章，被大量引用，非常出色，再次建议大家去阅读。&lt;/p&gt;

&lt;p&gt;Transformer 中用了 8 个头，也就是 8 组不同的 Q-K-V：&lt;/p&gt;

\[\begin{aligned}
Q_0 = W_0^Q \cdot X ;\enspace K_0 = &amp;amp;W_0^K \cdot X ;\enspace V_0 = W_0^V \cdot X \\
Q_1 = W_1^Q \cdot X ;\enspace K_1 = &amp;amp;W_0^K \cdot X ;\enspace V_1 = W_1^V \cdot X \\
&amp;amp;.... \\
Q_7 = W_7^Q \cdot X ;\enspace K_7 = &amp;amp;W_0^K \cdot X ;\enspace V_7 = W_7^V \cdot X
\end{aligned}\]

&lt;p&gt;这样我们就能得到 8 个 Z：&lt;/p&gt;

\[\begin{aligned}
&amp;amp;Z_0 = Attention(Q_0,K_0,V_0) = Softmax(\frac{Q_0 \cdot K_0^T}{\sqrt{\smash[b]{d_k}}}) \cdot V_0 \\
&amp;amp;Z_1 = Attention(Q_1,K_1,V_1) = Softmax(\frac{Q_1 \cdot K_1^T}{\sqrt{\smash[b]{d_k}}}) \cdot V_1 \\
&amp;amp;... \\
&amp;amp;Z_7 = Attention(Q_7,K_7,V_7) = Softmax(\frac{Q_7 \cdot K_7^T}{\sqrt{\smash[b]{d_k}}}) \cdot V_7 \\
\end{aligned}\]

&lt;p&gt;然后我们把  \(Z_0\)  到  \(Z_7\)  沿着行数不变的方向全部连接起来，如下图所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023-01-04-language-model-5-3.png&quot; alt=&quot;image&quot; width=&quot;464&quot; /&gt;&lt;/p&gt;

&lt;p&gt;我们再训练一个权重矩阵  \(W^O\) ，然后用上面拼接的  \(Z_{0-7}\)  乘以这个权重矩阵：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023-01-04-language-model-5-4.png&quot; alt=&quot;image&quot; width=&quot;135&quot; /&gt;&lt;/p&gt;

&lt;p&gt;于是我们会得到一个 Z 矩阵：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023-01-04-language-model-5-5.png&quot; alt=&quot;image&quot; width=&quot;100&quot; /&gt;&lt;/p&gt;

&lt;p&gt;到这里就是多头注意力机制的全部内容，与单头注意力相比，都是为了得到一个 Z 矩阵，但是多头用了多组 Q-K-V，然后经过拼接、乘以权重矩阵得到最后的 Z。我们总览一下整个过程：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023-01-04-language-model-5-6.png&quot; alt=&quot;image&quot; width=&quot;935&quot; /&gt;&lt;/p&gt;

&lt;p&gt;通过多头注意力，每个头都会关注到不同的信息，可以如下类似表示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023-01-04-language-model-5-7.png&quot; alt=&quot;image&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;p&gt;这通过两种方式提高了注意力层的性能：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;多头注意力机制，扩展了模型关注不同位置的能力。 \(Z\)  矩阵中的每个向量  \(z_i\)  包含了与  \(X\)  中所有向量  \(x_i\)  有关的一点编码信息。反过来说，不要认为  \(z_i\)  只与  \(x_i\)  有关。&lt;/li&gt;
  &lt;li&gt;多头注意力机制，为注意力层提供了多个「表示子空间 Q-K-V」，以及 Z。这样一个输入矩阵  \(X\) ，就会被表示成 8 种不同的矩阵 Z，都包含了原始数据信息的某种解读暗含其中。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;第-9-节--退化现象残差网络与-short-cut&quot;&gt;第 9 节 · 退化现象、残差网络与 Short-Cut&lt;/h3&gt;

&lt;h4 id=&quot;91退化现象&quot;&gt;9.1、退化现象&lt;/h4&gt;

&lt;p&gt;对于一个 56 层的神经网路，我们很自然地会觉得应该比 20 层的神经网络的效果要好，比如说从误差率（error）的量化角度看。但是华人学者何凯明等人的论文&lt;a href=&quot;https://arxiv.org/pdf/1512.03385.pdf&quot;&gt;《Deep Residual Learning for Image Recognition》&lt;/a&gt;中给我们呈现了相反的结果，而这个问题的原因并不是因为层数多带来的梯度爆炸/梯度消失（毕竟已经用了归一化解决了这个问题），而是因为一种反常的现象，这种现象我们称之为「退化现象」。何凯明等人认为这是因为存在「难以优化好的网络层」。&lt;/p&gt;

&lt;h4 id=&quot;92恒等映射&quot;&gt;9.2、恒等映射&lt;/h4&gt;

&lt;p&gt;如果这 36 层还帮了倒忙，那还不如没有，是不是？所以这多出来的 36 个网络层，如果对于提升性能（例如误差率）毫无影响，甚至更进一步，这 36 层前的输入数据，和经过这 36 层后的输出数据，完全相同，那么如果将这 36 层抽象成一个函数  \(f_{36}\) ，这就是一个恒等映射的函数：&lt;/p&gt;

\[f_{36}(x) = x\]

&lt;p&gt;回到实际应用中。如果我们对于一个神经网络中的连续 N 层是提升性能，还是降低性能，是未知的，那么则可以建立一个跳过这些层的连接，实现：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;如果这 N 层可以提升性能，则采用这 N 层；否则就跳过。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;这就像给了这 N 层神经网络一个试错的空间，待我们确认它们的性能后再决定是否采用它们。同时也可以理解成，这些层可以去单独优化，如果性能提升，则不被跳过。&lt;/p&gt;

&lt;h4 id=&quot;93残差网络residual-network与捷径short-cut&quot;&gt;9.3、残差网络（Residual Network）与捷径（Short-Cut）&lt;/h4&gt;

&lt;p&gt;如果前面 20 层已经可以实现 99% 的准确率，那么引入了这 36 层能否再提升「残差剩余那 1%」的准确率从而达到 100% 呢？所以这 36 层的网络，就被称为「残差网络（Residual Network，常简称为 ResNet）」，这个叫法非常形象。&lt;/p&gt;

&lt;p&gt;而那个可以跳过 N 层残差网络的捷径，则常被称为 Short-Cut，也会被叫做跳跃链接（Skip Conntection），这就解决了上述深度学习中的「退化现象」。&lt;/p&gt;

&lt;h3 id=&quot;第-10-节--transformer-的位置编码positional-embedding&quot;&gt;第 10 节 · Transformer 的位置编码（Positional Embedding）&lt;/h3&gt;

&lt;p&gt;还记得我在第二部分最后提到的吗：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;这个注意力机制忽略了位置信息。比如 Tigers love rabbits 和 Rabbits love tigers 会产生一样的注意力分数。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4 id=&quot;101transformer-论文中的三角式位置编码sinusoidal-positional-encoding&quot;&gt;10.1、Transformer 论文中的三角式位置编码（Sinusoidal Positional Encoding）&lt;/h4&gt;

&lt;p&gt;现在我们来解决这个问题，为每一个输入向量  \(x_i\)  生成一个位置编码向量  \(t_i\) ，这个位置编码向量的维度，与输入向量（词的嵌入式向量表示）的维度是相同的：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023-01-04-language-model-5-8.png&quot; alt=&quot;image&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Transformer 论文中给出了如下的公式，来计算位置编码向量的每一位的值：&lt;/p&gt;

\[\begin{aligned}
P_{pos,2i} &amp;amp;= sin(\frac{pos}{10000^{\frac{2i}{d_{model}}}}) \\
P_{pos,2i+1} &amp;amp;= cos(\frac{pos}{10000^{\frac{2i}{d_{model}}}})
\end{aligned}\]

&lt;p&gt;这样对于一个 embedding，如果它在输入内容中的位置是 pos，那么其编码向量就表示为：&lt;/p&gt;

\[\begin{aligned}
[P_{pos,0}, P_{pos,1}, ... , P_{pos,d_x-1}]
\end{aligned}\]

&lt;p&gt;延展开的话，位置编码其实还分为绝对位置编码（Absolute Positional Encoding）、相对位置编码（Relative Positional Encoding）。前者是专门生成位置编码，并想办法融入到输入中，我们上面看到的就是一种。后者是微调 Attention 结构，使得它可以分辨不同位置的数据。另外其实还有一些无法分类到这两种的位置编码方法。&lt;/p&gt;

&lt;h4 id=&quot;102绝对位置编码&quot;&gt;10.2、绝对位置编码&lt;/h4&gt;

&lt;p&gt;绝对位置编码，如上面提到的，就是定义一个位置编码向量  \(t_i\) ，通过  \(x_i + t_i\)  就得到了一个含有位置信息的向量。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;习得式位置编码（Learned Positional Encoding）：将位置编码当做训练参数，生成一个「最大长度 x 编码维度」的位置编码矩阵，随着训练进行更新。目前 Google BERT、OpenAI GPT 模型都是用的这种位置编码。缺点是「外推性」差，如果文本长度超过之前训练时用的「最大长度」则无法处理。目前有一些给出优化方案的论文，比如「&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&amp;amp;mid=2247515573&amp;amp;idx=1&amp;amp;sn=2d719108244ada7db3a535a435631210&amp;amp;chksm=96ea6235a19deb23babde5eaac484d69e4c2f53bab72d2e350f75bed18323eea3cf9be30615b#rd&quot;&gt;层次分解位置编码&lt;/a&gt;」。&lt;/li&gt;
  &lt;li&gt;三角式位置编码（Sinusoidal Positional Encodign）：上面提过了。&lt;/li&gt;
  &lt;li&gt;循环式位置编码（Recurrent Positional Encoding）：通过一个 RNN 再接一个 Transformer，那么 RNN 暗含的「顺序」就导致不再需要额外编码了。但这样牺牲了并行性，毕竟 RNN 的两大缺点之一就有这个。&lt;/li&gt;
  &lt;li&gt;相乘式位置编码（Product Positional Encoding）：用「 \(x_i \odot t_i\) 」代替「 \(x_i + t_i\) 」。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;103相对位置编码和其他位置编码&quot;&gt;10.3、相对位置编码和其他位置编码&lt;/h4&gt;

&lt;p&gt;最早来自于 Google 的论文&lt;a href=&quot;https://arxiv.org/abs/1803.02155&quot;&gt;《Self-Attention with Relative Position Representations》&lt;/a&gt;相对位置编码，考虑的是当前 position 与被 attention 的 position 之前的相对位置。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;常见相对位置编码：经典式、XLNET 式、T5 式、DeBERTa 式等。&lt;/li&gt;
  &lt;li&gt;其他位置编码：CNN 式、复数式、融合式等。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;到此我们都是在讲 Encoder，目前我们知道一个 Encoder 可以用如下的示意图表示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023-01-04-language-model-5-12.png&quot; alt=&quot;image&quot; width=&quot;680&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;第-11-节--transformer-的编码器-encoder-和解码器-decoder&quot;&gt;第 11 节 · Transformer 的编码器 Encoder 和解码器 Decoder&lt;/h3&gt;

&lt;h4 id=&quot;111encoder-和-decoder-的图示结构&quot;&gt;11.1、Encoder 和 Decoder 的图示结构&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023-01-04-language-model-5-15.png&quot; alt=&quot;image&quot; width=&quot;165&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;第一层是多头注意力层（Multi-Head Attention Layer）。&lt;/li&gt;
  &lt;li&gt;第二层是经过一个前馈神经网络（Feed Forward Neural Network，简称 FFNN）。&lt;/li&gt;
  &lt;li&gt;这两层，每一层都有「Add &amp;amp; Normalization」和 ResNet。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023-01-04-language-model-5-14.png&quot; alt=&quot;image&quot; width=&quot;179&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;解码器有两个多头注意力层。第一个多头注意力层是 Masked Multi-Head Attention 层，即在自注意力计算的过程中只有前面位置上的内容。第二个多头注意力层买有被 Masked，是个正常多头注意力层。&lt;/li&gt;
  &lt;li&gt;可以看出来，第一个注意力层是一个自注意力层（Self Attention Layer），第二个是 Encoder-Decoder Attention 层（它的 K、V 来自 Encoder，Q 来自自注意力层），有些文章里会用这个角度来指代。&lt;/li&gt;
  &lt;li&gt;FNN、Add &amp;amp; Norm、ResNet 都与 Encoder 类似。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;112decoder-的第一个输出结果&quot;&gt;11.2、Decoder 的第一个输出结果&lt;/h4&gt;

&lt;p&gt;产出第一个最终输出结果的过程：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;不需要经过 Masked Multi-Head Attention Layer（自注意力层）。&lt;/li&gt;
  &lt;li&gt;只经过 Encoder-Decoder Attention Layer。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023-01-04-language-model-5-13.png&quot; alt=&quot;image&quot; width=&quot;695&quot; /&gt;&lt;/p&gt;

&lt;p&gt;这样我们就像前面的 Encoder-Decoder Attention 模型一样，得到第一个输出。但是最终的输出结果，还会经过一层「Linear + Softmax」。&lt;/p&gt;

&lt;h4 id=&quot;113decoder-后续的所有输出&quot;&gt;11.3、Decoder 后续的所有输出&lt;/h4&gt;

&lt;p&gt;从产出第二个输出结果开始：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Decoder 的自注意力层，会用到前面的输出结果。&lt;/li&gt;
  &lt;li&gt;可以看到，这是一个串行过程。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;114decoder-之后的-linear-和-softmax&quot;&gt;11.4、Decoder 之后的 Linear 和 Softmax&lt;/h4&gt;

&lt;p&gt;经过所有 Decoder 之后，我们得到了一大堆浮点数的结果。最后的 Linear &amp;amp; Softmax 就是来解决「怎么把它变成文本」的问题的。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Linear 是一个全连接神经网络，把 Decoders 输出的结果投影到一个超大的向量上，我们称之为 logits 向量。&lt;/li&gt;
  &lt;li&gt;如果我们的输出词汇表有 1 万个词，那么 logits 向量的每一个维度就有 1 万个单元，每个单元都对应输出词汇表的一个词的概率。&lt;/li&gt;
  &lt;li&gt;Softmax 将 logits 向量中的每一个维度都做归一化，这样每个维度都能从 1 万个单元对应的词概率中选出最大的，对应的词汇表里的词，就是输出词。最终得到一个输出字符串。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;第-12-节--transformer-模型整体&quot;&gt;第 12 节 · Transformer 模型整体&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023-01-04-language-model-5-16.png&quot; alt=&quot;image&quot; width=&quot;660&quot; /&gt;&lt;/p&gt;

&lt;p&gt;最后我们再来整体看一下 Transformer：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;首先输入数据生成词的嵌入式向量表示（Embedding），生成位置编码（Positional Encoding，简称 PE）。&lt;/li&gt;
  &lt;li&gt;进入 Encoders 部分。先进入多头注意力层（Multi-Head Attention），是自注意力处理，然后进入全连接层（又叫前馈神经网络层），每层都有 ResNet、Add &amp;amp; Norm。&lt;/li&gt;
  &lt;li&gt;每一个 Encoder 的输入，都来自前一个 Encoder 的输出，但是第一个 Encoder 的输入就是 Embedding + PE。&lt;/li&gt;
  &lt;li&gt;进入 Decoders 部分。先进入第一个多头注意力层（是 Masked 自注意力层），再进入第二个多头注意力层（是 Encoder-Decoder 注意力层），每层都有 ResNet、Add &amp;amp; Norm。&lt;/li&gt;
  &lt;li&gt;每一个 Decoder 都有两部分输入。&lt;/li&gt;
  &lt;li&gt;Decoder 的第一层（Maksed 多头自注意力层）的输入，都来自前一个 Decoder 的输出，但是第一个 Decoder 是不经过第一层的（因为经过算出来也是 0）。&lt;/li&gt;
  &lt;li&gt;Decoder 的第二层（Encoder-Decoder 注意力层）的输入，Q 都来自该 Decoder 的第一层，且每个 Decoder 的这一层的 K、V 都是一样的，均来自最后一个 Encoder。&lt;/li&gt;
  &lt;li&gt;最后经过 Linear、Softmax 归一化。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;第-13-节--transformer-的性能&quot;&gt;第 13 节 · Transformer 的性能&lt;/h3&gt;

&lt;p&gt;Google 在其博客于 2017.08.31 发布如下测试数据：&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;img src=&quot;/img/src/2023-01-04-language-model-5-9.png&quot; alt=&quot;image&quot; /&gt;&lt;/th&gt;
      &lt;th&gt;&lt;img src=&quot;/img/src/2023-01-04-language-model-5-10.png&quot; alt=&quot;image&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;第三章--一个基于-tensorflow-架构的-transformer-实现&quot;&gt;第三章 · 一个基于 TensorFlow 架构的 Transformer 实现&lt;/h2&gt;

&lt;p&gt;我们来看看一个简单的 Transformer 模型，就是比较早出现的 Kyubyong 实现的 Transformer 模型：https://github.com/Kyubyong/transformer/tree/master/tf1.2_legacy&lt;/p&gt;

&lt;h3 id=&quot;第-14-节--先训练和测试一下-kyubyong-transformer&quot;&gt;第 14 节 · 先训练和测试一下 Kyubyong Transformer&lt;/h3&gt;

&lt;p&gt;下载一个「德语-英语翻译」的数据集：https://drive.google.com/uc?id=1l5y6Giag9aRPwGtuZHswh3w5v3qEz8D8&lt;/p&gt;

&lt;p&gt;把 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;de-en&lt;/code&gt; 下面的 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tgz&lt;/code&gt; 解压后放在 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;corpora/&lt;/code&gt; 目录下。如果需要先修改超参数，需要修改 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;hyperparams.py&lt;/code&gt;。然后运行如下命令，生成词汇文件（vocabulary files），默认到 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;preprocessed&lt;/code&gt; 目录下：&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;python prepro.py
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;然后开始训练：&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;python train.py
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;也可以跳过训练，直接&lt;a href=&quot;https://www.dropbox.com/s/fo5wqgnbmvalwwq/logdir.zip?dl=0&quot;&gt;下载预训练过的文件&lt;/a&gt;，是一个 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;logdir/&lt;/code&gt; 目录，把它放到项目根目录下。然后可以对训练出来的结果，运行评价程序啦：&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;python eval.py
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;会生成「德语-英语」测试结果文件在 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;results/&lt;/code&gt; 目录下，内容如下：&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;- source: Sie war eine jährige Frau namens Alex
- expected: She was a yearold woman named Alex
- got: She was a &amp;lt;UNK&amp;gt; of vote called &amp;lt;UNK&amp;gt;

- source: Und als ich das hörte war ich erleichtert
- expected: Now when I heard this I was so relieved
- got: And when I was I &amp;lt;UNK&amp;gt; &apos;s

- source: Meine Kommilitonin bekam nämlich einen Brandstifter als ersten Patienten
- expected: My classmate got an arsonist for her first client
- got: Because my first eye was a first show

- source: Das kriege ich hin dachte ich mir
- expected: This I thought I could handle
- got: I would give it to me a day

- source: Aber ich habe es nicht hingekriegt
- expected: But I didn&apos;t handle it
- got: But I didn&apos;t &amp;lt;UNK&amp;gt; &amp;lt;UNK&amp;gt;

- source: Ich hielt dagegen
- expected: I pushed back
- got: I &amp;lt;UNK&amp;gt;

...

Bleu Score = 6.598452846670836
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;评估结果文件的最后一行是 Bleu Score：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;这是用来评估机器翻译质量的一种度量方式。它是由几个不同的 BLEU 分数组成的，每个 BLEU 分数都表示翻译结果中与参考翻译的重叠程度。&lt;/li&gt;
  &lt;li&gt;一个常用的 BLEU 分数是 BLEU-4，它计算翻译结果中与参考翻译的 N 元文法语言模型 n-gram（n 为 4）的重叠程度。分数越高表示翻译结果越接近参考翻译。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;第-15-节--kyubyong-transformer-源码分析&quot;&gt;第 15 节 · Kyubyong Transformer 源码分析&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;hparams.py&lt;/code&gt;：超参数都在这里，仅 30 行。将在下面 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;2.1&lt;/code&gt; 部分解读。&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;data_load.py&lt;/code&gt;：装载、批处理数据的相关函数，代码仅 92 行。主要在下面 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;2.2&lt;/code&gt; 部分解读。&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;prepro.py&lt;/code&gt;：为 source 和 target 创建词汇文件（vocabulary file），代码仅 39 行。下面 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;2.3&lt;/code&gt; 部分会为大家解读。&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;train.py&lt;/code&gt;：代码仅 184 行。在下面 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;2.4&lt;/code&gt; 部分解读。&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;modules.py&lt;/code&gt;：Encoding / Decoding 网络的构建模块，代码仅 329 行。与 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;modules.py&lt;/code&gt; 一起会在 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;2.4&lt;/code&gt; 部分解读。&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;eval.py&lt;/code&gt;：评估效果，代码仅 82 行。将在 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;2.5&lt;/code&gt; 部分解读&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;总计 700 多行代码。&lt;/p&gt;

&lt;h4 id=&quot;151超参数&quot;&gt;15.1、超参数&lt;/h4&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;hyperparams.py&lt;/code&gt; 文件中定义了 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Hyperparams&lt;/code&gt; 超参数类，其中包含的参数我们逐一来解释一下：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;source_train&lt;/code&gt;：训练数据集的源输入文件，默认是 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&apos;corpora/train.tags.de-en.de&apos;&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;target_train&lt;/code&gt;：训练数据集的目标输出文件，默认是 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&apos;corpora/train.tags.de-en.en&apos;&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;source_test&lt;/code&gt;：测试数据集的源输入文件，默认是 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&apos;corpora/IWSLT16.TED.tst2014.de-en.de.xml&apos;&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;target_test&lt;/code&gt;：测试数据集的目标输出文件，默认是 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&apos;corpora/IWSLT16.TED.tst2014.de-en.en.xml&apos;&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;batch_size&lt;/code&gt;：设置每批数据的大小。&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;lr&lt;/code&gt;：设置学习率 learning rate。&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;logdir&lt;/code&gt;：设置日志文件保存的目录。&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;maxlen&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;min_cnt&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;hidden_units&lt;/code&gt;：设置编码器和解码器中隐藏层单元的数量。&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;num_blocks&lt;/code&gt;：编码器（encoder block）、解码器（decoder block）的数量&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;num_epochs&lt;/code&gt;：训练过程中迭代的次数。&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;num_heads&lt;/code&gt;：还记得上面文章里我们提到的 Transformer 中用到了多头注意力吧，这里就是多头注意力的头数。&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;droupout_rate&lt;/code&gt;：设置 dropout 层的 dropout rate，具体 dropout 请看 2.4.1 部分。&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sinusoid&lt;/code&gt;：设置为 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;True&lt;/code&gt; 时表示使用正弦函数计算位置编码，否则为 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;False&lt;/code&gt; 时表示直接用 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;position&lt;/code&gt; 做位置编码。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;152预处理&quot;&gt;15.2、预处理&lt;/h4&gt;

&lt;p&gt;文件 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;prepro.py&lt;/code&gt; 实现了预处理的过程，根据 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;hp.source_train&lt;/code&gt; 和 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;hp.target_train&lt;/code&gt; 分别创建 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&quot;de.vocab.tsv&quot;&lt;/code&gt; 和 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&quot;en.vocab.tsv&quot;&lt;/code&gt; 两个词汇表。&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;make_vocab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fpath&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fname&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# 使用 codecs.open 函数读取指定文件路径(fpath)的文本内容，并将其存储在 text 变量中
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;text&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;codecs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fpath&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&apos;r&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&apos;utf-8&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# 将 text 中的非字母和空格的字符去掉
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;text&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;regex&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sub&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;[^\s\p{Latin}&apos;]&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# 将 text 中的文本按照空格分割，并将每个单词存储在 words 变量中
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;words&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# words 中每个单词的词频
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;word2cnt&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Counter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;words&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# 检查是否存在 preprocessed 文件夹，如果不存在就创建
&lt;/span&gt;    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exists&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;preprocessed&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mkdir&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;preprocessed&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;codecs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;preprocessed/{}&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fname&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&apos;w&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&apos;utf-8&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;

    	&lt;span class=&quot;c1&quot;&gt;# 按出现次数从多到少的顺序写入每个单词和它的出现次数
&lt;/span&gt;    	&lt;span class=&quot;c1&quot;&gt;# 在文件最前面写入四个特殊字符 &amp;lt;PAD&amp;gt;, &amp;lt;UNK&amp;gt;, &amp;lt;S&amp;gt;, &amp;lt;/S&amp;gt; 分别用于填充，未知单词，句子开始和句子结束
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;fout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;write&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;{}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\t&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;1000000000&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;{}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\t&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;1000000000&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;{}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\t&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;1000000000&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;{}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\t&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;1000000000&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&amp;lt;PAD&amp;gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&amp;lt;UNK&amp;gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&amp;lt;S&amp;gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&amp;lt;/S&amp;gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;word&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cnt&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;word2cnt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;most_common&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;word2cnt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)):&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;fout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;write&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;u&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;{}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\t&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;{}&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;word&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cnt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__name__&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&apos;__main__&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;make_vocab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;source_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;de.vocab.tsv&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;make_vocab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;target_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;en.vocab.tsv&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Done&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;在主函数中调用 make_vocab 函数，在目录 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;preprocessed&lt;/code&gt; 生成 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;de.vocab.tsv&lt;/code&gt; 和 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;en.vocab.tsv&lt;/code&gt; 两个词汇表文件。&lt;/li&gt;
  &lt;li&gt;在函数 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;make_vocab&lt;/code&gt; 中，先使用 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;codecs.open&lt;/code&gt; 函数读取指定文件路径 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fpath&lt;/code&gt; 的文本内容，并将其存储在 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;text&lt;/code&gt; 变量中，再使用正则表达式 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;regex&lt;/code&gt; 将 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;text&lt;/code&gt; 中的非字母和空格的字符去掉，接着将 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;text&lt;/code&gt; 中的文本按照空格分割，并将每个单词存储在 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;words&lt;/code&gt; 变量中。&lt;/li&gt;
  &lt;li&gt;接下来，使用 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Counter&lt;/code&gt; 函数统计 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;words&lt;/code&gt; 中每个单词的出现次数，并将统计结果存储在 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;word2cnt&lt;/code&gt; 变量中。&lt;/li&gt;
  &lt;li&gt;最后所有词与词频，存储在 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;de.vocab.tsv&lt;/code&gt; 和 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;en.vocab.tsv&lt;/code&gt; 两个文件中。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;153训练测试数据集的加载&quot;&gt;15.3、训练/测试数据集的加载&lt;/h4&gt;

&lt;p&gt;我们先看下 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;train.py&lt;/code&gt;、&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;data_load.py&lt;/code&gt;、&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;eval.py&lt;/code&gt; 三个文件：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;train.py&lt;/code&gt;：该文件包含了 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Graph&lt;/code&gt; 类的定义，并在其构造函数中调用 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;load_data.py&lt;/code&gt; 文件中的 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;get_batch_data&lt;/code&gt; 函数加载训练数据。&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;data_load.py&lt;/code&gt;：定义了加载训练数据、加载测试数据的函数。&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;eval.py&lt;/code&gt;：测试结果的评价函数定义在这个文件里。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;下面是函数调用的流程：&lt;/p&gt;

&lt;div style=&quot;text-align: center;&quot;&gt;
&lt;div class=&quot;graphviz-wrapper&quot;&gt;

&lt;!-- Generated by graphviz version 2.43.0 (0)
 --&gt;
&lt;!-- Title: G Pages: 1 --&gt;
&lt;svg role=&quot;img&quot; aria-label=&quot;graphviz-9559986008ed2e1e47e8729260efda61&quot; width=&quot;830pt&quot; height=&quot;98pt&quot; viewBox=&quot;0.00 0.00 830.00 98.00&quot;&gt;
&lt;title&gt;graphviz-9559986008ed2e1e47e8729260efda61&lt;/title&gt;
&lt;desc&gt;
digraph G {
	rankdir=LR
	splines=ortho
	node [shape=&amp;quot;box&amp;quot;]

	训练 -&amp;gt; Graph构造函数 -&amp;gt; get_batch_data -&amp;gt; load_train_data
	测试 -&amp;gt; eval -&amp;gt; load_test_data

	load_train_data -&amp;gt; create_data
	load_test_data -&amp;gt; create_data

	create_data -&amp;gt; load_de_vocab
	create_data -&amp;gt; load_en_vocab
}
&lt;/desc&gt;

&lt;g id=&quot;graph0&quot; class=&quot;graph&quot; transform=&quot;scale(1 1) rotate(0) translate(4 94)&quot;&gt;
&lt;title&gt;G&lt;/title&gt;
&lt;polygon fill=&quot;white&quot; stroke=&quot;transparent&quot; points=&quot;-4,4 -4,-94 826,-94 826,4 -4,4&quot; /&gt;
&lt;!-- 训练 --&gt;
&lt;g id=&quot;node1&quot; class=&quot;node&quot;&gt;
&lt;title&gt;训练&lt;/title&gt;
&lt;polygon fill=&quot;none&quot; stroke=&quot;black&quot; points=&quot;54,-90 0,-90 0,-54 54,-54 54,-90&quot; /&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;27&quot; y=&quot;-68.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;训练&lt;/text&gt;
&lt;/g&gt;
&lt;!-- Graph构造函数 --&gt;
&lt;g id=&quot;node2&quot; class=&quot;node&quot;&gt;
&lt;title&gt;Graph构造函数&lt;/title&gt;
&lt;polygon fill=&quot;none&quot; stroke=&quot;black&quot; points=&quot;208,-90 90,-90 90,-54 208,-54 208,-90&quot; /&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;149&quot; y=&quot;-68.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;Graph构造函数&lt;/text&gt;
&lt;/g&gt;
&lt;!-- 训练&amp;#45;&amp;gt;Graph构造函数 --&gt;
&lt;g id=&quot;edge1&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;训练&amp;#45;&amp;gt;Graph构造函数&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M54.08,-72C54.08,-72 79.54,-72 79.54,-72&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;79.54,-75.5 89.54,-72 79.54,-68.5 79.54,-75.5&quot; /&gt;
&lt;/g&gt;
&lt;!-- get_batch_data --&gt;
&lt;g id=&quot;node3&quot; class=&quot;node&quot;&gt;
&lt;title&gt;get_batch_data&lt;/title&gt;
&lt;polygon fill=&quot;none&quot; stroke=&quot;black&quot; points=&quot;369,-90 244,-90 244,-54 369,-54 369,-90&quot; /&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;306.5&quot; y=&quot;-68.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;get_batch_data&lt;/text&gt;
&lt;/g&gt;
&lt;!-- Graph构造函数&amp;#45;&amp;gt;get_batch_data --&gt;
&lt;g id=&quot;edge2&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;Graph构造函数&amp;#45;&amp;gt;get_batch_data&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M208.1,-72C208.1,-72 233.91,-72 233.91,-72&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;233.91,-75.5 243.91,-72 233.91,-68.5 233.91,-75.5&quot; /&gt;
&lt;/g&gt;
&lt;!-- load_train_data --&gt;
&lt;g id=&quot;node4&quot; class=&quot;node&quot;&gt;
&lt;title&gt;load_train_data&lt;/title&gt;
&lt;polygon fill=&quot;none&quot; stroke=&quot;black&quot; points=&quot;531,-90 405,-90 405,-54 531,-54 531,-90&quot; /&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;468&quot; y=&quot;-68.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;load_train_data&lt;/text&gt;
&lt;/g&gt;
&lt;!-- get_batch_data&amp;#45;&amp;gt;load_train_data --&gt;
&lt;g id=&quot;edge3&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;get_batch_data&amp;#45;&amp;gt;load_train_data&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M369.4,-72C369.4,-72 394.74,-72 394.74,-72&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;394.74,-75.5 404.74,-72 394.74,-68.5 394.74,-75.5&quot; /&gt;
&lt;/g&gt;
&lt;!-- create_data --&gt;
&lt;g id=&quot;node8&quot; class=&quot;node&quot;&gt;
&lt;title&gt;create_data&lt;/title&gt;
&lt;polygon fill=&quot;none&quot; stroke=&quot;black&quot; points=&quot;667,-63 567,-63 567,-27 667,-27 667,-63&quot; /&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;617&quot; y=&quot;-41.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;create_data&lt;/text&gt;
&lt;/g&gt;
&lt;!-- load_train_data&amp;#45;&amp;gt;create_data --&gt;
&lt;g id=&quot;edge6&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;load_train_data&amp;#45;&amp;gt;create_data&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M531.19,-58.5C531.19,-58.5 556.81,-58.5 556.81,-58.5&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;556.81,-62 566.81,-58.5 556.81,-55 556.81,-62&quot; /&gt;
&lt;/g&gt;
&lt;!-- 测试 --&gt;
&lt;g id=&quot;node5&quot; class=&quot;node&quot;&gt;
&lt;title&gt;测试&lt;/title&gt;
&lt;polygon fill=&quot;none&quot; stroke=&quot;black&quot; points=&quot;176,-36 122,-36 122,0 176,0 176,-36&quot; /&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;149&quot; y=&quot;-14.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;测试&lt;/text&gt;
&lt;/g&gt;
&lt;!-- eval --&gt;
&lt;g id=&quot;node6&quot; class=&quot;node&quot;&gt;
&lt;title&gt;eval&lt;/title&gt;
&lt;polygon fill=&quot;none&quot; stroke=&quot;black&quot; points=&quot;333.5,-36 279.5,-36 279.5,0 333.5,0 333.5,-36&quot; /&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;306.5&quot; y=&quot;-14.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;eval&lt;/text&gt;
&lt;/g&gt;
&lt;!-- 测试&amp;#45;&amp;gt;eval --&gt;
&lt;g id=&quot;edge4&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;测试&amp;#45;&amp;gt;eval&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M176.08,-18C176.08,-18 269.25,-18 269.25,-18&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;269.25,-21.5 279.25,-18 269.25,-14.5 269.25,-21.5&quot; /&gt;
&lt;/g&gt;
&lt;!-- load_test_data --&gt;
&lt;g id=&quot;node7&quot; class=&quot;node&quot;&gt;
&lt;title&gt;load_test_data&lt;/title&gt;
&lt;polygon fill=&quot;none&quot; stroke=&quot;black&quot; points=&quot;527.5,-36 408.5,-36 408.5,0 527.5,0 527.5,-36&quot; /&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;468&quot; y=&quot;-14.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;load_test_data&lt;/text&gt;
&lt;/g&gt;
&lt;!-- eval&amp;#45;&amp;gt;load_test_data --&gt;
&lt;g id=&quot;edge5&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;eval&amp;#45;&amp;gt;load_test_data&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M333.53,-18C333.53,-18 398.34,-18 398.34,-18&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;398.34,-21.5 408.34,-18 398.34,-14.5 398.34,-21.5&quot; /&gt;
&lt;/g&gt;
&lt;!-- load_test_data&amp;#45;&amp;gt;create_data --&gt;
&lt;g id=&quot;edge7&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;load_test_data&amp;#45;&amp;gt;create_data&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M527.75,-31.5C527.75,-31.5 556.82,-31.5 556.82,-31.5&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;556.82,-35 566.82,-31.5 556.81,-28 556.82,-35&quot; /&gt;
&lt;/g&gt;
&lt;!-- load_de_vocab --&gt;
&lt;g id=&quot;node9&quot; class=&quot;node&quot;&gt;
&lt;title&gt;load_de_vocab&lt;/title&gt;
&lt;polygon fill=&quot;none&quot; stroke=&quot;black&quot; points=&quot;822,-90 703,-90 703,-54 822,-54 822,-90&quot; /&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;762.5&quot; y=&quot;-68.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;load_de_vocab&lt;/text&gt;
&lt;/g&gt;
&lt;!-- create_data&amp;#45;&amp;gt;load_de_vocab --&gt;
&lt;g id=&quot;edge8&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;create_data&amp;#45;&amp;gt;load_de_vocab&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M667.07,-58.5C667.07,-58.5 692.8,-58.5 692.8,-58.5&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;692.8,-62 702.8,-58.5 692.8,-55 692.8,-62&quot; /&gt;
&lt;/g&gt;
&lt;!-- load_en_vocab --&gt;
&lt;g id=&quot;node10&quot; class=&quot;node&quot;&gt;
&lt;title&gt;load_en_vocab&lt;/title&gt;
&lt;polygon fill=&quot;none&quot; stroke=&quot;black&quot; points=&quot;822,-36 703,-36 703,0 822,0 822,-36&quot; /&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;762.5&quot; y=&quot;-14.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;load_en_vocab&lt;/text&gt;
&lt;/g&gt;
&lt;!-- create_data&amp;#45;&amp;gt;load_en_vocab --&gt;
&lt;g id=&quot;edge9&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;create_data&amp;#45;&amp;gt;load_en_vocab&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M667.07,-31.5C667.07,-31.5 692.8,-31.5 692.8,-31.5&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;692.8,-35 702.8,-31.5 692.8,-28 692.8,-35&quot; /&gt;
&lt;/g&gt;
&lt;/g&gt;
&lt;/svg&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;load_de_vocab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;vocab&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;line&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;line&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;codecs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;preprocessed/de.vocab.tsv&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&apos;r&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&apos;utf-8&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;splitlines&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;line&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;min_cnt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;word2idx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;word&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;word&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;enumerate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vocab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)}&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;idx2word&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;word&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;word&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;enumerate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vocab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)}&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;word2idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;idx2word&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;load_en_vocab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;vocab&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;line&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;line&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;codecs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;preprocessed/en.vocab.tsv&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&apos;r&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&apos;utf-8&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;splitlines&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;line&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;min_cnt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;word2idx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;word&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;word&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;enumerate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vocab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)}&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;idx2word&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;word&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;word&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;enumerate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vocab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)}&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;word2idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;idx2word&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;将 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;preprocessed/de.vocab.tsv&lt;/code&gt; 和 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;preprocessed/en.vocab.tsv&lt;/code&gt; 中储存的德语、英语的词汇、词频，载入成 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;word2idx&lt;/code&gt; 和 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;idx2word&lt;/code&gt;。前者是通过词查询词向量，后者通过词向量查询词。&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;load_de_vocab&lt;/code&gt; 和 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;load_en_vocab&lt;/code&gt; 函数被 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;create_data&lt;/code&gt; 函数引用，该函数将输入的源语言和目标语言句子转换为索引表示，并对过长的句子进行截断或填充。详细的解释看下面代码里的注释。&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# 输入参数是翻译模型的源语言语句、目标语言语句
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;create_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;source_sents&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target_sents&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;de2idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;idx2de&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;load_de_vocab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;en2idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;idx2en&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;load_en_vocab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    
    &lt;span class=&quot;c1&quot;&gt;# 用 zip 函数将源语言和目标语言句子对应起来，并对句子进行截断或填充
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;x_list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Sources&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Targets&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;source_sent&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target_sent&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;source_sents&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target_sents&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;de2idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;word&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;word&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;source_sent&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;sa&quot;&gt;u&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot; &amp;lt;/S&amp;gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()]&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# 1: OOV, &amp;lt;/S&amp;gt;: End of Text
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;en2idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;word&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;word&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;target_sent&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;sa&quot;&gt;u&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot; &amp;lt;/S&amp;gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()]&lt;/span&gt; 

        &lt;span class=&quot;c1&quot;&gt;# 将句子的词的编号，原句以及编号后的句子存储下来，以供之后使用
&lt;/span&gt;        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;maxlen&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;

        	&lt;span class=&quot;c1&quot;&gt;# 将 x 和 y 转换成 numpy 数组并加入 x_list 和 y_list 中
&lt;/span&gt;            &lt;span class=&quot;n&quot;&gt;x_list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;y_list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

            &lt;span class=&quot;c1&quot;&gt;# 将原始的 source_sent 和 target_sent 加入 Sources 和 Targets 列表中
&lt;/span&gt;            &lt;span class=&quot;n&quot;&gt;Sources&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;source_sent&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;Targets&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;target_sent&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
    &lt;span class=&quot;c1&quot;&gt;# 对于每个 (x, y) 对，使用 np.lib.pad 函数将 x 和 y 分别用 0 进行填充，直到长度为 hp.maxlen
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# 这样做的目的是使得每个句子长度都相等，方便后续的训练
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;maxlen&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;int32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;maxlen&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;int32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;enumerate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lib&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;maxlen&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)],&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&apos;constant&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;constant_values&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lib&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;maxlen&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)],&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&apos;constant&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;constant_values&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# 返回转换后的索引表示，以及未经处理的源语言和目标语言句子
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# X 是原始句子中德语的索引
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# Y 是原始句子中英语的索引
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# Sources 是源原始句子列表，并与 X 一一对应
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# Targets 是目标原始句子列表，并与 Y 一一对应
&lt;/span&gt;    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Sources&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Targets&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 返回原始句子中德语、英语的索引
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;load_train_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;de_sents&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;regex&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sub&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;[^\s\p{Latin}&apos;]&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;line&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;line&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;codecs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;source_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&apos;r&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&apos;utf-8&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;line&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;line&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&amp;lt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;en_sents&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;regex&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sub&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;[^\s\p{Latin}&apos;]&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;line&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;line&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;codecs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;target_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&apos;r&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&apos;utf-8&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;line&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;line&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&amp;lt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    
    &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Sources&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Targets&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;create_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;de_sents&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;en_sents&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;下面的 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;get_batch_data&lt;/code&gt; 则从文本数据中读取并生成 batch：&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;get_batch_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    
    &lt;span class=&quot;c1&quot;&gt;# 加载数据
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;load_train_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    
    &lt;span class=&quot;c1&quot;&gt;# calc total batch count
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;num_batch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;//&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;
    
    &lt;span class=&quot;c1&quot;&gt;# 将 X 和 Y 转换成张量
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;convert_to_tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;int32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;convert_to_tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;int32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
    &lt;span class=&quot;c1&quot;&gt;# 创建输入队列
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;input_queues&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;slice_input_producer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
            
    &lt;span class=&quot;c1&quot;&gt;# 创建 batch 队列，利用 shuffle_batch 将一组 tensor 随机打乱，并将它们分为多个 batch
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# 使用 shuffle_batch 是为了防止模型过拟合
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shuffle_batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_queues&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                &lt;span class=&quot;n&quot;&gt;num_threads&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                                &lt;span class=&quot;n&quot;&gt;capacity&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;   
                                &lt;span class=&quot;n&quot;&gt;min_after_dequeue&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                                &lt;span class=&quot;n&quot;&gt;allow_smaller_final_batch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_batch&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# (N, T), (N, T), ()
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;154构建模型并训练&quot;&gt;15.4、构建模型并训练&lt;/h4&gt;

&lt;p&gt;Graph 的构造函数流程，就是模型的构建流程，下面船长来分析这部分代码。&lt;/p&gt;

&lt;div style=&quot;text-align: center;&quot;&gt;
&lt;div class=&quot;graphviz-wrapper&quot;&gt;

&lt;!-- Generated by graphviz version 2.43.0 (0)
 --&gt;
&lt;!-- Title: G Pages: 1 --&gt;
&lt;svg role=&quot;img&quot; aria-label=&quot;graphviz-d43b60bdbb0d309ff41c0875976a1d4b&quot; width=&quot;526pt&quot; height=&quot;44pt&quot; viewBox=&quot;0.00 0.00 526.00 44.00&quot;&gt;
&lt;title&gt;graphviz-d43b60bdbb0d309ff41c0875976a1d4b&lt;/title&gt;
&lt;desc&gt;
digraph G {
	rankdir=LR
	splines=ortho
	node [shape=&amp;quot;box&amp;quot;]

	Graph构造函数 -&amp;gt; 编码器 -&amp;gt; 解码器 -&amp;gt; Linear -&amp;gt; Softmax
}
&lt;/desc&gt;

&lt;g id=&quot;graph0&quot; class=&quot;graph&quot; transform=&quot;scale(1 1) rotate(0) translate(4 40)&quot;&gt;
&lt;title&gt;G&lt;/title&gt;
&lt;polygon fill=&quot;white&quot; stroke=&quot;transparent&quot; points=&quot;-4,4 -4,-40 522,-40 522,4 -4,4&quot; /&gt;
&lt;!-- Graph构造函数 --&gt;
&lt;g id=&quot;node1&quot; class=&quot;node&quot;&gt;
&lt;title&gt;Graph构造函数&lt;/title&gt;
&lt;polygon fill=&quot;none&quot; stroke=&quot;black&quot; points=&quot;118,-36 0,-36 0,0 118,0 118,-36&quot; /&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;59&quot; y=&quot;-14.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;Graph构造函数&lt;/text&gt;
&lt;/g&gt;
&lt;!-- 编码器 --&gt;
&lt;g id=&quot;node2&quot; class=&quot;node&quot;&gt;
&lt;title&gt;编码器&lt;/title&gt;
&lt;polygon fill=&quot;none&quot; stroke=&quot;black&quot; points=&quot;213,-36 154,-36 154,0 213,0 213,-36&quot; /&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;183.5&quot; y=&quot;-14.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;编码器&lt;/text&gt;
&lt;/g&gt;
&lt;!-- Graph构造函数&amp;#45;&amp;gt;编码器 --&gt;
&lt;g id=&quot;edge1&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;Graph构造函数&amp;#45;&amp;gt;编码器&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M118.33,-18C118.33,-18 143.7,-18 143.7,-18&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;143.7,-21.5 153.7,-18 143.7,-14.5 143.7,-21.5&quot; /&gt;
&lt;/g&gt;
&lt;!-- 解码器 --&gt;
&lt;g id=&quot;node3&quot; class=&quot;node&quot;&gt;
&lt;title&gt;解码器&lt;/title&gt;
&lt;polygon fill=&quot;none&quot; stroke=&quot;black&quot; points=&quot;308,-36 249,-36 249,0 308,0 308,-36&quot; /&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;278.5&quot; y=&quot;-14.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;解码器&lt;/text&gt;
&lt;/g&gt;
&lt;!-- 编码器&amp;#45;&amp;gt;解码器 --&gt;
&lt;g id=&quot;edge2&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;编码器&amp;#45;&amp;gt;解码器&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M213.04,-18C213.04,-18 238.98,-18 238.98,-18&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;238.98,-21.5 248.98,-18 238.98,-14.5 238.98,-21.5&quot; /&gt;
&lt;/g&gt;
&lt;!-- Linear --&gt;
&lt;g id=&quot;node4&quot; class=&quot;node&quot;&gt;
&lt;title&gt;Linear&lt;/title&gt;
&lt;polygon fill=&quot;none&quot; stroke=&quot;black&quot; points=&quot;406,-36 344,-36 344,0 406,0 406,-36&quot; /&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;375&quot; y=&quot;-14.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;Linear&lt;/text&gt;
&lt;/g&gt;
&lt;!-- 解码器&amp;#45;&amp;gt;Linear --&gt;
&lt;g id=&quot;edge3&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;解码器&amp;#45;&amp;gt;Linear&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M308.24,-18C308.24,-18 333.85,-18 333.85,-18&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;333.85,-21.5 343.85,-18 333.85,-14.5 333.85,-21.5&quot; /&gt;
&lt;/g&gt;
&lt;!-- Softmax --&gt;
&lt;g id=&quot;node5&quot; class=&quot;node&quot;&gt;
&lt;title&gt;Softmax&lt;/title&gt;
&lt;polygon fill=&quot;none&quot; stroke=&quot;black&quot; points=&quot;518,-36 442,-36 442,0 518,0 518,-36&quot; /&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;480&quot; y=&quot;-14.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;Softmax&lt;/text&gt;
&lt;/g&gt;
&lt;!-- Linear&amp;#45;&amp;gt;Softmax --&gt;
&lt;g id=&quot;edge4&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;Linear&amp;#45;&amp;gt;Softmax&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M406.22,-18C406.22,-18 431.65,-18 431.65,-18&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;431.65,-21.5 441.65,-18 431.65,-14.5 431.65,-21.5&quot; /&gt;
&lt;/g&gt;
&lt;/g&gt;
&lt;/svg&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;整体这个流程，主要涉及 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;train.py&lt;/code&gt; 文件和 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;modules.py&lt;/code&gt; 文件。所有模型所需的主要函数定义，都是在 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;modules.py&lt;/code&gt; 中实现的。我们先看下编码器（Encoder）的流程：&lt;/p&gt;

&lt;div style=&quot;text-align: center;&quot;&gt;
&lt;div class=&quot;graphviz-wrapper&quot;&gt;

&lt;!-- Generated by graphviz version 2.43.0 (0)
 --&gt;
&lt;!-- Title: G Pages: 1 --&gt;
&lt;svg role=&quot;img&quot; aria-label=&quot;graphviz-4459d4df0778b105cf972d664023b546&quot; width=&quot;169pt&quot; height=&quot;332pt&quot; viewBox=&quot;0.00 0.00 169.00 332.00&quot;&gt;
&lt;title&gt;graphviz-4459d4df0778b105cf972d664023b546&lt;/title&gt;
&lt;desc&gt;
digraph G {
	rankdir=BT
	splines=ortho
	node [shape=&amp;quot;box&amp;quot;]

	embedding -&amp;gt; positional_encoding -&amp;gt; dropout -&amp;gt; multihead_attention -&amp;gt; feedforward
}
&lt;/desc&gt;

&lt;g id=&quot;graph0&quot; class=&quot;graph&quot; transform=&quot;scale(1 1) rotate(0) translate(4 328)&quot;&gt;
&lt;title&gt;G&lt;/title&gt;
&lt;polygon fill=&quot;white&quot; stroke=&quot;transparent&quot; points=&quot;-4,4 -4,-328 165,-328 165,4 -4,4&quot; /&gt;
&lt;!-- embedding --&gt;
&lt;g id=&quot;node1&quot; class=&quot;node&quot;&gt;
&lt;title&gt;embedding&lt;/title&gt;
&lt;polygon fill=&quot;none&quot; stroke=&quot;black&quot; points=&quot;128.5,-36 32.5,-36 32.5,0 128.5,0 128.5,-36&quot; /&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;80.5&quot; y=&quot;-14.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;embedding&lt;/text&gt;
&lt;/g&gt;
&lt;!-- positional_encoding --&gt;
&lt;g id=&quot;node2&quot; class=&quot;node&quot;&gt;
&lt;title&gt;positional_encoding&lt;/title&gt;
&lt;polygon fill=&quot;none&quot; stroke=&quot;black&quot; points=&quot;159.5,-108 1.5,-108 1.5,-72 159.5,-72 159.5,-108&quot; /&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;80.5&quot; y=&quot;-86.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;positional_encoding&lt;/text&gt;
&lt;/g&gt;
&lt;!-- embedding&amp;#45;&amp;gt;positional_encoding --&gt;
&lt;g id=&quot;edge1&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;embedding&amp;#45;&amp;gt;positional_encoding&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M80.5,-36.17C80.5,-36.17 80.5,-61.59 80.5,-61.59&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;77,-61.59 80.5,-71.59 84,-61.59 77,-61.59&quot; /&gt;
&lt;/g&gt;
&lt;!-- dropout --&gt;
&lt;g id=&quot;node3&quot; class=&quot;node&quot;&gt;
&lt;title&gt;dropout&lt;/title&gt;
&lt;polygon fill=&quot;none&quot; stroke=&quot;black&quot; points=&quot;117,-180 44,-180 44,-144 117,-144 117,-180&quot; /&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;80.5&quot; y=&quot;-158.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;dropout&lt;/text&gt;
&lt;/g&gt;
&lt;!-- positional_encoding&amp;#45;&amp;gt;dropout --&gt;
&lt;g id=&quot;edge2&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;positional_encoding&amp;#45;&amp;gt;dropout&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M80.5,-108.17C80.5,-108.17 80.5,-133.59 80.5,-133.59&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;77,-133.59 80.5,-143.59 84,-133.59 77,-133.59&quot; /&gt;
&lt;/g&gt;
&lt;!-- multihead_attention --&gt;
&lt;g id=&quot;node4&quot; class=&quot;node&quot;&gt;
&lt;title&gt;multihead_attention&lt;/title&gt;
&lt;polygon fill=&quot;none&quot; stroke=&quot;black&quot; points=&quot;161,-252 0,-252 0,-216 161,-216 161,-252&quot; /&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;80.5&quot; y=&quot;-230.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;multihead_attention&lt;/text&gt;
&lt;/g&gt;
&lt;!-- dropout&amp;#45;&amp;gt;multihead_attention --&gt;
&lt;g id=&quot;edge3&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;dropout&amp;#45;&amp;gt;multihead_attention&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M80.5,-180.17C80.5,-180.17 80.5,-205.59 80.5,-205.59&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;77,-205.59 80.5,-215.59 84,-205.59 77,-205.59&quot; /&gt;
&lt;/g&gt;
&lt;!-- feedforward --&gt;
&lt;g id=&quot;node5&quot; class=&quot;node&quot;&gt;
&lt;title&gt;feedforward&lt;/title&gt;
&lt;polygon fill=&quot;none&quot; stroke=&quot;black&quot; points=&quot;132.5,-324 28.5,-324 28.5,-288 132.5,-288 132.5,-324&quot; /&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;80.5&quot; y=&quot;-302.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;feedforward&lt;/text&gt;
&lt;/g&gt;
&lt;!-- multihead_attention&amp;#45;&amp;gt;feedforward --&gt;
&lt;g id=&quot;edge4&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;multihead_attention&amp;#45;&amp;gt;feedforward&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M80.5,-252.17C80.5,-252.17 80.5,-277.59 80.5,-277.59&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;77,-277.59 80.5,-287.59 84,-277.59 77,-277.59&quot; /&gt;
&lt;/g&gt;
&lt;/g&gt;
&lt;/svg&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;下面是 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;train.py&lt;/code&gt; 中实现的 Transformer 流程，其中的每一段代码，船长都会做详细解释，先不用急。这个流程里，首先定义了编码器，先使用了 Embedding 层将输入数据转换为词向量，使用 Positional Encoding 层对词向量进行位置编码，使用 Dropout 层进行 dropout 操作，然后进行多层 Multihead Attention 和 Feed Forward 操作。&lt;/p&gt;

&lt;p&gt;在构建模型前，先执行 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;train.py&lt;/code&gt; 的主程序段，首先 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;if __name__ == &apos;__main__&apos;&lt;/code&gt; 这句代码是在 Python 中常用的一种编写方式，它的意思是当一个文件被直接运行时，&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;if&lt;/code&gt; 语句下面的代码会被执行。请看下面代码的注释。&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__name__&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&apos;__main__&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;                
    
    &lt;span class=&quot;c1&quot;&gt;# 加载词汇表   
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;de2idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;idx2de&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;load_de_vocab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;en2idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;idx2en&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;load_en_vocab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    
    &lt;span class=&quot;c1&quot;&gt;# 构建模型并训练
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;g&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Graph&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;train&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Graph loaded&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
    &lt;span class=&quot;c1&quot;&gt;# 创建了一个 Supervisor 对象来管理训练过程
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;sv&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Supervisor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;graph&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;graph&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                             &lt;span class=&quot;n&quot;&gt;logdir&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;logdir&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                             &lt;span class=&quot;n&quot;&gt;save_model_secs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# 使用 with 语句打开一个会话
&lt;/span&gt;    &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;managed_session&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sess&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;

    	&lt;span class=&quot;c1&quot;&gt;# 训练迭代 hp.num_epochs 次
&lt;/span&gt;        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;epoch&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_epochs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; 
            &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;should_stop&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;break&lt;/span&gt;

            &lt;span class=&quot;c1&quot;&gt;# tqdm 是一个 Python 库，用来在循环执行训练操作时在命令行中显示进度条
&lt;/span&gt;            &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;step&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tqdm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;total&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ncols&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;70&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;leave&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;unit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;b&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;

            	&lt;span class=&quot;c1&quot;&gt;# 每次迭代都会运行训练操作 g.train_op
&lt;/span&gt;                &lt;span class=&quot;n&quot;&gt;sess&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_op&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

            &lt;span class=&quot;c1&quot;&gt;# 获取训练的步数，通过 sess.run() 函数获取 global_step 的当前值并赋值给 gs。这样可在后面使用 gs 保存模型时用这个值命名模型
&lt;/span&gt;            &lt;span class=&quot;n&quot;&gt;gs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sess&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;global_step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

            &lt;span class=&quot;c1&quot;&gt;# 每个 epoch 结束时，它使用 saver.save() 函数保存当前模型的状态
&lt;/span&gt;            &lt;span class=&quot;n&quot;&gt;sv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;saver&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;save&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sess&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;logdir&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&apos;/model_epoch_%02d_gs_%d&apos;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;epoch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Done&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;num_epochs&lt;/code&gt; 是训练过程中迭代的次数，它表示训练模型需要在训练数据上跑多少遍。每一次迭代都会在训练数据集上进行训练，通常来说，训练数据集会被重复多次迭代，直到达到 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;num_epochs&lt;/code&gt; 次。这样可以确保模型能够充分地学习数据的特征。设置 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;num_epochs&lt;/code&gt; 的值过大或过小都会导致模型性能下降。&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;1541编码过程&quot;&gt;15.4.1、编码过程&lt;/h5&gt;

&lt;h6 id=&quot;embedding&quot;&gt;Embedding&lt;/h6&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;embedding&lt;/code&gt; 用来把输入生成词嵌入向量：&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# 词语转换为对应的词向量表示
&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;enc&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;embedding&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                      &lt;span class=&quot;n&quot;&gt;vocab_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;de2idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; 
                      &lt;span class=&quot;n&quot;&gt;num_units&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hidden_units&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                      &lt;span class=&quot;n&quot;&gt;scale&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                      &lt;span class=&quot;n&quot;&gt;scope&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;enc_embed&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;vocab_size&lt;/code&gt; 是词汇表的大小。&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;num_units&lt;/code&gt; 是词向量的维度。&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;scale&lt;/code&gt; 是一个布尔值，用来确定是否对词向量进行标准化。&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;scope&lt;/code&gt; 是变量作用域的名称。&lt;/li&gt;
&lt;/ul&gt;

&lt;h6 id=&quot;key-masks&quot;&gt;Key Masks&lt;/h6&gt;

&lt;p&gt;接着生成一个 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;key_masks&lt;/code&gt; 用于在之后的计算中屏蔽掉某些位置的信息，以便模型只关注有效的信息。&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;key_masks&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;expand_dims&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sign&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reduce_sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;abs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;enc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;先对 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;self.enc&lt;/code&gt; 张量进行对每个元素求绝对值的操作&lt;/li&gt;
  &lt;li&gt;沿着最后一阶作为轴，进行 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;reduce_sum&lt;/code&gt; 操作，得到一个 (batch, sequence_length) 形状的张量。&lt;/li&gt;
  &lt;li&gt;再进行 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tf.sign&lt;/code&gt; 操作，对刚得到的每个元素进行符号函数的变换。&lt;/li&gt;
  &lt;li&gt;最后再扩展阶数，变成形状 (batch, sequence_length, 1) 的张量。&lt;/li&gt;
&lt;/ul&gt;

&lt;h6 id=&quot;positional-encoding&quot;&gt;Positional Encoding&lt;/h6&gt;

&lt;p&gt;下面生成 Transformer 的位置编码：&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# 位置编码
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sinusoid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;enc&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;positional_encoding&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                      &lt;span class=&quot;n&quot;&gt;num_units&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hidden_units&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                      &lt;span class=&quot;n&quot;&gt;zero_pad&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                      &lt;span class=&quot;n&quot;&gt;scale&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                      &lt;span class=&quot;n&quot;&gt;scope&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;enc_pe&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;enc&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;embedding&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;expand_dims&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
    							 &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]),&lt;/span&gt;
                      &lt;span class=&quot;n&quot;&gt;vocab_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;maxlen&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                      &lt;span class=&quot;n&quot;&gt;num_units&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hidden_units&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                      &lt;span class=&quot;n&quot;&gt;zero_pad&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                      &lt;span class=&quot;n&quot;&gt;scale&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                      &lt;span class=&quot;n&quot;&gt;scope&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;enc_pe&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;如果超参数 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;hp.sinusoid=True&lt;/code&gt;，使用 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;positional_encoding&lt;/code&gt; 函数，通过使用正弦和余弦函数来生成位置编码，可以为输入序列添加位置信息。如果 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;hp.sinusoid=False&lt;/code&gt;，使用 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;embedding&lt;/code&gt; 函数，通过学习的词嵌入来生成位置编码。&lt;/p&gt;

&lt;p&gt;位置编码生成后，用 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;key_masks&lt;/code&gt; 处理一下。注意 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;key_masks&lt;/code&gt; 的生成一定要用最初的 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;self.enc&lt;/code&gt;，所以在前面执行而不是这里：&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;enc&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;key_masks&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;这个不是矩阵乘法，而是对应元素相乘。这里乘上 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;key_masks&lt;/code&gt; 的目的是将 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;key_masks&lt;/code&gt; 中值为 0 的位置对应的 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;self.enc&lt;/code&gt; 中的元素置为 0，这样就可以排除这些位置对计算的影响。&lt;/p&gt;

&lt;h6 id=&quot;drop-out&quot;&gt;Drop out&lt;/h6&gt;

&lt;p&gt;下面调用了 TensorFlow 的 drop out 操作：&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;enc&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dropout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;enc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                            &lt;span class=&quot;n&quot;&gt;rate&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dropout_rate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                            &lt;span class=&quot;n&quot;&gt;training&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;convert_to_tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;is_training&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;drop out 是一种在深度学习中常用的正则化技巧。它通过在训练过程中随机地「关闭」一些神经元来减少 &lt;strong&gt;过拟合&lt;/strong&gt;。这样做是为了防止模型过于依赖于某些特定的特征，而导致在新数据上的表现不佳。&lt;/p&gt;

&lt;p&gt;在这个函数中，&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dropout&lt;/code&gt; 层通过在训练过程中随机地将一些神经元的输出值设置为 0，来减少模型的过拟合。这个函数中使用了一个参数 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;rate&lt;/code&gt;，表示每个神经元被「关闭」的概率。这样做是为了防止模型过于依赖于某些特定的特征，而导致在新数据上的表现不佳。&lt;/p&gt;

&lt;h6 id=&quot;encoder-blocks-multi-head-attention--feed-forward&quot;&gt;Encoder Blocks: Multi-Head Attention &amp;amp; Feed Forward&lt;/h6&gt;

&lt;p&gt;然后看下 encoder blocks 代码：&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;## Blocks
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_blocks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;variable_scope&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;num_blocks_{}&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)):&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# 多头注意力
&lt;/span&gt;        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;enc&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;multihead_attention&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;queries&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;enc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                                        &lt;span class=&quot;n&quot;&gt;keys&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;enc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                                        &lt;span class=&quot;n&quot;&gt;num_units&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hidden_units&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                                        &lt;span class=&quot;n&quot;&gt;num_heads&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_heads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                                        &lt;span class=&quot;n&quot;&gt;dropout_rate&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dropout_rate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                        &lt;span class=&quot;n&quot;&gt;is_training&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;is_training&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                        &lt;span class=&quot;n&quot;&gt;causality&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        
        &lt;span class=&quot;c1&quot;&gt;# 前馈神经网络
&lt;/span&gt;        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;enc&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;feedforward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;enc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_units&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hidden_units&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hidden_units&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;上述代码是编码器（Encoder）的实现函数调用的流程，也是与船长上面的模型原理介绍一致的，在定义时同样使用了 Embedding 层、Positional Encoding 层、Dropout 层、Multihead Attention 和 Feed Forward 操作。其中 Multihead Attention 在编码、解码中是不一样的，待会儿我们会在 Decoder 部分再提到，有自注意力层和 Encoder-Decoder 层。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;超参数 hp.num_blocks 表示 Encoder Blocks 的层数，每一层都有一个 Multi-Head Attention 和一个 Feed Forward。&lt;/li&gt;
  &lt;li&gt;这个 Encoder 中的 Multi-Head Attention 是基于自注意力的（注意与后面的 Decoder 部分有区别）&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;causality&lt;/code&gt; 参数的意思是否使用 Causal Attention，它是 Self-Attention 的一种，但是只使用过去的信息，防止模型获取未来信息的干扰。一般对于预测序列中的某个时间步来说，只关注之前的信息，而不是整个序列的信息。这段代码中 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;causality&lt;/code&gt; 设置为了 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;False&lt;/code&gt;，即会关注整个序列的信息。&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;1542解码过程&quot;&gt;15.4.2、解码过程&lt;/h5&gt;

&lt;p&gt;再看一下解码的流程：&lt;/p&gt;

&lt;div style=&quot;text-align: center;&quot;&gt;
&lt;div class=&quot;graphviz-wrapper&quot;&gt;

&lt;!-- Generated by graphviz version 2.43.0 (0)
 --&gt;
&lt;!-- Title: G Pages: 1 --&gt;
&lt;svg role=&quot;img&quot; aria-label=&quot;graphviz-0ec29ea83329c971f433bc6641585297&quot; width=&quot;372pt&quot; height=&quot;404pt&quot; viewBox=&quot;0.00 0.00 372.00 404.00&quot;&gt;
&lt;title&gt;graphviz-0ec29ea83329c971f433bc6641585297&lt;/title&gt;
&lt;desc&gt;
digraph G {
	rankdir=BT
	splines=ortho
	node [shape=&amp;quot;box&amp;quot;]
	decoder_attn1 [label=&amp;quot;multihead_attention (self-attention)&amp;quot;]
	decoder_attn2 [label=&amp;quot;multihead_attention (encoder-decoder attention)&amp;quot;]

	embedding -&amp;gt; positional_encoding -&amp;gt; dropout -&amp;gt; decoder_attn1 -&amp;gt; decoder_attn2 -&amp;gt; feedforward
}
&lt;/desc&gt;

&lt;g id=&quot;graph0&quot; class=&quot;graph&quot; transform=&quot;scale(1 1) rotate(0) translate(4 400)&quot;&gt;
&lt;title&gt;G&lt;/title&gt;
&lt;polygon fill=&quot;white&quot; stroke=&quot;transparent&quot; points=&quot;-4,4 -4,-400 368,-400 368,4 -4,4&quot; /&gt;
&lt;!-- decoder_attn1 --&gt;
&lt;g id=&quot;node1&quot; class=&quot;node&quot;&gt;
&lt;title&gt;decoder_attn1&lt;/title&gt;
&lt;polygon fill=&quot;none&quot; stroke=&quot;black&quot; points=&quot;317,-252 47,-252 47,-216 317,-216 317,-252&quot; /&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;182&quot; y=&quot;-230.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;multihead_attention (self&amp;#45;attention)&lt;/text&gt;
&lt;/g&gt;
&lt;!-- decoder_attn2 --&gt;
&lt;g id=&quot;node2&quot; class=&quot;node&quot;&gt;
&lt;title&gt;decoder_attn2&lt;/title&gt;
&lt;polygon fill=&quot;none&quot; stroke=&quot;black&quot; points=&quot;364,-324 0,-324 0,-288 364,-288 364,-324&quot; /&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;182&quot; y=&quot;-302.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;multihead_attention (encoder&amp;#45;decoder attention)&lt;/text&gt;
&lt;/g&gt;
&lt;!-- decoder_attn1&amp;#45;&amp;gt;decoder_attn2 --&gt;
&lt;g id=&quot;edge4&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;decoder_attn1&amp;#45;&amp;gt;decoder_attn2&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M182,-252.17C182,-252.17 182,-277.59 182,-277.59&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;178.5,-277.59 182,-287.59 185.5,-277.59 178.5,-277.59&quot; /&gt;
&lt;/g&gt;
&lt;!-- feedforward --&gt;
&lt;g id=&quot;node6&quot; class=&quot;node&quot;&gt;
&lt;title&gt;feedforward&lt;/title&gt;
&lt;polygon fill=&quot;none&quot; stroke=&quot;black&quot; points=&quot;234,-396 130,-396 130,-360 234,-360 234,-396&quot; /&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;182&quot; y=&quot;-374.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;feedforward&lt;/text&gt;
&lt;/g&gt;
&lt;!-- decoder_attn2&amp;#45;&amp;gt;feedforward --&gt;
&lt;g id=&quot;edge5&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;decoder_attn2&amp;#45;&amp;gt;feedforward&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M182,-324.17C182,-324.17 182,-349.59 182,-349.59&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;178.5,-349.59 182,-359.59 185.5,-349.59 178.5,-349.59&quot; /&gt;
&lt;/g&gt;
&lt;!-- embedding --&gt;
&lt;g id=&quot;node3&quot; class=&quot;node&quot;&gt;
&lt;title&gt;embedding&lt;/title&gt;
&lt;polygon fill=&quot;none&quot; stroke=&quot;black&quot; points=&quot;230,-36 134,-36 134,0 230,0 230,-36&quot; /&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;182&quot; y=&quot;-14.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;embedding&lt;/text&gt;
&lt;/g&gt;
&lt;!-- positional_encoding --&gt;
&lt;g id=&quot;node4&quot; class=&quot;node&quot;&gt;
&lt;title&gt;positional_encoding&lt;/title&gt;
&lt;polygon fill=&quot;none&quot; stroke=&quot;black&quot; points=&quot;261,-108 103,-108 103,-72 261,-72 261,-108&quot; /&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;182&quot; y=&quot;-86.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;positional_encoding&lt;/text&gt;
&lt;/g&gt;
&lt;!-- embedding&amp;#45;&amp;gt;positional_encoding --&gt;
&lt;g id=&quot;edge1&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;embedding&amp;#45;&amp;gt;positional_encoding&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M182,-36.17C182,-36.17 182,-61.59 182,-61.59&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;178.5,-61.59 182,-71.59 185.5,-61.59 178.5,-61.59&quot; /&gt;
&lt;/g&gt;
&lt;!-- dropout --&gt;
&lt;g id=&quot;node5&quot; class=&quot;node&quot;&gt;
&lt;title&gt;dropout&lt;/title&gt;
&lt;polygon fill=&quot;none&quot; stroke=&quot;black&quot; points=&quot;218.5,-180 145.5,-180 145.5,-144 218.5,-144 218.5,-180&quot; /&gt;
&lt;text text-anchor=&quot;middle&quot; x=&quot;182&quot; y=&quot;-158.3&quot; font-family=&quot;Times,serif&quot; font-size=&quot;14.00&quot;&gt;dropout&lt;/text&gt;
&lt;/g&gt;
&lt;!-- positional_encoding&amp;#45;&amp;gt;dropout --&gt;
&lt;g id=&quot;edge2&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;positional_encoding&amp;#45;&amp;gt;dropout&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M182,-108.17C182,-108.17 182,-133.59 182,-133.59&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;178.5,-133.59 182,-143.59 185.5,-133.59 178.5,-133.59&quot; /&gt;
&lt;/g&gt;
&lt;!-- dropout&amp;#45;&amp;gt;decoder_attn1 --&gt;
&lt;g id=&quot;edge3&quot; class=&quot;edge&quot;&gt;
&lt;title&gt;dropout&amp;#45;&amp;gt;decoder_attn1&lt;/title&gt;
&lt;path fill=&quot;none&quot; stroke=&quot;black&quot; d=&quot;M182,-180.17C182,-180.17 182,-205.59 182,-205.59&quot; /&gt;
&lt;polygon fill=&quot;black&quot; stroke=&quot;black&quot; points=&quot;178.5,-205.59 182,-215.59 185.5,-205.59 178.5,-205.59&quot; /&gt;
&lt;/g&gt;
&lt;/g&gt;
&lt;/svg&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;h6 id=&quot;embedding-1&quot;&gt;Embedding&lt;/h6&gt;

&lt;p&gt;下面我们逐一看每段代码，主要关注与编码阶段的区别即可：&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dec&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;embedding&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;decoder_inputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                      &lt;span class=&quot;n&quot;&gt;vocab_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;en2idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; 
                      &lt;span class=&quot;n&quot;&gt;num_units&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hidden_units&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                      &lt;span class=&quot;n&quot;&gt;scale&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                      &lt;span class=&quot;n&quot;&gt;scope&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;dec_embed&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;embedding&lt;/code&gt; 输入用的是 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;self.decoder_inputs&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;词汇表尺寸用翻译后的输出语言英语词汇表长度 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;len(en2idx)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h6 id=&quot;key-masks-1&quot;&gt;Key Masks&lt;/h6&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;key_masks&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;expand_dims&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sign&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reduce_sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;abs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dec&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;key_masks&lt;/code&gt; 输入变量用 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;self.dec&lt;/code&gt;。&lt;/li&gt;
&lt;/ul&gt;

&lt;h6 id=&quot;positional-encoding--drop-out&quot;&gt;Positional Encoding &amp;amp; Drop out&lt;/h6&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# 位置编码
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sinusoid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dec&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;positional_encoding&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;decoder_inputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                      &lt;span class=&quot;n&quot;&gt;vocab_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;maxlen&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                      &lt;span class=&quot;n&quot;&gt;num_units&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hidden_units&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                      &lt;span class=&quot;n&quot;&gt;zero_pad&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                      &lt;span class=&quot;n&quot;&gt;scale&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                      &lt;span class=&quot;n&quot;&gt;scope&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;dec_pe&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dec&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;embedding&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;expand_dims&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;decoder_inputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
    							 &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;decoder_inputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]),&lt;/span&gt;
                      &lt;span class=&quot;n&quot;&gt;vocab_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;maxlen&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                      &lt;span class=&quot;n&quot;&gt;num_units&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hidden_units&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                      &lt;span class=&quot;n&quot;&gt;zero_pad&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                      &lt;span class=&quot;n&quot;&gt;scale&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                      &lt;span class=&quot;n&quot;&gt;scope&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;dec_pe&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dec&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;key_masks&lt;/span&gt;

&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dec&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dropout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dec&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                            &lt;span class=&quot;n&quot;&gt;rate&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dropout_rate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                            &lt;span class=&quot;n&quot;&gt;training&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;convert_to_tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;is_training&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;输入 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;self.decoder_inputs&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;指定 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;vocab_size&lt;/code&gt; 参数 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;hp.maxlen&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h6 id=&quot;decoder-blocks-multi-head-attention--feed-forward&quot;&gt;Decoder Blocks: Multi-Head Attention &amp;amp; Feed Forward&lt;/h6&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;## 解码器模块
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_blocks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;variable_scope&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;num_blocks_{}&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)):&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# 多头注意力（自注意力）
&lt;/span&gt;        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dec&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;multihead_attention&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;queries&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dec&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                                        &lt;span class=&quot;n&quot;&gt;keys&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dec&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                                        &lt;span class=&quot;n&quot;&gt;num_units&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hidden_units&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                                        &lt;span class=&quot;n&quot;&gt;num_heads&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_heads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                                        &lt;span class=&quot;n&quot;&gt;dropout_rate&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dropout_rate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                        &lt;span class=&quot;n&quot;&gt;is_training&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;is_training&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                        &lt;span class=&quot;n&quot;&gt;causality&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                                        &lt;span class=&quot;n&quot;&gt;scope&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;self_attention&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        
        &lt;span class=&quot;c1&quot;&gt;# 多头注意力（Encoder-Decoder 注意力）
&lt;/span&gt;        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dec&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;multihead_attention&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;queries&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dec&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                                        &lt;span class=&quot;n&quot;&gt;keys&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;enc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                                        &lt;span class=&quot;n&quot;&gt;num_units&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hidden_units&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                                        &lt;span class=&quot;n&quot;&gt;num_heads&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_heads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                        &lt;span class=&quot;n&quot;&gt;dropout_rate&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dropout_rate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                        &lt;span class=&quot;n&quot;&gt;is_training&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;is_training&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                                        &lt;span class=&quot;n&quot;&gt;causality&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                        &lt;span class=&quot;n&quot;&gt;scope&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;vanilla_attention&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# 前馈神经网络
&lt;/span&gt;        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dec&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;feedforward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dec&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_units&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hidden_units&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hidden_units&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;在用 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;multihead_attention&lt;/code&gt; 函数解码器模块时，注意传入的参数 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;scope&lt;/code&gt; 区别，先是自注意力层，用参数 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;self_attention&lt;/code&gt;，对应的 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;queries&lt;/code&gt; 是 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;self.dec&lt;/code&gt;，&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;keys&lt;/code&gt; 也是 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;self.dec&lt;/code&gt;。再是「Encoder-Decder Attention」用的是参数 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;vanilla_attention&lt;/code&gt;，对应的 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;queries&lt;/code&gt; 来自解码器是 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;self.dec&lt;/code&gt;，但 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;keys&lt;/code&gt; 来自编码器是是 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;self.enc&lt;/code&gt;。&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;1543embeddingpositional-encodingmulti-head-attentionfeed-forward&quot;&gt;15.4.3、Embedding、Positional Encoding、Multi-Head Attention、Feed Forward&lt;/h5&gt;

&lt;h6 id=&quot;embedding-函数实现&quot;&gt;Embedding 函数实现&lt;/h6&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;embedding&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
              &lt;span class=&quot;n&quot;&gt;vocab_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
              &lt;span class=&quot;n&quot;&gt;num_units&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
              &lt;span class=&quot;n&quot;&gt;zero_pad&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
              &lt;span class=&quot;n&quot;&gt;scale&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
              &lt;span class=&quot;n&quot;&gt;scope&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;embedding&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
              &lt;span class=&quot;n&quot;&gt;reuse&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;variable_scope&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scope&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reuse&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reuse&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;

    	&lt;span class=&quot;c1&quot;&gt;# 创建一个名为 `lookup_table`、形状为 (vocab_size, num_units) 的矩阵
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;lookup_table&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_variable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;lookup_table&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                       &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                       &lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vocab_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_units&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
                                       &lt;span class=&quot;n&quot;&gt;initializer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;contrib&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xavier_initializer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# lookup_table 的第一行插入一个全零行，作为 PAD 的词向量
&lt;/span&gt;        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;zero_pad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;lookup_table&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;concat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_units&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]),&lt;/span&gt;
                                      &lt;span class=&quot;n&quot;&gt;lookup_table&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:]),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# 在词向量矩阵 lookup_table 中查找 inputs
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;embedding_lookup&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lookup_table&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        
        &lt;span class=&quot;c1&quot;&gt;# 对输出的词向量进行除以根号 num_units 的操作，可以控制词向量的统计稳定性。
&lt;/span&gt;        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scale&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_units&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
            
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h6 id=&quot;positional-encoding-函数实现&quot;&gt;Positional Encoding 函数实现&lt;/h6&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;positional_encoding&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                        &lt;span class=&quot;n&quot;&gt;num_units&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                        &lt;span class=&quot;n&quot;&gt;zero_pad&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                        &lt;span class=&quot;n&quot;&gt;scale&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                        &lt;span class=&quot;n&quot;&gt;scope&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;positional_encoding&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                        &lt;span class=&quot;n&quot;&gt;reuse&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;as_list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;variable_scope&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scope&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reuse&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reuse&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;

    	&lt;span class=&quot;c1&quot;&gt;# tf.range(T) 生成一个 0~T-1 的数组
&lt;/span&gt;    	&lt;span class=&quot;c1&quot;&gt;# tf.tile() 将其扩展成 N*T 的矩阵，表示每个词的位置
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;position_ind&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;expand_dims&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# First part of the PE function: sin and cos argument
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;position_enc&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;
            &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pos&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;power&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;2.&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_units&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_units&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pos&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)])&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# 用 numpy 的 sin 和 cos 函数对每个位置进行编码
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;position_enc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;position_enc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# dim 2i
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;position_enc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cos&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;position_enc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# dim 2i+1
&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# 将编码结果转为张量
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;lookup_table&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;convert_to_tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;position_enc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# 将编码的结果与位置索引相关联，得到最终的位置编码
&lt;/span&gt;        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;zero_pad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        	&lt;span class=&quot;c1&quot;&gt;# 如果 zero_pad 参数为 True，则在编码结果的开头添加一个全 0 的向量
&lt;/span&gt;            &lt;span class=&quot;n&quot;&gt;lookup_table&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;concat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_units&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]),&lt;/span&gt;
                                      &lt;span class=&quot;n&quot;&gt;lookup_table&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:]),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;embedding_lookup&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lookup_table&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;position_ind&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# scale 参数为 True，则将编码结果乘上 num_units 的平方根
&lt;/span&gt;        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scale&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_units&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;

        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h6 id=&quot;multi-head-attention-函数实现&quot;&gt;Multi-Head Attention 函数实现&lt;/h6&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;multihead_attention&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;queries&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                        &lt;span class=&quot;n&quot;&gt;keys&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                        &lt;span class=&quot;n&quot;&gt;num_units&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                        &lt;span class=&quot;n&quot;&gt;num_heads&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                        &lt;span class=&quot;n&quot;&gt;dropout_rate&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                        &lt;span class=&quot;n&quot;&gt;is_training&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                        &lt;span class=&quot;n&quot;&gt;causality&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                        &lt;span class=&quot;n&quot;&gt;scope&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;multihead_attention&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                        &lt;span class=&quot;n&quot;&gt;reuse&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;variable_scope&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scope&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reuse&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reuse&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# Set the fall back option for num_units
&lt;/span&gt;        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_units&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;num_units&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;queries&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;as_list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        
        &lt;span class=&quot;c1&quot;&gt;# Linear Projections
&lt;/span&gt;        &lt;span class=&quot;c1&quot;&gt;# 使用三个全连接层对输入的 queries、keys 分别进行线性变换，将其转换为三个维度相同的张量 Q/K/V
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;Q&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;queries&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_units&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;relu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# (N, T_q, C)
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;K&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keys&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_units&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;relu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# (N, T_k, C)
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;V&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keys&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_units&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;relu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# (N, T_k, C)
&lt;/span&gt;        
        &lt;span class=&quot;c1&quot;&gt;# Split and concat
&lt;/span&gt;        &lt;span class=&quot;c1&quot;&gt;# 按头数 split Q/K/V，再各自连接起来
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;Q_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;concat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_heads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# (h*N, T_q, C/h) 
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;K_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;concat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;K&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_heads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# (h*N, T_k, C/h) 
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;V_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;concat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;V&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_heads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# (h*N, T_k, C/h) 
&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# Multiplication
&lt;/span&gt;        &lt;span class=&quot;c1&quot;&gt;# 计算 Q_, K_, V_ 的点积来获得注意力权重
&lt;/span&gt;        &lt;span class=&quot;c1&quot;&gt;# 其中 Q_ 的维度为 (hN, T_q, C/h)
&lt;/span&gt;        &lt;span class=&quot;c1&quot;&gt;# K_ 的维度为 (hN, T_k, C/h)
&lt;/span&gt;        &lt;span class=&quot;c1&quot;&gt;# 计算出来的结果 outputs 的维度为 (h*N, T_q, T_k)
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matmul&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Q_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transpose&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;K_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# (h*N, T_q, T_k)
&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# Scale
&lt;/span&gt;        &lt;span class=&quot;c1&quot;&gt;# 对权重进行 scale，这里除以了 K_ 的第三维的平方根，用于缩放权重
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;K_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;as_list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        
        &lt;span class=&quot;c1&quot;&gt;# Key Masking
&lt;/span&gt;        &lt;span class=&quot;c1&quot;&gt;# 这里需要将 keys 的有效部分标记出来，将无效部分设置为极小值，以便在之后的 softmax 中被忽略
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;key_masks&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sign&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reduce_sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;abs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keys&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# (N, T_k)
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;key_masks&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;key_masks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_heads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# (h*N, T_k)
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;key_masks&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;expand_dims&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;key_masks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;queries&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# (h*N, T_q, T_k)
&lt;/span&gt;        
        &lt;span class=&quot;n&quot;&gt;paddings&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ones_like&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;32&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;where&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;equal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;key_masks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;paddings&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# (h*N, T_q, T_k)
&lt;/span&gt;  
        &lt;span class=&quot;c1&quot;&gt;# Causality = Future blinding
&lt;/span&gt;        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;causality&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;

        	&lt;span class=&quot;c1&quot;&gt;# 创建一个与 outputs[0, :, :] 相同形状的全 1 矩阵
&lt;/span&gt;            &lt;span class=&quot;n&quot;&gt;diag_vals&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ones_like&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:])&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# (T_q, T_k)
&lt;/span&gt;
            &lt;span class=&quot;c1&quot;&gt;# 对 diag_vals 进行处理，返回一个下三角线矩阵
&lt;/span&gt;            &lt;span class=&quot;n&quot;&gt;tril&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linalg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;LinearOperatorLowerTriangular&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;diag_vals&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to_dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# (T_q, T_k)
&lt;/span&gt;            &lt;span class=&quot;n&quot;&gt;masks&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;expand_dims&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tril&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# (h*N, T_q, T_k)
&lt;/span&gt;   
   			&lt;span class=&quot;c1&quot;&gt;# 将 masks 为 0 的位置的 outputs 值设置为一个非常小的数
&lt;/span&gt;   			&lt;span class=&quot;c1&quot;&gt;# 这样会导致这些位置在之后的计算中对结果产生非常小的影响，从而实现了遮盖未来信息的功能
&lt;/span&gt;            &lt;span class=&quot;n&quot;&gt;paddings&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ones_like&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;masks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;32&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;where&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;equal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;masks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;paddings&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# (h*N, T_q, T_k)
&lt;/span&gt;  
        &lt;span class=&quot;c1&quot;&gt;# 对于每个头的输出，应用 softmax 激活函数，这样可以得到一个概率分布
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;softmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# (h*N, T_q, T_k)
&lt;/span&gt;         
        &lt;span class=&quot;c1&quot;&gt;# Query Masking
&lt;/span&gt;        &lt;span class=&quot;c1&quot;&gt;# 对于查询（queries）进行 masking，这样可以避免输入序列后面的词对之前词的影响
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;query_masks&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sign&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reduce_sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;abs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;queries&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# (N, T_q)
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;query_masks&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;query_masks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_heads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# (h*N, T_q)
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;query_masks&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;expand_dims&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;query_masks&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keys&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# (h*N, T_q, T_k)
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;query_masks&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# broadcasting. (N, T_q, C)
&lt;/span&gt;          
        &lt;span class=&quot;c1&quot;&gt;# Dropouts &amp;amp; Weighted Sum
&lt;/span&gt;        &lt;span class=&quot;c1&quot;&gt;# 对于每个头的输出，应用 dropout 以及进行残差连接
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dropout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rate&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dropout_rate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;training&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;convert_to_tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;is_training&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matmul&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;V_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# ( h*N, T_q, C/h)
&lt;/span&gt;        
        &lt;span class=&quot;c1&quot;&gt;# Restore shape
&lt;/span&gt;        &lt;span class=&quot;c1&quot;&gt;# 将每个头的输出拼接起来，使用 tf.concat 函数，将不同头的结果按照第二维拼接起来
&lt;/span&gt;        &lt;span class=&quot;c1&quot;&gt;# 得到最终的输出结果，即经过多头注意力计算后的结果
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;concat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_heads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# (N, T_q, C)
&lt;/span&gt;              
        &lt;span class=&quot;c1&quot;&gt;# Residual connection
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;queries&lt;/span&gt;
              
        &lt;span class=&quot;c1&quot;&gt;# Normalize
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;normalize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# (N, T_q, C)
&lt;/span&gt; 
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h6 id=&quot;feed-forward-函数实现&quot;&gt;Feed Forward 函数实现&lt;/h6&gt;

&lt;p&gt;下面是 &lt;strong&gt;前馈神经网络层&lt;/strong&gt; 的定义，这是一个非线性变换，这里用到了一些卷积神经网络（CNN）的知识，我们来看下代码再解释：&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;feedforward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                &lt;span class=&quot;n&quot;&gt;num_units&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2048&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;512&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;scope&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;multihead_attention&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                &lt;span class=&quot;n&quot;&gt;reuse&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;variable_scope&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scope&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reuse&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reuse&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# Inner layer
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;inputs&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;filters&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_units&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;kernel_size&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                  &lt;span class=&quot;s&quot;&gt;&quot;activation&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;relu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;use_bias&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv1d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        
        &lt;span class=&quot;c1&quot;&gt;# Readout layer
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;inputs&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;filters&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_units&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;kernel_size&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                  &lt;span class=&quot;s&quot;&gt;&quot;activation&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;use_bias&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv1d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        
        &lt;span class=&quot;c1&quot;&gt;# 连接一个残差网络 ResNet
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inputs&lt;/span&gt;
        
        &lt;span class=&quot;c1&quot;&gt;# 归一化后输出
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;normalize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;先是使用了一个卷积层（conv1d）作为 inner layer、一个卷积层作为 readout layer，卷积核大小都为 1。&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;filters&lt;/code&gt; 参数用来控制卷积层中输出通道数量，inner layer 的输出通道数设置为 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;num_units[0]&lt;/code&gt; ，readout layer 的设置为 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;num_units[1]&lt;/code&gt;。有时也会把这个解释为神经元数量。这两个的默认分别为 2048、512，调用时传入的是超参数的 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[4 * hidden_units, hidden_units]&lt;/code&gt;。&lt;/li&gt;
  &lt;li&gt;其中 inner layer 用 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ReLU&lt;/code&gt; 作为激活函数，然后连接一个残差网络 RedNet，把 readout layer 的输出加上原始的输入。&lt;/li&gt;
  &lt;li&gt;最后使用 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;normalize&lt;/code&gt; 归一化处理输出，再返回。下面来看下 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;normalize&lt;/code&gt; 函数。&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;normalize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
              &lt;span class=&quot;n&quot;&gt;epsilon&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1e-8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
              &lt;span class=&quot;n&quot;&gt;scope&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;ln&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
              &lt;span class=&quot;n&quot;&gt;reuse&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;variable_scope&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scope&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reuse&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reuse&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;

    	&lt;span class=&quot;c1&quot;&gt;# 输入数据的形状
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;inputs_shape&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;params_shape&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inputs_shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt;
    
    	&lt;span class=&quot;c1&quot;&gt;# 平均数、方差
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;variance&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;moments&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;keep_dims&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# 拉伸因子 beta
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;beta&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Variable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;params_shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# 缩放因子 gamma
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;gamma&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Variable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ones&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;params_shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# 归一化：加上一个非常小的 epsilon，是为了防止除以 0
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;normalized&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inputs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;variance&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;epsilon&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gamma&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;normalized&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;beta&lt;/span&gt;
        
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;该函数实现了 Layer Normalization，用于在深度神经网络中解决数据的不稳定性问题。&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;1544编码和解码完成后的操作&quot;&gt;15.4.4、编码和解码完成后的操作&lt;/h5&gt;

&lt;p&gt;解码器后的 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Linear &amp;amp; Softmax&lt;/code&gt;：&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# 全连接层得到的未经过归一化的概率值
&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;logits&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dec&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;en2idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 预测的英文单词 idx
&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;preds&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to_int32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arg_max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;logits&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dimension&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;istarget&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to_float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;not_equal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 正确预测数量，除以所有样本数，得到准确率
&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;acc&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reduce_sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to_float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;equal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;preds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;istarget&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reduce_sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;istarget&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;#  记录了模型的准确率的值，用于 tensorboard 可视化
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;summary&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scalar&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;acc&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;acc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;训练集数据处理时，经过 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Linear &amp;amp; Softmax&lt;/code&gt; 之后的最后处理如下。这里用到了 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tf.nn.softmax_cross_entropy_with_logits&lt;/code&gt; 交叉熵损失，来计算模型的错误率 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mean_loss&lt;/code&gt;，并使用 Adam 优化器 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;AdamOptimizer&lt;/code&gt; 来优化模型参数。&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# 使用 label_smoothing 函数对真实标签进行标签平滑，得到 self.y_smoothed
&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_smoothed&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label_smoothing&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;one_hot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;depth&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;en2idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;下面这段代码实现了一种叫做「label Smoothing」的技巧。&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;label_smoothing&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;epsilon&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;

	&lt;span class=&quot;c1&quot;&gt;# 获取输入的类别数，并将其赋值给变量 K
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;K&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;as_list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# number of channels
&lt;/span&gt;    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;epsilon&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;epsilon&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;K&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;在训练过程中，样本的标签被表示为一个二维矩阵，其中第一维表示样本的编号，第二维表示样本的标签。这个矩阵的形状就是 (样本数, 类别数)，所以类别数对应的就是最后一维。具体到这个模型用例里，第一个维度是德语样本句子数，最后一维就是英语词汇量的大小。&lt;/p&gt;

&lt;p&gt;用于解决在训练模型时出现的过拟合问题。在标签平滑中，我们给每个样本的标签加上一些噪声，使得模型不能完全依赖于样本的标签来进行训练，从而减少过拟合的可能性。具体来说，这段代码将输入的标签 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;inputs&lt;/code&gt; 乘上 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;1-epsilon&lt;/code&gt;，再加上 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;epsilon / K&lt;/code&gt;，其中 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;epsilon&lt;/code&gt; 是平滑因子，&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;K&lt;/code&gt; 是标签类别数（英语词汇量大小）。这样就可以在训练过程中让模型对标签的预测更加平稳，并且降低过拟合的风险。&lt;/p&gt;

&lt;p&gt;然后我们看后续的操作。&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# 对于分类问题来说，常用的损失函数是交叉熵损失
&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;softmax_cross_entropy_with_logits&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;logits&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;logits&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_smoothed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean_loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reduce_sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;istarget&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reduce_sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;istarget&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Training Scheme
&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;global_step&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Variable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;global_step&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;trainable&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Adam 优化器 self.optimizer，用于优化损失函数
&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;AdamOptimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;beta1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;beta2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.98&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;epsilon&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1e-8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 使用优化器的 minimize() 函数创建一个训练操作 self.train_op，用于更新模型参数。这个函数会自动计算梯度并应用更新
&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_op&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;minimize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean_loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;global_step&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;global_step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
   
&lt;span class=&quot;c1&quot;&gt;# 将平均损失写入 TensorFlow 的 Summary 中，用于 tensorboard 可视化
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;summary&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scalar&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;mean_loss&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean_loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 将所有的 summary 合并到一起，方便在训练过程中写入事件文件
&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;merged&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;summary&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;merge_all&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;155效果评价&quot;&gt;15.5、效果评价&lt;/h4&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;eval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt; 
    &lt;span class=&quot;c1&quot;&gt;# 创建一个处理测试数据集的 Graph 实例
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;g&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Graph&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;is_training&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Graph loaded&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
    &lt;span class=&quot;c1&quot;&gt;# 加载测试数据
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Sources&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Targets&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;load_test_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;de2idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;idx2de&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;load_de_vocab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;en2idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;idx2en&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;load_en_vocab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
     
    &lt;span class=&quot;c1&quot;&gt;# Start session         
&lt;/span&gt;    &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;graph&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;as_default&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;

    	&lt;span class=&quot;c1&quot;&gt;# TensorFlow 中用于管理训练的一个类
&lt;/span&gt;    	&lt;span class=&quot;c1&quot;&gt;# 它可以帮助你轻松地管理训练过程中的各种资源，如模型参数、检查点和日志
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;sv&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Supervisor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# 创建一个会话
&lt;/span&gt;        &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;managed_session&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ConfigProto&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;allow_soft_placement&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sess&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;

            &lt;span class=&quot;c1&quot;&gt;# 恢复模型参数
&lt;/span&gt;            &lt;span class=&quot;n&quot;&gt;sv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;saver&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;restore&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sess&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;latest_checkpoint&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;logdir&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Restored!&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
              
            &lt;span class=&quot;c1&quot;&gt;# 获取模型名称
&lt;/span&gt;            &lt;span class=&quot;n&quot;&gt;mname&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;logdir&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&apos;/checkpoint&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&apos;r&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;&quot;&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# model name
&lt;/span&gt;             
            &lt;span class=&quot;c1&quot;&gt;## Inference
&lt;/span&gt;            &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exists&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;results&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mkdir&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;results&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

            &lt;span class=&quot;c1&quot;&gt;# 初始化结果文件
&lt;/span&gt;            &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;codecs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;results/&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mname&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;w&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;utf-8&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;list_of_refs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hypotheses&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;

                &lt;span class=&quot;c1&quot;&gt;# 循环处理数据
&lt;/span&gt;                &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;//&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
                     
                    &lt;span class=&quot;c1&quot;&gt;# 获取小批量数据
&lt;/span&gt;                    &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;sources&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Sources&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;targets&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Targets&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
                     
                    &lt;span class=&quot;c1&quot;&gt;# 使用自回归推理（Autoregressive inference）得到预测结果
&lt;/span&gt;                    &lt;span class=&quot;n&quot;&gt;preds&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;maxlen&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;int32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
                    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;maxlen&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
                        &lt;span class=&quot;n&quot;&gt;_preds&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sess&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;preds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;preds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;
                        &lt;span class=&quot;n&quot;&gt;preds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_preds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
                     
                    &lt;span class=&quot;c1&quot;&gt;# 将预测结果写入文件
&lt;/span&gt;                    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;source&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pred&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sources&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;targets&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;preds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# sentence-wise
&lt;/span&gt;                        &lt;span class=&quot;n&quot;&gt;got&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot; &quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;idx2en&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&amp;lt;/S&amp;gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;strip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
                        &lt;span class=&quot;n&quot;&gt;fout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;write&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;- source: &quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;source&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
                        &lt;span class=&quot;n&quot;&gt;fout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;write&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;- expected: &quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
                        &lt;span class=&quot;n&quot;&gt;fout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;write&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;- got: &quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;got&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
                        &lt;span class=&quot;n&quot;&gt;fout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;flush&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
                          
                        &lt;span class=&quot;c1&quot;&gt;# bleu score
&lt;/span&gt;                        &lt;span class=&quot;n&quot;&gt;ref&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
                        &lt;span class=&quot;n&quot;&gt;hypothesis&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;got&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
                        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ref&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hypothesis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
                            &lt;span class=&quot;n&quot;&gt;list_of_refs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ref&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
                            &lt;span class=&quot;n&quot;&gt;hypotheses&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hypothesis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
              
                &lt;span class=&quot;c1&quot;&gt;# 计算 BLEU 分数，并将其写入文件
&lt;/span&gt;                &lt;span class=&quot;n&quot;&gt;score&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;corpus_bleu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;list_of_refs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hypotheses&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;fout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;write&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Bleu Score = &quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
                                          
&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__name__&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&apos;__main__&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;eval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Done&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;第-16-节--kyubyong-transformer-的性能表现和一些问题&quot;&gt;第 16 节 · Kyubyong Transformer 的性能表现和一些问题&lt;/h3&gt;

&lt;p&gt;评估结果文件的最后一行有 Bleu Score = 6.598452846670836 表示这个翻译模型的翻译结果与参考翻译重叠程度比较高，翻译质量较好。不过需要注意的是，BLEU 分数不能完全反映翻译质量，因为它不能评估语法，语义，语调等方面的问题。&lt;/p&gt;

&lt;p&gt;另外前面我们在代码中已经将过程数据保存在 logdir 下了，就是为了后续方便可视化，我们可以用 TensorBoard 来可视化，具体使用方法如下：&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;mikecaptain@local &lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;tensorboard &lt;span class=&quot;nt&quot;&gt;--logdir&lt;/span&gt; logdir
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;然后在浏览器里查看 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;http://localhost:6006&lt;/code&gt;，示例如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023-01-04-language-model-5-17.gif&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;我们可以看到这个 Transformer 能够较好地捕捉长距离依赖关系，提高翻译质量。然而，Kyubyong Transformer 的实现存在一些问题。该 Transformer 模型在训练过程中还需要调整许多超参数，如学习率（learning rate）、batch size 等，不同的任务可能需要不同的超参数调整。&lt;/p&gt;

&lt;h2 id=&quot;结尾--transformer-问世后的这些年&quot;&gt;结尾 · Transformer 问世后的这些年&lt;/h2&gt;

&lt;p&gt;Transformer 的优势显而易见：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;更快 —— 并行性好：在 Transformer 诞生之前，RNN 是 NLP 领域的主流模型，但是 RNN 并行性差（序列串行处理）。&lt;/li&gt;
  &lt;li&gt;不健忘 —— 词距离缩短为 1：RNN 模型处理长文本内容已丢失（在 RNN 模型中意味着词的空间距离长）。&lt;/li&gt;
  &lt;li&gt;处理不同长度序列：不需要输入数据的序列是固定长度的。&lt;/li&gt;
  &lt;li&gt;易于转移学习。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;因此基于 Transformer 原理的模型，在众多 NLP 任务中都取得了卓越的表现。&lt;/p&gt;

&lt;p&gt;说到底机器学习（Machine Learning）领域还是一个实验科学，并且是离工业界极近的实验科学。机器学习看待实验结果的角度，不是为了拿实验结果总结抽象后推动理论科学发展。机器学习的实验结果是要被评价的，其效果有客观量化评估标准。所以机器学习，一切以结果说话。基于 Transformer 架构 Decoder 部分诞生了 OpenAI 的 GPT 大模型，基于其架构的 Encoder 部分诞生了 Google 的 BERT 大模型，他们两个都诞生于 2018 年。这几年基于 Transformer 的各种优化思想不断出现，其集大成者便是 2022 年年底基于 GPT-3.5 或者说基于 InstructGPT 的 ChatGPT。&lt;/p&gt;

&lt;p&gt;感谢你有耐心看完本篇近 10 万字长文，因为是船涨的技术笔记，所以对于关键点梳理得细致了些。后续，我讲和大家一起聊聊 AIGC 的当下，如果说本篇内容更像一个教程（对缘起技术的深入），那么后续我们的探讨则可能更像一篇报告了（对眼前学界与业界发展现状的综述），我们将更关注文章「前言」部分的两个议题：1）如果认为通过图灵测试代表着 AGI（Artificial General Intelligence，通用人工智能）的话，当下 NLP，乃至 AGI 发展到什么程度了？2）未来一些年内，AGI 的发展路线可能会是怎样的？&lt;/p&gt;

&lt;p&gt;AI 终将颠覆各行各业，阿里人有责任花些时间关注前沿的发展脉搏，欢迎大家在钉钉或微信（id：sinosuperman）上与我交流。&lt;/p&gt;

&lt;p&gt;最后，船涨祝大家兔年里，健康又快乐。&lt;/p&gt;

&lt;p&gt;PS：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;UPDATED：2023 年 1 月 27 日，本文登上 ATA 头条。（注：ATA 全称 Alibaba Technology Associate，是阿里集团最大的技术社区）&lt;/li&gt;
    &lt;li&gt;UPDATED：2023 年 2 月 2 日，本文在 ATA 获得鲁肃点赞。（注：鲁肃，本名程立，是阿里合伙人、阿里集团上一任 CTO）&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023/2023-01-29-ata-headline-top-1.jpg&quot; alt=&quot;image&quot; /&gt;
&lt;img src=&quot;/img/src/2023/2023-01-29-ata-headline-top-2.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;参考&quot;&gt;参考&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;https://web.stanford.edu/~jurafsky/slp3/3.pdf&lt;/li&gt;
  &lt;li&gt;https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html&lt;/li&gt;
  &lt;li&gt;《自然语言处理：基于预训练模型的方法》车万翔 等著&lt;/li&gt;
  &lt;li&gt;https://cs.stanford.edu/people/karpathy/convnetjs/&lt;/li&gt;
  &lt;li&gt;https://arxiv.org/abs/1706.03762&lt;/li&gt;
  &lt;li&gt;https://arxiv.org/abs/1512.03385&lt;/li&gt;
  &lt;li&gt;https://github.com/Kyubyong/transformer/&lt;/li&gt;
  &lt;li&gt;http://jalammar.github.io/illustrated-transformer/&lt;/li&gt;
  &lt;li&gt;https://towardsdatascience.com/this-is-how-to-train-better-transformer-models-d54191299978&lt;/li&gt;
  &lt;li&gt;《自然语言处理实战：预训练模型应用及其产品化》安库·A·帕特尔 等著&lt;/li&gt;
  &lt;li&gt;https://lilianweng.github.io/posts/2018-06-24-attention/&lt;/li&gt;
  &lt;li&gt;https://github.com/lilianweng/transformer-tensorflow/&lt;/li&gt;
  &lt;li&gt;《基于深度学习的道路短期交通状态时空序列预测》崔建勋 著&lt;/li&gt;
  &lt;li&gt;https://www.zhihu.com/question/325839123&lt;/li&gt;
  &lt;li&gt;https://luweikxy.gitbook.io/machine-learning-notes/self-attention-and-transformer&lt;/li&gt;
  &lt;li&gt;《Python 深度学习（第 2 版）》弗朗索瓦·肖莱 著&lt;/li&gt;
  &lt;li&gt;https://en.wikipedia.org/wiki/Attention_(machine_learning)&lt;/li&gt;
  &lt;li&gt;https://zhuanlan.zhihu.com/p/410776234&lt;/li&gt;
  &lt;li&gt;https://www.tensorflow.org/tensorboard/get_started&lt;/li&gt;
  &lt;li&gt;https://paperswithcode.com/method/multi-head-attention&lt;/li&gt;
  &lt;li&gt;https://zhuanlan.zhihu.com/p/48508221&lt;/li&gt;
  &lt;li&gt;https://www.joshbelanich.com/self-attention-layer/&lt;/li&gt;
  &lt;li&gt;https://learning.rasa.com/transformers/kvq/&lt;/li&gt;
  &lt;li&gt;http://deeplearning.stanford.edu/tutorial/supervised/ConvolutionalNeuralNetwork/&lt;/li&gt;
  &lt;li&gt;https://zhuanlan.zhihu.com/p/352898810&lt;/li&gt;
  &lt;li&gt;https://towardsdatascience.com/beautifully-illustrated-nlp-models-from-rnn-to-transformer-80d69faf2109&lt;/li&gt;
  &lt;li&gt;https://medium.com/analytics-vidhya/understanding-q-k-v-in-transformer-self-attention-9a5eddaa5960&lt;/li&gt;
&lt;/ul&gt;</content><author><name>麦克船长</name></author><category term="ai" /><category term="AI" /><category term="人工智能" /><category term="NLP" /><category term="自然语言处理" /><category term="神经网络" /><category term="Attention" /><category term="注意力" /><category term="AIGC" /><category term="Transformer" /><category term="自注意力" /><category term="Self-Attention" /><category term="多头注意力" /><category term="Multiple Head Attention" /><category term="残差网络" /><category term="Short-Cut" /><category term="位置编码" /><category term="Bahdanau" /><category term="Encoder-Decoder" /><summary type="html">基于 RNN 的 Encoder-Decoder 模型存在无法处理过长文本、并行性差的两大痛点。2015 年 Bahdanau 等人在其论文中提出 Attention 机制，再到 2017 年 Transformer 模型的论文《Attention is All You Need》横空出世，其并行速度极快，而且每两个词之间的词间距都是 1。此后 NLP 领域 Transformer 彻底成为主流。如果你已经了解 Encoder-Decoder 模型，本文将基于此带你深入浅出的搞清楚 Attention、Transformer。</summary></entry><entry><title type="html">【编译】三万字长文！LSTM 之父 Jürgen 带我们回顾深度学习发展史</title><link href="https://www.mikecaptain.com/2023/01/14/juergen-deep-learning-history/" rel="alternate" type="text/html" title="【编译】三万字长文！LSTM 之父 Jürgen 带我们回顾深度学习发展史" /><published>2023-01-14T20:21:55+00:00</published><updated>2023-01-14T20:21:55+00:00</updated><id>https://www.mikecaptain.com/2023/01/14/juergen-deep-learning-history</id><content type="html" xml:base="https://www.mikecaptain.com/2023/01/14/juergen-deep-learning-history/">&lt;p&gt;&lt;img src=&quot;/img/src/2023-01-17-juergen-deep-learning-history-1.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;本文译自 LSTM 作者 &lt;a href=&quot;https://people.idsia.ch/~juergen/deep-learning-history.html#gan&quot;&gt;Jürgen Schmidhuber, KAUST AII, Swiss AI Lab IDSIA, USI&lt;/a&gt;，中文译文由 AI 及麦克船长完成翻译。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;本文目录&lt;/strong&gt;&lt;/p&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#现代人工智能和深度学习的注释历史&quot; id=&quot;markdown-toc-现代人工智能和深度学习的注释历史&quot;&gt;现代人工智能和深度学习的注释历史&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#介绍&quot; id=&quot;markdown-toc-介绍&quot;&gt;介绍&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#1676向后信用分配的链式规则&quot; id=&quot;markdown-toc-1676向后信用分配的链式规则&quot;&gt;1676：向后信用分配的链式规则&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#1800第一个神经网络线性回归浅层学习&quot; id=&quot;markdown-toc-1800第一个神经网络线性回归浅层学习&quot;&gt;~1800：第一个神经网络/线性回归/浅层学习&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#1920-1925第一个循环网络架构&quot; id=&quot;markdown-toc-1920-1925第一个循环网络架构&quot;&gt;1920-1925：第一个循环网络架构&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#1972首次发布学习人工-rnn&quot; id=&quot;markdown-toc-1972首次发布学习人工-rnn&quot;&gt;~1972：首次发布学习人工 RNN&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#1958-年多层前馈神经网络没有深度学习&quot; id=&quot;markdown-toc-1958-年多层前馈神经网络没有深度学习&quot;&gt;1958 年：多层前馈神经网络（没有深度学习）&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#1965-年第一次深度学习&quot; id=&quot;markdown-toc-1965-年第一次深度学习&quot;&gt;1965 年：第一次深度学习&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#1967-68通过随机梯度下降进行深度学习&quot; id=&quot;markdown-toc-1967-68通过随机梯度下降进行深度学习&quot;&gt;1967-68：通过随机梯度下降进行深度学习&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#1970-年反向传播-1982-年对于神经网络-1960-年先驱&quot; id=&quot;markdown-toc-1970-年反向传播-1982-年对于神经网络-1960-年先驱&quot;&gt;1970 年：反向传播。 1982 年：对于神经网络。 1960 年：先驱。&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#1979-年第一个深度卷积神经网络1969-年relu&quot; id=&quot;markdown-toc-1979-年第一个深度卷积神经网络1969-年relu&quot;&gt;1979 年：第一个深度卷积神经网络（1969 年：ReLU）&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#1980-年代至-90-年代图形神经网络随机增量规则dropout&quot; id=&quot;markdown-toc-1980-年代至-90-年代图形神经网络随机增量规则dropout&quot;&gt;1980 年代至 90 年代：图形神经网络/随机增量规则（Dropout）/…&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#1990-年-2-月生成对抗网络好奇心&quot; id=&quot;markdown-toc-1990-年-2-月生成对抗网络好奇心&quot;&gt;1990 年 2 月：生成对抗网络/好奇心&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#1990-年-2-月生成对抗网络好奇心-1&quot; id=&quot;markdown-toc-1990-年-2-月生成对抗网络好奇心-1&quot;&gt;1990 年 2 月：生成对抗网络/好奇心&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#1991-年-3-月具有线性化自注意力的变形金刚&quot; id=&quot;markdown-toc-1991-年-3-月具有线性化自注意力的变形金刚&quot;&gt;1991 年 3 月：具有线性化自注意力的变形金刚&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#1991-年-4-月通过自监督预训练进行深度学习&quot; id=&quot;markdown-toc-1991-年-4-月通过自监督预训练进行深度学习&quot;&gt;1991 年 4 月：通过自监督预训练进行深度学习&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#1991-年-6-月基本问题梯度消失&quot; id=&quot;markdown-toc-1991-年-6-月基本问题梯度消失&quot;&gt;1991 年 6 月：基本问题：梯度消失&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#1991-年-6-月lstm--highway-nets--resnets-的根源&quot; id=&quot;markdown-toc-1991-年-6-月lstm--highway-nets--resnets-的根源&quot;&gt;1991 年 6 月：LSTM / Highway Nets / ResNets 的根源&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#1995神经概率语言模型&quot; id=&quot;markdown-toc-1995神经概率语言模型&quot;&gt;1995：神经概率语言模型&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#lstm--highway-net-原理是现代深度学习的核心&quot; id=&quot;markdown-toc-lstm--highway-net-原理是现代深度学习的核心&quot;&gt;LSTM / Highway Net 原理是现代深度学习的核心&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#是硬件笨蛋&quot; id=&quot;markdown-toc-是硬件笨蛋&quot;&gt;是硬件，笨蛋！&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#不要忽视-1931-年以来的人工智能理论&quot; id=&quot;markdown-toc-不要忽视-1931-年以来的人工智能理论&quot;&gt;不要忽视 1931 年以来的人工智能理论&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#从大爆炸到遥远的未来的更广泛的历史背景&quot; id=&quot;markdown-toc-从大爆炸到遥远的未来的更广泛的历史背景&quot;&gt;从大爆炸到遥远的未来的更广泛的历史背景&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;现代人工智能和深度学习的注释历史&quot;&gt;现代人工智能和深度学习的注释历史&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;摘要&lt;/strong&gt;。 机器学习（ML）是信用分配的科学：在观察中发现预测行动后果的模式，并帮助提高未来的表现。信用分配也是人类理解世界如何运作的必要条件，不仅对于每天生活中的个人，而且对于像历史学家这样的学术专业人士来说也是如此。在这里，我主要关注现代人工智能（AI）的历史，它由人工神经网络（NNs）和深度学习（DL）主导，在概念上更接近早期的控制论，而不是自1956年以来被称为 AI（例如专家系统和逻辑编程）的领域。现代AI的历史重点将强调传统AI教科书以外的突破，特别是当今NNs的数学基础，如链式规则（1676 年），第一个NNs（线性回归，约1800年）和第一个工作的深度学习器（1965-）。从2022年的角度来看，我提供了一个时间表，阐述了NNs，深度学习，AI，计算机科学和数学领域中事后看来最重要的相关事件，并对那些奠定了这一领域基础的人进行了赞扬。文章中包含了许多与我的AI博客相关的概述网站的超链接。它还揭示了深度学习的一些流行但是误导性的历史条目，并补充了我之前的深度学习调查[DL1]，其中提供了数百条额外的参考资料。最后，为了结束这篇文章，我将把事情放在更广泛的历史背景中，跨越从大爆炸开始到宇宙将比现在老很多倍的时间。本文也是我即将出版的AI书籍的一章的草稿。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;免责声明&lt;/strong&gt;。 有人说深度学习的历史不应该由帮助塑造它的人来写——“你是历史的一部分，而不是历史学家。”[CONN21] 我不同意这种观点。 由于我似乎比其他人更了解深度学习的历史，[S20][DL3,DL3a][T22][DL1-2] 我认为记录和推广这些知识是我的责任，即使这似乎暗示着与 兴趣，因为这意味着突出提及我自己团队的工作，因为（截至 2022 年）引用最多的神经网络都是基于它的。[MOST] 未来的人工智能历史学家可能会纠正任何时代特定的潜在偏见。&lt;/p&gt;

&lt;h2 id=&quot;介绍&quot;&gt;介绍&lt;/h2&gt;

&lt;p&gt;随着时间的推移，某些历史事件在某些旁观者眼中变得更加重要。 例如，138 亿年前的大爆炸现在被广泛认为是万物历史上的重要时刻。 然而，直到几十年前，地球人还完全不知道它，长期以来，地球人对宇宙的起源抱有相当错误的看法（有关世界历史的更多信息，请参见最后一节）。 目前接受的许多更有限主题的历史是类似激进修订的结果。 在这里，我将重点关注人工智能 (AI) 的历史，它也与过去不同。&lt;/p&gt;

&lt;p&gt;1980 年代写的 AI 历史会强调定理证明、[GOD][GOD34][ZU48][NS56] 逻辑编程、专家系统和启发式搜索等主题。[FEI63,83][LEN83] 这将是 与 1956 年达特茅斯会议的主题一致，约翰麦卡锡在会上创造了“人工智能”一词，用来描述一个旧的研究领域重新引起人们的兴趣。 实用 AI 至少可以追溯到 1914 年，当时 Leonardo Torres y Quevedo（见下文）构建了第一个工作的国际象棋终端游戏玩家 [BRU1-4]（当时国际象棋被认为是一种仅限于智能生物领域的活动）。 AI 理论至少可以追溯到 1931-34 年，当时 Kurt Gödel（见下文）确定了任何类型的基于计算的 AI 的基本限制。[GOD][BIB3][GOD21,a,b]&lt;/p&gt;

&lt;p&gt;2000 年代初期编写的 AI 历史会更加强调支持向量机和内核方法等主题，[SVM1-4] 贝叶斯（实际上是拉普拉斯或可能是桑德森[STI83-85]）推理[BAY1-8][ FI22]和其他概率论和统计概念，[MM1-5][NIL98][RUS95]决策树，例如[MIT97]集成方法，[ENS1-4]群体智能，[SW1]和进化计算。&lt;a href=&quot;[TUR1],未发表&quot;&gt;EVO1 -7&lt;/a&gt;为什么？ 因为在当时，此类技术推动了许多成功的 AI 应用。&lt;/p&gt;

&lt;p&gt;写于 2020 年代的 AI 历史必须强调诸如更古老的链式法则 [LEI07] 和通过梯度下降训练的深度非线性人工神经网络 (NN) [GD’] 等概念，特别是基于反馈的循环网络，它们是 其程序是权重矩阵的通用计算机。[AC90] 为什么？ 因为最近许多最著名和最商业化的 AI 应用程序都依赖于它们。[DL4]&lt;/p&gt;

&lt;p&gt;这样的 NN 概念实际上在概念上接近 MACY 会议 (1946-1953)[MACY51] 和 1951 年关于计算机器和人类思想的巴黎会议的主题，现在通常被视为关于 AI 的第一次会议。[AI51][BRO21][ BRU4] 然而，在 1956 年之前，现在称为 AI 的大部分内容仍被称为控制论，重点与基于神经网络“深度学习”的现代 AI 非常一致。[DL1-2][DEC]&lt;/p&gt;

&lt;p&gt;过去的一些神经网络研究受到人脑的启发，人脑有大约 1000 亿个神经元，每个神经元平均连接到 10,000 个其他神经元。 有些是输入神经元，为其余神经元提供数据（声音、视觉、触觉、疼痛、饥饿）。 其他的是控制肌肉的输出神经元。 大多数神经元隐藏在两者之间，思考发生的地方。 你的大脑显然通过改变连接的强度或权重来学习，这决定了神经元相互影响的强度，并且似乎编码了你一生的所有经历。 与我们的人工 NN 类似，它比以前的方法学习得更好，可以识别语音或手写或视频、最小化痛苦、最大化快乐、驾驶汽车等。[MIR]（第 0 节）[DL1-4]&lt;/p&gt;

&lt;p&gt;NN 如何学习所有这些？ 在下文中，我将强调使这一切成为可能的重要历史贡献。 由于现代 AI 的几乎所有基本概念都源于前几千年，因此下面的章节标题只强调到 2000 年的发展。然而，许多章节都提到了这项工作在新千年的后期影响，这带来了许多 硬件和软件的改进，有点像 20 世纪对 19 世纪发明的汽车进行了大量改进。&lt;/p&gt;

&lt;p&gt;本文还揭穿了一个经常重复的、误导性的“深度学习的历史”[S20][DL3,3a]，它忽略了下面提到的大部分开创性工作。[T22]见脚注 6。本文的标题图片是一个 对一条错误的常识的反应，该常识说 [T19] 使用神经网络“作为帮助计算机识别模式和模拟人类智能的工具是在 1980 年代引入的”，尽管这种神经网络早在 1980 年代就出现了。 [T22 ] 确保在所有科学中正确分配学分对我来说非常重要——就像对所有科学家一样——我鼓励有兴趣的读者也看看我在《科学》和《自然》杂志上就此发表的一些信件，例如， 关于航空史，[NASC1-2] 电话，[NASC3] 计算机，[NASC4-7] 弹性机器人，[NASC8] 和 19 世纪的科学家。[NASC9]&lt;/p&gt;

&lt;p&gt;最后，为了圆满结束，我将把事情放在更广泛的历史背景下，跨越从大爆炸到宇宙比现在古老许多倍的时间。&lt;/p&gt;

&lt;h2 id=&quot;1676向后信用分配的链式规则&quot;&gt;1676：向后信用分配的链式规则&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023-01-17-juergen-deep-learning-history-2.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;莱布尼茨，大约 1670 年的计算机科学之父，于 1676 年发表了链式法则&lt;/p&gt;

&lt;p&gt;1676年，戈特弗里德·威廉·莱布尼茨在回忆录中发表了微积分的链式法则（尽管万物皆有符号错误！）； Guillaume de l’Hopital 在他 1696 年关于莱布尼茨微积分的教科书中对此进行了描述。[LEI07-10][L84] 今天，这条规则是深度神经网络 (NN) 中信用分配的核心。 为什么？ 最流行的 NN 具有计算来自其他神经元的输入的可微函数的节点或神经元，这些节点或神经元又计算来自其他神经元的输入的可微函数，等等。 问题是：如果我们稍微修改早期函数的参数或权重，最终函数的输出将如何变化？ 链式法则是计算答案的基本工具。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023-01-17-juergen-deep-learning-history-3.jpg&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Cauchy 这个答案被梯度下降 (GD) 技术使用，显然是由 Augustin-Louis Cauchy 于 1847 年首次提出 [GD’]（后来由 Jacques Hadamard [GD’’] 提出；称为 SGD 的随机版本归功于 Herbert 罗宾斯和萨顿门罗 (1951)[STO51-52])。 为了教会神经网络将来自训练集的输入模式转换为所需的输出模式，所有神经网络权重都朝着最大局部改进的方向迭代改变一点，以创建稍微更好的神经网络，依此类推，直到获得令人满意的解决方案 成立。&lt;/p&gt;

&lt;p&gt;脚注 1. 1684 年，莱布尼茨也是第一个发表“现代”微积分的人；[L84][SON18][MAD05][LEI21,a,b] 后来艾萨克·牛顿也因其未发表的工作而受到赞誉。[SON18] 他们的优先事项 然而，争议 [SON18] 并不包含链式法则。[LEI07-10] 当然，两者都建立在早期工作的基础上：在公元前 2 世纪，阿基米德（也许是有史以来最伟大的科学家 [ARC06]）为 无穷小并发表了微积分的特例，例如球体和抛物线段，建立在古希腊更早的工作之上。 14 世纪，Sangamagrama 的 Madhava 和印度喀拉拉邦学派的同事也进行了微积分的基础工作。[MAD86-05]&lt;/p&gt;

&lt;p&gt;脚注 2. 值得注意的是，莱布尼茨（1646-1714 年，又名“世界上第一位计算机科学家”[LA14]）也奠定了现代计算机科学的基础。 他设计了第一台可以执行所有四种算术运算的机器（1673），以及第一台带有内部存储器的机器。[BL16] 他描述了二进制计算机的原理（1679）[L79][L03][LA14][HO66][ LEI21,a,b] 几乎被所有现代机器所采用。 他的正式思想代数 (1686)[L86][WI48] 与后来的布尔代数 (1847) 演绎等价[LE18]。[BOO] 他的 Characteristica Universalis &amp;amp; Calculus Ratiocinator 旨在通过计算回答所有可能的问题；[WI48 】 他的《微积分！ 是启蒙时代的标志性名言之一。 值得注意的是，他还负责链式法则，这是“现代”深度学习的基础，是现代计算机科学的一个重要子领域。&lt;/p&gt;

&lt;p&gt;脚注 3. 有人声称反向传播算法（进一步讨论；现在广泛用于训练深度神经网络）只是 Leibniz (1676) &amp;amp; L’Hopital (1696) 的链式法则。[CONN21] 不，这是有效的方法 将链式法则应用于具有可微分节点的大型网络（也有许多低效的方法）。[T22] 直到 1970 年才发布，如下所述。[BP1,4,5]&lt;/p&gt;

&lt;h2 id=&quot;1800第一个神经网络线性回归浅层学习&quot;&gt;~1800：第一个神经网络/线性回归/浅层学习&lt;/h2&gt;

&lt;p&gt;1805 年，Adrien-Marie Legendre 发表了现在通常称为线性神经网络 (NN) 的内容。 约翰·卡尔·弗里德里希·高斯 (Johann Carl Friedrich Gauss) 也因在大约 1795 年完成的早期未发表的工作而受到赞誉&lt;/p&gt;

&lt;p&gt;1805 年，Adrien-Marie Legendre 发表了现在通常称为线性神经网络 (NN) 的内容。 后来，约翰·卡尔·弗里德里希·高斯 (Johann Carl Friedrich Gauss) 也因在大约 1795 年完成的这项未发表的工作而受到赞誉。[STI81]&lt;/p&gt;

&lt;p&gt;这个来自 2 个多世纪前的神经网络有两层：一个具有多个输入单元的输入层和一个输出层。 为简单起见，我们假设后者由单个输出单元组成。 每个输入单元都可以保存一个实数值，并通过具有实数值权重的连接连接到输出。 NN 的输出是输入与其权重的乘积之和。 给定输入向量的训练集和每个向量的期望目标值，调整 NN 权重，使 NN 输出与相应目标之间的平方误差之和最小化。&lt;/p&gt;

&lt;p&gt;1795 年，高斯使用了现在称为线性神经网络的东西，但勒让德于 1805 年首次发表了它。高斯通常被称为自古以来最伟大的数学家，当然，那时候还不叫神经网络。 它被称为最小二乘法，也被广泛称为线性回归。 但它在数学上与今天的线性神经网络相同：相同的基本算法、相同的误差函数、相同的自适应参数/权重。 这种简单的神经网络执行“浅层学习”（与具有许多非线性层的“深度学习”相反）。 事实上，许多神经网络课程都是从介绍这种方法开始的，然后转向更复杂、更深入的神经网络。&lt;/p&gt;

&lt;p&gt;也许第一个通过浅层学习进行模式识别的著名例子可以追溯到 200 多年前：1801 年通过高斯重新发现矮行星谷神星，他从以前的天文观测中获得了数据点，然后使用各种技巧来调整模型的参数 预测器，它基本上学会了从训练数据中进行归纳以正确预测谷神星的新位置。&lt;/p&gt;

&lt;p&gt;脚注 4. 今天，所有技术学科的学生都必须上数学课，尤其是分析、线性代数和统计学。 在所有这些领域中，重要的结果和方法（至少部分）归功于高斯：代数基本定理、高斯消去法、统计的高斯分布等。这位号称“自古以来最伟大的数学家”的人也开创了微分 几何、数论（他最喜欢的科目）和非欧几何。 此外，他对天文学和物理学做出了重大贡献。 如果没有他的成果，包括 AI 在内的现代工程将不可想象。&lt;/p&gt;

&lt;p&gt;脚注 5. 神经网络的“浅层学习”在 1950 年代后期经历了新一波的流行。 Rosenblatt 的感知器 (1958)[R58] 将上述线性 NN 与输出阈值函数相结合以获得模式分类器（比较他在下面讨论的多层网络上更先进的工作）。 Joseph[R61] 提到了 Farley &amp;amp; Clark 更早的类似感知器的设备。 Widrow &amp;amp; Hoff 的类似 Adaline 在 1962 年学到。[WID62]&lt;/p&gt;

&lt;h2 id=&quot;1920-1925第一个循环网络架构&quot;&gt;1920-1925：第一个循环网络架构&lt;/h2&gt;

&lt;p&gt;1924 年，Ernst Ising 发表了第一个循环网络架构：Ising 模型或 Lenz-Ising 模型。 与人脑相似，但与更有限的前馈神经网络 (FNN) 不同，循环神经网络 (RNN) 具有反馈连接，因此可以遵循从某些内部节点到其他节点的定向连接，并最终在起点处结束。 这对于在序列处理期间实现对过去事件的记忆是必不可少的。&lt;/p&gt;

&lt;p&gt;第一个非学习 RNN 架构（Ising 模型或 Lenz-Ising 模型）是由物理学家 Ernst Ising 和 Wilhelm Lenz 在 1920 年代引入和分析的[L20][I24,I25][K41][W45][T22] 它 响应输入条件进入平衡状态，并且是第一个学习 RNN 的基础（见下文）。&lt;/p&gt;

&lt;p&gt;非学习 RNN 也在 1943 年由神经科学家 Warren McCulloch 和 Walter Pitts [MC43] 进行了讨论，并在 1956 年由 Stephen Cole Kleene 进行了正式分析。 [K56]&lt;/p&gt;

&lt;p&gt;1972 年，Shun-Ichi Amari 使 Ising 递归网络自适应。 这是第一个发表的学习人工递归神经网络&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;1972首次发布学习人工-rnn&quot;&gt;~1972：首次发布学习人工 RNN&lt;/h3&gt;

&lt;p&gt;1972 年，Shun-Ichi Amari 使 Lenz-Ising 循环架构具有自适应性，这样它就可以通过改变连接权重来学习将输入模式与输出模式相关联。[AMH1] 另见 Stephen Grossberg 关于生物网络的工作，[GRO69] David Marr 的 [MAR71]和Teuvo Kohonen的[KOH72]工作，以及Kaoru Nakano的学习RNN。[NAK72]&lt;/p&gt;

&lt;p&gt;艾伦·图灵
10 年后，Amari 网络被重新发布（并分析了它的存储容量）。[AMH2] 有人称它为 Hopfield 网络（！）或 Amari-Hopfield 网络。[AMH3] 它不处理序列，但在响应中达到平衡 到静态输入模式。 然而，Amari (1972) 也对其进行了序列处理推广[AMH1]&lt;/p&gt;

&lt;p&gt;值得注意的是，早在 1948 年，艾伦图灵就提出了与人工进化和学习 RNN 相关的想法。 然而，这在几十年后首次发表，[TUR1] 这解释了他在这里思想的晦涩。[TUR21]（边注：有人指出，著名的“图灵测试”实际上应该称为“笛卡尔测试” .[TUR3,a,b][TUR21])&lt;/p&gt;

&lt;p&gt;今天最流行的RNN就是下面提到的长短期记忆（LSTM），它已经成为20世纪被引用最多的NN[MOST]&lt;/p&gt;

&lt;h2 id=&quot;1958-年多层前馈神经网络没有深度学习&quot;&gt;1958 年：多层前馈神经网络（没有深度学习）&lt;/h2&gt;

&lt;p&gt;1958 年，弗兰克·罗森布拉特 (Frank Rosenblatt) 拥有多层感知器，其最后一层学习&lt;/p&gt;

&lt;p&gt;1958 年，Frank Rosenblatt 不仅结合了线性 NN 和阈值函数（参见 1800 年以来的浅层学习部分），他还有更有趣、更深层的多层感知器 (MLP)。[R58] 他的 MLP 有一个非学习的第一层 随机权重和自适应输出层。 虽然这还不是深度学习，因为只有最后一层学习了，[DL1] Rosenblatt 基本上拥有了后来被重新命名为极限学习机 (ELM) 的东西，但没有适当的归因。[ELM1-2][CONN21][T22]&lt;/p&gt;

&lt;p&gt;1961 年，Karl Steinbuch [ST61-95] 和 Roger David Joseph [R61] (1961) 也讨论了 MLP。 另见 Oliver Selfridge 的多层 Pandemonium [SE59] (1959)。&lt;/p&gt;

&lt;p&gt;Rosenblatt (1962) 甚至写了关于带有隐藏层的 MLP 中的“反向传播错误”[R62]，尽管他还没有针对深度 MLP 的通用深度学习算法。 现在称为反向传播的东西完全不同，它于 1970 年首次发布，如下所述。[BP1-BP5][BPA-C]&lt;/p&gt;

&lt;p&gt;今天，最流行的 FNN 是基于 LSTM 的 Highway Net（下文提到）的一个版本，称为 ResNet，[HW1-3]，它已成为 21 世纪被引用最多的 NN。[MOST]&lt;/p&gt;

&lt;h2 id=&quot;1965-年第一次深度学习&quot;&gt;1965 年：第一次深度学习&lt;/h2&gt;

&lt;p&gt;1965 年，Alexey Ivakhnenko 和 Valentin Lapa 推出了第一个适用于具有任意多个隐藏层的深度 MLP 的深度学习算法
深度前馈网络架构的成功学习始于 1965 年的乌克兰（当时的苏联），当时 Alexey Ivakhnenko 和 Valentin Lapa 为具有任意多个隐藏层（已经包含现在流行的乘法门）的深度 MLP 引入了第一个通用的工作学习算法 .[DEEP1-2][DL1-2][FDL] 1971年的一篇论文[DEEP2]已经描述了一个8层的深度学习网络，用他们被高度引用的方法训练，这种方法在新千年仍然很流行，[DL2]尤其是 在东欧，那里诞生了很多机器学习。[MIR]（第 1 节）[R8]&lt;/p&gt;

&lt;p&gt;给定一组具有相应目标输出向量的输入向量训练集，层逐渐增长并通过回归分析进行训练，然后借助单独的验证集进行修剪，其中正则化用于清除多余的单元。 层数和每层单元以问题相关的方式学习。&lt;/p&gt;

&lt;p&gt;与后来的深度神经网络一样，Ivakhnenko 的网络学会了为传入数据创建分层的、分布式的、内部表示。&lt;/p&gt;

&lt;p&gt;他没有称它们为深度学习神经网络，但它们就是这样。 事实上，“深度学习”这个古老的术语最早是由 Dechter (1986) 引入机器学习的，Aizenberg 等人 (2000) 引入神经网络的。[DL2]（边注：我们 2005 年关于深度学习的论文 [DL6 ,6a] 是第一本机器学习出版物，标题中包含“深入学习”这个词组合。[T22])&lt;/p&gt;

&lt;h2 id=&quot;1967-68通过随机梯度下降进行深度学习&quot;&gt;1967-68：通过随机梯度下降进行深度学习&lt;/h2&gt;

&lt;p&gt;1967-68 年，Shun-Ichi Amari 通过随机梯度下降训练深度 MLP
Ivakhnenko 和 Lapa（1965 年，见上文）逐层训练他们的深层网络。 然而，在 1967 年，Shun-Ichi Amari 建议通过随机梯度下降 (SGD)[GD1] 从头开始以非增量端到端方式训练多层 MLP，这是 Robbins 和 Monro 于 1951 年提出的一种方法。 STO51-52]&lt;/p&gt;

&lt;p&gt;Amari 的实现 [GD2,GD2a]（与他的学生 Saito）在具有两个可修改层的五层 MLP 中学习了内部表示，该层被训练为对非线性可分离模式类进行分类。 那时候的计算成本是今天的数十亿倍。&lt;/p&gt;

&lt;p&gt;另见 Iakov Zalmanovich Tsypkin 更早的关于非线性系统的基于梯度下降的在线学习的工作。[GDa-b]&lt;/p&gt;

&lt;p&gt;值得注意的是，如上所述，Amari 还在 1972 年发表了学习 RNN。[AMH1]&lt;/p&gt;

&lt;h2 id=&quot;1970-年反向传播-1982-年对于神经网络-1960-年先驱&quot;&gt;1970 年：反向传播。 1982 年：对于神经网络。 1960 年：先驱。&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023-01-17-juergen-deep-learning-history-12.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;谁发明了反向传播？&lt;/p&gt;

&lt;p&gt;1970 年，Seppo Linnainmaa 是第一个发布现在称为反向传播的算法，这是一种著名的可微节点网络信用分配算法，[BP1,4,5] 也称为“自动微分的反向模式”。 它现在是广泛使用的神经网络软件包的基础，例如 PyTorch 和谷歌的 Tensorflow。&lt;/p&gt;

&lt;p&gt;1960年，Henry J. Kelley在控制理论领域有了反向传播的先驱
1982 年，Paul Werbos 在他 1974 年的论文中提出了使用该方法训练神经网络，[BP2] 扩展了思想。&lt;/p&gt;

&lt;p&gt;1960 年，Henry J. Kelley 在控制理论领域已经有了反向传播的先驱；[BPA] 另请参阅 Stuart Dreyfus 和 Arthur E. Bryson 在 1960 年代早期的后期工作。[BPB][BPC][R7] 不同于 Linnainmaa 的一般方法，[BP1] 1960 年代的系统[BPA-C] 通过标准雅可比矩阵计算从一个“阶段”到前一个“阶段”反向传播导数信息，既没有解决跨多个阶段的直接链接，也没有解决由于网络导致的潜在额外效率增益 稀疏性。&lt;/p&gt;

&lt;p&gt;反向传播本质上是为深度网络实施莱布尼茨链式法则 [LEI07-10] (1676)（见上文）的有效方式。 Cauchy 的梯度下降 [GD’] 使用它在许多试验过程中逐渐削弱某些 NN 连接并加强其他连接，这样 NN 的行为越来越像某个老师，可能是一个人，也可能是另一个 NN，[UN- UN2] 或其他东西。&lt;/p&gt;

&lt;p&gt;到 1985 年，计算成本已比 1970 年便宜约 1,000 倍，而第一台台式计算机刚刚在富裕的学术实验室中普及。 David E. Rumelhart 等人对已知方法[BP1-2] 的实验分析。 然后证明反向传播可以在 NN 的隐藏层中产生有用的内部表示。[RUM] 至少对于监督学习，反向传播通常比 Amari 的上述深度学习更有效，通过更一般的 SGD 方法（1967），它学习了有用的内部 大约 2 年前 NN 中的表示。[GD1-2a]&lt;/p&gt;

&lt;p&gt;直到 1970 年 [BP1-2] 的反向传播方法被广泛接受作为深度神经网络的训练方法，花了 4 年时间。 在 2010 年之前，许多人认为训练多层神经网络需要无监督预训练，这是我自己在 1991 年提出的方法[UN][UN0-3]（见下文），后来得到其他人的支持（2006 年）。[UN4 ] 事实上，据称 [VID1] “没有任何头脑正常的人会建议”将简单的反向传播应用于深度神经网络。 然而，在 2010 年，我们的团队与我出色的罗马尼亚博士后 Dan Ciresan [MLP1-2] 表明，深度 FNN 可以通过简单的反向传播进行训练，并且根本不需要对重要应用进行无监督预训练。 [MLP2]&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023-01-17-juergen-deep-learning-history-14.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;我们的系统在当时著名且广泛使用的图像识别基准 MNIST 上创造了新的性能记录 [MLP1]。 这是通过在称为 GPU 的高度并行图形处理单元上极大地加速深度 FNN 来实现的（正如 Jung 和 Oh 在 2004 年 [GPUNN] 首次对层数较少的浅层 NN 所做的那样）。 一位评论家称这是“机器学习社区的警钟”。 今天，该领域的每个人都在追求这种方法。&lt;/p&gt;

&lt;p&gt;脚注 6. 不幸的是，在 1980 年代重新发表反向传播的几位作者没有引用现有技术——甚至在后来的调查中也没有。[T22] 事实上，正如引言中提到的，有一个更广泛的、经常重复的、误导性的“ 深度学习的历史[S20]忽略了前面章节中提到的大部分开创性工作。[T22][DLC]这个“替代历史”基本上是这样的：“1969 年，Minsky &amp;amp; Papert[M69] 表明浅 没有隐藏层的神经网络非常有限，该领域被放弃，直到新一代神经网络研究人员在 1980 年代重新审视这个问题。[S20] 然而，1969 年的书 [M69] 解决了高斯的“问题” &amp;amp; Legendre 的浅层学习（大约 1800 年）[DL1-2] 已经在 4 年前被 Ivakhnenko &amp;amp; Lapa 流行的深度学习方法 [DEEP1-2][DL2] 解决了，然后 Amari 的 SGD 也解决了 MLPs。[GD1- 2] Minsky 既没有引用这项工作，也没有在后来更正他的书。&lt;a href=&quot;Sec. I&quot;&gt;HIN&lt;/a&gt;[T22] 甚至 r 最近的论文宣扬了这种对深度学习的修正主义叙述，显然是为了美化其作者后来的贡献（例如玻尔兹曼机[BM][HIN][SK75][G63][T22]），而没有将它们与原始作品联系起来，[DLC ][S20][T22]虽然真实历史众所周知。 深度学习研究在 1960 年代至 70 年代非常活跃，尤其是在英语圈之外。[DEEP1-2][GD1-3][CNN1][DL1-2][T22] 明显的错误归因和无意的[PLAG1][CONN21] 或故意 [FAKE2] 剽窃仍在污染整个深度学习领域。[T22] 科学期刊“需要对自我纠正做出更明确、更坚定的承诺”，[SV20] 这已经是其他科学领域的标准。&lt;/p&gt;

&lt;h2 id=&quot;1979-年第一个深度卷积神经网络1969-年relu&quot;&gt;1979 年：第一个深度卷积神经网络（1969 年：ReLU）&lt;/h2&gt;

&lt;p&gt;1979 年，Kunihiko Fukushima 引入了卷积神经网络 (CNN) 架构。计算机视觉在 2010 年代被称为卷积神经网络 (CNN) 的特殊前馈神经网络彻底改变了。[CNN1-4] 具有交替卷积层和下采样层的基本 CNN 架构 这要归功于福岛邦彦 (1979)。 他称之为 Neocognitron。[CNN1]&lt;/p&gt;

&lt;p&gt;值得注意的是，早在 10 年前，Fukushima 还为神经网络引入了整流线性单元 (ReLU) (1969)。[RELU1] 它们现在广泛用于 CNN 和其他神经网络。&lt;/p&gt;

&lt;p&gt;1987 年，Alex Waibel 将带卷积的神经网络与权重共享和反向传播相结合（见上文），[BP1-2] 并将其应用于语音。[CNN1a] Waibel 没有称此为 CNN，而是 TDNN。&lt;/p&gt;

&lt;p&gt;Yamaguchi 等人介绍了一种流行的下采样变体，称为最大池化。 1990 年的 TDNN [CNN3a] 和 Juan Weng 等人。 1993 年用于高维 CNN。[CNN3]&lt;/p&gt;

&lt;p&gt;自 1989 年以来，Yann LeCun 的团队为 CNN 的改进做出了贡献，尤其是在图像方面。[CNN2,4][T22] Baldi 和 Chauvin (1993) 首次将具有反向传播功能的 CNN 应用于生物医学/生物特征图像。[BA93]&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023-01-17-juergen-deep-learning-history-16.gif&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;2011 年晚些时候，CNN 在 ML 社区变得更加流行，当时我自己的团队大大加快了深度 CNN 的训练（Dan Ciresan 等人，2011）。[GPUCNN1,3,5] 我们基于 GPU 的快速 [GPUNN][ GPUCNN5] 2011 年的 CNN [GPUCNN1] 被称为 DanNet[DAN,DAN1][R6] 是一个实际的突破，比 2006 年早期的 GPU 加速 CNN 更深更快。[GPUCNN] 2011 年，DanNet 成为第一个纯深度 CNN 赢得计算机视觉竞赛。[GPUCNN2-3,5]&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Competition[GPUCNN5]&lt;/th&gt;
      &lt;th&gt;Date/Deadline&lt;/th&gt;
      &lt;th&gt;Image size&lt;/th&gt;
      &lt;th&gt;Improvement&lt;/th&gt;
      &lt;th&gt;Winner&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;IJCNN 2011 traffic signs&lt;/td&gt;
      &lt;td&gt;Aug 06, 2011&lt;/td&gt;
      &lt;td&gt;variable&lt;/td&gt;
      &lt;td&gt;68.0% (superhuman)&lt;/td&gt;
      &lt;td&gt;DanNet[DAN,DAN1]&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;ISBI 2012 image segmentation&lt;/td&gt;
      &lt;td&gt;Mar 01, 2012&lt;/td&gt;
      &lt;td&gt;512x512&lt;/td&gt;
      &lt;td&gt;26.1%&lt;/td&gt;
      &lt;td&gt;DanNet[GPUCNN3a]&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;ICPR 2012 medical imaging&lt;/td&gt;
      &lt;td&gt;Sep 10, 2012&lt;/td&gt;
      &lt;td&gt;2048x2048x3&lt;/td&gt;
      &lt;td&gt;8.9%&lt;/td&gt;
      &lt;td&gt;DanNet[GPUCNN3a]&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;ImageNet 2012&lt;/td&gt;
      &lt;td&gt;Sep 30, 2012&lt;/td&gt;
      &lt;td&gt;256x256x3&lt;/td&gt;
      &lt;td&gt;41.4%&lt;/td&gt;
      &lt;td&gt;AlexNet[GPUCNN4]&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;MICCAI 2013 Grand Challenge&lt;/td&gt;
      &lt;td&gt;Sep 08, 2013&lt;/td&gt;
      &lt;td&gt;2048x2048x3&lt;/td&gt;
      &lt;td&gt;26.5%&lt;/td&gt;
      &lt;td&gt;DanNet[GPUCNN8]&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;ImageNet 2014&lt;/td&gt;
      &lt;td&gt;Aug 18, 2014&lt;/td&gt;
      &lt;td&gt;256x256x3&lt;/td&gt;
      &lt;td&gt;15.8%&lt;/td&gt;
      &lt;td&gt;VGG Net[GPUCNN9]&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;ImageNet 2015&lt;/td&gt;
      &lt;td&gt;Sep 30, 2015&lt;/td&gt;
      &lt;td&gt;256x256x&lt;/td&gt;
      &lt;td&gt;315.8%&lt;/td&gt;
      &lt;td&gt;ResNet,[HW2] a Highway Net[HW1] with open gates&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;有一段时间，DanNet 享有垄断地位。 从 2011 年到 2012 年，它赢得了它参加的所有比赛，并连续赢得了四场比赛（2011 年 5 月 15 日、2011 年 8 月 6 日、2012 年 3 月 1 日、2012 年 9 月 10 日）。[GPUCNN5] 特别是在硅谷的 IJCNN 2011 上，DanNet 吹响了比赛，并在国际比赛中取得了第一个超人视觉模式识别[DAN1]。 DanNet 也是第一个获胜的深度 CNN：中国手写比赛（ICDAR 2011）、图像分割比赛（ISBI，2012 年 5 月）、大图像物体检测比赛（ICPR，2012 年 9 月 10 日），以及——在 同一时间——关于癌症检测的医学成像竞赛。[GPUCNN8] 2010 年，我们将 DanNet 介绍给世界上最大的钢铁生产商 Arcelor Mittal，并且能够大大提高钢铁缺陷检测。[ST] 据我所知， 这是重工业的第一个深度学习突破。 2012 年 7 月，我们关于 DanNet[GPUCNN3] 的 CVPR 论文引起了计算机视觉社区的注意。 5 个月后，类似的 GPU 加速 AlexNet 赢得了 ImageNet[IM09] 2012 竞赛。[GPUCNN4-5][R6] 我们的 CNN 图像扫描仪比以前的方法快 1000 倍。[SCAN] 这引起了医疗保健行业的极大兴趣 . 今天，IBM、西门子、谷歌和许多初创公司都在采用这种方法。 VGG 网络（ImageNet 2014 获胜者）[GPUCNN9] 和其他高引用的 CNNs[RCNN1-3] 进一步扩展了 2011 年的 DanNet。[MIR]（第 19 节）[MOST]&lt;/p&gt;

&lt;p&gt;ResNet，ImageNet 2015 的赢家[HW2]（2015 年 12 月）和目前被引用最多的神经网络，[MOST] 是我们早期 Highway Net（2015 年 5 月）的一个版本（开门）[HW1-3][R5] Highway Net（见下文）实际上是我们的 vanilla LSTM（见下文）的前馈网络版本。[LSTM2] 它是第一个有效的、真正具有数百层的深度前馈神经网络（以前的神经网络最多只有几十层） .&lt;/p&gt;

&lt;h2 id=&quot;1980-年代至-90-年代图形神经网络随机增量规则dropout&quot;&gt;1980 年代至 90 年代：图形神经网络/随机增量规则（Dropout）/…&lt;/h2&gt;

&lt;p&gt;v.d. 引入了具有快速变化的“快速权重”的神经网络。 Malsburg (1981) 等人。[FAST,a,b] 1987 年由 Pollack [PO87-90] 提出并由 Sperduti、Goller 和 Küchler 扩展/改进的可以操纵图形等结构化数据的深度学习架构 [T22] 在 1990 年代初期。[SP93-97][GOL][KU][T22] 另见我们的图 NN-like, Transformer-like Fast Weight Programmers of 1991[FWP0-1][FWP6][FWP] 学习不断 重写从输入到输出的映射（见下文），以及 Baldi 及其同事的工作。[BA96-03] 如今，图 NN 用于许多应用程序。&lt;/p&gt;

&lt;p&gt;Werbos,[BP2][BPTT1] Williams,[BPTT2][CUB0-2]等人[ROB87][BPTT3][DL1]分析了梯度下降的实现方式[GD’][STO51-52][GDa-b][ GD1-2a] 在 RNN 中。 Kohonen 的自组织映射开始流行。[KOH82-89]&lt;/p&gt;

&lt;p&gt;80 年代和 90 年代还看到了各种生物学上更合理的深度学习算法的提议，与反向传播不同，这些算法在空间和时间上是局部的。[BB2][NAN1-4][NHE][HEL] 参见概述[MIR]（第 15 节） ，第 17 节）以及最近对此类方法重新产生的兴趣。[NAN5][FWPMETA6][HIN22]&lt;/p&gt;

&lt;p&gt;1990 年，Hanson 引入了随机增量法则，这是一种通过反向传播训练神经网络的随机方法。 几十年后，这个版本在绰号“dropout”下流行起来。[Drop1-4][GPUCNN4]&lt;/p&gt;

&lt;p&gt;1980 年代和 90 年代发表了许多关于 NN（包括 RNN）的其他论文——请参阅 2015 年调查中的大量参考文献。[DL1] 然而，在这里，我们主要将自己限制在——事后看来——最重要的论文，鉴于目前 （短暂的？）2022 年的前景。&lt;/p&gt;

&lt;h2 id=&quot;1990-年-2-月生成对抗网络好奇心&quot;&gt;1990 年 2 月：生成对抗网络/好奇心&lt;/h2&gt;

&lt;p&gt;生成对抗网络 (GAN) 已经变得非常流行。[MOST] 它们于 1990 年首次在慕尼黑以人工好奇心的名义发表。[AC90-20][GAN1] 两个决斗的神经网络（一个概率生成器和一个预测器）正试图 在 minimax 游戏中最大化彼此的损失。[AC]（第 1 节）生成器（称为控制器）生成概率输出（使用随机单位 [AC90]，就像在后来的 StyleGANs[GAN2] 中一样）。 预测器（称为世界模型）看到控制器的输出并预测环境对它们的反应。 使用梯度下降，预测器 NN 最小化它的错误，而生成器 NN 试图使输出最大化这个错误：一个网络的损失是另一个网络的收益。[AC90]（世界模型也可以用于连续在线行动规划。 [AC90][计划 2-3][计划])&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023-01-17-juergen-deep-learning-history-17.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;在 2014 年一篇关于 GAN 的论文之前 4 年，[GAN1] 我著名的 2010 年调查 [AC10] 将 1990 年的生成对抗性神经网络总结如下：“神经网络作为预测世界模型用于最大化控制器的内在奖励，这 与模型的预测误差成正比”（已最小化）。&lt;/p&gt;

&lt;p&gt;2014 年的 GAN 就是这样的一个例子，其中试验非常短（就像老虎机问题）并且环境简单地返回 1 或 0，这取决于控制器（或生成器）的输出是否在给定的集合中。[AC20][AC] [T22]（第十七节）&lt;/p&gt;

&lt;p&gt;其他早期的对抗性机器学习设置 [S59][H90] 非常不同——它们既不涉及无监督神经网络，也不涉及建模数据，也不使用梯度下降。[AC20]&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023-01-17-juergen-deep-learning-history-18.jpg&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;可预测性最小化：无监督极小极大博弈，其中一个神经网络最小化另一个最大化的目标函数&lt;/p&gt;

&lt;p&gt;1990 年的原则已被广泛用于强化学习 [SIN5][OUD13][PAT17][BUR18] 和逼真图像合成 [GAN1,2] 的探索，尽管后者最近被 Rombach 等人接管。 s Latent Diffusion，另一种在慕尼黑发表的方法，[DIF1] 建立在 Jarzynski 上个千年的早期物理学工作 [DIF2] 和最近的论文的基础上。 [DIF3-5]&lt;/p&gt;

&lt;p&gt;1991 年，我发布了另一种基于两个称为可预测性最小化的对抗性神经网络的 ML 方法，用于创建部分冗余数据的分离表示，并于 1996 年应用于图像。[PM0-2][AC20][R2][MIR]（第 7 节） )&lt;/p&gt;

&lt;h2 id=&quot;1990-年-2-月生成对抗网络好奇心-1&quot;&gt;1990 年 2 月：生成对抗网络/好奇心&lt;/h2&gt;

&lt;p&gt;生成对抗网络 (GAN) 已经变得非常流行。[MOST] 它们于 1990 年首次在慕尼黑以人工好奇心的名义发表。[AC90-20][GAN1] 两个决斗的神经网络（一个概率生成器和一个预测器）正试图 在 minimax 游戏中最大化彼此的损失。[AC]（第 1 节）生成器（称为控制器）生成概率输出（使用随机单位 [AC90]，就像在后来的 StyleGANs[GAN2] 中一样）。 预测器（称为世界模型）看到控制器的输出并预测环境对它们的反应。 使用梯度下降，预测器 NN 最小化它的错误，而生成器 NN 试图使输出最大化这个错误：一个网络的损失是另一个网络的收益。[AC90]（世界模型也可以用于连续在线行动规划。 [AC90][计划 2-3][计划])&lt;/p&gt;

&lt;p&gt;1990-91 年以来的人工好奇心和创造力&lt;/p&gt;

&lt;p&gt;在 2014 年一篇关于 GAN 的论文之前 4 年，[GAN1] 我著名的 2010 年调查 [AC10] 将 1990 年的生成对抗性神经网络总结如下：“神经网络作为预测世界模型用于最大化控制器的内在奖励，这 与模型的预测误差成正比”（已最小化）。&lt;/p&gt;

&lt;p&gt;2014 年的 GAN 就是这样的一个例子，其中试验非常短（就像老虎机问题）并且环境简单地返回 1 或 0，这取决于控制器（或生成器）的输出是否在给定的集合中。[AC20][AC] [T22]（第十七节）&lt;/p&gt;

&lt;p&gt;其他早期的对抗性机器学习设置 [S59][H90] 非常不同——它们既不涉及无监督神经网络，也不涉及建模数据，也不使用梯度下降。[AC20]&lt;/p&gt;

&lt;p&gt;可预测性最小化：无监督极小极大博弈，其中一个神经网络最小化另一个最大化的目标函数&lt;/p&gt;

&lt;p&gt;1990 年的原则已被广泛用于强化学习 [SIN5][OUD13][PAT17][BUR18] 和逼真图像合成 [GAN1,2] 的探索，尽管后者最近被 Rombach 等人接管。 s Latent Diffusion，另一种在慕尼黑发表的方法，[DIF1] 建立在 Jarzynski 上个千年的早期物理学工作 [DIF2] 和最近的论文的基础上。 [DIF3-5]&lt;/p&gt;

&lt;p&gt;1991 年，我发布了另一种基于两个称为可预测性最小化的对抗性神经网络的 ML 方法，用于创建部分冗余数据的分离表示，并于 1996 年应用于图像。[PM0-2][AC20][R2][MIR]（第 7 节） )&lt;/p&gt;

&lt;h3 id=&quot;1991-年-3-月具有线性化自注意力的变形金刚&quot;&gt;1991 年 3 月：具有线性化自注意力的变形金刚&lt;/h3&gt;

&lt;p&gt;最近，Transformers[TR1] 风靡一时，例如生成听起来像人类的文本。[GPT3] Transformers with “linearized self-attention”[TR5-6] 发表于 1991 年 3 月[FWP0-1][FWP6] [FWP]（除了正常化——见 2022 年 30 周年推文）。 这些所谓的“Fast Weight Programmers”或“Fast Weight Controllers”[FWP0-1] 像传统计算机一样将存储和控制分开，但是以端到端可微分的、自适应的、完全神经的方式（而不是 混合时尚[PDA1-2][DNC])。 标准变形金刚 [TR1-4] 中的“自注意力”将其与投影和 softmax 相结合（使用像我在 1993 年 [ATT][FWP2][R4] 中介绍的那样的注意力术语）。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023-01-17-juergen-deep-learning-history-19.png&quot; alt=&quot;image&quot; /&gt;
1991 年 3 月 26 日：神经网络学习使用快速权重对神经网络进行编程——就像今天的 Transformer 变体一样。 2021 年：新东西！&lt;/p&gt;

&lt;p&gt;今天的变形金刚大量使用无监督预训练[UN0-3]（见下一节），这是另一种深度学习方法，首次发表于我们 1990-1991 年的奇迹年[MIR][MOST]&lt;/p&gt;

&lt;p&gt;1991 年的快速权重程序员还导致了元学习自参照神经网络，它们可以在自己身上运行自己的权重变化算法或学习算法，并对其进行改进，并改进它们改进它的方式，等等。 这项工作自 1992 年以来[FWPMETA1-9][HO1] 扩展了我 1987 年的毕业论文，[META1] 介绍了不仅用于学习而且用于元学习或学习学习的算法，[META] 通过经验学习更好的学习算法。 这在 2010 年代[DEC] 变得非常流行，当时计算机的速度快了一百万倍。&lt;/p&gt;

&lt;h2 id=&quot;1991-年-4-月通过自监督预训练进行深度学习&quot;&gt;1991 年 4 月：通过自监督预训练进行深度学习&lt;/h2&gt;

&lt;p&gt;今天最强大的 NN 往往非常深，也就是说，它们有很多层神经元或许多后续计算阶段。[MIR] 然而，在 1990 年代之前，基于梯度的训练对深度 NN 效果不佳，仅适用于浅层 NN [DL1-2]（但请参阅 1989 年的一篇论文 [MOZ]）。 这个深度学习问题对于循环神经网络最为明显。 与人脑相似，但与更有限的前馈神经网络 (FNN) 不同，RNN 具有反馈连接。 这使得 RNN 成为功能强大的通用并行顺序计算机，可以处理任意长度的输入序列（想想语音数据或视频）。 RNN 原则上可以实现任何可以在您的笔记本电脑或任何其他现有计算机上运行的程序。 如果我们想要构建通用人工智能 (AGI)，那么它的底层计算基础必须更像 RNN 而不是 FNN，因为 FNN 从根本上是不够的； RNN 和类似系统之于 FNN 就像通用计算机之于袖珍计算器一样。 特别是，与 FNN 不同，RNN 原则上可以处理任意深度的问题。[DL1] 然而，在 1990 年代之前，RNN 在实践中未能学习深度问题。[MIR]（第 0 节）&lt;/p&gt;

&lt;p&gt;为了通过基于 RNN 的“一般深度学习”克服这个缺点，我构建了一个自我监督的 RNN 层次结构，它在多个抽象层次和多个自组织时间尺度上学习表示：[LEC] 神经序列分块器 [UN0] 或神经网络 History Compressor.[UN1] 每个 RNN 都试图解决预测其下一个输入的借口任务，仅将意外输入（因此也是目标）发送到上面的下一个 RNN。 由此产生的压缩序列表示极大地促进了下游监督深度学习，例如序列分类。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023-01-17-juergen-deep-learning-history-20.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;尽管当时的计算机每一美元的计算速度比今天慢了大约一百万倍，但到 1993 年，上面的神经历史压缩器已经能够解决以前无法解决的深度 &amp;gt; 1000[UN2]（需要超过 1,000 个后续计算阶段）的“非常深度学习”任务 ——这样的阶段越多，学习越深入）。 1993 年，我们还发布了神经历史压缩器的连续版本。[UN3]（另请参阅最近关于无监督的基于神经网络的抽象的工作。[OBJ1-5]）&lt;/p&gt;

&lt;p&gt;这项工作十多年后，[UN1] 发布了一种类似的用于更有限的前馈神经网络 (FNN) 的无监督方法，通过对称为深度信念网络 (DBN) 的 FNN 堆栈进行无监督预训练来促进监督学习。[UN4] 2006 年的理由基本上是我在 1990 年代初期为我的 RNN 堆栈使用的理由：每个更高级别都试图减少下面级别中数据表示的描述长度（或负对数概率）。[HIN][T22][MIR]&lt;/p&gt;

&lt;p&gt;1991 年 4 月：将一个 NN 提炼成另一个 NN
使用我 1991 年的 NN 蒸馏程序，可以将上述神经历史压缩器的分层内部表示折叠成单个循环神经网络 (RNN)。[UN0-1][MIR] 在这里，教师神经网络的知识被“蒸馏”成 一个学生 NN，通过训练学生 NN 模仿老师 NN 的行为（同时还对学生 NN 重新训练以前学过的技能，这样它就不会忘记它们）。 NN 蒸馏也在多年后重新发表，[DIST2][MIR][HIN][T22] 并在今天被广泛使用。&lt;/p&gt;

&lt;p&gt;如今，无监督预训练被 Transformers[TR1-6] 大量用于自然语言处理和其他领域。 值得注意的是，具有线性化自注意力的 Transformers 也首次在我们的 Annus Mirabilis of 1990-1991 中发表[FWP0-6]，[MIR][MOST] 以及用于深度学习的无监督/自监督预训练。[UN0-3 ] 见上一节。&lt;/p&gt;

&lt;h2 id=&quot;1991-年-6-月基本问题梯度消失&quot;&gt;1991 年 6 月：基本问题：梯度消失&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023-01-17-juergen-deep-learning-history-21.jpg&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Sepp Hochreiter 对基本深度学习问题的分析 (1991) 深度学习之所以困难，是因为我的第一个学生 Sepp Hochreiter 在他的毕业论文中于 1991 年确定并分析了基本深度学习问题，我很高兴能够监督。[VAN1] 首先 他实现了上面的神经历史压缩器，但后来做了更多：他表明深度神经网络存在现在著名的梯度消失或爆炸问题：在典型的深度或循环网络中，反向传播的误差信号要么迅速缩小，要么从中消失 界限。 在这两种情况下，学习都会失败（比较[VAN2]）。 这种分析导致了现在称为 LSTM 的基本原理（见下文）。&lt;/p&gt;

&lt;h2 id=&quot;1991-年-6-月lstm--highway-nets--resnets-的根源&quot;&gt;1991 年 6 月：LSTM / Highway Nets / ResNets 的根源&lt;/h2&gt;

&lt;p&gt;长短期记忆 (LSTM) 循环神经网络 [LSTM1-6] 克服了 Sepp 在其上述 1991 年毕业论文 [VAN1] 中确定的基本深度学习问题，我认为这是历史上最重要的文献之一 机器学习。 它还通过我们在 1995 年的一份技术报告中称为 LSTM 的基本原理（例如恒定错误流）提供了解决问题的重要见解。[LSTM0] 在 1997 年主要同行评审出版物之后 [LSTM1][25y97] （现在是 20 世纪被引用次数最多的 NN 文章 [MOST]），LSTM 及其训练程序在我在 IDSIA 的瑞士 LSTM 资助下通过我后来的学生 Felix Gers、Alex Graves 和其他人的工作得到了进一步改进。 一个里程碑是带有遗忘门 [LSTM2] 的“香草 LSTM 架构”——今天每个人都在使用的 1999-2000 的 LSTM 变体，例如在谷歌的 Tensorflow 中。 Alex 是我们首次将 LSTM 成功应用于语音（2004 年）的主要作者。[LSTM10] 2005 年首次发布了具有全时间反向传播的 LSTM 和双向 LSTM[LSTM3]（现已广泛使用）。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023-01-17-juergen-deep-learning-history-22.gif&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;2006 年的另一个里程碑是用于同时比对和识别序列的训练方法“Connectionist Temporal Classification”或 CTC[CTC]。 我们的团队在 2007 年成功地将 CTC 训练的 LSTM 应用于语音 [LSTM4]（也使用分层 LSTM 堆栈 [LSTM14]）。 这导致了第一个卓越的端到端神经语音识别。 它与 20 世纪 80 年代末以来将神经网络与传统方法（如隐马尔可夫模型 (HMM)）相结合的混合方法有很大不同。[BW][BRI][BOU][HYB12][T22] 在 2009 年，通过 Alex 的努力， CTC训练的LSTM成为第一个赢得国际比赛的RNN，即三项ICDAR 2009 Connected Handwriting Competitions（法语，波斯语，阿拉伯语）。 这引起了业界的极大兴趣。 LSTM 很快被用于涉及序列数据的所有事物，例如语音 [LSTM10-11][LSTM4][DL1] 和视频。 2015 年，CTC-LSTM 组合显着改善了谷歌在 Android 智能手机上的语音识别。[GSR15] 许多其他公司采用了这一点。[DL4] 谷歌 2019 年新的设备语音识别（现在在你的手机上，而不是在服务器上） 仍然是基于LSTM。&lt;/p&gt;

&lt;h3 id=&quot;1995神经概率语言模型&quot;&gt;1995：神经概率语言模型&lt;/h3&gt;

&lt;p&gt;第一个出色的端到端神经机器翻译也是基于 LSTM。 1995 年，我们已经有了一个优秀的神经概率文本模型 [SNT]，其基本概念在 2003 年 [NPM][T22] 中得到了重用——另请参阅 Pollack 早期关于词嵌入和其他结构的工作 [PO87][PO90] 以及 Nakamura 和 Shikano 1989 年的词类别预测模型。[NPMa] 2001 年，我们表明 LSTM 可以学习 HMM 等传统模型无法学习的语言，[LSTM13] 即神经“亚符号”模型突然擅长学习“符号”任务。 计算仍然必须便宜 1000 倍，但到 2016 年，谷歌翻译 [GT16]——其白皮书 [WU] 提到 LSTM 超过 50 次——基于两个连接的 LSTM，[S2S] 一个用于传入文本，一个用于传出翻译 - 比以前好得多。[DL4] 到 2017 年，LSTM 还支持 Facebook 的机器翻译（每周超过 300 亿次翻译——最受欢迎的 YouTube 视频需要数年时间才能实现仅 100 亿次点击），[FB17][DL4] 苹果的 大约 10 亿部 iPhone 上的 Quicktype，[DL4] 亚马逊 Alexa 的声音，[DL4] 谷歌的图像标题生成 [DL4] 和自动电子邮件回复 [DL4] 等。《商业周刊》称 LSTM “可以说是最商业化的 AI 成就”。[AV1 ] 到 2016 年，谷歌数据中心超过四分之一的强大推理计算能力用于 LSTM（5% 用于另一种流行的深度学习技术，称为 CNN——见上文）。[JOU17] 当然，我们的 LSTM 也是 大量用于医疗保健和医疗诊断——一个简单的谷歌 e Scholar search 出现了无数标题中带有“LSTM”的医学文章。[DEC]&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023-01-17-juergen-deep-learning-history-23.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;通过我的学生 Rupesh Kumar Srivastava 和 Klaus Greff 的工作，LSTM 原理也促成了我们 2015 年 5 月的 Highway Network[HW1]，这是第一个具有数百层的非常深的 FNN（以前的 NN 最多只有几十层） ). Microsoft 的 ResNet[HW2]（赢得了 ImageNet 2015 竞赛）是其一个版本（ResNet 是大门始终敞开的 Highway Net）。 早期的 Highway Nets 在 ImageNet 上的表现与它们的 ResNet 版本大致相同。[HW3] highway gates 的变体也用于某些算法任务，其中纯残差层不能很好地工作。[NDR]&lt;/p&gt;

&lt;h3 id=&quot;lstm--highway-net-原理是现代深度学习的核心&quot;&gt;LSTM / Highway Net 原理是现代深度学习的核心&lt;/h3&gt;

&lt;p&gt;深度学习完全是关于神经网络深度的。[DL1] 在 1990 年代，LSTM 为受监督的循环神经网络带来了本质上无限的深度； 在 2000 年代，受 LSTM 启发的 Highway Nets 将其引入前馈神经网络。 LSTM 已成为 20 世纪引用最多的神经网络； 名为 ResNet 的 Highway Net 版本是 21 世纪引用最多的神经网络。[MOST]（然而，引用是衡量真实影响的一个非常值得怀疑的衡量标准。[NAT1]）&lt;/p&gt;

&lt;p&gt;1980s-：在没有老师的情况下学习行动的神经网络
前面的部分主要关注用于被动模式识别/分类的深度学习。 然而，NN 也与强化学习 (RL)、[KAE96][BER96][TD3][UNI][GM3][LSTMPG] 最通用的学习类型相关。 一般的 RL 智能体必须在没有教师帮助的情况下发现如何与动态的、最初未知的、部分可观察的环境进行交互，以最大化其预期的累积奖励信号。[DL1] 动作之间可能存在任意的、先验未知的延迟 和可察觉的后果。 RL 问题与计算机科学的任何问题一样困难，因为任何具有可计算描述的任务都可以在通用 RL 框架中制定。[UNI]&lt;/p&gt;

&lt;p&gt;某些强化学习问题可以通过 80 年代之前发明的非神经技术来解决：蒙特卡洛（树）搜索（MC，1949 年）、[MOC1-5] 动态规划（DP，1953 年）、[BEL53] 人工进化（1954 年） ,&lt;a href=&quot;[TUR1],未发表&quot;&gt;EVO1-7&lt;/a&gt; alpha-beta-pruning (1959),[S59] 控制理论与系统辨识 (1950s),[KAL59][GLA85] 随机梯度下降 (SGD, 1951),[ STO51-52]和通用搜索技术（1973）。[AIT7]&lt;/p&gt;

&lt;p&gt;然而，深度 FNN 和 RNN 是改进某些类型的 RL 的有用工具。 在 1980 年代，函数逼近和 NN 的概念与系统识别相结合，[WER87-89][MUN87][NGU89] DP 及其在线变体 Temporal Differences (TD)，[TD1-3] 人工进化，[EVONN1- 3] 和策略梯度。[GD1][PG1-3] 可以在第 1 节中找到有关此的许多其他参考资料。 2015 年调查的 6 [DL1]&lt;/p&gt;

&lt;p&gt;当环境存在马尔可夫接口 [PLAN3]，使得 RL 机器的当前输入传达了确定下一个最佳动作所需的所有信息时，基于 DP/TD/MC 的 FNN 的 RL 可以非常成功，如图所示 1994 年 [TD2]（大师级西洋双陆棋玩家）和 2010 年代 [DM1-2a]（围棋、国际象棋和其他游戏的超人玩家）。&lt;/p&gt;

&lt;p&gt;对于没有马尔可夫接口的更复杂的情况，学习机不仅要考虑当前输入，还要考虑以前输入的历史，我们的 RL 算法和 LSTM[LSTM-RL][RPG] 的组合已经成为标准，特别是， 我们的 LSTM 通过策略梯度训练 (2007).[RPG07][RPG][LSTMPG]&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023-01-17-juergen-deep-learning-history-24.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;例如，2018 年，PG 训练的 LSTM 是 OpenAI 著名的 Dactyl 的核心，它学会了在没有老师的情况下控制灵巧的机器人手。[OAI1][OAI1a] 视频游戏类似：2019 年，DeepMind（由 我实验室的一名学生）在星际争霸游戏中击败了一名职业玩家，这在理论上比国际象棋或围棋 [DM2] 在许多方面都更难，使用的是 Alphastar，其大脑具有由 PG 训练的深层 LSTM 核心。[DM3] 强化学习 LSTM（占模型总参数数的 84%）也是著名的 OpenAI Five 的核心，它学会了在 Dota 2 视频游戏（2018 年）中击败人类专家。[OAI2] 比尔·盖茨称这是“进步的巨大里程碑” 人工智能”。[OAI2a][MIR]（第 4 节）[LSTMPG]&lt;/p&gt;

&lt;p&gt;RL 的未来将是关于使用复杂输入流的紧凑时空抽象进行学习/组合/规划——关于常识推理[MAR15] 和学习思考。[PLAN4-5] 分层方式，在多个抽象级别和多个时间尺度？[LEC] 我们在 1990-91 年发表了这些问题的答案：自我监督的神经历史压缩器 [UN][UN0-3] learn to represent percepts at multiple levels 抽象和多个时间尺度（见上文），而端到端可区分的基于神经网络的子目标生成器[HRL3] [MIR]（第 10 节）通过梯度下降学习分层行动计划（见上文）。 更复杂的学习抽象思考的方法发表于 1997[AC97][AC99][AC02] 和 2015-18.[PLAN4-5]&lt;/p&gt;

&lt;h2 id=&quot;是硬件笨蛋&quot;&gt;是硬件，笨蛋！&lt;/h2&gt;

&lt;p&gt;如果没有不断改进和加速计算机硬件，深度学习算法在过去千年中的最新突破（见前几节）是不可能的。 如果不提及这种已经运行了至少两千年的进化，任何人工智能和深度学习的历史都是不完整的。&lt;/p&gt;

&lt;p&gt;第一个已知的基于齿轮的计算设备是 2000 多年前古希腊的 Antikythera 机制（一种天文钟）。&lt;/p&gt;

&lt;p&gt;也许世界上第一台实用的可编程机器是 1 世纪 [SHA7a][RAU1] 由亚历山大的赫伦制造的自动剧院（显然他还拥有第一台已知的工作蒸汽机 - Aeolipile）。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023-01-17-juergen-deep-learning-history-25.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Banu Musa 兄弟在 9 世纪在巴格达制造的音乐自动机可能是第一台具有存储程序的机器。[BAN][KOE1] 它使用旋转圆柱体上的销来存储控制蒸汽驱动长笛的程序——比较 Al-Jazari 的可编程 1206.[SHA7b] 的鼓机&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023-01-17-juergen-deep-learning-history-26.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;1600 年代带来了更灵活的机器，可以根据输入数据计算答案。 第一个用于简单算术的基于数据处理齿轮的专用计算器是由 Wilhelm Schickard 于 1623 年建造的，Wilhelm Schickard 是“自动计算之父”称号的候选人之一，其次是 Blaise Pascal 的高级 Pascaline（1642 年）。 1673 年，已经提到的戈特弗里德·威廉·莱布尼茨（被称为“有史以来最聪明的人”[SMO13]）设计了第一台可以执行所有四种算术运算的机器（计步器），并且是第一台带有记忆的机器。[BL16] 他还描述了由穿孔卡 (1679)、[L79][L03][LA14][HO66] 控制的二进制计算机的原理，并发表了链式法则[LEI07-10]（见上文），深度学习和现代的基本要素 人工智能。&lt;/p&gt;

&lt;p&gt;大约 1800 年，约瑟夫-玛丽·雅卡尔 (Joseph-Marie Jacquard) 和其他人在法国制造了第一台商业程序控制机器（基于打孔卡的织机）——他们可能是编写世界上第一个工业软件的第一批“现代”程序员。 他们启发了 Ada Lovelace 和她的导师 Charles Babbage（英国，大约 1840 年）。 他计划但无法构建一台可编程的通用计算机（只有他的非通用专用计算器导致了 20 世纪的工作复制品）。&lt;/p&gt;

&lt;p&gt;Leonardo Torres y Quevedo，20 世纪第一个实用 AI 的先驱 1914 年，西班牙人 Leonardo Torres y Quevedo（在介绍中提到）成为 20 世纪第一个 AI 先驱，他创造了第一个工作的国际象棋终端玩家（当时国际象棋被认为 作为一种仅限于智能生物领域的活动）。 几十年后，当另一位 AI 先驱 Norbert Wiener [WI48] 在 1951 年巴黎 AI 会议上与它对战时，这台机器仍然被认为令人印象深刻。 [AI51][BRO21][BRU4]&lt;/p&gt;

&lt;p&gt;1935 年至 1941 年间，Konrad Zuse 创造了世界上第一台可运行的可编程通用计算机：Z3。 1936 年的相应专利 [ZU36-38][RO98][ZUS21] 描述了可编程物理硬件所需的数字电路，早于克劳德香农 1937 年关于数字电路设计的论文。[SHA37] 与巴贝奇不同，Zuse 使用了莱布尼茨的二进制计算原理 (1679)[L79][LA14][HO66][L03] 代替传统的十进制计算。 这极大地简化了硬件。[LEI21,a,b] 忽略任何物理计算机不可避免的存储限制，Z3 的物理硬件在哥德尔 [GOD][GOD34, GOD34, 21,21a] (1931-34)、Church[CHU] (1935)、Turing[TUR] (1936) 和 Post[POS] (1936)。 简单的算术技巧可以弥补 Z3 缺少明确的条件跳转指令。[RO98] 今天，大多数计算机都像 Z3 一样是二进制的。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023-01-17-juergen-deep-learning-history-28.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Z3 使用带有明显移动开关的电磁继电器。 第一个电子专用计算器（其运动部件是电子，太小以至于看不见）是约翰·阿塔纳索夫（“管基计算之父”[NASC6a]）的二进制 ABC（美国，1942 年）。 与 1600 年代基于齿轮的机器不同，ABC 使用真空管——今天的机器使用 Julius Edgar Lilienfeld 于 1925 年获得专利的晶体管原理。[LIL1-2] 但与 Zuse 的 Z3 不同，ABC 不能自由编程。 Tommy Flowers（英国，1943-45 年）的电子巨像机器也没有用来破解纳粹密码。[NASC6]&lt;/p&gt;

&lt;p&gt;由 Zuse (1941)[RO98] 以外的人建造的第一台通用工作可编程机器是 Howard Aiken 的十进制 MARK I（美国，1944 年）。 由 Eckert 和 Mauchly (1945/46) 开发的速度快得多的十进制 ENIAC 是通过重新布线来编程的。 数据和程序都被“曼彻斯特宝贝”（Williams, Kilburn &amp;amp; Tootill, UK, 1948）和 1948 年升级的 ENIAC 存储在电子存储器中，通过将数字指令代码输入只读存储器来重新编程。 [HAI14b]&lt;/p&gt;

&lt;p&gt;从那时起，计算机通过集成电路 (IC) 变得更快。 1949 年，西门子的 Werner Jacobi 为在公共基板上具有多个晶体管的 IC 半导体申请了专利（于 1952 年授予）。[IC49-14] 1958 年，Jack Kilby 展示了带有外部导线的 IC。 1959 年，罗伯特·诺伊斯 (Robert Noyce) 提出了单片 IC。[IC14] 自 1970 年代以来，图形处理单元 (GPU) 已被用于通过并行处理来加速计算。 今天（2022 年）的 IC/GPU 包含数十亿个晶体管（几乎所有晶体管都是 Lilienfeld 的 1925 FET 类型[LIL1-2]）。&lt;/p&gt;

&lt;p&gt;1941 年，Zuse 的 Z3 每秒可以执行大约一个基本运算（例如加法）。 从那时起，每 5 年，计算成本就会降低 10 倍（请注意，他的定律比摩尔定律要古老得多，摩尔定律指出每个芯片的晶体管 [LIL1-2] 数量每 18 个月翻一番）。 截至 2021 年，即 Z3 之后的 80 年，现代计算机每秒可以以相同（经通货膨胀调整后）的价格执行约 1000 万亿条指令。 对这种指数趋势的天真推断预测，21 世纪将出现廉价计算机，其原始计算能力是所有人类大脑总和的一千倍。[RAW]&lt;/p&gt;

&lt;p&gt;物理极限在哪里？ 根据 Bremermann (1982)，[BRE] 一台质量为 1 千克和体积为 1 升的计算机最多可以在最多 1032 位上每秒执行最多 1051 次操作。 上述趋势将在 Z3 之后大约 25 年，即 2200 年左右达到布雷默曼极限。但是，由于太阳系中只有 2 x 1030 千克的质量，因此趋势势必会在几个世纪内打破，因为光速 将极大地限制额外质量的获取，例如，以其他太阳系的形式，通过及时的函数多项式，如先前在 2004 年指出的那样。[OOPS2][ZUS21]&lt;/p&gt;

&lt;p&gt;物理学似乎要求未来高效的计算硬件必须像大脑一样，在 3 维空间中有许多紧凑放置的处理器，由许多短线和少量长线稀疏地连接，以最小化总连接成本（即使“线” 实际上是光束）。[DL2] 基本架构本质上是一种深度的、稀疏连接的 3 维 RNN，这种 RNN 的深度学习方法有望变得比今天更加重要。[DL2 ]&lt;/p&gt;

&lt;h2 id=&quot;不要忽视-1931-年以来的人工智能理论&quot;&gt;不要忽视 1931 年以来的人工智能理论&lt;/h2&gt;

&lt;p&gt;现代人工智能和深度学习的核心主要基于近几个世纪的简单数学：微积分/线性代数/统计学。 然而，要在上一节中提到的现代硬件上有效地实现这个核心，并为数十亿人推出它，需要大量的软件工程，基于上个世纪发明的大量智能算法。 这里没有余地一一提及。 然而，至少我会列出人工智能和计算机科学理论的一些最重要的亮点。&lt;/p&gt;

&lt;p&gt;1930 年代初期，哥德尔创立了现代理论计算机科学。[GOD][GOD34][LEI21,21a] 他介绍了一种通用编码语言 (1931-34)。[GOD][GOD34-21a] 它基于整数， 并允许以公理形式形式化任何数字计算机的操作。 哥德尔用它来表示数据（例如公理和定理）和程序 [VAR13]（例如数据操作的证明生成序列）。 他著名地构造了关于其他形式陈述的计算的形式陈述——特别是暗示它们不可判定的自引用陈述，给定一个计算定理证明器，系统地从一组可枚举的公理中列举所有可能的定理。 因此，他确定了算法定理证明、计算和任何类型的基于计算的 AI 的基本限制。[GOD][BIB3][MIR]（第 18 节）[GOD21,21a]&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023-01-17-juergen-deep-learning-history-29.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;像大多数伟大的科学家一样，哥德尔建立在早期工作的基础上。 他将 Georg Cantor 的对角化技巧 [CAN]（在 1891 年表明存在不同类型的无穷大）与 Gottlob Frege [FRE]（他在 1879 年引入了第一种形式语言）、Thoralf Skolem [SKO23]（他 在 1923 年引入了原始递归函数）和 Jacques Herbrand [GOD86]（他发现了 Skolem 方法的局限性）。 这些作者又建立在 Gottfried Wilhelm Leibniz[L86][WI48]（见上文）的正式思想代数（1686 年）的基础上，它与后来的 1847 年布尔代数演绎等价[LE18]。[BOO]&lt;/p&gt;

&lt;p&gt;1935 年，Alonzo Church 通过证明 Hilbert &amp;amp; Ackermann 的 Entscheidungsproblem（决策问题）没有一般解决方案，得出了哥德尔结果的推论/扩展。[CHU] 为此，他使用了他的替代通用编码语言，称为 Untyped Lambda Calculus， 它构成了极具影响力的编程语言 LISP 的基础。 1936年，Alan M. Turing引入了另一个通用模型：图灵机。[TUR]他重新推导了上述结果。[CHU][TUR][HIN][GOD21,21a][TUR21][LEI21,21a] 在 1936 年的同一年，Emil Post 发表了另一个独立的通用计算模型。[POS] 今天我们知道很多这样的模型。&lt;/p&gt;

&lt;p&gt;Konrad Zuse 不仅创造了世界上第一台可工作的可编程通用计算机，[ZU36-38][RO98][ZUS21]，他还设计了第一种高级编程语言 Plankalkül。[BAU][KNU]，他将其应用于国际象棋 在 1945 年 [KNU] 和 1948 年的定理证明。[ZU48] 比较 Newell 和 Simon 在定理证明方面的后期工作（1956）。[NS56] 1940 年代至 70 年代的许多早期人工智能实际上是关于哥德尔风格的定理证明和演绎 [GOD][GOD34,21,21a] 通过专家系统和逻辑编程。&lt;/p&gt;

&lt;p&gt;1964 年，Ray Solomonoff 将贝叶斯（实际上是拉普拉斯[STI83-85]）概率推理与理论计算机科学[GOD][CHU][TUR][POS] 相结合，推导出一种数学上最优（但计算上不可行）的学习预测未来的方式 [AIT1][AIT10] 与 Andrej Kolmogorov 一起创立了 Kolmogorov 复杂性理论或算法信息论 (AIT)，[AIT1-22] 通过形式化概念超越了传统信息论 [SHA48][KUL] 奥卡姆剃刀原理，通过计算数据的最短程序的概念，支持对给定数据进行最简单的解释。 这个概念有许多可计算的、有时间限制的版本，[AIT7][AIT5][AIT12-13][AIT16-17] 以及神经网络的应用。[KO2][CO1-3]&lt;/p&gt;

&lt;p&gt;在 2000 年代初期，Marcus Hutter（在我的瑞士国家科学基金会资助 [UNI] 下工作时）通过最佳动作选择器（通用 AI）增强了 Solomonoff 的通用预测器 [AIT1][AIT10]，用于强化学习代理，这些代理最初未知（ 但至少是可计算的）环境。[AIT20,22] 他还推导出了所有明确定义的计算问题的渐近最快算法，[AIT21] 解决任何问题的速度与此类问题的未知最快求解器一样快，除了加法常数 不依赖于问题的大小。&lt;/p&gt;

&lt;p&gt;自参考 2003 哥德尔机 [GM3-9] 的更一般的最优性不限于渐近最优性。&lt;/p&gt;

&lt;p&gt;然而，由于各种原因，这种数学上最优的 AI 在实践中尚不可行。 相反，实用的现代 AI 是基于次优的、有限的，但还不是很容易理解的技术，例如神经网络和深度学习，这是本文的重点。 但谁知道 20 年后会出现什么样的 AI 历史呢？&lt;/p&gt;

&lt;h2 id=&quot;从大爆炸到遥远的未来的更广泛的历史背景&quot;&gt;从大爆炸到遥远的未来的更广泛的历史背景&lt;/h2&gt;

&lt;p&gt;信用分配是关于在历史数据中寻找模式，并弄清楚以前的事件是如何促成某些事件的。 历史学家这样做。 物理学家这样做。 AI 也会这样做。 让我们退后一步，在最广泛的历史背景下审视人工智能：自大爆炸以来的所有时间。 2014 年，我在其中发现了一个美丽的指数加速模式，[OMG] 从那以后我在许多演讲中都提出了它，它也被写进了 Sibylle Berg 的获奖书籍“GRM：Brainfuck”。[OMG2] 以前出版 这种模式跨越的时间间隔要短得多：只有几十年或几个世纪或最多几千年。[OMG1]&lt;/p&gt;

&lt;p&gt;事实证明，从人类的角度来看，自宇宙诞生以来最重要的事件都整齐地排列在指数加速的时间轴上（误差线大多低于 10%）。 事实上，历史似乎在 2040 年左右汇聚在一个欧米茄点。 我喜欢叫它Omega，因为一个世纪前，Teilhard de Chardin称Omega是人类将达到下一个层次的点。[OMG0]另外，Omega听起来比“Singularity”[SING1-2]好听多了——听起来有点 就像“哦，我的上帝。”[OMG]&lt;/p&gt;

&lt;p&gt;让我们从138亿年前的大爆炸说起。 我们将这个时间除以 4 得到大约 35 亿年。 欧米茄是2040年左右。 在欧米茄负 35 亿年时，发生了一件非常重要的事情：生命出现在这个星球上。&lt;/p&gt;

&lt;p&gt;我们再次花费四分之一的时间。 我们在 9 亿年前出现，当时发生了一件非常重要的事情：类似动物的移动生命出现了。&lt;/p&gt;

&lt;p&gt;我们再除以 4。我们在 2.2 亿年前，当哺乳动物被发明时，我们就出来了，我们的祖先。&lt;/p&gt;

&lt;p&gt;我们再次除以 4。5500 万年前，第一批灵长类动物出现了，我们的祖先。&lt;/p&gt;

&lt;p&gt;自宇宙诞生以来最重要的事件似乎整齐地排列在 2040 年左右收敛于 Omega 点的指数加速时间线上（J Schmidhuber，2014 年）&lt;/p&gt;

&lt;p&gt;我们再次除以 4。1300 万年前，第一批原始人出现了，我们的祖先。 我不知道为什么所有这些除以 4 的除法总是在历史上出现这些决定性的时刻。 但他们确实如此。 我也试过三度、五度和谐波比例，但似乎只有四分之一奏效。&lt;/p&gt;

&lt;p&gt;我们再次除以 4。350 万年前发生了一件非常重要的事情：技术的黎明，正如大自然所说：第一批石器。&lt;/p&gt;

&lt;p&gt;我们除以 4。80 万年前，下一个伟大的技术突破发生了：可控火力。&lt;/p&gt;

&lt;p&gt;我们除以 4。 20 万年前，解剖学上的现代人变得突出，我们的祖先。&lt;/p&gt;

&lt;p&gt;我们除以 4. 5 万年前，出现了行为上现代的人，我们的祖先，并开始在世界上殖民。&lt;/p&gt;

&lt;p&gt;我们再次除以 4。我们在 13000 年前出现，当时发生了一件非常重要的事情：动物的驯化、农业、第一批定居点——文明的开始。 现在我们看到，所有的文明只是世界历史上的一瞬间，只是大爆炸以来时间的百万分之一。 农业和航天器几乎是同时发明的。&lt;/p&gt;

&lt;p&gt;我们除以 4。 3300 年前，铁器时代出现了第一次人口爆炸。&lt;/p&gt;

&lt;p&gt;我们除以 4。请记住，收敛点 Omega 是 2040 年左右。 欧米茄负 800 年——那是在 13 世纪，在中国，铁和火以枪炮、大炮和火箭的形式结合在一起。 从那时起，这就定义了世界，西方仍然远远落后于欠中国的许可费。&lt;/p&gt;

&lt;p&gt;我们再次除以 4。 欧米茄减去 200 年——我们来到了 19 世纪中叶，当时铁和火以越来越复杂的形式结合在一起，通过改进的蒸汽机为工业革命提供动力，基于博蒙特、帕潘、纽科门的工作 、瓦特和其他人（1600 年代至 1700 年代，超越了 1 世纪亚历山大港的 Heron [RAU1] 的第一台简单蒸汽机）。 电话（例如 Meucci 1857、Reis 1860、Bell 1876）[NASC3] 开始彻底改变通信方式。 疾病的细菌理论（巴斯德和科赫，1800 年代后期）彻底改变了医疗保健并使人们的平均寿命更长。 大约在 1850 年，以化肥为基础的农业革命（Sprengel &amp;amp; von Liebig，1800 年代初期）帮助引发了第二次人口爆炸，并在 20 世纪达到顶峰，当时世界人口翻了两番，让 20 世纪在所有世纪中脱颖而出 人类的历史，由制造人造肥料的 Haber-Bosch 工艺驱动，如果没有人造肥料，世界最多只能养活 40 亿人。[HAB1-2]&lt;/p&gt;

&lt;p&gt;我们再除以 4。 欧米茄减去 50 年——差不多是 1990 年，20 世纪 3 场大战的结束：第一次世界大战、第二次世界大战和冷战。 最有价值的 7 家上市公司都是日本公司（如今大多数都在美国）； 然而，中国和美国西海岸都开始迅速崛起，为 21 世纪奠定了基础。 通过手机和无线革命（基于 1800 年代发现的无线电波）以及面向所有人的廉价个人电脑，数字神经系统开始席卷全球。 WWW 是由 Tim Berners-Lee 在瑞士的欧洲粒子对撞机上创建的。 现代人工智能也大约在这个时候开始：第一辆真正的自动驾驶汽车于 1980 年代由 Ernst Dickmanns 团队在慕尼黑制造（到 1994 年，他们的机器人汽车以最高 180 公里/小时的速度在高速公路上行驶）。 [AUT] 那时候，我在写我 1987 年的毕业论文 [META1]，它介绍了不仅用于学习而且用于元学习或学习学习的算法，[META] 通过经验学习更好的学习算法（现在很流行 主题 [DEC])。 然后是我们在 TU Munich 的奇迹年 1990-91[MIR]，这是当今被引用最多的神经网络 [MOST] 和通过自我监督/无监督学习（见上文）进行现代深度学习的根源，[UN][UN0-3 ] LSTM/Highway Net/ResNet 原理（现在放在你智能手机的口袋里——见上文），[DL4][DEC][MOST] 人工好奇心和针对发明自己问题的代理的生成对抗性神经网络（见上文），[ AC90-AC20][PP-PP2][SA17] 具有线性化自注意力的变压器（见上文），[FWP0-6][TR5-6] 将教师 NN 提取为学生 NN（见上文），[UN][UN0- 3] 在多个抽象层次和多个时间尺度上学习行动计划（见上文），[HRL0-2][LEC] 和其他令人兴奋的东西。 其中大部分已经变得非常流行，并改善了数十亿人的生活。[DL4][DEC][MOST]&lt;/p&gt;

&lt;p&gt;我们再次除以 4。Omega 减去 13 年——这是不久的将来的一个时间点，大约在 2030 年，届时许多人预测廉价的 AI 将具有人类的脑力。 然后是最后 13 年左右，直到 Omega，那时不可思议的事情将会发生（尽管对这一切持保留态度 [OMG1]）。&lt;/p&gt;

&lt;p&gt;但当然，时间不会因欧米茄而停止。 也许只有人类主导的历史才会结束。 在 Omega 之后，许多好奇的元学习 AI 发明了自己的目标（这些目标已经在我的实验室中存在了几十年[AC][AC90,AC90b]）将迅速改进自己，仅受限于可计算性和物理学的基本限制。&lt;/p&gt;

&lt;p&gt;超级智能人工智能会做什么？ 太空对人类充满敌意，但对设计合理的机器人友好，它提供的资源比我们的生物圈薄膜要多得多，后者吸收的太阳能量不到太阳能的十亿分之一。 虽然一些好奇的 AI 会继续对生命着迷，至少只要他们还没有完全理解生命，[ACM16][FA15][SP16][SA17] 大多数人会对机器人和软件生命的令人难以置信的新机会更感兴趣 在太空中。 通过小行星带及更远地区无数的自我复制机器人工厂，它们将改造太阳系，然后在几十万年内改造整个银河系，并在数百亿年内改造可及宇宙的其余部分。 尽管存在光速限制，但不断扩大的 AI 领域将有足够的时间来殖民和塑造整个可见宇宙。&lt;/p&gt;

&lt;p&gt;让我稍微扩展一下你的想法。 宇宙还很年轻，只有 138 亿岁。 还记得我们一直除以 4 吗？ 现在让我们乘以 4！ 让我们展望未来，宇宙的年龄将是现在的 4 倍：大约 550 亿年。 届时，可见的宇宙将充满智慧。 因为在 Omega 之后，大多数 AI 将不得不去大多数物理资源所在的地方，以制造更多更大的 AI。 那些没有的不会有影响。[ACM16][FA15][SP16]&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023-01-17-juergen-deep-learning-history-31.jpg&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;</content><author><name>Jürgen Schmidhuber | [译] AI &amp; 麦克船长</name></author><category term="ai" /><category term="AI" /><category term="人工智能" /><category term="深度学习" /><category term="Deep Learning" /><category term="神经网络" /><category term="Artificial Neural Network" /><category term="机器学习" /><category term="Machine Learning" /><category term="ML" /><category term="ANN" /><category term="Transformer" /><category term="CNN" /><category term="RNN" /><category term="循环神经网络" /><category term="神经网络" /><category term="人工神经网络" /><category term="Artificial Intelligence" /><category term="LSTM" /><category term="长短时记忆" /><summary type="html">本文译自 LSTM 作者 Jürgen Schmidhuber，全文主要由 AI 翻译生成，麦克船长进行部分校对，这篇超长文章为了串联起了深度学习领域的大事件，以及那些引领我们的优秀科学家们。</summary></entry><entry><title type="html">【编译】当下生成式 AI（AIGC）领域的应用图景</title><link href="https://www.mikecaptain.com/2023/01/13/antler-generative-ai/" rel="alternate" type="text/html" title="【编译】当下生成式 AI（AIGC）领域的应用图景" /><published>2023-01-13T18:09:43+00:00</published><updated>2023-01-13T18:09:43+00:00</updated><id>https://www.mikecaptain.com/2023/01/13/antler-generative-ai</id><content type="html" xml:base="https://www.mikecaptain.com/2023/01/13/antler-generative-ai/">&lt;p&gt;&lt;img src=&quot;/img/src/2023-01-15-antler-generative-ai-1.jpg&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;本文译自 Antler Blog，原作者 Ollie Forsyth，中文译文由 AI 及麦克船长完成翻译。&lt;/p&gt;

&lt;p&gt;随着 ChatGPT 和 DALL-E 的发布，2022 年社交媒体平台上最热门的话题之一在最近几周爆发，引发了关于其对全球人员、职业和行业影响的激烈辩论。 争议的核心是什么？ 生成式 AI (Gen-AI)——可以快速创建新内容的系统，例如大学论文、歌曲和数字艺术作品。 这些能力令人印象深刻，但它们也引发了关于工作的未来以及人类在 AI 主导的世界中的作用的重要问题。 随着生成式人工智能的不断发展，考虑伦理意义和对社会的潜在影响将变得至关重要。 如果创造性工作在很大程度上被人工智能机器取代，会发生什么？&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;本文目录&lt;/strong&gt;&lt;/p&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#什么是-gen-ai&quot; id=&quot;markdown-toc-什么是-gen-ai&quot;&gt;什么是 Gen-AI？&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#人工智能与生成人工智能&quot; id=&quot;markdown-toc-人工智能与生成人工智能&quot;&gt;人工智能与生成人工智能&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#广阔的机遇正在展开&quot; id=&quot;markdown-toc-广阔的机遇正在展开&quot;&gt;广阔的机遇正在展开&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#gen-ai的影响&quot; id=&quot;markdown-toc-gen-ai的影响&quot;&gt;Gen-AI的影响&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#培训模型在实践中如何运作&quot; id=&quot;markdown-toc-培训模型在实践中如何运作&quot;&gt;培训模型在实践中如何运作？&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#语言模型是如何创建的&quot; id=&quot;markdown-toc-语言模型是如何创建的&quot;&gt;语言模型是如何创建的？&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#为什么-gen-ai-存在&quot; id=&quot;markdown-toc-为什么-gen-ai-存在&quot;&gt;为什么 Gen-AI 存在？&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#展望未来gen-ai收入模式&quot; id=&quot;markdown-toc-展望未来gen-ai收入模式&quot;&gt;展望未来——Gen-AI收入模式&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#为什么现在&quot; id=&quot;markdown-toc-为什么现在&quot;&gt;为什么现在？&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#gen-ai筹款格局&quot; id=&quot;markdown-toc-gen-ai筹款格局&quot;&gt;Gen-AI筹款格局&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#gen-ai独角兽格局&quot; id=&quot;markdown-toc-gen-ai独角兽格局&quot;&gt;Gen-AI独角兽格局&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#趋势&quot; id=&quot;markdown-toc-趋势&quot;&gt;趋势：&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#gen-ai-如何用于艺术和音乐&quot; id=&quot;markdown-toc-gen-ai-如何用于艺术和音乐&quot;&gt;Gen-AI 如何用于艺术和音乐？&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#gen-ai-如何用于游戏&quot; id=&quot;markdown-toc-gen-ai-如何用于游戏&quot;&gt;Gen-AI 如何用于游戏？&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#生成式-ai-将会如何影响创作者经济&quot; id=&quot;markdown-toc-生成式-ai-将会如何影响创作者经济&quot;&gt;生成式 AI 将会如何影响创作者经济？&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#这个空间的未来是什么它可能面临什么挑战&quot; id=&quot;markdown-toc-这个空间的未来是什么它可能面临什么挑战&quot;&gt;这个空间的未来是什么，它可能面临什么挑战？&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#gen-ai-将影响元宇宙具体如何影响还有待观察&quot; id=&quot;markdown-toc-gen-ai-将影响元宇宙具体如何影响还有待观察&quot;&gt;Gen-AI 将影响元宇宙——具体如何影响还有待观察。&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#让我们一起塑造未来&quot; id=&quot;markdown-toc-让我们一起塑造未来&quot;&gt;让我们一起塑造未来&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#参考链接&quot; id=&quot;markdown-toc-参考链接&quot;&gt;参考链接&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这份报告深入探讨了 Gen-AI 的世界，并且是第一份面向所有人的综合市场地图。 我们概述了该领域的 160 多个平台及其投资者，以及领先思想领袖对这项技术潜力的见解。 这为读者提供了一个独特的机会，可以全面了解生成人工智能市场以及新玩家挑战谷歌等老牌玩家的潜力。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“生成式 AI 是一项基础技术，并且与这些新平台一样，它带来的机会很多——我们已经过了‘如果’的阶段，我们正处于‘何时’和‘如何’的阶段。” 随着 LLM 开源，我们看到基础设施层日趋成熟和民主化，这加速了应用层。”——Irina Elena Haivas，Atomico 的投资者和合伙人&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;请注意：本文提供的信息基于 Antler 的零投资日方法和我们为全球创始人提供的支持。 我们行业地图中的特色平台来自 Crunchbase。 值得注意的是，其中一些平台可能与 AI 和 Gen-AI 相交。 如果您认为您的平台应该包含在我们未来的映射中，请通过 Ollie.Forsyth@antler.co 与我们联系。&lt;/p&gt;

&lt;h2 id=&quot;什么是-gen-ai&quot;&gt;什么是 Gen-AI？&lt;/h2&gt;

&lt;p&gt;想象这样一个世界，您可以使用生成式辅助工具在几分钟内完成您的项目，而不是花几天时间写一篇博客文章、一周时间创建演示文稿或几个月时间写一篇学术论文。 这些工具不仅帮助我们完成项目，还支持我们做出更好的决策。&lt;/p&gt;

&lt;p&gt;以下是 Gen-AI 平台可能变得多么强大的一个例子：对于那些熟悉我们关于创作者经济的报告的人来说，想象一个世界，在这个世界里，创作者可以将他们的内容上传到任何语言，并用他们自己的声音作为画外音，而不是依赖 在机器人或本地翻译器上。 这是一个美丽的新世界，在这里我们可以获得强大的工具，可以节省我们无数的时间并提高我们的工作效率。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“我们正处于生成人工智能的转折点，原因有二：计算机可以比以往任何时候都更好地创造，而且人们与它们的互动从未如此简单。”——Molly Welch，Radical Ventures 的投资者。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023-01-15-antler-generative-ai-2.jpg&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“在 Media Monks，我们相信生成式 AI 将对我们的行业产生重大影响，尽管很难想象这项惊人技术的真正范围。 我们研究生成式人工智能已有大约五年时间，创新速度呈指数级增长。 技术的进步发生在我们的生产时间表内，范围从 1 到 6 个月不等。 这意味着我们在项目开始时使用的工具在我们上线时已经过时了。” — Media Monks 的创意 AI 设计师兼工程师 Samuel Snider Held。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;人工智能与生成人工智能&quot;&gt;人工智能与生成人工智能&lt;/h2&gt;

&lt;p&gt;人工智能 (AI) 是一个广义术语，指的是任何能够实现智能行为的技术。 这可能包括范围广泛的技术，从可以对数据进行排序的简单算法，到可以模仿类人思维过程的更先进的系统。&lt;/p&gt;

&lt;p&gt;另一方面，生成式人工智能 (Gen-AI) 是一种特定类型的人工智能，专注于生成新内容，例如文本、图像或音乐。 这些系统在大型数据集上进行训练，并使用机器学习算法生成与训练数据相似的新内容。 这在各种应用程序中都很有用，例如创作艺术、音乐，甚至为聊天机器人生成文本。&lt;/p&gt;

&lt;p&gt;从本质上讲，人工智能是一个广义的术语，涵盖了许多不同的技术，而生成人工智能是一种专注于创造新内容的特定类型的人工智能。&lt;/p&gt;

&lt;h2 id=&quot;广阔的机遇正在展开&quot;&gt;广阔的机遇正在展开&lt;/h2&gt;

&lt;p&gt;未来，Gen-AI 很可能会对创意产业产生重大影响。 虽然一些创意可能会被 Gen-AI 系统取代，但其他创意可能会找到新的机会来使用这些系统或创建由 Gen-AI 支持的内容。 在许多情况下，它实际上可以增强创意人员的工作，使他们能够创建更加个性化或独特的内容，或者产生新的想法和概念，如果不使用 AI，这些想法和概念可能是不可能的。&lt;/p&gt;

&lt;p&gt;Gen-AI 对创意人员的一个潜在好处是，它可以使他们能够更快、更高效地创建内容。 例如，作家可以使用 Gen-AI 系统生成文章或故事的草稿，然后他们可以对其进行编辑和完善。 这可以节省时间并让创意人员专注于工作中最重要的方面。&lt;/p&gt;

&lt;p&gt;“生成式 AI 是一股巨大的浪潮，它将在几乎所有行业中产生不可避免的涟漪，对于其中的绝大多数，我们认为这将带来难以置信的增值。我们看到了最大的机会，因为平台是建立在基础之上的 模型，其中用户体验、可访问性和嵌入性将成为这场比赛的关键差异化因素。所有这些都需要由杀手级的上市战略提供动力，最重要的是，速度！下半年将是关键。” ——Stephanie Chan，Samaipata Ventures 投资人。&lt;/p&gt;

&lt;h2 id=&quot;gen-ai的影响&quot;&gt;Gen-AI的影响&lt;/h2&gt;

&lt;p&gt;根据使用方式的不同，这项技术可能会产生许多不同的影响。 例如，Gen-AI 可用于创建新的内容，如音乐或图像，这些内容可用于多种用途，例如为创意者提供更多的灵活性和想象力。 它还可用于通过生成新的训练数据来改进机器学习算法。 总的来说，Gen-AI 的影响肯定是巨大的，因为它有潜力创造新的有用内容并提高机器学习系统的性能。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“我们正在走向人工智能广泛应用的时代。 但广泛可用和实际可用于实现业务成果是两件截然不同的事情。” —Dave Rogenmoser，Jasper 的首席执行官兼联合创始人。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;培训模型在实践中如何运作&quot;&gt;培训模型在实践中如何运作？&lt;/h2&gt;

&lt;p&gt;Gen-AI 训练模型通过从大量示例数据集中学习并使用该知识生成与训练数据集中示例相似的新数据来工作。 这通常是使用一种称为生成模型的机器学习算法来完成的。有许多不同类型的生成模型，每种模型都使用不同的方法来生成新数据。 一些常见类型的生成模型包括生成对抗网络 (GAN)、变分自动编码器 (VAE) 和自回归模型。&lt;/p&gt;

&lt;p&gt;例如，在人脸图像数据集上训练的生成模型可能会学习人脸的一般结构和外观，然后使用这些知识生成新的、以前未见过的看起来真实可信的人脸。&lt;/p&gt;

&lt;p&gt;生成模型用于各种应用程序，包括图像生成、自然语言处理和音乐生成。 它们对于手动生成新数据困难或昂贵的任务特别有用，例如在为产品创建新设计或生成逼真的语音的情况下。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“这些新的基础模型以及建立在其上的应用程序加快了许多行业的步伐：为游戏和社交媒体公司生成创意内容，自动化企业内部的手动流程，帮助扩大以前无法想象的业务，如电影、音乐和漫画制作—— 可能性是无限的。”——Manjot Pahwa，Lightspeed Venture Partners 的投资者&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;语言模型是如何创建的&quot;&gt;语言模型是如何创建的？&lt;/h2&gt;

&lt;p&gt;创建语言模型的方法有多种，但最常见的方法是使用机器学习算法在现有文本的大型数据集上训练模型。 此过程通常包括以下步骤：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;收集现有文本的大型数据集。 此数据集应代表您希望模型能够生成的语言或文本样式。&lt;/li&gt;
  &lt;li&gt;预处理文本数据以清理并准备训练。 这通常涉及将文本标记为单个单词或短语，并将所有单词转换为小写。&lt;/li&gt;
  &lt;li&gt;在预处理的文本数据上训练机器学习算法。 这可以使用多种算法来完成，包括递归神经网络 (RNN) 和长短期记忆 (LSTM) 网络。&lt;/li&gt;
  &lt;li&gt;通过调整模型的参数和超参数以及在必要时使用额外的训练数据来微调训练模型。&lt;/li&gt;
  &lt;li&gt;通过使用经过训练的模型生成示例文本并评估结果来测试模型。 这可以通过将生成的文本与原始训练数据进行比较，或使用其他指标（例如困惑度或 BLEU 分数）来完成。&lt;/li&gt;
  &lt;li&gt;通过重复步骤 4 和 5 来优化模型，直到生成的文本具有高质量并匹配所需的语言或样式。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;“重要的是要注意，创建语言模型需要大量的计算资源和机器学习方面的专业知识——尽管这个空间还很早，但平台正在花费数百万美元来微调他们的产品和服务。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;生成式 AI 类别的创始人当前面临的挑战不仅是要构建产品，还要构建具有持久能力的可防御商业模型。 任何有能力的开发人员都可以围绕这些底层生成引擎包装应用程序皮肤。 解决方案是通过嵌入网络效应、提高转换成本、根深蒂固的产品合作伙伴关系等策略，整合可持续的竞争差异化。”——David Beisel，NextView Ventures 合伙人。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;为什么-gen-ai-存在&quot;&gt;为什么 Gen-AI 存在？&lt;/h2&gt;

&lt;p&gt;Gen-AI 的存在是因为它有可能解决许多重要问题，并为广泛领域的无数新机遇打开大门。 Gen-AI 成为一个不断发展的研发领域的一些关键原因包括：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Gen-AI 可以创造新的内容。 Gen-AI 的主要优势之一是它能够生成新内容，例如文本、图像或音乐。 这可用于创造新的艺术、音乐和其他形式的创造性表达，并生成用于训练机器学习模型的数据。&lt;/li&gt;
  &lt;li&gt;Gen-AI 可以提高效率和生产力。 通过自动生成内容，Gen-AI 可以帮助节省时间并减少人工劳动。 这可以提高各个领域的效率和生产力，从新闻和内容创建到数据注释和分析。&lt;/li&gt;
  &lt;li&gt;Gen-AI 可以提高生成内容的质量。 随着机器学习和自然语言处理的进步，Gen-AI 变得越来越复杂，能够生成人类难以与真实内容区分开来的高质量内容。&lt;/li&gt;
  &lt;li&gt;Gen-AI 可以启用新的应用程序和用途。 Gen-AI 创造新内容的能力为新的应用和用途开辟了许多可能性。 例如，它可用于创建个性化体验，例如个性化新闻文章或个性化音乐推荐。&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;“这并不广为人知。 我的观点是，生成式 AI 模型现在很神奇，因为它们已经能够通过语言接收人们的输入。因为它们能够代表如此多的不同概念——并将它们结合起来——它们可以产生美丽、狂野和创造性的结果。 这令人兴奋、激动，也许还有点可怕。 对于创意人员来说，这意味着通过灵感来寻找灵感，更快地创建原型，并结合模型 (Photoshop++) 的技能来完善作品。’’——Sharon Zhou。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;展望未来gen-ai收入模式&quot;&gt;展望未来——Gen-AI收入模式&lt;/h2&gt;

&lt;p&gt;使用 Gen-AI 技术的公司有几种潜在的收入模式。 一些可能的收入来源包括：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;将技术许可给可以使用它来改进其产品或服务的其他公司或组织。&lt;/li&gt;
  &lt;li&gt;将 AI 系统的输出（例如生成的图像、视频或文本）出售给可以将它们用于各种目的的客户。&lt;/li&gt;
  &lt;li&gt;提供对人工智能系统的访问作为订阅服务，客户可以使用它来生成自己的输出&lt;/li&gt;
  &lt;li&gt;使用 AI 系统提高公司现有产品或服务的效率或有效性，然后向客户收取这些增强产品的费用。&lt;/li&gt;
  &lt;li&gt;创建利用 AI 系统功能的新产品或服务，并将其直接销售给客户。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;为什么现在&quot;&gt;为什么现在？&lt;/h2&gt;

&lt;p&gt;现在是 Gen-AI 时代的几个原因。 首先，机器学习和自然语言处理的进步使人工智能系统能够生成高质量的、类似人类的内容。 其次，艺术、营销和娱乐等领域对个性化和独特内容的需求不断增长，增加了对 Gen-AI 平台的需求。 第三，大量数据和强大计算资源的可用性使得大规模训练和部署这些类型的模型成为可能。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“人们曾承诺人工智能将改变世界，自 2012 年以来我们一直在等待。在过去的两三年里，终于发生了一些变化。 虽然最近围绕生成 AI 的兴奋一直是文本到图像，但我相信 AI 驱动的文本生成将被证明更具变革性。 现在，随着越来越多地使用尖端语言模型，我们看到这项技术扩散到日常产品中——彻底改变了公司开展业务的方式，并重新构想了人类体验技术的方式。”——Aidan Gomez，Cohere 联合创始人兼首席执行官。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023-01-15-antler-generative-ai-3.jpg&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;阅读我们的 Gen-AI 初创公司完整列表（定期更新）&lt;/p&gt;

&lt;p&gt;Gen-AI 类别说明：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;文本：总结或自动化内容。&lt;/li&gt;
  &lt;li&gt;图像：生成图像。&lt;/li&gt;
  &lt;li&gt;音频：总结、生成或转换音频中的文本。&lt;/li&gt;
  &lt;li&gt;视频：生成或编辑视频。&lt;/li&gt;
  &lt;li&gt;代码：生成代码。&lt;/li&gt;
  &lt;li&gt;聊天机器人：自动化客户服务等。&lt;/li&gt;
  &lt;li&gt;机器学习平台：应用程序/机器学习平台。&lt;/li&gt;
  &lt;li&gt;搜索：人工智能驱动的洞察力。&lt;/li&gt;
  &lt;li&gt;游戏：Gen-AI 游戏工作室或应用程序。&lt;/li&gt;
  &lt;li&gt;数据：设计、收集或总结数据。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;gen-ai筹款格局&quot;&gt;Gen-AI筹款格局&lt;/h2&gt;

&lt;p&gt;由于许多投资者专注于 Gen-AI 领域，我们列出了最活跃的投资者：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023-01-15-antler-generative-ai-4.jpg&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;少数投资于 Gen-AI 领域的投资者。 这些投资者也可能投资于后期或早期阶段的公司。&lt;/p&gt;

&lt;h2 id=&quot;gen-ai独角兽格局&quot;&gt;Gen-AI独角兽格局&lt;/h2&gt;

&lt;p&gt;尽管该行业仍在兴起，但一些独角兽已经出现。 到目前为止，2019 年生产了两只独角兽，2020 年生产了一只，2022 年生产了四只。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/src/2023-01-15-antler-generative-ai-5.jpg&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;趋势&quot;&gt;趋势：&lt;/h2&gt;

&lt;h3 id=&quot;gen-ai-如何用于艺术和音乐&quot;&gt;Gen-AI 如何用于艺术和音乐？&lt;/h3&gt;

&lt;p&gt;Gen-AI 正以几种不同的方式用于艺术和音乐。 一个常见的应用是使用生成模型来创造新的艺术和音乐，方法是从头开始生成全新的作品，或者以现有作品为起点并向其中添加新元素。 例如，生成模型可能会在大型绘画数据集上进行训练，然后用于生成与数据集中的作品相似但又独特且原创的新绘画。&lt;/p&gt;

&lt;h3 id=&quot;gen-ai-如何用于游戏&quot;&gt;Gen-AI 如何用于游戏？&lt;/h3&gt;

&lt;p&gt;Gen-AI 正以多种方式用于游戏，包括创建新的关卡或地图、生成新的对话或故事情节，以及创建新的虚拟环境。 例如，游戏可能会使用 Gen-AI 模型来创建一个新的、独特的关卡，供玩家在每次玩游戏时探索，或者根据玩家的动作为非玩家角色生成新的对话选项。 此外，Gen-AI 可用于创建新的、逼真的虚拟环境供玩家探索，例如城市、森林或行星。 总的来说，它可以用来为游戏体验增加一定程度的活力和多样性，使它们对玩家来说更具吸引力和身临其境。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;‘“一般而言，短期的创新领域会非常积极。 众所周知，游戏和在线 3D 体验难以构建——生成式 AI 将彻底颠覆这一现状，让游戏资产的创建变得更加容易。 在游戏中应用生成式 AI 的潜在缺点，或者更确切地说是后果，更为现实。 虽然像 AI 生成的文案或图像创建这样的单维应用程序只是我们执行的现有任务的放大器，但仍然允许我们控制输出的应用程序（即，我们可以决定接受/拒绝一份副本并决定在哪里 使用副本），我们在游戏中与 AI 的交互将更加多维。 随着时间的推移，AI（无论是环境、行为还是 NPC 角色）将进化并适应人类的注意，同样，人类将习惯于在这些 AI 生成的领域中进行社交和定期互动。”——Roblox 的 Annie Zhang。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;生成式-ai-将会如何影响创作者经济&quot;&gt;生成式 AI 将会如何影响创作者经济？&lt;/h3&gt;

&lt;p&gt;创作者经济已经是一个价值 1000 亿美元的行业，正准备持续颠覆，Gen-AI 可能会对创意产生重大影响，尤其是那些创作音乐、艺术或写作的人。 然而，它确实为创作者提供了从第一天起就走向全球的机会，允许他们的内容使用创作者的声音转化为任何语言，或者将他们的创造力转化为更具吸引力的内容。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“生成式 AI 会将创作者变成超级英雄，并扩大他们不那么强大的领域。更多地将其视为创作者的副驾驶，而不是创作者的替代者。” ——Jim Louderback，Inside The Creator Economy 的作者。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;为了让创作者经济取得成功，平台需要适应创作者的个性，以便在内容可能主要由 AI 平台支持时，创作者与他们的粉丝建立某种形式的联系。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“我认为人的因素对于艺术具有价值是必不可少的。 当 AI 生成的艺术是由算法和机器创造的，而不是由具有自己的经验、情感和观点的个人创造时，它可以被视为缺乏通常被视为伟大艺术必不可少的真实性和人性。 这可能会使一些观众难以在情感层面上与 AI 生成的艺术产生联系，从而降低其影响力和重要性。”——创作者 Ivona Tau。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;然而，当我们问创作者 Gen-AI 将对他们产生什么影响时，一位创作者说：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“不多。 也就是说，我正怀着极大的兴趣关注正在发生的事情。 其他人在生成模型的帮助下获得的结果让我深受启发。 你经常听到艺术家将 AI 图像模型称为“工具”，但 AI 不仅仅是一种工具。 它是创意伙伴、合成精灵或鼓舞人心的盟友。”——艺术家詹姆斯·格尼 (James Gurney)。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;这个空间的未来是什么它可能面临什么挑战&quot;&gt;这个空间的未来是什么，它可能面临什么挑战？&lt;/h2&gt;

&lt;p&gt;Gen-AI 面临许多挑战，包括提高这些模型产生的输出的质量和多样性，提高它们生成输出的速度，并使它们更加健壮和可靠。 另一个主要挑战是开发生成式 Gen-AI 模型，这些模型能够更好地理解和整合他们正在处理的数据的底层结构和上下文，以便产生更准确和连贯的输出。 此外，对于生成式人工智能的伦理和社会影响，以及如何确保以负责任和有益的方式使用这些技术，也存在持续的担忧。&lt;/p&gt;

&lt;p&gt;让我们仔细看看其中的一些问题：&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;版权&lt;/strong&gt;。 截至今天，要了解这些平台如何识别真实的原始来源或艺术作品的来源是一项挑战——这些模型是由数亿个数据点训练的。 创作者担心这些平台将如何减轻对创作者作品的版权侵权。 正如我们在 Lauryn Ipsum 发布的最近一个案例中看到的那样，Lensa 应用程序中使用的图像具有原始艺术家签名的背景。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“目前生成人工智能中最紧迫的问题之一是系统可信度。 像 OpenAI 的 ChatGPT 这样的大型语言模型很容易分享不正确或错误的响应。 在图像生成中，系统已经接受了大量图像的训练，系统输出存在版权和知识产权问题，使企业用户不确定将它们集成到产品或工作流程中。”——Molly Welch，Radical Ventures 的投资者。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;学生写论文&lt;/strong&gt;。 随着这些平台变得更加智能，精明的年轻学生将在日常生活中采用它们。 这将如何影响他们的学术工作，他们的教授将如何确定这是否真的是他们的工作？ Gen-AI 将对教育领域产生巨大影响，这还有待观察。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“假设 ChatGPT 模型不断改进，学生使用 chatGPT 来补充学习的机会是无穷无尽的。 学生可以使用它来生成测验和抽认卡的内容，以帮助他们学习、优化现有代码，甚至为学习指南编写摘要。 这里的关键词是补充。 除了他们自己已经投入的原创作品之外，学生还应该使用 ChatGPT。当学生使用 ChatGPT 内容代替他们的作品，甚至提交 ChatGPT 内容作为他们自己的原创想法时，ChatGPT 可能会出现问题。 大学行政部门和学生需要共同努力制定政策，明确说明这个新世界可以接受的内容。 上周我参加了一次开卷考试，明确禁止使用 ChatGPT 或任何其他人工智能支持。” —Cherie Lou，斯坦福大学的创作者和学生。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;虚假信息与错误信息&lt;/strong&gt;。尽管这些系统非常聪明，但有时它们不可避免地会提供错误信息。 例如，最近在英国第 4 频道的一次采访中，主持人向 Open AI 询问他的职业道路，聊天机器人助手给出了不准确的信息。 随着训练模型变得更具适应性并更多地了解我们，最终算法中的错误将会减少。&lt;/p&gt;

&lt;p&gt;Gen-AI 的缺点包括：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;如果训练数据不够多样化或不够具有代表性，则生成的数据存在偏差风险。&lt;/li&gt;
  &lt;li&gt;对生成人工智能在某些行业取代人类劳动的潜力的担忧，导致失业。&lt;/li&gt;
  &lt;li&gt;Gen-AI 被用于恶意目的的可能性，例如制造假新闻或冒充个人。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Gen-AI 有可能取代从设计师到制作人再到艺术家的数百万个工作岗位； 但是，创意总是会在某些方面存在。&lt;/p&gt;

&lt;h2 id=&quot;gen-ai-将影响元宇宙具体如何影响还有待观察&quot;&gt;Gen-AI 将影响元宇宙——具体如何影响还有待观察。&lt;/h2&gt;

&lt;p&gt;很难准确预测生成式 AI 将如何影响元宇宙，因为后者在很大程度上仍是一个理论概念，并且对于它的外观或功能尚无共识。 然而，Gen-AI 将在其创造和发展中发挥重要作用，因为它将允许在虚拟世界中自动生成内容和体验。 这可能会导致更加身临其境和动态的元宇宙，几乎可以无限地提供新的和独特的体验供用户享受。 Gen-AI 也有可能用于在元宇宙中自动执行各种任务，例如管理虚拟经济并确保虚拟世界保持稳定和正常运行。 总体而言，Gen-AI 对元宇宙的影响可能是重大而广泛的。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“人工智能堆栈的不同层级将存在商机，我们已经看到一些商业模式正在出现。 显然，生产像 GPT-3 这样的基础模型非常昂贵和复杂，少数能够做到这一点的公司将获得丰厚的报酬。 但是，有无数机会开发更专业的模型并将通用功能捆绑到特定目标市场需要的东西中。 这相当于垂直SaaS，应用于AI。 我们可能会看到许多支持 AI 的 SaaS 游戏，它们为特定市场提供具有出色 UX 的整体解决方案。在堆栈的更下方，提供正确类型的训练数据，使 ML 工程师能够快速构建专业模型并 确保模型的稳健性都是非常可行的业务。”—Andreas Goeldi，BTOV Ventures 的合伙人。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;让我们一起塑造未来&quot;&gt;让我们一起塑造未来&lt;/h2&gt;

&lt;p&gt;准备好迎接将彻底改变未来工作方式的技术转变！ 我们正处在一个新时代的边缘，成千上万的工作岗位将被改变，新的工作岗位将被创造出来。 这些尖端的 Gen-AI 平台无疑将支持和改善我们的日常生活，但我们需要时间才能完全适应它们。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“这种前所未有的人机协作水平正在如火如荼地进行，无论你身处哪个行业，无论你身处哪个行业，无论谁率先全面整合生成式 AI 方法，游戏现在都向他们开放。”——Gabrielle Chou，副教授 上海纽约大学。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;参考链接&quot;&gt;参考链接&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;https://www.antler.co/blog/generative-ai&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Ollie Forsyth | [译] AI &amp; 麦克船长</name></author><category term="ai" /><category term="AI" /><category term="人工智能" /><category term="生成式AI" /><category term="Generative AI" /><category term="游戏" /><category term="game" /><category term="Gen-AI" /><category term="AIGC" /><summary type="html">随着 ChatGPT 和 DALL-E 的发布，2022 年社交媒体平台上最热门的话题之一在最近几周爆发，引发了关于其对全球人员、职业和行业影响的激烈辩论。 争议的核心是什么？ 生成式 AI (Gen-AI)——可以快速创建新内容的系统，例如大学论文、歌曲和数字艺术作品。 这些能力令人印象深刻，但它们也引发了关于工作的未来以及人类在 AI 主导的世界中的作用的重要问题。 随着生成式人工智能的不断发展，考虑伦理意义和对社会的潜在影响将变得至关重要。 如果创造性工作在很大程度上被人工智能机器取代，会发生什么？</summary></entry></feed>
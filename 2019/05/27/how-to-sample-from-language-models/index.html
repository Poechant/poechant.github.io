<!DOCTYPE html>
<html>

<head>
	<!-- Meta -->
	<meta charset="UTF-8"/>
	<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
	<meta name="generator" content="Jekyll">

	<title>如何从语言模型中抽样：标准采样技术和新核采样的探索</title>
  	<meta name="description" content="麦克船长对于技术、产品、商业等领域的分享|AI,A.I.,NLP,神经网络,人工智能,自然语言处理,BERT,GPT,ChatGPT,OpenAI,阿里巴巴,P9,运营,淘宝,天猫,总监,高管">

	<!-- CSS & fonts -->
	<link rel="stylesheet" href="/css/main.css">

	<!-- RSS -->
	<link href="/atom.xml" type="application/atom+xml" rel="alternate" title="ATOM Feed" />

  	<!-- Favicon -->
 	 <link rel="shortcut icon" type="image/png" href="/img/favicon.png">

 	 <!-- Syntax highlighter -->
  	<link rel="stylesheet" href="/css/syntax.css" />

  	<!--KaTeX-->
  	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
  	<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>
  	<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"></script>
  	<script>
  		document.addEventListener("DOMContentLoaded", function() {
  			renderMathInElement(document.body, {
  				// ...options...
  			});
  		});
  	</script>

  	
  	<!-- KaTeX -->
  	<link rel="stylesheet" href="/assets/plugins/katex.0.11.1/katex.min.css">
  	

</head>

<body>
	<div id="wrap">
	  	
	  	<!-- Navigation -->
	  	<nav id="nav">
	<div id="nav-list">
		<a href="/">Home</a>

		<!-- Nav pages -->
	  <!-- 
	    
	  
	    
	      <a href="/about/" title="关于我">关于我</a>
	    
	  
	    
	  
	    
	  
	    
	      <a href="/booklist/" title="读书行路">读书行路</a>
	    
	  
	    
	  
	    
	      <a href="/categories/" title="Categories">Categories</a>
	    
	  
	    
	  
	    
	  
	    
	  
	    
	  
	    
	  
	    
	  
	    
	  
	    
	  
	    
	  
	    
	   -->

	  <!-- Tech category pages -->






  <a href="/category/ai" title="人工智能">人工智能</a>















  <a href="/category/rt_tech" title="实时技术">实时技术</a>





  <a href="/category/web" title="前端技术">前端技术</a>














<!-- Non-tech category pages -->












  <a href="/category/design" title="设计">设计</a>











  <a href="/category/thinking" title="思考与生活">思考与生活</a>















	  
        
      
        
          <a href="/about/" title="关于我">关于我</a>
        
      
        
      
        
      
        
          <a href="/booklist/" title="读书行路">读书行路</a>
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    <!-- Nav links -->
	  <!-- <a href="https://github.com/thereviewindex/monochrome/archive/master.zip">Download</a>
<a href="https://github.com/thereviewindex/monochrome">Project on Github</a> -->

	</div>
  
  <!-- Nav footer -->
	
	  <footer>
	
	<span>version 1.0.0</span>

</footer>
	

</nav>

    
    <!-- Icon menu -->
	  <a id="nav-menu">
	  	<div id="menu"></div>
	  </a>

      <!-- Header -->
      
        <header id="header" class="parent justify-spaceBetween">
  <div class="inner w100 relative">
    <span class="f-left">  
      <a href="/">
        <h1>
          <span>Mike</span>Captain
        </h1>
      </a>
    </span>
    <span id="nav-links" class="absolute right bottom">

      <!-- Tech category pages -->






  <a href="/category/ai" title="人工智能">人工智能</a>















  <a href="/category/rt_tech" title="实时技术">实时技术</a>





  <a href="/category/web" title="前端技术">前端技术</a>














<!-- Non-tech category pages -->












  <a href="/category/design" title="设计">设计</a>











  <a href="/category/thinking" title="思考与生活">思考与生活</a>















      &nbsp;&nbsp;&nbsp;丨&nbsp;

      <!-- Nav pages -->
      
        
      
        
          <a href="/about/" title="关于我">关于我</a>
        
      
        
      
        
      
        
          <a href="/booklist/" title="读书行路">读书行路</a>
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
      
      <!-- Nav links -->
      <!-- <a href="https://github.com/thereviewindex/monochrome/archive/master.zip">Download</a>
<a href="https://github.com/thereviewindex/monochrome">Project on Github</a> -->

    </span>
  </div>
</header>




      

    <!-- Main content -->
	  <div id="container">
		  
		<main>

			<article id="post-page">
	<h2>如何从语言模型中抽样：标准采样技术和新核采样的探索</h2>		
	<time datetime="2019-05-27T15:24:58+00:00" class="by-line">27 May 2019, 杭州 | Ben Mann | [译] AI & 麦克船长 | 总计 3108 字</time>
	<div class="content">
		<p><strong>本文目录</strong></p>

<ul>
  <li>原文链接：https://towardsdatascience.com/how-to-sample-from-language-models-682bceb97277</li>
</ul>

<p><img src="" alt="" /></p>

<p>↑ 人类通常会选择令语言模型感到惊讶的词（Holtzman 等人，2019 年）</p>

<p>像 GPT-2 这样的因果语言模型被训练来预测在给定上下文的情况下下一个单词的概率。例如，给定「我吃了美味的热 ()」，模型可能以 80% 的概率预测「狗」，以 5% 的概率预测「煎饼」等。这种结构的妙处在于它们可用于生成<strong>任意序列长度</strong>。我可以给模型「我吃了 ()」，从结果分布中抽取一个标记以获得「我吃了一个 ()」，然后再次将其放入模型以获得另一个分布和结果标记，可以一直这样重复下去。事实证明，这一代语言模型往往是，要么陷入重复的循环，要么跑题。为什么会发生这种情况，我们如何更好地采样以生成更像人类的文本？</p>

<p>这篇文章是 Holtzman 等人在 2019 年对<a href="https://arxiv.org/abs/1904.09751">《The Curious Case of Neural Text De generation》</a>的总结和探索。我发现它是我最近读过的最透彻和可读性最强的论文之一，所以如果这篇文章引起共鸣，一定记得去读读它！</p>

<p>如果我们总是对最有可能的词进行采样，标准语言模型训练目标会让我们陷入「我不知道。我不知道。我不知道。」 这是不自然的，但现代语言模型中模型的大部分注意力只集中在最近的几个标记上。相反，流行的生成采样方法是基于从分布中采样的。但是抽样也会遇到一个问题：如果我们有 50K 个可能的选择，即使底部的 25K 个标记每个都极不可能，它们加起来可能具有例如 30% 的概率质量。这意味着对于每个样本，我们有三分之一的机会完全偏离我们的「思路」。由于前面提到的短上下文，这将导致不可恢复的错误级联，因为每个下一个单词都严重依赖于最近的错误单词。</p>

<p>为了对抗尾部采样，最流行的方法是温度（temperature）采样和前 k 采样。</p>

<p>温度采样的灵感来自统计热力学，其中高温意味着更有可能遇到低能量状态。在概率模型中，logits 扮演能量的角色，我们可以通过将 logits 除以温度来实现温度采样，然后将它们输入 softmax 并获得我们的采样概率。例如：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="kn">import</span> <span class="nn">torch</span>
<span class="o">&gt;&gt;&gt;</span> <span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mf">4.</span><span class="p">])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">F</span><span class="p">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">a</span><span class="o">/</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([</span><span class="mf">0.0708</span><span class="p">,</span> <span class="mf">0.1378</span><span class="p">,</span> <span class="mf">0.2685</span><span class="p">,</span> <span class="mf">0.5229</span><span class="p">])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">F</span><span class="p">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([</span><span class="mf">0.0321</span><span class="p">,</span> <span class="mf">0.0871</span><span class="p">,</span> <span class="mf">0.2369</span><span class="p">,</span> <span class="mf">0.6439</span><span class="p">])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">F</span><span class="p">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">a</span><span class="o">/</span><span class="p">.</span><span class="mi">5</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([</span><span class="mf">0.0021</span><span class="p">,</span> <span class="mf">0.0158</span><span class="p">,</span> <span class="mf">0.1171</span><span class="p">,</span> <span class="mf">0.8650</span><span class="p">])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">F</span><span class="p">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">a</span><span class="o">/</span><span class="mf">1e-6</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">])</span>
</code></pre></div></div>

<p>或者直观点：</p>

<p><img src="" alt="" /></p>

<p>较低的温度使模型对其最佳选择更置信，而大于 1 的温度会降低置信。0 温度相当于 argmax/max likelihood，无限大温度对应均匀采样。</p>

<p><strong>Top K Sampling（前 k 采样）</strong>意味着按概率排序并将第 k 个标记以下的任何事物的概率归零。它似乎通过去尾并使其不太可能偏离主题来提升效果。但在某些情况下，确实有很多我们可以合理地从中采样的词（下面的广泛分布），而在某些情况下则没有（下面的狭窄分布）。</p>

<p><img src="" alt="" /></p>

<p>↑ 霍尔兹曼等人 2019</p>

<p>为了解决这个问题，作者提出了「top p sampling」，又名「nucleus sampling」，其中我们计算累积分布并在 CDF 超过 P 时立即切断。在上面的广泛分布示例中，可能需要前 100 个令牌超过 top_p = .9。在窄分布中，我们的样本分布中可能已经超过了 top_p = .9，只有“热”和“暖”。通过这种方式，我们仍然可以避免对严重错误的标记进行采样，但可以在得分最高的标记置信度较低时保持多样性。</p>

<p>为什么最大似然抽样不起作用？在训练过程中，永远不可能看到复合错误。该模型经过训练，可以根据人工生成的上下文预测下一个标记。如果它通过生成错误的分布而导致一个标记错误，则下一个标记将使用独立于上一个预测的“正确”人类生成的上下文。在生成期间，它被迫完成自己自动生成的上下文，这是它在训练期间没有考虑的设置。</p>

<p>定性结果
以下是使用 top_k=40 和上下文“I a​​te a delicious”的示例</p>

<p>以下是使用 top_p=0​​.9 和相同的“我吃了一顿美味”上下文的示例：</p>

<p>在这里自己试试吧！您可以在Runtime &gt; Change runtime type中启用 GPU并获得大批量，无需额外的运行时间。</p>

<p>超越论文：自动选择 p 和 k
我发现很难确定这些样本中的哪一个更像人类。为此我设计了一个实验来确定top_k和top_p凭经验。</p>

<p>我们的目标是使用 top_k 和 top_p 来最大化选择我们提供的实际下一个单词的概率。在搜索最佳 k 和 p 值时，实际上很容易通过分析确定给定样本。对于 k，我们找到出现“黄金”标记的排序索引。对于 p，我们找到黄金代币的 CDF。例如，如果上下文是“I ate a delicious hot”，而实际单词是“dog”，但模型的预测分布最有可能是“pancake”，我们将搜索概率，直到在以下位置找到“dog”索引 3。在索引 1 处，CDF 可能为 62%。在索引 3 处，CDF 可能约为 86%，因此我们将其记录为最佳 p。</p>

<p>在许多示例中，我们可以计算最佳 p 和 k 值的直方图，并计算它们的汇总统计量。我在维基百科的随机部分进行了测试，上下文长度为 15。这比模型训练的长度 (1024) 短得多，但对于https://duet.li或聊天机器人等设置很常见。</p>

<p>===== ks =====
最高 29094.00
均值 233.69
中位数 3.00
只有 13376.00
===== ps =====
最大 1.00
均值 0.59
中位数 0.60
只有 13376.00
随意在我的colab notebook中自己尝试。</p>

<p>如果模型在其训练集上进行评估，则选择 top_k = 1 是最佳选择。但是由于模型稍微超出了域，因此最有可能的标记有时会出现在列表的更下方。此外，我们还有 50K 的 token 词汇表。在许多数据集中，我们永远不会看到所有标记，但模型对此并不确定。通过使用 top_p 或 top_k 将大部分概率质量归零，我们合并了我们的先验，从不选择这些从未见过的甚至在训练中的标记。</p>

<p>也就是说，这种对 k 和 p 的搜索仍然在模型的世界观的背景下，因此它只是一个创可贴。我们真正想要的是修复训练。</p>

<p>固定训练
我也开始考虑改变训练目标以更好地匹配生成任务。例如，当模型生成看起来不像人类的整个序列时，我们是否可以训练某种鉴别器来惩罚模型？如何将 GAN 架构应用于非连续域并不简单。我遇到了Adversarial Text Generation without Reinforcement Learning和RL-based idea，但似乎这些还没有成为主流。我认为将这些想法应用于过去几个月席卷最先进技术的大型变形金刚会很有趣。</p>


	</div>
</article>



	  </main>
		
		  <!-- Pagination links -->
      

	  </div>
	    
	    <!-- Footer -->
	    <footer><span>@2022 - MikeCaptain.com</span></footer>


	    <!-- Script -->
      <script src="/js/main.js"></script>	


	</div>
</body>
</html>

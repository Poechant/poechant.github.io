---
layout: post
title: 【编译】图解 RLHF（人工反馈的强化学习）
date:   2023-02-12 23:24:58 +0800
categories: ai
tags: [AI, 人工智能, NLP, 自然语言处理, 神经网络, LLM, 大型语言模型, 语言模型, 大模型, AGI, 通用人工智能]
description: 
excerpt: 
katex: True
location: 杭州
author: 麦克船长
---

* 原文链接：[Illustrating Reinforcement Learning from Human Feedback (RLHF)](https://huggingface.co/blog/rlhf)
* 原文作者：Nathan Lambert, Leandro von Werra

在过去的几年中，语言模型通过根据人类输入提示生成多样化且引人注目的文本显示出令人印象深刻的能力。 然而，什么才是「好」文本本质上很难定义，因为它是主观的并且依赖于上下文的。有许多应用程序，例如编写创意故事、理应真实的信息片段，或者期待可执行的代码片段。

似乎很难编写一个损失函数来解决问题，而且大多数语言模型（Language Models，LM）仍然在使用简单的、预测下一个标记的损失函数（例如 Cross Entropy 交叉熵）进行训练。为了弥补损失函数的缺点，人们定义了更易捕捉人类偏好的指标，例如 BLEU 或 ROUGE。她们虽然比损失函数更适合衡量模型的表现，但这些指标只是用简单的规则比较一下生成文本与示例数据，因此也有局限性。如果我们使用生成文本的人工反馈作来衡量表现，或者更进一步使用该反馈作代替损失函数来优化模型，那不是更好吗？这就是从人类反馈中强化学习（Reinforcement Learning with Human Feedback，RLHF）的想法； 使用 RL 方法直接优化带有 HF 的语言模型。RLHF 使语言模型能够开始将在一般文本数据语料库上训练的模型与复杂人类价值观的模型对齐。

RLHF 最近的成功应用是 ChatGPT。我们也直接问了 ChatGPT 如何解释 RLHF：

![image](/img/src/2023/2023-02-12-illustrated-rlhf-1.png)

它回答得非常好，但是并未涵盖所有内容。那么这篇文章我们就和大家展开聊聊。

## 一、RLHF：让我们一步步地看下

人工反馈的强化学习（Reinforcement learning from Human Feedback，RLHF），有时也被称为「人工偏好（preferences）的强化学习」是一个充满挑战的概念，因为它引入了多模型训练过程和部署的不同阶段。在这篇博文中，我们将把训练过程拆解为三个核心步骤：

1. 预训练一个语言模型（LM）；
2. 收集数据并训练一个奖励模型（RM）；
3. 以强化学习的训练方法精调 LM。

我们先看看怎么预训练 LM 的。

### 1、语言模型（LM）的预训练

As a starting point RLHF use a language model that has already been pretrained with the classical pretraining objectives (see this blog post for more details). OpenAI used a smaller version of GPT-3 for its first popular RLHF model, InstructGPT. Anthropic used transformer models from 10 million to 52 billion parameters trained for this task. DeepMind used their 280 billion parameter model Gopher.

This initial model can also be fine-tuned on additional text or conditions, but does not necessarily need to be. For example, OpenAI fine-tuned on human-generated text that was “preferable” and Anthropic generated their initial LM for RLHF by distilling an original LM on context clues for their “helpful, honest, and harmless” criteria. These are both sources of what I refer to as expensive, augmented data, but it is not a required technique to understand RLHF.

一般来说，对于哪种模型最适合作为 RLHF 的起点，并没有明确的答案。这将是本博客的一个议题 ———— RLHF 训练中一些可能性的设计并没有得到比较彻底的探索。

接下来，使用语言模型生成数据来训练一个 RM，通过这个过程人类的偏好就被整合进系统里了。

![](/img/src/2023/2023-02-12-illustrated-rlhf-2.png){: width="490" }

### 2、奖励模型的训练

Generating a reward model (RM, also referred to as a preference model) calibrated with human preferences is where the relatively new research in RLHF begins. The underlying goal is to get a model or system that takes in a sequence of text, and returns a scalar reward which should numerically represent the human preference. The system can be an end-to-end LM, or a modular system outputting a reward (e.g. a model ranks outputs, and the ranking is converted to reward). The output being a scalar reward is crucial for existing RL algorithms being integrated seamlessly later in the RLHF process.

These LMs for reward modeling can be both another fine-tuned LM or a LM trained from scratch on the preference data. For example, Anthropic uses a specialized method of fine-tuning to initialize these models after pretraining (preference model pretraining, PMP) because they found it be more sample efficient than fine-tuning, but no one variation of reward modeling is considered the clear best choice today.

The training dataset of prompt-generation pairs for the RM is generated by sampling a set of prompts from a predefined dataset (Anthropic’s data generated primarily with a chat tool on Amazon Mechanical Turk is available on the Hub, and OpenAI used prompts submitted by users to the GPT API). The prompts are passed through the initial language model to generate new text.

Human annotators are used to rank the generated text outputs from the LM. One may initially think that humans should apply a scalar score directly to each piece of text in order to generate a reward model, but this is difficult to do in practice. The differing values of humans cause these scores to uncalibrated and noisy. Instead, rankings are used to compare the outputs of multiple models and create a much better regularized dataset.

There are multiple methods for ranking the text. One method that has been successful is to have users compare generated text from two language models conditioned on the same prompt. By comparing model outputs in head-to-head matchups, an Elo system can be used to generate a ranking of the models and outputs relative to each-other. These different methods of ranking are normalized into a scalar reward signal for training.

An interesting artifact of this process is that the successful RLHF systems to date have used reward language models with varying sizes relative to the text generation (e.g. OpenAI 175B LM, 6B reward model, Anthropic used LM and reward models from 10B to 52B, DeepMind uses 70B Chinchilla models for both LM and reward). An intuition would be that these preference models need to have similar capacity to understand the text given to them as a model would need in order to generate said text.

此时在 RLHF 系统中，我们有一个可用于生成文本的初始 LM 和一个接收任何文本并为其分配人类感知程度分数的偏好模型。接下来，我们使用强化学习 (RL) 来针对 RM 优化原始的 LM。

![image](/img/src/2023/2023-02-12-illustrated-rlhf-3.png)

### 3、强化学习精调

曾经很长时间内，出于工程和算法原因，用强化学习训练语言模型被认为是不可能的。而现在很多公司或研究机构，已经在用策略梯度 RL 算法、近端策略优化 (PPO) 微调初始 LM 的部分或全部参数了。LM 的参数被冻结，因为微调整个 10B 或 100B+ 参数模型的成本过高（有关更多信息，请参阅 LM 的低秩适应 (LoRA) 或 DeepMind 的 Sparrow LM）。 PPO 已经存在了相对较长的时间 - 有大量关于其工作原理的指南。 这种方法的相对成熟度使其成为扩展到 RLHF 分布式训练新应用的有利选择。 事实证明，RLHF 的许多核心 RL 进步一直在弄清楚如何使用熟悉的算法更新如此大的模型（稍后会详细介绍）。

Parameters of the LM are frozen because fine-tuning an entire 10B or 100B+ parameter model is prohibitively expensive (for more, see Low-Rank Adaptation (LoRA) for LMs or the Sparrow LM from DeepMind). PPO has been around for a relatively long time – there are tons of guides on how it works. The relative maturity of this method made it a favorable choice for scaling up to the new application of distributed training for RLHF. 事实证明，RLHF 的许多核心 RL 优化，都在试图用用我们熟悉的算法更新这么大的模型（稍后会详细介绍）。

让我们首先将此微调任务表述为 RL 问题。首先，the policy is a language model that takes in a prompt and returns a sequence of text (or just probability distributions over text). The action space of this policy is all the tokens corresponding to the vocabulary of the language model (often on the order of 50k tokens) and the observation space is the possible input token sequences, which is also quite large (size of vocabulary ^ number of input tokens). The reward function is a combination of the preference model and a constraint on policy shift.

我们讨论过的所有模型在 RLHF 训练方法中，都交汇于奖励函数。给定数据集中的提示 x，生成两个文本 y1 和 y2 —— 其中一个来自初始语言模型，另一个来自精调策略（fine-tuned policy）的当前迭代。当前精调策略生成的文本被输入给 RM，并得到一个表示偏好度的反馈值 {% raw %}$$ r_{\theta} $${% endraw %}。将该文本与来自初始 LM 生成的文本进行比较，计算它们的差异作为惩罚。在 OpenAI、Anthropic 和 DeepMind 的多篇论文中，这个惩罚被设计为这些文本 tokens 的分布序列间 KL 散度的缩放版 {% raw %} $$ r_{KL} $$ {% endraw %}。KL 散度项惩罚 RL 策略在每个训练批次中大幅偏离初始预训练模型，这有助于确保模型输出合理连贯的文本片段。如果没有这种惩罚，继续这样优化可能会导致出现文本乱码，即愚弄 RM 以提供高奖励。在实践中，KL 散度是通过从两个分布中采样来近似的（[John Schulman 的博文中有解释](http://joschu.net/blog/kl-approx.html)）。发送到 RL 更新规则的最终奖励是 {% raw %} $$ r=r_\theta-\lambda \cdot r_{KL} $$ {% endraw %}.

一些 RLHF 系统在奖励函数中添加了额外的项。例如，OpenAI 通过将额外的预训练梯度（来自人类标注数据集）混合到 PPO 的更新规则中，在 InstructGPT 上成功进行了实验。随着 RLHF 的进一步研究，这种奖励函数的公式可能会继续演进。

最后，更新规则是来自 PPO 的参数更新，它最大化当前 batch 数据中的奖励指标（PPO 是 on-policy，这意味着参数只用当前 batch 的 prompt-generation pairts 更新）。PPO 是一种信赖域优化算法，它使用梯度约束来确保更新步骤不会破坏学习过程的稳定性。DeepMind 对 Gopher 使用了类似的奖励设置，但使用 synchronous advantage actor-critic（A2C）来优化梯度，这明显不同但未在 DeepMind 公司外部复现过。

![image](/img/src/2023/2023-02-12-illustrated-rlhf-4.png)

Optionally, RLHF can continue from this point by iteratively updating the reward model and the policy together. As the RL policy updates, users can continue ranking these outputs versus the model's earlier versions. Most papers have yet to discuss implementing this operation, as the deployment mode needed to collect this type of data only works for dialogue agents with access to an engaged user base. Anthropic discusses this option as Iterated Online RLHF (see the original paper), where iterations of the policy are included in the ELO ranking system across models. This introduces complex dynamics of the policy and reward model evolving, which represents a complex and open research question.

## 二、开源的 RLHF 工具

The first code released to perform RLHF on LMs was from OpenAI in TensorFlow in 2019.

Today, there are already a few active repositories for RLHF in PyTorch that grew out of this. The primary repositories are Transformers Reinforcement Learning (TRL), TRLX which originated as a fork of TRL, and Reinforcement Learning for Language models (RL4LMs).

TRL is designed to fine-tune pretrained LMs in the Hugging Face ecosystem with PPO. TRLX is an expanded fork of TRL built by CarperAI to handle larger models for online and offline training. At the moment, TRLX has an API capable of production-ready RLHF with PPO and Implicit Language Q-Learning ILQL at the scales required for LLM deployment (e.g. 33 billion parameters). Future versions of TRLX will allow for language models up to 200B parameters. As such, interfacing with TRLX is optimized for machine learning engineers with experience at this scale.

RL4LMs offers building blocks for fine-tuning and evaluating LLMs with a wide variety of RL algorithms (PPO, NLPO, A2C and TRPO), reward functions and metrics. Moreover, the library is easily customizable, which allows training of any encoder-decoder or encoder transformer-based LM on any arbitrary user-specified reward function. Notably, it is well-tested and benchmarked on a broad range of tasks in recent work amounting up to 2000 experiments highlighting several practical insights on data budget comparison (expert demonstrations vs. reward modeling), handling reward hacking and training instabilities, etc. RL4LMs current plans include distributed training of larger models and new RL algorithms.

Both TRLX and RL4LMs are under heavy further development, so expect more features beyond these soon.

Hub 上有一个由 Anthropic 创建的大型数据集。

## 三、RLHF 将走向何方？

While these techniques are extremely promising and impactful and have caught the attention of the biggest research labs in AI, there are still clear limitations. The models, while better, can still output harmful or factually inaccurate text without any uncertainty. This imperfection represents a long-term challenge and motivation for RLHF – operating in an inherently human problem domain means there will never be a clear final line to cross for the model to be labeled as complete.

When deploying a system using RLHF, gathering the human preference data is quite expensive due to the mandatory and thoughtful human component. RLHF performance is only as good as the quality of its human annotations, which takes on two varieties: human-generated text, such as fine-tuning the initial LM in InstructGPT, and labels of human preferences between model outputs.

Generating well-written human text answering specific prompts is very costly, as it often requires hiring part-time staff (rather than being able to rely on product users or crowdsourcing). Thankfully, the scale of data used in training the reward model for most applications of RLHF (~50k labeled preference samples) is not as expensive. However, it is still a higher cost than academic labs would likely be able to afford. Currently, there only exists one large-scale dataset for RLHF on a general language model (from Anthropic) and a couple of smaller-scale task-specific datasets (such as summarization data from OpenAI). The second challenge of data for RLHF is that human annotators can often disagree, adding a substantial potential variance to the training data without ground truth.

With these limitations, huge swaths of unexplored design options could still enable RLHF to take substantial strides. Many of these fall within the domain of improving the RL optimizer. PPO is a relatively old algorithm, but there are no structural reasons that other algorithms could offer benefits and permutations on the existing RLHF workflow. One large cost of the feedback portion of fine-tuning the LM policy is that every generated piece of text from the policy needs to be evaluated on the reward model (as it acts like part of the environment in the standard RL framework). To avoid these costly forward passes of a large model, offline RL could be used as a policy optimizer. Recently, new algorithms have emerged, such as implicit language Q-learning (ILQL) [Talk on ILQL at CarperAI], that fit particularly well with this type of optimization. Other core trade-offs in the RL process, like exploration-exploitation balance, have also not been documented. Exploring these directions would at least develop a substantial understanding of how RLHF functions and, if not, provide improved performance.

We hosted a lecture on Tuesday 13 December 2022 that expanded on this post; you can watch it here!

### 延伸阅读

以下是迄今为止关于 RLHF 最流行的论文列表。该领域最近随着深度强化学习（DeepRL）在 2017 年左右的出现而得到普及，并已发展成为对许多大型科技公司 LLM 应用领域更广泛的研究。 

以下是一些早于 LM 焦点的关于 RLHF 的论文：

* TAMER: Training an Agent Manually via Evaluative Reinforcement (Knox and Stone 2008): Proposed a learned agent where humans provided scores on the actions taken iteratively to learn a reward model.
* Interactive Learning from Policy-Dependent Human Feedback (MacGlashan et al. 2017): Proposed an actor-critic algorithm, COACH, where human feedback (both positive and negative) is used to tune the advantage function.
* Deep Reinforcement Learning from Human Preferences (Christiano et al. 2017): RLHF applied on preferences between Atari trajectories.
* Deep TAMER: Interactive Agent Shaping in High-Dimensional State Spaces (Warnell et al. 2018): Extends the TAMER framework where a deep neural network is used to model the reward prediction.

以下这些论文展示了 RLHF 在 LM 方面的表现：

* Fine-Tuning Language Models from Human Preferences (Zieglar et al. 2019): An early paper that studies the impact of reward learning on four specific tasks.
* Learning to summarize with human feedback (Stiennon et al., 2020): RLHF applied to the task of summarizing text. Also, Recursively Summarizing Books with Human Feedback (OpenAI Alignment Team 2021), follow on work summarizing books.
* WebGPT: Browser-assisted question-answering with human feedback (OpenAI, 2021): Using RLHF to train an agent to navigate the web.
* InstructGPT: Training language models to follow instructions with human feedback (OpenAI Alignment Team 2022): RLHF applied to a general language model [Blog post on InstructGPT].
* GopherCite: Teaching language models to support answers with verified quotes (Menick et al. 2022): Train a LM with RLHF to return answers with specific citations.
* Sparrow: Improving alignment of dialogue agents via targeted human judgements (Glaese et al. 2022): Fine-tuning a dialogue agent with RLHF
* [《ChatGPT: Optimizing Language Models for Dialogue》](https://openai.com/blog/chatgpt/) （OpenAI 2022）：用 RLHF 方法训练 LM 以用于通用聊天机器人（All-Purpose Chatbot）。
* Scaling Laws for Reward Model Overoptimization (Gao et al. 2022): studies the scaling properties of the learned preference model in RLHF.
* Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback (Anthropic, 2022): A detailed documentation of training a LM assistant with RLHF.
* Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned (Ganguli et al. 2022): A detailed documentation of efforts to “discover, measure, and attempt to reduce [language models] potentially harmful outputs.”
* Dynamic Planning in Open-Ended Dialogue using Reinforcement Learning (Cohen at al. 2022): Using RL to enhance the conversational skill of an open-ended dialogue agent.
* Is Reinforcement Learning (Not) for Natural Language Processing?: Benchmarks, Baselines, and Building Blocks for Natural Language Policy Optimization (Ramamurthy and Ammanabrolu et al. 2022): Discusses the design space of open-source tools in RLHF and proposes a new algorithm NLPO (Natural Language Policy Optimization) as an alternative to PPO.


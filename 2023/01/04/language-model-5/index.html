<!DOCTYPE html>
<html>

<head>
	<!-- Meta -->
	<meta charset="UTF-8"/>
	<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
	<meta name="generator" content="Jekyll">

	<title>麦克船长 NLP 语言模型技术笔记 5：注意力机制（Attention Mechanism）</title>
  	<meta name="description" content="基于 RNN 的 Encoder-Decoder 模型存在无法处理过长文本、并行性差的两大痛点。2015 年 Bahdanau 等人在其论文中提出 Attention 机制，再到 2017 年 Transformer 模型的论文《Attention is All You Need》横空出世，其并行速度极快，而且每两个词之间的词间距都是 1。此后 NLP 领域 Transformer 彻底成为主流。如果你已经了解 Encoder-Decoder 模型，本文将基于此带你深入浅出的搞清楚 Attention、Transformer。">

	<!-- CSS & fonts -->
	<link rel="stylesheet" href="/css/main.css">

	<!-- RSS -->
	<link href="/atom.xml" type="application/atom+xml" rel="alternate" title="ATOM Feed" />

  	<!-- Favicon -->
 	 <link rel="shortcut icon" type="image/png" href="/img/favicon.png">

 	 <!-- Syntax highlighter -->
  	<link rel="stylesheet" href="/css/syntax.css" />

  	<!--KaTeX-->
  	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
  	<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>
  	<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"></script>
  	<script>
  		document.addEventListener("DOMContentLoaded", function() {
  			renderMathInElement(document.body, {
  				// ...options...
  			});
  		});
  	</script>

  	
  	<!-- KaTeX -->
  	<link rel="stylesheet" href="/assets/plugins/katex.0.11.1/katex.min.css">
  	

</head>

<body>
	<div id="wrap">
	  	
	  	<!-- Navigation -->
	  	<nav id="nav">
	<div id="nav-list">
		<a href="/">Home</a>

		<!-- Nav pages -->
	  <!-- 
	    
	  
	    
	      <a href="/about/" title="关于我">关于我</a>
	    
	  
	    
	  
	    
	      <a href="/booklist/" title="我的书单">我的书单</a>
	    
	  
	    
	      <a href="/categories/" title="Categories">Categories</a>
	    
	  
	    
	  
	    
	  
	    
	  
	    
	  
	    
	  
	    
	  
	    
	  
	    
	  
	    
	  
	    
	   -->

	  <!-- Tech category pages -->






  <a href="/category/ai" title="人工智能">人工智能</a>











  <a href="/category/rt_tech" title="实时技术">实时技术</a>





  <a href="/category/web" title="前端技术">前端技术</a>














<!-- Non-tech category pages -->


















  <a href="/category/thinking" title="思考与生活">思考与生活</a>















	  
        
      
        
          <a href="/about/" title="关于我">关于我</a>
        
      
        
      
        
          <a href="/booklist/" title="我的书单">我的书单</a>
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    <!-- Nav links -->
	  <!-- <a href="https://github.com/thereviewindex/monochrome/archive/master.zip">Download</a>
<a href="https://github.com/thereviewindex/monochrome">Project on Github</a> -->

	</div>
  
  <!-- Nav footer -->
	
	  <footer>
	
	<span>version 1.0.0</span>

</footer>
	

</nav>

    
    <!-- Icon menu -->
	  <a id="nav-menu">
	  	<div id="menu"></div>
	  </a>

      <!-- Header -->
      
        <header id="header" class="parent justify-spaceBetween">
  <div class="inner w100 relative">
    <span class="f-left">  
      <a href="/">
        <h1>
          <span>Mike</span>Captain
        </h1>
      </a>
    </span>
    <span id="nav-links" class="absolute right bottom">

      <!-- Tech category pages -->






  <a href="/category/ai" title="人工智能">人工智能</a>











  <a href="/category/rt_tech" title="实时技术">实时技术</a>





  <a href="/category/web" title="前端技术">前端技术</a>














<!-- Non-tech category pages -->


















  <a href="/category/thinking" title="思考与生活">思考与生活</a>















      &nbsp;&nbsp;&nbsp;丨&nbsp;

      <!-- Nav pages -->
      
        
      
        
          <a href="/about/" title="关于我">关于我</a>
        
      
        
      
        
          <a href="/booklist/" title="我的书单">我的书单</a>
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
      
      <!-- Nav links -->
      <!-- <a href="https://github.com/thereviewindex/monochrome/archive/master.zip">Download</a>
<a href="https://github.com/thereviewindex/monochrome">Project on Github</a> -->

    </span>
  </div>
</header>




      

    <!-- Main content -->
	  <div id="container">
		  
		<main>

			<article id="post-page">
	<h2>麦克船长 NLP 语言模型技术笔记 5：注意力机制（Attention Mechanism）</h2>		
	<time datetime="2023-01-04T18:13:09+00:00" class="by-line">04 Jan 2023, 杭州 | 作者 麦克船长 | 总计 14548 字</time>
	<div class="content">
		<p><strong>本文目录</strong></p>
<ul id="markdown-toc">
  <li><a href="#一为什么说-rnn-模型没有体现注意力" id="markdown-toc-一为什么说-rnn-模型没有体现注意力">一、为什么说 RNN 模型没有体现「注意力」？</a></li>
  <li><a href="#二基于-attention-机制的-encoder-decoder-模型" id="markdown-toc-二基于-attention-机制的-encoder-decoder-模型">二、基于 Attention 机制的 Encoder-Decoder 模型</a></li>
  <li><a href="#三transformer-在-2017-年横空出世" id="markdown-toc-三transformer-在-2017-年横空出世">三、Transformer 在 2017 年横空出世</a>    <ul>
      <li><a href="#1自注意力机制self-attention" id="markdown-toc-1自注意力机制self-attention">1、自注意力机制（Self-Attention）</a>        <ul>
          <li><a href="#11一段自然语言内容其自身就暗含很多内部关联信息" id="markdown-toc-11一段自然语言内容其自身就暗含很多内部关联信息">1.1、一段自然语言内容，其自身就「暗含」很多内部关联信息</a></li>
          <li><a href="#12如何计算-qkv" id="markdown-toc-12如何计算-qkv">1.2、如何计算 Q、K、V</a></li>
          <li><a href="#13注意力函数如何通过-qv-得到-z" id="markdown-toc-13注意力函数如何通过-qv-得到-z">1.3、注意力函数：如何通过 Q、V 得到 Z</a></li>
          <li><a href="#14其他注意力函数" id="markdown-toc-14其他注意力函数">1.4、其他注意力函数</a></li>
        </ul>
      </li>
      <li><a href="#2多头注意力" id="markdown-toc-2多头注意力">2、多头注意力</a></li>
      <li><a href="#3退化现象残差网络与-short-cut" id="markdown-toc-3退化现象残差网络与-short-cut">3、退化现象、残差网络与 Short-Cut</a>        <ul>
          <li><a href="#31退化现象" id="markdown-toc-31退化现象">3.1、退化现象</a></li>
          <li><a href="#32恒等映射" id="markdown-toc-32恒等映射">3.2、恒等映射</a></li>
          <li><a href="#33残差网络residual-network与捷径short-cut" id="markdown-toc-33残差网络residual-network与捷径short-cut">3.3、残差网络（Residual Network）与捷径（Short-Cut）</a></li>
        </ul>
      </li>
      <li><a href="#4位置编码positional-embedding" id="markdown-toc-4位置编码positional-embedding">4、位置编码（Positional Embedding）</a>        <ul>
          <li><a href="#41transformer-论文中的三角式位置编码sinusoidal-positional-encoding" id="markdown-toc-41transformer-论文中的三角式位置编码sinusoidal-positional-encoding">4.1、Transformer 论文中的三角式位置编码（Sinusoidal Positional Encoding）</a></li>
          <li><a href="#42绝对位置编码" id="markdown-toc-42绝对位置编码">4.2、绝对位置编码</a></li>
          <li><a href="#43相对位置编码和其他位置编码" id="markdown-toc-43相对位置编码和其他位置编码">4.3、相对位置编码和其他位置编码</a></li>
        </ul>
      </li>
      <li><a href="#5编码器-encoder-和解码器-decoder" id="markdown-toc-5编码器-encoder-和解码器-decoder">5、编码器 Encoder 和解码器 Decoder</a>        <ul>
          <li><a href="#51encoder-和-decoder-的图示结构" id="markdown-toc-51encoder-和-decoder-的图示结构">5.1、Encoder 和 Decoder 的图示结构</a></li>
          <li><a href="#52decoder-的第一个输出结果" id="markdown-toc-52decoder-的第一个输出结果">5.2、Decoder 的第一个输出结果</a></li>
          <li><a href="#53decoder-后续的所有输出" id="markdown-toc-53decoder-后续的所有输出">5.3、Decoder 后续的所有输出</a></li>
          <li><a href="#54decoder-之后的-linear-和-softmax" id="markdown-toc-54decoder-之后的-linear-和-softmax">5.4、Decoder 之后的 Linear 和 Softmax</a></li>
        </ul>
      </li>
      <li><a href="#6transformer-模型整体" id="markdown-toc-6transformer-模型整体">6、Transformer 模型整体</a></li>
      <li><a href="#6transformer-的性能" id="markdown-toc-6transformer-的性能">6、Transformer 的性能</a></li>
      <li><a href="#7来看一段用-pytorch-实现的-transformer-示例" id="markdown-toc-7来看一段用-pytorch-实现的-transformer-示例">7、来看一段用 PyTorch 实现的 Transformer 示例</a></li>
    </ul>
  </li>
  <li><a href="#参考" id="markdown-toc-参考">参考</a></li>
</ul>

<h3 id="一为什么说-rnn-模型没有体现注意力">一、为什么说 RNN 模型没有体现「注意力」？</h3>

<p>Encoder-Decoder 的一个非常严重的问题，是依赖中间那个 context 向量，则无法处理特别长的输入序列 —— 记忆力不足，会忘事儿。而忘事儿的根本原因，是没有「注意力」。</p>

<p>对于一般的 RNN 模型，Encoder-Decoder 结构并没有体现「注意力」—— 这句话怎么理解？当输入序列经过 Encoder 生成的中间结果（上下文 C），被喂给 Decoder 时，这些中间结果对所生成序列里的哪个词，都没有区别（没有特别关照谁）。这相当于在说：输入序列里的每个词，对于生成任何一个输出的词的影响，是一样的，而不是输出某个词时是聚焦特定的一些输入词。这就是模型没有注意力机制。</p>

<p>人脑的注意力模型，其实是资源分配模型。NLP 领域的注意力模型，是在 2014 年被提出的，后来逐渐成为 NLP 领域的一个广泛应用的机制。可以应用的场景，比如对于一个电商平台中很常见的白底图，其边缘的白色区域都是无用的，那么就不应该被关注（关注权重为 0）。比如机器翻译中，翻译词都是对局部输入重点关注的。</p>

<p>所以 Attention 机制，就是在 Decoder 时，不是所有输出都依赖相同的「上下文  \(\bm{C}_t\) 」，而是时刻 t 的输出，使用  \(\bm{C}_t\) ，而这个  \(\bm{C}_t\)  来自对每个输入数据项根据「注意力」进行的加权。</p>

<h3 id="二基于-attention-机制的-encoder-decoder-模型">二、基于 Attention 机制的 Encoder-Decoder 模型</h3>

<p>2015 年 Dzmitry Bahdanau 等人在论文<a href="https://arxiv.org/abs/1409.0473">《Neural Machine Translation by Jointly Learning to Align and Translate》</a> 中提出了「Attention」机制，下面请跟着麦克船长，我会深入浅出地为你解释清楚。</p>

<p>下图中  \(e_i\)  表示编码器的隐藏层输出， \(d_i\)  表示解码器的隐藏层输出</p>

<div style="text-align: center;">
<div class="graphviz-wrapper">

<!-- Generated by graphviz version 2.43.0 (0)
 -->
<!-- Title: G Pages: 1 -->
<svg role="img" aria-label="graphviz-f66c634a9c7c02915e5610af76c3b1b7" width="436pt" height="336pt" viewBox="0.00 0.00 436.00 336.00">
<title>graphviz-f66c634a9c7c02915e5610af76c3b1b7</title>
<desc>
digraph G {
	rankdir=BT
	splines=ortho
	{rank=same e1 e2 eddd en}
	{rank=same d1 d2 dddd dt0 dt dddd2}

	eddd[label=&quot;...&quot;]
	dddd[label=&quot;...&quot;]
	xddd[label=&quot;...&quot;]
	yddd[label=&quot;...&quot;]
	dt[label=&quot;d_t&quot;]
	dt0[label=&quot;d_t-1&quot;]
	yt[label=&quot;y_t&quot;]
	yt0[label=&quot;y_t-1&quot;]
	Ct[shape=plaintext]
	x1[shape=plaintext]
	x2[shape=plaintext]
	xddd[shape=plaintext]
	xn[shape=plaintext]
	y1[shape=plaintext]
	y2[shape=plaintext]
	yddd[shape=plaintext]
	dddd2[shape=plaintext, label=&quot;&quot;]
	Ct[label=&quot;C_t&quot;, shape=&quot;square&quot;]

	x1 -&gt; e1
	x2 -&gt; e2
	xddd -&gt; eddd
	xn -&gt; en

	e1 -&gt; e2
	e2 -&gt; eddd
	eddd -&gt; en

	Ct -&gt; dt

	d1 -&gt; y1
	d2 -&gt; y2
	dddd -&gt; yddd
	dt0 -&gt; yt0
	dt -&gt; yt

	d1 -&gt; d2
	d2 -&gt; dddd
	dddd -&gt; dt0
	dt0 -&gt; dt

	e1 -&gt; Ct
	e2 -&gt; Ct
	eddd -&gt; Ct
	en -&gt; Ct

	dt -&gt; dddd2
	dt0 -&gt; Ct
}
</desc>

<g id="graph0" class="graph" transform="scale(1 1) rotate(0) translate(4 332)">
<title>G</title>
<polygon fill="white" stroke="transparent" points="-4,4 -4,-332 432,-332 432,4 -4,4" />
<!-- e1 -->
<g id="node1" class="node">
<title>e1</title>
<ellipse fill="none" stroke="black" cx="181" cy="-90" rx="27" ry="18" />
<text text-anchor="middle" x="181" y="-86.3" font-family="Times,serif" font-size="14.00">e1</text>
</g>
<!-- e2 -->
<g id="node2" class="node">
<title>e2</title>
<ellipse fill="none" stroke="black" cx="253" cy="-90" rx="27" ry="18" />
<text text-anchor="middle" x="253" y="-86.3" font-family="Times,serif" font-size="14.00">e2</text>
</g>
<!-- e1&#45;&gt;e2 -->
<g id="edge5" class="edge">
<title>e1&#45;&gt;e2</title>
<path fill="none" stroke="black" d="M208.22,-90C208.22,-90 215.74,-90 215.74,-90" />
<polygon fill="black" stroke="black" points="215.74,-93.5 225.74,-90 215.74,-86.5 215.74,-93.5" />
</g>
<!-- Ct -->
<g id="node15" class="node">
<title>Ct</title>
<polygon fill="none" stroke="black" points="309,-184 269,-184 269,-144 309,-144 309,-184" />
<text text-anchor="middle" x="289" y="-160.3" font-family="Times,serif" font-size="14.00">C_t</text>
</g>
<!-- e1&#45;&gt;Ct -->
<g id="edge18" class="edge">
<title>e1&#45;&gt;Ct</title>
<path fill="none" stroke="black" d="M203,-100.6C203,-121.06 203,-164 203,-164 203,-164 258.62,-164 258.62,-164" />
<polygon fill="black" stroke="black" points="258.62,-167.5 268.62,-164 258.62,-160.5 258.62,-167.5" />
</g>
<!-- eddd -->
<g id="node3" class="node">
<title>eddd</title>
<ellipse fill="none" stroke="black" cx="325" cy="-90" rx="27" ry="18" />
<text text-anchor="middle" x="325" y="-86.3" font-family="Times,serif" font-size="14.00">...</text>
</g>
<!-- e2&#45;&gt;eddd -->
<g id="edge6" class="edge">
<title>e2&#45;&gt;eddd</title>
<path fill="none" stroke="black" d="M280.22,-90C280.22,-90 287.74,-90 287.74,-90" />
<polygon fill="black" stroke="black" points="287.74,-93.5 297.74,-90 287.74,-86.5 287.74,-93.5" />
</g>
<!-- e2&#45;&gt;Ct -->
<g id="edge19" class="edge">
<title>e2&#45;&gt;Ct</title>
<path fill="none" stroke="black" d="M274.5,-100.92C274.5,-100.92 274.5,-133.82 274.5,-133.82" />
<polygon fill="black" stroke="black" points="271,-133.82 274.5,-143.82 278,-133.82 271,-133.82" />
</g>
<!-- en -->
<g id="node4" class="node">
<title>en</title>
<ellipse fill="none" stroke="black" cx="397" cy="-90" rx="27" ry="18" />
<text text-anchor="middle" x="397" y="-86.3" font-family="Times,serif" font-size="14.00">en</text>
</g>
<!-- eddd&#45;&gt;en -->
<g id="edge7" class="edge">
<title>eddd&#45;&gt;en</title>
<path fill="none" stroke="black" d="M352.22,-90C352.22,-90 359.74,-90 359.74,-90" />
<polygon fill="black" stroke="black" points="359.74,-93.5 369.74,-90 359.74,-86.5 359.74,-93.5" />
</g>
<!-- eddd&#45;&gt;Ct -->
<g id="edge20" class="edge">
<title>eddd&#45;&gt;Ct</title>
<path fill="none" stroke="black" d="M303.5,-100.92C303.5,-100.92 303.5,-133.82 303.5,-133.82" />
<polygon fill="black" stroke="black" points="300,-133.82 303.5,-143.82 307,-133.82 300,-133.82" />
</g>
<!-- en&#45;&gt;Ct -->
<g id="edge21" class="edge">
<title>en&#45;&gt;Ct</title>
<path fill="none" stroke="black" d="M399,-108.29C399,-130.21 399,-164 399,-164 399,-164 319.18,-164 319.18,-164" />
<polygon fill="black" stroke="black" points="319.18,-160.5 309.18,-164 319.18,-167.5 319.18,-160.5" />
</g>
<!-- d1 -->
<g id="node5" class="node">
<title>d1</title>
<ellipse fill="none" stroke="black" cx="27" cy="-238" rx="27" ry="18" />
<text text-anchor="middle" x="27" y="-234.3" font-family="Times,serif" font-size="14.00">d1</text>
</g>
<!-- d2 -->
<g id="node6" class="node">
<title>d2</title>
<ellipse fill="none" stroke="black" cx="99" cy="-238" rx="27" ry="18" />
<text text-anchor="middle" x="99" y="-234.3" font-family="Times,serif" font-size="14.00">d2</text>
</g>
<!-- d1&#45;&gt;d2 -->
<g id="edge14" class="edge">
<title>d1&#45;&gt;d2</title>
<path fill="none" stroke="black" d="M54.22,-238C54.22,-238 61.74,-238 61.74,-238" />
<polygon fill="black" stroke="black" points="61.74,-241.5 71.74,-238 61.74,-234.5 61.74,-241.5" />
</g>
<!-- y1 -->
<g id="node19" class="node">
<title>y1</title>
<text text-anchor="middle" x="27" y="-306.3" font-family="Times,serif" font-size="14.00">y1</text>
</g>
<!-- d1&#45;&gt;y1 -->
<g id="edge9" class="edge">
<title>d1&#45;&gt;y1</title>
<path fill="none" stroke="black" d="M27,-256.17C27,-256.17 27,-281.59 27,-281.59" />
<polygon fill="black" stroke="black" points="23.5,-281.59 27,-291.59 30.5,-281.59 23.5,-281.59" />
</g>
<!-- dddd -->
<g id="node7" class="node">
<title>dddd</title>
<ellipse fill="none" stroke="black" cx="171" cy="-238" rx="27" ry="18" />
<text text-anchor="middle" x="171" y="-234.3" font-family="Times,serif" font-size="14.00">...</text>
</g>
<!-- d2&#45;&gt;dddd -->
<g id="edge15" class="edge">
<title>d2&#45;&gt;dddd</title>
<path fill="none" stroke="black" d="M126.22,-238C126.22,-238 133.74,-238 133.74,-238" />
<polygon fill="black" stroke="black" points="133.74,-241.5 143.74,-238 133.74,-234.5 133.74,-241.5" />
</g>
<!-- y2 -->
<g id="node20" class="node">
<title>y2</title>
<text text-anchor="middle" x="99" y="-306.3" font-family="Times,serif" font-size="14.00">y2</text>
</g>
<!-- d2&#45;&gt;y2 -->
<g id="edge10" class="edge">
<title>d2&#45;&gt;y2</title>
<path fill="none" stroke="black" d="M99,-256.17C99,-256.17 99,-281.59 99,-281.59" />
<polygon fill="black" stroke="black" points="95.5,-281.59 99,-291.59 102.5,-281.59 95.5,-281.59" />
</g>
<!-- dt0 -->
<g id="node8" class="node">
<title>dt0</title>
<ellipse fill="none" stroke="black" cx="250" cy="-238" rx="33.6" ry="18" />
<text text-anchor="middle" x="250" y="-234.3" font-family="Times,serif" font-size="14.00">d_t&#45;1</text>
</g>
<!-- dddd&#45;&gt;dt0 -->
<g id="edge16" class="edge">
<title>dddd&#45;&gt;dt0</title>
<path fill="none" stroke="black" d="M198.19,-238C198.19,-238 206.2,-238 206.2,-238" />
<polygon fill="black" stroke="black" points="206.2,-241.5 216.2,-238 206.2,-234.5 206.2,-241.5" />
</g>
<!-- yddd -->
<g id="node12" class="node">
<title>yddd</title>
<text text-anchor="middle" x="171" y="-306.3" font-family="Times,serif" font-size="14.00">...</text>
</g>
<!-- dddd&#45;&gt;yddd -->
<g id="edge11" class="edge">
<title>dddd&#45;&gt;yddd</title>
<path fill="none" stroke="black" d="M171,-256.17C171,-256.17 171,-281.59 171,-281.59" />
<polygon fill="black" stroke="black" points="167.5,-281.59 171,-291.59 174.5,-281.59 167.5,-281.59" />
</g>
<!-- dt -->
<g id="node9" class="node">
<title>dt</title>
<ellipse fill="none" stroke="black" cx="329" cy="-238" rx="27" ry="18" />
<text text-anchor="middle" x="329" y="-234.3" font-family="Times,serif" font-size="14.00">d_t</text>
</g>
<!-- dt0&#45;&gt;dt -->
<g id="edge17" class="edge">
<title>dt0&#45;&gt;dt</title>
<path fill="none" stroke="black" d="M283.96,-238C283.96,-238 291.98,-238 291.98,-238" />
<polygon fill="black" stroke="black" points="291.98,-241.5 301.98,-238 291.98,-234.5 291.98,-241.5" />
</g>
<!-- yt0 -->
<g id="node14" class="node">
<title>yt0</title>
<ellipse fill="none" stroke="black" cx="250" cy="-310" rx="33.29" ry="18" />
<text text-anchor="middle" x="250" y="-306.3" font-family="Times,serif" font-size="14.00">y_t&#45;1</text>
</g>
<!-- dt0&#45;&gt;yt0 -->
<g id="edge12" class="edge">
<title>dt0&#45;&gt;yt0</title>
<path fill="none" stroke="black" d="M250,-256.17C250,-256.17 250,-281.59 250,-281.59" />
<polygon fill="black" stroke="black" points="246.5,-281.59 250,-291.59 253.5,-281.59 246.5,-281.59" />
</g>
<!-- dt0&#45;&gt;Ct -->
<g id="edge23" class="edge">
<title>dt0&#45;&gt;Ct</title>
<path fill="none" stroke="black" d="M276.4,-226.44C276.4,-226.44 276.4,-194.12 276.4,-194.12" />
<polygon fill="black" stroke="black" points="279.9,-194.12 276.4,-184.12 272.9,-194.12 279.9,-194.12" />
</g>
<!-- dddd2 -->
<g id="node10" class="node">
<title>dddd2</title>
</g>
<!-- dt&#45;&gt;dddd2 -->
<g id="edge22" class="edge">
<title>dt&#45;&gt;dddd2</title>
<path fill="none" stroke="black" d="M356.22,-238C356.22,-238 363.74,-238 363.74,-238" />
<polygon fill="black" stroke="black" points="363.74,-241.5 373.74,-238 363.74,-234.5 363.74,-241.5" />
</g>
<!-- yt -->
<g id="node13" class="node">
<title>yt</title>
<ellipse fill="none" stroke="black" cx="329" cy="-310" rx="27" ry="18" />
<text text-anchor="middle" x="329" y="-306.3" font-family="Times,serif" font-size="14.00">y_t</text>
</g>
<!-- dt&#45;&gt;yt -->
<g id="edge13" class="edge">
<title>dt&#45;&gt;yt</title>
<path fill="none" stroke="black" d="M329,-256.17C329,-256.17 329,-281.59 329,-281.59" />
<polygon fill="black" stroke="black" points="325.5,-281.59 329,-291.59 332.5,-281.59 325.5,-281.59" />
</g>
<!-- xddd -->
<g id="node11" class="node">
<title>xddd</title>
<text text-anchor="middle" x="325" y="-14.3" font-family="Times,serif" font-size="14.00">...</text>
</g>
<!-- xddd&#45;&gt;eddd -->
<g id="edge3" class="edge">
<title>xddd&#45;&gt;eddd</title>
<path fill="none" stroke="black" d="M325,-36.17C325,-36.17 325,-61.59 325,-61.59" />
<polygon fill="black" stroke="black" points="321.5,-61.59 325,-71.59 328.5,-61.59 321.5,-61.59" />
</g>
<!-- Ct&#45;&gt;dt -->
<g id="edge8" class="edge">
<title>Ct&#45;&gt;dt</title>
<path fill="none" stroke="black" d="M305.5,-184.22C305.5,-184.22 305.5,-218.8 305.5,-218.8" />
<polygon fill="black" stroke="black" points="302,-218.8 305.5,-228.8 309,-218.8 302,-218.8" />
</g>
<!-- x1 -->
<g id="node16" class="node">
<title>x1</title>
<text text-anchor="middle" x="181" y="-14.3" font-family="Times,serif" font-size="14.00">x1</text>
</g>
<!-- x1&#45;&gt;e1 -->
<g id="edge1" class="edge">
<title>x1&#45;&gt;e1</title>
<path fill="none" stroke="black" d="M181,-36.17C181,-36.17 181,-61.59 181,-61.59" />
<polygon fill="black" stroke="black" points="177.5,-61.59 181,-71.59 184.5,-61.59 177.5,-61.59" />
</g>
<!-- x2 -->
<g id="node17" class="node">
<title>x2</title>
<text text-anchor="middle" x="253" y="-14.3" font-family="Times,serif" font-size="14.00">x2</text>
</g>
<!-- x2&#45;&gt;e2 -->
<g id="edge2" class="edge">
<title>x2&#45;&gt;e2</title>
<path fill="none" stroke="black" d="M253,-36.17C253,-36.17 253,-61.59 253,-61.59" />
<polygon fill="black" stroke="black" points="249.5,-61.59 253,-71.59 256.5,-61.59 249.5,-61.59" />
</g>
<!-- xn -->
<g id="node18" class="node">
<title>xn</title>
<text text-anchor="middle" x="397" y="-14.3" font-family="Times,serif" font-size="14.00">xn</text>
</g>
<!-- xn&#45;&gt;en -->
<g id="edge4" class="edge">
<title>xn&#45;&gt;en</title>
<path fill="none" stroke="black" d="M397,-36.17C397,-36.17 397,-61.59 397,-61.59" />
<polygon fill="black" stroke="black" points="393.5,-61.59 397,-71.59 400.5,-61.59 393.5,-61.59" />
</g>
</g>
</svg>
</div>
</div>

<p>更进一步细化关于  \(\bm{C}_t\)  部分，我们引用《基于深度学习的道路短期交通状态时空序列预测》一书中的图：</p>

<p><img src="/img/src/2023-01-04-captain-nlp-5.png" alt="image" /></p>

<p>这个图里的  \(\widetilde{h}_i\)  与上一个图里的  \(d_i\)  对应， \(h_i\)  与上一个图里的  \(e_i\)  对应。</p>

<p>针对时刻  \(t\)  要产出的输出，隐藏层每一个隐藏细胞都与  \(\bm{C}_t\)  有一个权重关系  \(\alpha_{t,i}\)  其中  \(1\le i\le n\) ，这个权重值与「输入项经过编码器后隐藏层后的输出 \(e_i（1\le i\le n）\) 、解码器的前一时刻隐藏层输出  \(d_{t-1}\) 」两者有关：</p>

\[\begin{aligned}
&amp;s_{i,t} = score(\bm{e}_i,\bm{d}_{t-1}) \\
&amp;\alpha_{i,t} = \frac{exp(s_{i,t})}{\textstyle\sum_{j=1}^n exp(s_{j,t})}
\end{aligned}\]

<p>常用的  \(score\)  函数有：</p>

<ul>
  <li>点积（Dot Product）模型： \(s_{i,t} = {\bm{d}_{t-1}}^T \cdot \bm{e}_i\)</li>
  <li>缩放点积（Scaled Dot-Product）模型： \(s_{i,t} = \frac{{\bm{d}_{t-1}}^T \cdot \bm{e}_i}{\sqrt{\smash[b]{dimensions\:of\:d_{t-1}\:or\:e_i}}}\) ，可避免因为向量维度过大导致点积结果太大</li>
</ul>

<p>然后上下文向量就表示成：</p>

\[\begin{aligned}
&amp;\bm{C}_t = \displaystyle\sum_{i=1}^n \alpha_{i,t} \bm{e}_i
\end{aligned}\]

<p>还记得 RNN 那部分里我们讲到的 Encoder-Decoder 模型的公式表示吗？</p>

\[\begin{aligned}
e_t &amp;= Encoder_{LSTM/GRU}(x_t, e_{t-1}) \\
\bm{C} &amp;= f_1(e_n) \\
d_t &amp;= f_2(d_{t-1}, \bm{C}) \\
y_t &amp;= Decoder_{LSTM/GRU}(y_{t-1}, d_{t-1}, \bm{C})
\end{aligned}\]

<p>加入 Attention 机制的 Encoder-Decoder 模型如下：</p>

\[\begin{aligned}
e_t &amp;= Encoder_{LSTM/GRU}(x_t, e_{t-1}) \\
\bm{C}_t &amp;= f_1(e_1,e_2...e_n,d_{t-1}) \\
d_t &amp;= f_2(d_{t-1}, \bm{C}_t) \\
y_t &amp;= Decoder_{LSTM/GRU}(y_{t-1}, d_{t-1}, \bm{C}_t)
\end{aligned}\]

<p>可以看到最核心的区别是第二个公式  \(C_t\) 。加入 Attention 后，对所有数据给予不同的注意力分布。具体地，比如我们用如下的函数来定义这个模型：</p>

\[\begin{aligned}
\bm{e} &amp;= tanh(\bm{W}^{xe} \cdot \bm{x} + \bm{b}^{xe}) \\
s_{i,t} &amp;= score(\bm{e}_i,\bm{d}_{t-1}) \\
\alpha_{i,t} &amp;= \frac{e^{s_{i,t}}}{\textstyle\sum_{j=1}^n e^{s_{j,t}}} \\
\bm{C}_t &amp;= \displaystyle\sum_{i=1}^n \alpha_{i,t} \bm{e}_i \\
\bm{d}_t &amp;= tanh(\bm{W}^{dd} \cdot \bm{d}_{t-1} + \bm{b}^{dd} +
				 \bm{W}^{yd} \cdot \bm{y}_{t-1} + \bm{b}^{yd} +
				 \bm{W}^{cd} \cdot \bm{C}_t + \bm{b}^{cd}) \\
\bm{y} &amp;= Softmax(\bm{W}^{dy} \cdot \bm{d} + \bm{b}^{dy})
\end{aligned}\]

<p>到这里你能发现注意力机制的什么问题不？</p>

<ul>
  <li>这个注意力机制忽略了位置信息。比如 Tigers love rabbits 和 Rabbits love tigers 会产生一样的注意力分数。</li>
</ul>

<h3 id="三transformer-在-2017-年横空出世">三、Transformer 在 2017 年横空出世</h3>

<p>我们先通过一个动画来看下 Transformer 是举例示意，该图来自 Google 的博客文章 <a href="https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html">《Transformer: A Novel Neural Network Architecture for Language Understanding》</a>：</p>

<p><img src="/img/src/2023-01-04-language-model-5-11.gif" alt="image" /></p>

<p>中文网络里找到的解释得比较好的 blogs、answers，几乎都指向了同一篇博客：Jay Alammar 的<a href="http://jalammar.github.io/illustrated-transformer/">《The Illustrated Transformer》</a>，所以建议读者搭配该篇文章阅读。</p>

<p>Transformer 模型中用到了自注意力（Self-Attention）、多头注意力（Multiple-Head Attention）、残差网络（ResNet）与捷径（Short-Cut）。下面我们先通过第 1 到第 4 小节把几个基本概念讲清楚，然后在第 5 小节讲解整体 Transformer 模型就会好理解很多了。最后第 6 小节我们来一段动手实践。</p>

<h4 id="1自注意力机制self-attention">1、自注意力机制（Self-Attention）</h4>

<p>自注意力是理解 Transformer 的关键，原作者在论文中限于篇幅，没有给出过多的解释。以下是我自己的理解，能够比较通透、符合常识地去理解 Transformer 中的一些神来之笔的概念。</p>

<h5 id="11一段自然语言内容其自身就暗含很多内部关联信息">1.1、一段自然语言内容，其自身就「暗含」很多内部关联信息</h5>

<p>在加入了 Attention 的 Encoder-Decoder 模型中，对输出序列 Y 中的一个词的注意力来自于输入序列 X，那么如果 X 和 Y 相等呢？什么场景会有这个需求？因为我们认为一段文字里某些词就是由于另外某些词而决定的，可以粗暴地理解为「完形填空」的原理。那么这样一段文字，其实就存在其中每个词的自注意力，举个例子：</p>

<blockquote>
  <p>老王是我的主管，我很喜欢他的平易近人。</p>
</blockquote>

<p>对这句话里的「他」，如果基于这句话计算自注意力的话，显然应该给予「老王」最多的注意力。受此启发，我们认为：</p>

<blockquote>
  <p>一段自然语言中，其实暗含了：为了得到关于某方面信息 Q，可以通过关注某些信息 K，进而得到某些信息（V）作为结果。</p>
</blockquote>

<p>Q 就是 query 检索/查询，K、V 分别是 key、value。所以类似于我们在图书检索系统里搜索「NLP书籍」（这是 Q），得到了一本叫《自然语言处理实战》的电子书，书名就是 key，这本电子书就是 value。只是对于自然语言的理解，我们认为任何一段内容里，都自身暗含了很多潜在 Q-K-V 的关联。这是整体受到信息检索领域里 query-key-value 的启发的。</p>

<p>基于这个启发，我们将自注意力的公式表示为：</p>

\[\begin{aligned}
Z = SelfAttention(X) = Attention(Q,K,V)
\end{aligned}\]

<p>X 经过自注意力计算后，得到的「暗含」了大量原数据内部信息的 Z。然后我们拿着这个带有自注意力信息的 Z 进行后续的操作。这里要强调的是，Z 向量中的每个元素 z_i 都与 X 的所有元素有某种关联，而不是只与 x_i 有关联。</p>

<h5 id="12如何计算-qkv">1.2、如何计算 Q、K、V</h5>

<p>Q、K、V 全部来自输入 X 的线性变换：</p>

\[\begin{aligned}
Q &amp;= W^Q \cdot X \\
K &amp;= W^K \cdot X \\
V &amp;= W^V \cdot X
\end{aligned}\]

<p>\(W^Q、W^K、W^V\)  以随机初始化开始，经过训练就会得到非常好的表现。对于  \(X\)  中的每一个词向量  \(x_i\) ，经过这个变换后得到了：</p>

\[\begin{aligned}
q_i &amp;= W^Q \cdot x_i \\
k_i &amp;= W^K \cdot x_i \\
v_i &amp;= W^V \cdot x_i
\end{aligned}\]

<h5 id="13注意力函数如何通过-qv-得到-z">1.3、注意力函数：如何通过 Q、V 得到 Z</h5>

<p>基于上面的启发，我们认为 X 经过自注意力的挖掘后，得到了：</p>

<ul>
  <li>暗含信息 1：一组 query 与一组 key 之间的关联，记作 qk（想一下信息检索系统要用 query 先招到 key）</li>
  <li>暗含信息 2：一组 value</li>
  <li>暗含信息 3：qk 与 value 的某种关联</li>
</ul>

<p>这三组信息，分别如何表示呢？这里又需要一些启发了，因为计算机科学其实是在「模拟还原」现实世界，在 AI 的领域目前的研究方向就是模拟还原人脑的思考。所以这种「模拟还原」都是寻找某一种近似方法，因此不能按照数学、物理的逻辑推理来理解，而应该按照「工程」或者「计算科学」来理解，想想我们大学时学的「计算方法」这门课，因此常需要一些启发来找到某种「表示」。</p>

<p>这里 Transformer 的作者，认为  \(Q\)  和  \(K\)  两个向量之间的关联，是我们在用  \(Q\)  找其在  \(K\)  上的投影，如果  \(Q\) 、 \(K\)  是单位长度的向量，那么这个投影其实可以理解为找「 \(Q\)  和  \(K\)  向量之间的相似度」：</p>

<ul>
  <li>如果  \(Q\)  和  \(K\)  垂直，那么两个向量正交，其点积（Dot Product）为 0；</li>
  <li>如果  \(Q\)  和  \(K\)  平行，那么两个向量点积为两者模积  \(\|Q\|\|K\|\) ；</li>
  <li>如果  \(Q\)  和  \(K\)  呈某个夹角，则点积就是  \(Q\)  在  \(K\)  上的投影的模。</li>
</ul>

<p>因此「暗含信息 1」就可以用「 \(Q\cdot K\) 」再经过 Softmax 归一化来表示。这个表示，是一个所有元素都是 0~1 的矩阵，可以理解成对应注意力机制里的「注意力分数」，也就是一个「注意力分数矩阵（Attention Score Matrix）」。</p>

<p>而「暗含信息 2」则是输入  \(X\)  经过的线性变换后的特征，看做  \(X\)  的另一种表示。然后我们用这个「注意力分数矩阵」来加持一下  \(V\) ，这个点积过程就表示了「暗含信息 3」了。所以我们有了如下公式：</p>

\[\begin{aligned}
Z = Attention(Q,K,V) = Softmax(Q \cdot K^T) \cdot V
\end{aligned}\]

<p>其实到这里，这个注意力函数已经可以用了。有时候，为了避免因为向量维度过大，导致  \(Q \cdot K^T\)  点积结果过大，我们再加一步处理：</p>

\[\begin{aligned}
Z = Attention(Q,K,V) = Softmax(\frac{Q \cdot K^T}{\sqrt{\smash[b]{d_k}}}) \cdot V
\end{aligned}\]

<p>这里  \(d_k\)  是 K 矩阵中向量  \(k_i\)  的维度。这一步修正还有进一步的解释，即如果经过 Softmax 归一化后模型稳定性存在问题。怎么理解？如果假设 Q 和 K 中的每个向量的每一维数据都具有零均值、单位方差，这样输入数据是具有稳定性的，那么如何让「暗含信息 1」计算后仍然具有稳定性呢？即运算结果依然保持零均值、单位方差，就是除以「 \(\sqrt{\smash[b]{d_k}}\) 」。</p>

<p>到这里我们注意到：</p>

<ul>
  <li>K、V 里的每一个向量，都是</li>
</ul>

<h5 id="14其他注意力函数">1.4、其他注意力函数</h5>

<p>为了提醒大家这种暗含信息的表示，都只是计算方法上的一种选择，好坏全靠结果评定，所以包括上面的在内，常见的注意力函数有（甚至你也可以自己定义）：</p>

\[Z = Attention(Q,K,V) =
\begin{cases}
\begin{aligned}
&amp;= Softmax(Q^T K) V \\
&amp;= Softmax(\frac{Q K^T}{\sqrt{\smash[b]{d_k}}}) V \\
&amp;= Softmax(\omega^T tanh(W[q;k])) V \\
&amp;= Softmax(Q^T W K) V \\
&amp;= cosine[Q^T K] V
\end{aligned}
\end{cases}\]

<p>到这里，我们就从原始的输入  \(X\)  得到了一个包含自注意力信息的  \(Z\)  了，后续就可以用  \(Z\)  了。</p>

<h4 id="2多头注意力">2、多头注意力</h4>

<p>到这里我们理解了「自注意力」，而 Transformer 这篇论文通过添加「多头」注意力的机制进一步提升了注意力层。我们先看下它是什么，然后看下它的优点。从本小节开始，本文大量插图引用自<a href="http://jalammar.github.io/illustrated-transformer/">《The Illustrated Transformer》</a>，作者 Jay Alammar 写出一篇非常深入浅出的图解文章，被大量引用，非常出色，再次建议大家去阅读。</p>

<p>Transformer 中用了 8 个头，也就是 8 组不同的 Q-K-V：</p>

\[\begin{aligned}
Q_0 = W_0^Q \cdot X ;\enspace K_0 = &amp;W_0^K \cdot X ;\enspace V_0 = W_0^V \cdot X \\
Q_1 = W_1^Q \cdot X ;\enspace K_1 = &amp;W_0^K \cdot X ;\enspace V_1 = W_1^V \cdot X \\
&amp;.... \\
Q_7 = W_7^Q \cdot X ;\enspace K_7 = &amp;W_0^K \cdot X ;\enspace V_7 = W_7^V \cdot X
\end{aligned}\]

<p>这样我们就能得到 8 个 Z：</p>

\[\begin{aligned}
&amp;Z_0 = Attention(Q_0,K_0,V_0) = Softmax(\frac{Q_0 \cdot K_0^T}{\sqrt{\smash[b]{d_k}}}) \cdot V_0 \\
&amp;Z_1 = Attention(Q_1,K_1,V_1) = Softmax(\frac{Q_1 \cdot K_1^T}{\sqrt{\smash[b]{d_k}}}) \cdot V_1 \\
&amp;... \\
&amp;Z_7 = Attention(Q_7,K_7,V_7) = Softmax(\frac{Q_7 \cdot K_7^T}{\sqrt{\smash[b]{d_k}}}) \cdot V_7 \\
\end{aligned}\]

<p>然后我们把  \(Z_0\)  到  \(Z_7\)  沿着行数不变的方向全部连接起来，如下图所示：</p>

<p><img src="/img/src/2023-01-04-language-model-5-3.png" alt="image" width="464" /></p>

<p>我们再训练一个权重矩阵  \(W^O\) ，然后用上面拼接的  \(Z_{0~7}\)  乘以这个权重矩阵：</p>

<p><img src="/img/src/2023-01-04-language-model-5-4.png" alt="image" width="135" /></p>

<p>于是我们会得到一个 Z 矩阵：</p>

<p><img src="/img/src/2023-01-04-language-model-5-5.png" alt="image" width="100" /></p>

<p>到这里就是多头注意力机制的全部内容，与单头注意力相比，都是为了得到一个 Z 矩阵，但是多头用了多组 Q-K-V，然后经过拼接、乘以权重矩阵得到最后的 Z。我们总览一下整个过程：</p>

<p><img src="/img/src/2023-01-04-language-model-5-6.png" alt="image" width="935" /></p>

<p>通过多头注意力，每个头都会关注到不同的信息，可以如下类似表示：</p>

<p><img src="/img/src/2023-01-04-language-model-5-7.png" alt="image" width="400" /></p>

<p>这通过两种方式提高了注意力层的性能：</p>

<ul>
  <li>多头注意力机制，扩展了模型关注不同位置的能力。 \(Z\)  矩阵中的每个向量  \(z_i\)  包含了与  \(X\)  中所有向量  \(x_i\)  有关的一点编码信息。反过来说，不要认为  \(z_i\)  只与  \(x_i\)  有关。</li>
  <li>多头注意力机制，为注意力层提供了多个「表示子空间 Q-K-V」，以及 Z。这样一个输入矩阵  \(X\) ，就会被表示成 8 种不同的矩阵 Z，都包含了原始数据信息的某种解读暗含其中。</li>
</ul>

<h4 id="3退化现象残差网络与-short-cut">3、退化现象、残差网络与 Short-Cut</h4>

<h5 id="31退化现象">3.1、退化现象</h5>

<p>对于一个 56 层的神经网路，我们很自然地会觉得应该比 20 层的神经网络的效果要好，比如说从误差率（error）的量化角度看。但是华人学者何凯明等人的论文<a href="https://arxiv.org/pdf/1512.03385.pdf">《Deep Residual Learning for Image Recognition》</a>中给我们呈现了相反的结果，而这个问题的原因并不是因为层数多带来的梯度爆炸/梯度消失（毕竟已经用了归一化解决了这个问题），而是因为一种反常的现象，这种现象我们称之为「退化现象」。何凯明等人认为这是因为存在「难以优化好的网络层」。</p>

<h5 id="32恒等映射">3.2、恒等映射</h5>

<p>如果这 36 层还帮了倒忙，那还不如没有，是不是？所以这多出来的 36 个网络层，如果对于提升性能（例如误差率）毫无影响，甚至更进一步，这 36 层前的输入数据，和经过这 36 层后的输出数据，完全相同，那么如果将这 36 层抽象成一个函数  \(f_{36}\) ，这就是一个恒等映射的函数：</p>

\[f_{36}(x) = x\]

<p>回到实际应用中。如果我们对于一个神经网络中的连续 N 层是提升性能，还是降低性能，是未知的，那么则可以建立一个跳过这些层的连接，实现：</p>

<blockquote>
  <p>如果这 N 层可以提升性能，则采用这 N 层；否则就跳过。</p>
</blockquote>

<p>这就像给了这 N 层神经网络一个试错的空间，待我们确认它们的性能后再决定是否采用它们。同时也可以理解成，这些层可以去单独优化，如果性能提升，则不被跳过。</p>

<h5 id="33残差网络residual-network与捷径short-cut">3.3、残差网络（Residual Network）与捷径（Short-Cut）</h5>

<p>如果前面 20 层已经可以实现 99% 的准确率，那么引入了这 36 层能否再提升「残差剩余那 1%」的准确率从而达到 100% 呢？所以这 36 层的网络，就被称为「残差网络（Residual Network，常简称为 ResNet）」，这个叫法非常形象。</p>

<p>而那个可以跳过 N 层残差网络的捷径，则常被称为 Short-Cut，也会被叫做跳跃链接（Skip Conntection），这就解决了上述深度学习中的「退化现象」。</p>

<h4 id="4位置编码positional-embedding">4、位置编码（Positional Embedding）</h4>

<p>还记得我在第二部分最后提到的吗：</p>

<blockquote>
  <p>这个注意力机制忽略了位置信息。比如 Tigers love rabbits 和 Rabbits love tigers 会产生一样的注意力分数。</p>
</blockquote>

<h5 id="41transformer-论文中的三角式位置编码sinusoidal-positional-encoding">4.1、Transformer 论文中的三角式位置编码（Sinusoidal Positional Encoding）</h5>

<p>现在我们来解决这个问题，为每一个输入向量  \(x_i\)  生成一个位置编码向量  \(t_i\) ，这个位置编码向量的维度，与输入向量（词的嵌入式向量表示）的维度是相同的：</p>

<p><img src="/img/src/2023-01-04-language-model-5-8.png" alt="image" width="500" /></p>

<p>Transformer 论文中给出了如下的公式，来计算位置编码向量的每一位的值：</p>

\[\begin{aligned}
P_{pos,2i} &amp;= sin(\frac{pos}{10000^{\frac{2i}{d_{model}}}}) \\
P_{pos,2i+1} &amp;= cos(\frac{pos}{10000^{\frac{2i}{d_{model}}}})
\end{aligned}\]

<p>这样对于一个 embedding，如果它在输入内容中的位置是 pos，那么其编码向量就表示为：</p>

\[\begin{aligned}
[P_{pos,0}, P_{pos,1}, ... , P_{pos,d_x-1}]
\end{aligned}\]

<p>延展开的话，位置编码其实还分为绝对位置编码（Absolute Positional Encoding）、相对位置编码（Relative Positional Encoding）。前者是专门生成位置编码，并想办法融入到输入中，我们上面看到的就是一种。后者是微调 Attention 结构，使得它可以分辨不同位置的数据。另外其实还有一些无法分类到这两种的位置编码方法。</p>

<h5 id="42绝对位置编码">4.2、绝对位置编码</h5>

<p>绝对位置编码，如上面提到的，就是定义一个位置编码向量  \(t_i\) ，通过  \(x_i + t_i\)  就得到了一个含有位置信息的向量。</p>

<ul>
  <li>习得式位置编码（Learned Positional Encoding）：将位置编码当做训练参数，生成一个「最大长度 x 编码维度」的位置编码矩阵，随着训练进行更新。目前 Google BERT、OpenAI GPT 模型都是用的这种位置编码。缺点是「外推性」差，如果文本长度超过之前训练时用的「最大长度」则无法处理。目前有一些给出优化方案的论文，比如「<a href="https://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&amp;mid=2247515573&amp;idx=1&amp;sn=2d719108244ada7db3a535a435631210&amp;chksm=96ea6235a19deb23babde5eaac484d69e4c2f53bab72d2e350f75bed18323eea3cf9be30615b#rd">层次分解位置编码</a>」。</li>
  <li>三角式位置编码（Sinusoidal Positional Encodign）：上面提过了。</li>
  <li>循环式位置编码（Recurrent Positional Encoding）：通过一个 RNN 再接一个 Transformer，那么 RNN 暗含的「顺序」就导致不再需要额外编码了。但这样牺牲了并行性，毕竟 RNN 的两大缺点之一就有这个。</li>
  <li>相乘式位置编码（Product Positional Encoding）：用「 \(x_i \odot t_i\) 」代替「 \(x_i + t_i\) 」。</li>
</ul>

<h5 id="43相对位置编码和其他位置编码">4.3、相对位置编码和其他位置编码</h5>

<p>最早来自于 Google 的论文<a href="https://arxiv.org/abs/1803.02155">《Self-Attention with Relative Position Representations》</a>相对位置编码，考虑的是当前 position 与被 attention 的 position 之前的相对位置。</p>

<ul>
  <li>常见相对位置编码：经典式、XLNET 式、T5 式、DeBERTa 式等。</li>
  <li>其他位置编码：CNN 式、复数式、融合式等。</li>
</ul>

<p>到此我们都是在讲 Encoder，目前我们知道一个 Encoder 可以用如下的示意图表示：</p>

<p><img src="/img/src/2023-01-04-language-model-5-12.png" alt="image" width="680" /></p>

<h4 id="5编码器-encoder-和解码器-decoder">5、编码器 Encoder 和解码器 Decoder</h4>

<h5 id="51encoder-和-decoder-的图示结构">5.1、Encoder 和 Decoder 的图示结构</h5>

<p><img src="/img/src/2023-01-04-language-model-5-15.png" alt="image" width="165" /></p>

<ul>
  <li>第一层是多头注意力层（Multi-Head Attention Layer）。</li>
  <li>第二层是经过一个前馈神经网络（Feed Forward Neural Network，简称 FFNN）。</li>
  <li>这两层，每一层都有「Add &amp; Normalization」和 ResNet。</li>
</ul>

<p><img src="/img/src/2023-01-04-language-model-5-14.png" alt="image" width="179" /></p>

<ul>
  <li>解码器有两个多头注意力层。第一个多头注意力层是 Masked Multi-Head Attention 层，即在自注意力计算的过程中只有前面位置上的内容。第二个多头注意力层买有被 Masked，是个正常多头注意力层。</li>
  <li>可以看出来，第一个注意力层是一个自注意力层（Self Attention Layer），第二个是 Encoder-Decoder Attention 层（它的 K、V 来自 Encoder，Q 来自自注意力层），有些文章里会用这个角度来指代。</li>
  <li>FNN、Add &amp; Norm、ResNet 都与 Encoder 类似。</li>
</ul>

<h5 id="52decoder-的第一个输出结果">5.2、Decoder 的第一个输出结果</h5>

<p>产出第一个最终输出结果的过程：</p>

<ul>
  <li>不需要经过 Masked Multi-Head Attention Layer（自注意力层）。</li>
  <li>只经过 Encoder-Decoder Attention Layer。</li>
</ul>

<p><img src="/img/src/2023-01-04-language-model-5-13.png" alt="image" width="695" /></p>

<p>这样我们就像前面的 Encoder-Decoder Attention 模型一样，得到第一个输出。但是最终的输出结果，还会经过一层「Linear + Softmax」。</p>

<h5 id="53decoder-后续的所有输出">5.3、Decoder 后续的所有输出</h5>

<p>从产出第二个输出结果开始：</p>

<ul>
  <li>Decoder 的自注意力层，会用到前面的输出结果。</li>
  <li>可以看到，这是一个串行过程。</li>
</ul>

<h5 id="54decoder-之后的-linear-和-softmax">5.4、Decoder 之后的 Linear 和 Softmax</h5>

<p>经过所有 Decoder 之后，我们得到了一大堆浮点数的结果。最后的 Linear &amp; Softmax 就是来解决「怎么把它变成文本」的问题的。</p>

<ul>
  <li>Linear 是一个全连接神经网络，把 Decoders 输出的结果投影到一个超大的向量上，我们称之为 logits 向量。</li>
  <li>如果我们的输出词汇表有 1 万个词，那么 logits 向量的每一个维度就有 1 万个单元，每个单元都对应输出词汇表的一个词的概率。</li>
  <li>Softmax 将 logits 向量中的每一个维度都做归一化，这样每个维度都能从 1 万个单元对应的词概率中选出最大的，对应的词汇表里的词，就是输出词。最终得到一个输出字符串。</li>
</ul>

<h4 id="6transformer-模型整体">6、Transformer 模型整体</h4>

<p><img src="/img/src/2023-01-04-language-model-5-16.png" alt="image" width="660" /></p>

<p>最后我们再来整体看一下 Transformer：</p>

<ul>
  <li>首先输入数据生成词的嵌入式向量表示（Embedding），生成位置编码（Positional Encoding，简称 PE）。</li>
  <li>进入 Encoders 部分。先进入多头注意力层（Multi-Head Attention），是自注意力处理，然后进入全连接层（又叫前馈神经网络层），每层都有 ResNet、Add &amp; Norm。</li>
  <li>每一个 Encoder 的输入，都来自前一个 Encoder 的输出，但是第一个 Encoder 的输入就是 Embedding + PE。</li>
  <li>进入 Decoders 部分。先进入第一个多头注意力层（是 Masked 自注意力层），再进入第二个多头注意力层（是 Encoder-Decoder 注意力层），每层都有 ResNet、Add &amp; Norm。</li>
  <li>每一个 Decoder 都有两部分输入。</li>
  <li>Decoder 的第一层（Maksed 多头自注意力层）的输入，都来自前一个 Decoder 的输出，但是第一个 Decoder 是不经过第一层的（因为经过算出来也是 0）。</li>
  <li>Decoder 的第二层（Encoder-Decoder 注意力层）的输入，Q 都来自该 Decoder 的第一层，且每个 Decoder 的这一层的 K、V 都是一样的，均来自最后一个 Encoder。</li>
  <li>最后经过 Linear、Softmax 归一化。</li>
</ul>

<h4 id="6transformer-的性能">6、Transformer 的性能</h4>

<p>Google 在其博客于 2017.08.31 发布如下测试数据：</p>

<table>
  <thead>
    <tr>
      <th><img src="/img/src/2023-01-04-language-model-5-9.png" alt="image" /></th>
      <th><img src="/img/src/2023-01-04-language-model-5-10.png" alt="image" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td> </td>
      <td> </td>
    </tr>
  </tbody>
</table>

<h4 id="7来看一段用-pytorch-实现的-transformer-示例">7、来看一段用 PyTorch 实现的 Transformer 示例</h4>

<p>—— 未完待续</p>

<h3 id="参考">参考</h3>

<ul>
  <li>http://jalammar.github.io/illustrated-transformer/</li>
  <li>《自然语言处理：基于预训练模型的方法》车万翔 等</li>
  <li>《自然语言处理实战：预训练模型应用及其产品化》安库·A·帕特尔 等</li>
  <li>https://lilianweng.github.io/posts/2018-06-24-attention/</li>
  <li>《基于深度学习的道路短期交通状态时空序列预测》</li>
  <li>https://www.zhihu.com/question/325839123</li>
  <li>https://zhuanlan.zhihu.com/p/410776234</li>
  <li>https://zhuanlan.zhihu.com/p/48508221</li>
  <li>https://luweikxy.gitbook.io/machine-learning-notes/self-attention-and-transformer</li>
  <li>https://zhuanlan.zhihu.com/p/352898810</li>
</ul>

	</div>
</article>



	  </main>
		
		  <!-- Pagination links -->
      

	  </div>
	    
	    <!-- Footer -->
	    <footer><span>@2022 - MikeCaptain.com</span></footer>


	    <!-- Script -->
      <script src="/js/main.js"></script>	


	</div>
</body>
</html>

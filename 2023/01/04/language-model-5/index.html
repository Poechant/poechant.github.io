<!DOCTYPE html>
<html>

<head>
	<!-- Meta -->
	<meta charset="UTF-8"/>
	<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
	<meta name="generator" content="Jekyll">

	<title>麦克船长 NLP 语言模型技术笔记 5：注意力机制（Attention Mechanism）</title>
  	<meta name="description" content="基于 RNN 的 Encoder-Decoder 模型存在无法处理过长文本、并行性差的两大痛点。2015 年 Bahdanau 等人在其论文中提出 Attention 机制，再到 2017 年 Transformer 模型的论文《Attention is All You Need》横空出世，其并行速度极快，而且每两个词之间的词间距都是 1。此后 NLP 领域 Transformer 彻底成为主流。如果你已经了解 Encoder-Decoder 模型，本文将基于此带你深入浅出的搞清楚 Attention、Transformer。">

	<!-- CSS & fonts -->
	<link rel="stylesheet" href="/css/main.css">

	<!-- RSS -->
	<link href="/atom.xml" type="application/atom+xml" rel="alternate" title="ATOM Feed" />

  	<!-- Favicon -->
 	 <link rel="shortcut icon" type="image/png" href="/img/favicon.png">

 	 <!-- Syntax highlighter -->
  	<link rel="stylesheet" href="/css/syntax.css" />

  	<!--KaTeX-->
  	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
  	<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>
  	<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"></script>
  	<script>
  		document.addEventListener("DOMContentLoaded", function() {
  			renderMathInElement(document.body, {
  				// ...options...
  			});
  		});
  	</script>

  	
  	<!-- KaTeX -->
  	<link rel="stylesheet" href="/assets/plugins/katex.0.11.1/katex.min.css">
  	

</head>

<body>
	<div id="wrap">
	  	
	  	<!-- Navigation -->
	  	<nav id="nav">
	<div id="nav-list">
		<a href="/">Home</a>

		<!-- Nav pages -->
	  <!-- 
	    
	  
	    
	      <a href="/about/" title="关于我">关于我</a>
	    
	  
	    
	  
	    
	      <a href="/booklist/" title="我的书单">我的书单</a>
	    
	  
	    
	      <a href="/categories/" title="Categories">Categories</a>
	    
	  
	    
	  
	    
	  
	    
	  
	    
	  
	    
	  
	    
	  
	    
	  
	    
	  
	    
	  
	    
	   -->

	  <!-- Tech category pages -->






  <a href="/category/ai" title="人工智能">人工智能</a>











  <a href="/category/rt_tech" title="实时技术">实时技术</a>





  <a href="/category/web" title="前端技术">前端技术</a>














<!-- Non-tech category pages -->


















  <a href="/category/thinking" title="思考与生活">思考与生活</a>















	  
        
      
        
          <a href="/about/" title="关于我">关于我</a>
        
      
        
      
        
          <a href="/booklist/" title="我的书单">我的书单</a>
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    <!-- Nav links -->
	  <!-- <a href="https://github.com/thereviewindex/monochrome/archive/master.zip">Download</a>
<a href="https://github.com/thereviewindex/monochrome">Project on Github</a> -->

	</div>
  
  <!-- Nav footer -->
	
	  <footer>
	
	<span>version 1.0.0</span>

</footer>
	

</nav>

    
    <!-- Icon menu -->
	  <a id="nav-menu">
	  	<div id="menu"></div>
	  </a>

      <!-- Header -->
      
        <header id="header" class="parent justify-spaceBetween">
  <div class="inner w100 relative">
    <span class="f-left">  
      <a href="/">
        <h1>
          <span>Mike</span>Captain
        </h1>
      </a>
    </span>
    <span id="nav-links" class="absolute right bottom">

      <!-- Tech category pages -->






  <a href="/category/ai" title="人工智能">人工智能</a>











  <a href="/category/rt_tech" title="实时技术">实时技术</a>





  <a href="/category/web" title="前端技术">前端技术</a>














<!-- Non-tech category pages -->


















  <a href="/category/thinking" title="思考与生活">思考与生活</a>















      &nbsp;&nbsp;&nbsp;丨&nbsp;

      <!-- Nav pages -->
      
        
      
        
          <a href="/about/" title="关于我">关于我</a>
        
      
        
      
        
          <a href="/booklist/" title="我的书单">我的书单</a>
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
      
      <!-- Nav links -->
      <!-- <a href="https://github.com/thereviewindex/monochrome/archive/master.zip">Download</a>
<a href="https://github.com/thereviewindex/monochrome">Project on Github</a> -->

    </span>
  </div>
</header>




      

    <!-- Main content -->
	  <div id="container">
		  
		<main>

			<article id="post-page">
	<h2>麦克船长 NLP 语言模型技术笔记 5：注意力机制（Attention Mechanism）</h2>		
	<time datetime="2023-01-04T18:13:09+00:00" class="by-line">04 Jan 2023, 杭州 | 麦克船长 | 总计 48078 字</time>
	<div class="content">
		<p><strong>本文目录</strong></p>
<ul id="markdown-toc">
  <li><a href="#一为什么说-rnn-模型没有体现注意力" id="markdown-toc-一为什么说-rnn-模型没有体现注意力">一、为什么说 RNN 模型没有体现「注意力」？</a></li>
  <li><a href="#二基于-attention-机制的-encoder-decoder-模型" id="markdown-toc-二基于-attention-机制的-encoder-decoder-模型">二、基于 Attention 机制的 Encoder-Decoder 模型</a></li>
  <li><a href="#三transformer-在-2017-年横空出世" id="markdown-toc-三transformer-在-2017-年横空出世">三、Transformer 在 2017 年横空出世</a>    <ul>
      <li><a href="#1自注意力机制self-attention" id="markdown-toc-1自注意力机制self-attention">1、自注意力机制（Self-Attention）</a>        <ul>
          <li><a href="#11一段自然语言内容其自身就暗含很多内部关联信息" id="markdown-toc-11一段自然语言内容其自身就暗含很多内部关联信息">1.1、一段自然语言内容，其自身就「暗含」很多内部关联信息</a></li>
          <li><a href="#12如何计算-qkv" id="markdown-toc-12如何计算-qkv">1.2、如何计算 Q、K、V</a></li>
          <li><a href="#13注意力函数如何通过-qv-得到-z" id="markdown-toc-13注意力函数如何通过-qv-得到-z">1.3、注意力函数：如何通过 Q、V 得到 Z</a></li>
          <li><a href="#14其他注意力函数" id="markdown-toc-14其他注意力函数">1.4、其他注意力函数</a></li>
        </ul>
      </li>
      <li><a href="#2多头注意力" id="markdown-toc-2多头注意力">2、多头注意力</a></li>
      <li><a href="#3退化现象残差网络与-short-cut" id="markdown-toc-3退化现象残差网络与-short-cut">3、退化现象、残差网络与 Short-Cut</a>        <ul>
          <li><a href="#31退化现象" id="markdown-toc-31退化现象">3.1、退化现象</a></li>
          <li><a href="#32恒等映射" id="markdown-toc-32恒等映射">3.2、恒等映射</a></li>
          <li><a href="#33残差网络residual-network与捷径short-cut" id="markdown-toc-33残差网络residual-network与捷径short-cut">3.3、残差网络（Residual Network）与捷径（Short-Cut）</a></li>
        </ul>
      </li>
      <li><a href="#4位置编码positional-embedding" id="markdown-toc-4位置编码positional-embedding">4、位置编码（Positional Embedding）</a>        <ul>
          <li><a href="#41transformer-论文中的三角式位置编码sinusoidal-positional-encoding" id="markdown-toc-41transformer-论文中的三角式位置编码sinusoidal-positional-encoding">4.1、Transformer 论文中的三角式位置编码（Sinusoidal Positional Encoding）</a></li>
          <li><a href="#42绝对位置编码" id="markdown-toc-42绝对位置编码">4.2、绝对位置编码</a></li>
          <li><a href="#43相对位置编码和其他位置编码" id="markdown-toc-43相对位置编码和其他位置编码">4.3、相对位置编码和其他位置编码</a></li>
        </ul>
      </li>
      <li><a href="#5编码器-encoder-和解码器-decoder" id="markdown-toc-5编码器-encoder-和解码器-decoder">5、编码器 Encoder 和解码器 Decoder</a>        <ul>
          <li><a href="#51encoder-和-decoder-的图示结构" id="markdown-toc-51encoder-和-decoder-的图示结构">5.1、Encoder 和 Decoder 的图示结构</a></li>
          <li><a href="#52decoder-的第一个输出结果" id="markdown-toc-52decoder-的第一个输出结果">5.2、Decoder 的第一个输出结果</a></li>
          <li><a href="#53decoder-后续的所有输出" id="markdown-toc-53decoder-后续的所有输出">5.3、Decoder 后续的所有输出</a></li>
          <li><a href="#54decoder-之后的-linear-和-softmax" id="markdown-toc-54decoder-之后的-linear-和-softmax">5.4、Decoder 之后的 Linear 和 Softmax</a></li>
        </ul>
      </li>
      <li><a href="#6transformer-模型整体" id="markdown-toc-6transformer-模型整体">6、Transformer 模型整体</a></li>
      <li><a href="#7transformer-的性能" id="markdown-toc-7transformer-的性能">7、Transformer 的性能</a></li>
    </ul>
  </li>
  <li><a href="#四一个基于-tensorflow-架构的-transformer-实现" id="markdown-toc-四一个基于-tensorflow-架构的-transformer-实现">四、一个基于 TensorFlow 架构的 Transformer 实现</a>    <ul>
      <li><a href="#1先训练和测试一下-kyubyong-transformer" id="markdown-toc-1先训练和测试一下-kyubyong-transformer">1、先训练和测试一下 Kyubyong Transformer</a></li>
      <li><a href="#2kyubyong-transformer-源码分析" id="markdown-toc-2kyubyong-transformer-源码分析">2、Kyubyong Transformer 源码分析</a>        <ul>
          <li><a href="#21超参数" id="markdown-toc-21超参数">2.1、超参数</a></li>
          <li><a href="#22预处理" id="markdown-toc-22预处理">2.2、预处理</a></li>
          <li><a href="#23训练测试数据集的加载" id="markdown-toc-23训练测试数据集的加载">2.3、训练/测试数据集的加载</a></li>
          <li><a href="#24构建模型并训练" id="markdown-toc-24构建模型并训练">2.4、构建模型并训练</a>            <ul>
              <li><a href="#241编码过程" id="markdown-toc-241编码过程">2.4.1、编码过程</a>                <ul>
                  <li><a href="#embedding" id="markdown-toc-embedding">Embedding</a></li>
                  <li><a href="#key-masks" id="markdown-toc-key-masks">Key Masks</a></li>
                  <li><a href="#positional-encoding" id="markdown-toc-positional-encoding">Positional Encoding</a></li>
                  <li><a href="#drop-out" id="markdown-toc-drop-out">Drop out</a></li>
                  <li><a href="#encoder-blocks-multi-head-attention--feed-forward" id="markdown-toc-encoder-blocks-multi-head-attention--feed-forward">Encoder Blocks: Multi-Head Attention &amp; Feed Forward</a></li>
                </ul>
              </li>
              <li><a href="#242解码过程" id="markdown-toc-242解码过程">2.4.2、解码过程</a>                <ul>
                  <li><a href="#embedding-1" id="markdown-toc-embedding-1">Embedding</a></li>
                  <li><a href="#key-masks-1" id="markdown-toc-key-masks-1">Key Masks</a></li>
                  <li><a href="#positional-encoding--drop-out" id="markdown-toc-positional-encoding--drop-out">Positional Encoding &amp; Drop out</a></li>
                  <li><a href="#decoder-blocks-multi-head-attention--feed-forward" id="markdown-toc-decoder-blocks-multi-head-attention--feed-forward">Decoder Blocks: Multi-Head Attention &amp; Feed Forward</a></li>
                </ul>
              </li>
              <li><a href="#243embeddingpositional-encodingmulti-head-attentionfeed-forward" id="markdown-toc-243embeddingpositional-encodingmulti-head-attentionfeed-forward">2.4.3、Embedding、Positional Encoding、Multi-Head Attention、Feed Forward</a>                <ul>
                  <li><a href="#embedding-函数实现" id="markdown-toc-embedding-函数实现">Embedding 函数实现</a></li>
                  <li><a href="#positional-encoding-函数实现" id="markdown-toc-positional-encoding-函数实现">Positional Encoding 函数实现</a></li>
                  <li><a href="#multi-head-attention-函数实现" id="markdown-toc-multi-head-attention-函数实现">Multi-Head Attention 函数实现</a></li>
                  <li><a href="#feed-forward-函数实现" id="markdown-toc-feed-forward-函数实现">Feed Forward 函数实现</a></li>
                </ul>
              </li>
              <li><a href="#244编码和解码完成后的操作" id="markdown-toc-244编码和解码完成后的操作">2.4.4、编码和解码完成后的操作</a></li>
            </ul>
          </li>
          <li><a href="#25效果评价" id="markdown-toc-25效果评价">2.5、效果评价</a></li>
        </ul>
      </li>
      <li><a href="#3kyubyong-transformer-的性能表现" id="markdown-toc-3kyubyong-transformer-的性能表现">3、Kyubyong Transformer 的性能表现</a></li>
      <li><a href="#4kyubyong-transformer-模型的一些问题" id="markdown-toc-4kyubyong-transformer-模型的一些问题">4、Kyubyong Transformer 模型的一些问题</a></li>
    </ul>
  </li>
  <li><a href="#参考" id="markdown-toc-参考">参考</a></li>
</ul>

<h2 id="一为什么说-rnn-模型没有体现注意力">一、为什么说 RNN 模型没有体现「注意力」？</h2>

<p>Encoder-Decoder 的一个非常严重的问题，是依赖中间那个 context 向量，则无法处理特别长的输入序列 —— 记忆力不足，会忘事儿。而忘事儿的根本原因，是没有「注意力」。</p>

<p>对于一般的 RNN 模型，Encoder-Decoder 结构并没有体现「注意力」—— 这句话怎么理解？当输入序列经过 Encoder 生成的中间结果（上下文 C），被喂给 Decoder 时，这些中间结果对所生成序列里的哪个词，都没有区别（没有特别关照谁）。这相当于在说：输入序列里的每个词，对于生成任何一个输出的词的影响，是一样的，而不是输出某个词时是聚焦特定的一些输入词。这就是模型没有注意力机制。</p>

<p>人脑的注意力模型，其实是资源分配模型。NLP 领域的注意力模型，是在 2014 年被提出的，后来逐渐成为 NLP 领域的一个广泛应用的机制。可以应用的场景，比如对于一个电商平台中很常见的白底图，其边缘的白色区域都是无用的，那么就不应该被关注（关注权重为 0）。比如机器翻译中，翻译词都是对局部输入重点关注的。</p>

<p>所以 Attention 机制，就是在 Decoder 时，不是所有输出都依赖相同的「上下文  \(\bm{C}_t\) 」，而是时刻 t 的输出，使用  \(\bm{C}_t\) ，而这个  \(\bm{C}_t\)  来自对每个输入数据项根据「注意力」进行的加权。</p>

<h2 id="二基于-attention-机制的-encoder-decoder-模型">二、基于 Attention 机制的 Encoder-Decoder 模型</h2>

<p>2015 年 Dzmitry Bahdanau 等人在论文<a href="https://arxiv.org/abs/1409.0473">《Neural Machine Translation by Jointly Learning to Align and Translate》</a> 中提出了「Attention」机制，下面请跟着麦克船长，船长会深入浅出地为你解释清楚。</p>

<p>下图中  \(e_i\)  表示编码器的隐藏层输出， \(d_i\)  表示解码器的隐藏层输出</p>

<div style="text-align: center;">
<div class="graphviz-wrapper">

<!-- Generated by graphviz version 2.43.0 (0)
 -->
<!-- Title: G Pages: 1 -->
<svg role="img" aria-label="graphviz-f66c634a9c7c02915e5610af76c3b1b7" width="436pt" height="336pt" viewBox="0.00 0.00 436.00 336.00">
<title>graphviz-f66c634a9c7c02915e5610af76c3b1b7</title>
<desc>
digraph G {
	rankdir=BT
	splines=ortho
	{rank=same e1 e2 eddd en}
	{rank=same d1 d2 dddd dt0 dt dddd2}

	eddd[label=&quot;...&quot;]
	dddd[label=&quot;...&quot;]
	xddd[label=&quot;...&quot;]
	yddd[label=&quot;...&quot;]
	dt[label=&quot;d_t&quot;]
	dt0[label=&quot;d_t-1&quot;]
	yt[label=&quot;y_t&quot;]
	yt0[label=&quot;y_t-1&quot;]
	Ct[shape=plaintext]
	x1[shape=plaintext]
	x2[shape=plaintext]
	xddd[shape=plaintext]
	xn[shape=plaintext]
	y1[shape=plaintext]
	y2[shape=plaintext]
	yddd[shape=plaintext]
	dddd2[shape=plaintext, label=&quot;&quot;]
	Ct[label=&quot;C_t&quot;, shape=&quot;square&quot;]

	x1 -&gt; e1
	x2 -&gt; e2
	xddd -&gt; eddd
	xn -&gt; en

	e1 -&gt; e2
	e2 -&gt; eddd
	eddd -&gt; en

	Ct -&gt; dt

	d1 -&gt; y1
	d2 -&gt; y2
	dddd -&gt; yddd
	dt0 -&gt; yt0
	dt -&gt; yt

	d1 -&gt; d2
	d2 -&gt; dddd
	dddd -&gt; dt0
	dt0 -&gt; dt

	e1 -&gt; Ct
	e2 -&gt; Ct
	eddd -&gt; Ct
	en -&gt; Ct

	dt -&gt; dddd2
	dt0 -&gt; Ct
}
</desc>

<g id="graph0" class="graph" transform="scale(1 1) rotate(0) translate(4 332)">
<title>G</title>
<polygon fill="white" stroke="transparent" points="-4,4 -4,-332 432,-332 432,4 -4,4" />
<!-- e1 -->
<g id="node1" class="node">
<title>e1</title>
<ellipse fill="none" stroke="black" cx="181" cy="-90" rx="27" ry="18" />
<text text-anchor="middle" x="181" y="-86.3" font-family="Times,serif" font-size="14.00">e1</text>
</g>
<!-- e2 -->
<g id="node2" class="node">
<title>e2</title>
<ellipse fill="none" stroke="black" cx="253" cy="-90" rx="27" ry="18" />
<text text-anchor="middle" x="253" y="-86.3" font-family="Times,serif" font-size="14.00">e2</text>
</g>
<!-- e1&#45;&gt;e2 -->
<g id="edge5" class="edge">
<title>e1&#45;&gt;e2</title>
<path fill="none" stroke="black" d="M208.22,-90C208.22,-90 215.74,-90 215.74,-90" />
<polygon fill="black" stroke="black" points="215.74,-93.5 225.74,-90 215.74,-86.5 215.74,-93.5" />
</g>
<!-- Ct -->
<g id="node15" class="node">
<title>Ct</title>
<polygon fill="none" stroke="black" points="309,-184 269,-184 269,-144 309,-144 309,-184" />
<text text-anchor="middle" x="289" y="-160.3" font-family="Times,serif" font-size="14.00">C_t</text>
</g>
<!-- e1&#45;&gt;Ct -->
<g id="edge18" class="edge">
<title>e1&#45;&gt;Ct</title>
<path fill="none" stroke="black" d="M203,-100.6C203,-121.06 203,-164 203,-164 203,-164 258.62,-164 258.62,-164" />
<polygon fill="black" stroke="black" points="258.62,-167.5 268.62,-164 258.62,-160.5 258.62,-167.5" />
</g>
<!-- eddd -->
<g id="node3" class="node">
<title>eddd</title>
<ellipse fill="none" stroke="black" cx="325" cy="-90" rx="27" ry="18" />
<text text-anchor="middle" x="325" y="-86.3" font-family="Times,serif" font-size="14.00">...</text>
</g>
<!-- e2&#45;&gt;eddd -->
<g id="edge6" class="edge">
<title>e2&#45;&gt;eddd</title>
<path fill="none" stroke="black" d="M280.22,-90C280.22,-90 287.74,-90 287.74,-90" />
<polygon fill="black" stroke="black" points="287.74,-93.5 297.74,-90 287.74,-86.5 287.74,-93.5" />
</g>
<!-- e2&#45;&gt;Ct -->
<g id="edge19" class="edge">
<title>e2&#45;&gt;Ct</title>
<path fill="none" stroke="black" d="M274.5,-100.92C274.5,-100.92 274.5,-133.82 274.5,-133.82" />
<polygon fill="black" stroke="black" points="271,-133.82 274.5,-143.82 278,-133.82 271,-133.82" />
</g>
<!-- en -->
<g id="node4" class="node">
<title>en</title>
<ellipse fill="none" stroke="black" cx="397" cy="-90" rx="27" ry="18" />
<text text-anchor="middle" x="397" y="-86.3" font-family="Times,serif" font-size="14.00">en</text>
</g>
<!-- eddd&#45;&gt;en -->
<g id="edge7" class="edge">
<title>eddd&#45;&gt;en</title>
<path fill="none" stroke="black" d="M352.22,-90C352.22,-90 359.74,-90 359.74,-90" />
<polygon fill="black" stroke="black" points="359.74,-93.5 369.74,-90 359.74,-86.5 359.74,-93.5" />
</g>
<!-- eddd&#45;&gt;Ct -->
<g id="edge20" class="edge">
<title>eddd&#45;&gt;Ct</title>
<path fill="none" stroke="black" d="M303.5,-100.92C303.5,-100.92 303.5,-133.82 303.5,-133.82" />
<polygon fill="black" stroke="black" points="300,-133.82 303.5,-143.82 307,-133.82 300,-133.82" />
</g>
<!-- en&#45;&gt;Ct -->
<g id="edge21" class="edge">
<title>en&#45;&gt;Ct</title>
<path fill="none" stroke="black" d="M399,-108.29C399,-130.21 399,-164 399,-164 399,-164 319.18,-164 319.18,-164" />
<polygon fill="black" stroke="black" points="319.18,-160.5 309.18,-164 319.18,-167.5 319.18,-160.5" />
</g>
<!-- d1 -->
<g id="node5" class="node">
<title>d1</title>
<ellipse fill="none" stroke="black" cx="27" cy="-238" rx="27" ry="18" />
<text text-anchor="middle" x="27" y="-234.3" font-family="Times,serif" font-size="14.00">d1</text>
</g>
<!-- d2 -->
<g id="node6" class="node">
<title>d2</title>
<ellipse fill="none" stroke="black" cx="99" cy="-238" rx="27" ry="18" />
<text text-anchor="middle" x="99" y="-234.3" font-family="Times,serif" font-size="14.00">d2</text>
</g>
<!-- d1&#45;&gt;d2 -->
<g id="edge14" class="edge">
<title>d1&#45;&gt;d2</title>
<path fill="none" stroke="black" d="M54.22,-238C54.22,-238 61.74,-238 61.74,-238" />
<polygon fill="black" stroke="black" points="61.74,-241.5 71.74,-238 61.74,-234.5 61.74,-241.5" />
</g>
<!-- y1 -->
<g id="node19" class="node">
<title>y1</title>
<text text-anchor="middle" x="27" y="-306.3" font-family="Times,serif" font-size="14.00">y1</text>
</g>
<!-- d1&#45;&gt;y1 -->
<g id="edge9" class="edge">
<title>d1&#45;&gt;y1</title>
<path fill="none" stroke="black" d="M27,-256.17C27,-256.17 27,-281.59 27,-281.59" />
<polygon fill="black" stroke="black" points="23.5,-281.59 27,-291.59 30.5,-281.59 23.5,-281.59" />
</g>
<!-- dddd -->
<g id="node7" class="node">
<title>dddd</title>
<ellipse fill="none" stroke="black" cx="171" cy="-238" rx="27" ry="18" />
<text text-anchor="middle" x="171" y="-234.3" font-family="Times,serif" font-size="14.00">...</text>
</g>
<!-- d2&#45;&gt;dddd -->
<g id="edge15" class="edge">
<title>d2&#45;&gt;dddd</title>
<path fill="none" stroke="black" d="M126.22,-238C126.22,-238 133.74,-238 133.74,-238" />
<polygon fill="black" stroke="black" points="133.74,-241.5 143.74,-238 133.74,-234.5 133.74,-241.5" />
</g>
<!-- y2 -->
<g id="node20" class="node">
<title>y2</title>
<text text-anchor="middle" x="99" y="-306.3" font-family="Times,serif" font-size="14.00">y2</text>
</g>
<!-- d2&#45;&gt;y2 -->
<g id="edge10" class="edge">
<title>d2&#45;&gt;y2</title>
<path fill="none" stroke="black" d="M99,-256.17C99,-256.17 99,-281.59 99,-281.59" />
<polygon fill="black" stroke="black" points="95.5,-281.59 99,-291.59 102.5,-281.59 95.5,-281.59" />
</g>
<!-- dt0 -->
<g id="node8" class="node">
<title>dt0</title>
<ellipse fill="none" stroke="black" cx="250" cy="-238" rx="33.6" ry="18" />
<text text-anchor="middle" x="250" y="-234.3" font-family="Times,serif" font-size="14.00">d_t&#45;1</text>
</g>
<!-- dddd&#45;&gt;dt0 -->
<g id="edge16" class="edge">
<title>dddd&#45;&gt;dt0</title>
<path fill="none" stroke="black" d="M198.19,-238C198.19,-238 206.2,-238 206.2,-238" />
<polygon fill="black" stroke="black" points="206.2,-241.5 216.2,-238 206.2,-234.5 206.2,-241.5" />
</g>
<!-- yddd -->
<g id="node12" class="node">
<title>yddd</title>
<text text-anchor="middle" x="171" y="-306.3" font-family="Times,serif" font-size="14.00">...</text>
</g>
<!-- dddd&#45;&gt;yddd -->
<g id="edge11" class="edge">
<title>dddd&#45;&gt;yddd</title>
<path fill="none" stroke="black" d="M171,-256.17C171,-256.17 171,-281.59 171,-281.59" />
<polygon fill="black" stroke="black" points="167.5,-281.59 171,-291.59 174.5,-281.59 167.5,-281.59" />
</g>
<!-- dt -->
<g id="node9" class="node">
<title>dt</title>
<ellipse fill="none" stroke="black" cx="329" cy="-238" rx="27" ry="18" />
<text text-anchor="middle" x="329" y="-234.3" font-family="Times,serif" font-size="14.00">d_t</text>
</g>
<!-- dt0&#45;&gt;dt -->
<g id="edge17" class="edge">
<title>dt0&#45;&gt;dt</title>
<path fill="none" stroke="black" d="M283.96,-238C283.96,-238 291.98,-238 291.98,-238" />
<polygon fill="black" stroke="black" points="291.98,-241.5 301.98,-238 291.98,-234.5 291.98,-241.5" />
</g>
<!-- yt0 -->
<g id="node14" class="node">
<title>yt0</title>
<ellipse fill="none" stroke="black" cx="250" cy="-310" rx="33.29" ry="18" />
<text text-anchor="middle" x="250" y="-306.3" font-family="Times,serif" font-size="14.00">y_t&#45;1</text>
</g>
<!-- dt0&#45;&gt;yt0 -->
<g id="edge12" class="edge">
<title>dt0&#45;&gt;yt0</title>
<path fill="none" stroke="black" d="M250,-256.17C250,-256.17 250,-281.59 250,-281.59" />
<polygon fill="black" stroke="black" points="246.5,-281.59 250,-291.59 253.5,-281.59 246.5,-281.59" />
</g>
<!-- dt0&#45;&gt;Ct -->
<g id="edge23" class="edge">
<title>dt0&#45;&gt;Ct</title>
<path fill="none" stroke="black" d="M276.4,-226.44C276.4,-226.44 276.4,-194.12 276.4,-194.12" />
<polygon fill="black" stroke="black" points="279.9,-194.12 276.4,-184.12 272.9,-194.12 279.9,-194.12" />
</g>
<!-- dddd2 -->
<g id="node10" class="node">
<title>dddd2</title>
</g>
<!-- dt&#45;&gt;dddd2 -->
<g id="edge22" class="edge">
<title>dt&#45;&gt;dddd2</title>
<path fill="none" stroke="black" d="M356.22,-238C356.22,-238 363.74,-238 363.74,-238" />
<polygon fill="black" stroke="black" points="363.74,-241.5 373.74,-238 363.74,-234.5 363.74,-241.5" />
</g>
<!-- yt -->
<g id="node13" class="node">
<title>yt</title>
<ellipse fill="none" stroke="black" cx="329" cy="-310" rx="27" ry="18" />
<text text-anchor="middle" x="329" y="-306.3" font-family="Times,serif" font-size="14.00">y_t</text>
</g>
<!-- dt&#45;&gt;yt -->
<g id="edge13" class="edge">
<title>dt&#45;&gt;yt</title>
<path fill="none" stroke="black" d="M329,-256.17C329,-256.17 329,-281.59 329,-281.59" />
<polygon fill="black" stroke="black" points="325.5,-281.59 329,-291.59 332.5,-281.59 325.5,-281.59" />
</g>
<!-- xddd -->
<g id="node11" class="node">
<title>xddd</title>
<text text-anchor="middle" x="325" y="-14.3" font-family="Times,serif" font-size="14.00">...</text>
</g>
<!-- xddd&#45;&gt;eddd -->
<g id="edge3" class="edge">
<title>xddd&#45;&gt;eddd</title>
<path fill="none" stroke="black" d="M325,-36.17C325,-36.17 325,-61.59 325,-61.59" />
<polygon fill="black" stroke="black" points="321.5,-61.59 325,-71.59 328.5,-61.59 321.5,-61.59" />
</g>
<!-- Ct&#45;&gt;dt -->
<g id="edge8" class="edge">
<title>Ct&#45;&gt;dt</title>
<path fill="none" stroke="black" d="M305.5,-184.22C305.5,-184.22 305.5,-218.8 305.5,-218.8" />
<polygon fill="black" stroke="black" points="302,-218.8 305.5,-228.8 309,-218.8 302,-218.8" />
</g>
<!-- x1 -->
<g id="node16" class="node">
<title>x1</title>
<text text-anchor="middle" x="181" y="-14.3" font-family="Times,serif" font-size="14.00">x1</text>
</g>
<!-- x1&#45;&gt;e1 -->
<g id="edge1" class="edge">
<title>x1&#45;&gt;e1</title>
<path fill="none" stroke="black" d="M181,-36.17C181,-36.17 181,-61.59 181,-61.59" />
<polygon fill="black" stroke="black" points="177.5,-61.59 181,-71.59 184.5,-61.59 177.5,-61.59" />
</g>
<!-- x2 -->
<g id="node17" class="node">
<title>x2</title>
<text text-anchor="middle" x="253" y="-14.3" font-family="Times,serif" font-size="14.00">x2</text>
</g>
<!-- x2&#45;&gt;e2 -->
<g id="edge2" class="edge">
<title>x2&#45;&gt;e2</title>
<path fill="none" stroke="black" d="M253,-36.17C253,-36.17 253,-61.59 253,-61.59" />
<polygon fill="black" stroke="black" points="249.5,-61.59 253,-71.59 256.5,-61.59 249.5,-61.59" />
</g>
<!-- xn -->
<g id="node18" class="node">
<title>xn</title>
<text text-anchor="middle" x="397" y="-14.3" font-family="Times,serif" font-size="14.00">xn</text>
</g>
<!-- xn&#45;&gt;en -->
<g id="edge4" class="edge">
<title>xn&#45;&gt;en</title>
<path fill="none" stroke="black" d="M397,-36.17C397,-36.17 397,-61.59 397,-61.59" />
<polygon fill="black" stroke="black" points="393.5,-61.59 397,-71.59 400.5,-61.59 393.5,-61.59" />
</g>
</g>
</svg>
</div>
</div>

<p>更进一步细化关于  \(\bm{C}_t\)  部分，船长在此引用《基于深度学习的道路短期交通状态时空序列预测》一书中的图：</p>

<p><img src="/img/src/2023-01-04-captain-nlp-5.png" alt="image" /></p>

<p>这个图里的  \(\widetilde{h}_i\)  与上一个图里的  \(d_i\)  对应， \(h_i\)  与上一个图里的  \(e_i\)  对应。</p>

<p>针对时刻  \(t\)  要产出的输出，隐藏层每一个隐藏细胞都与  \(\bm{C}_t\)  有一个权重关系  \(\alpha_{t,i}\)  其中  \(1\le i\le n\) ，这个权重值与「输入项经过编码器后隐藏层后的输出 \(e_i（1\le i\le n）\) 、解码器的前一时刻隐藏层输出  \(d_{t-1}\) 」两者有关：</p>

\[\begin{aligned}
&amp;s_{i,t} = score(\bm{e}_i,\bm{d}_{t-1}) \\
&amp;\alpha_{i,t} = \frac{exp(s_{i,t})}{\textstyle\sum_{j=1}^n exp(s_{j,t})}
\end{aligned}\]

<p>常用的  \(score\)  函数有：</p>

<ul>
  <li>点积（Dot Product）模型： \(s_{i,t} = {\bm{d}_{t-1}}^T \cdot \bm{e}_i\)</li>
  <li>缩放点积（Scaled Dot-Product）模型： \(s_{i,t} = \frac{{\bm{d}_{t-1}}^T \cdot \bm{e}_i}{\sqrt{\smash[b]{dimensions\:of\:d_{t-1}\:or\:e_i}}}\) ，可避免因为向量维度过大导致点积结果太大</li>
</ul>

<p>然后上下文向量就表示成：</p>

\[\begin{aligned}
&amp;\bm{C}_t = \displaystyle\sum_{i=1}^n \alpha_{i,t} \bm{e}_i
\end{aligned}\]

<p>还记得 RNN 那部分里船长讲到的 Encoder-Decoder 模型的公式表示吗？</p>

\[\begin{aligned}
e_t &amp;= Encoder_{LSTM/GRU}(x_t, e_{t-1}) \\
\bm{C} &amp;= f_1(e_n) \\
d_t &amp;= f_2(d_{t-1}, \bm{C}) \\
y_t &amp;= Decoder_{LSTM/GRU}(y_{t-1}, d_{t-1}, \bm{C})
\end{aligned}\]

<p>加入 Attention 机制的 Encoder-Decoder 模型如下。</p>

\[\begin{aligned}
e_t &amp;= Encoder_{LSTM/GRU}(x_t, e_{t-1}) \\
\bm{C}_t &amp;= f_1(e_1,e_2...e_n,d_{t-1}) \\
d_t &amp;= f_2(d_{t-1}, \bm{C}_t) \\
y_t &amp;= Decoder_{LSTM/GRU}(y_{t-1}, d_{t-1}, \bm{C}_t)
\end{aligned}\]

<p>这种同时考虑 Encoder、Decoder 的 Attention，就叫做「Encoder-Decoder Attention」，也常被叫做「Vanilla Attention」。可以看到上面最核心的区别是第二个公式  \(C_t\) 。加入 Attention 后，对所有数据给予不同的注意力分布。具体地，比如我们用如下的函数来定义这个模型：</p>

\[\begin{aligned}
\bm{e} &amp;= tanh(\bm{W}^{xe} \cdot \bm{x} + \bm{b}^{xe}) \\
s_{i,t} &amp;= score(\bm{e}_i,\bm{d}_{t-1}) \\
\alpha_{i,t} &amp;= \frac{e^{s_{i,t}}}{\textstyle\sum_{j=1}^n e^{s_{j,t}}} \\
\bm{C}_t &amp;= \displaystyle\sum_{i=1}^n \alpha_{i,t} \bm{e}_i \\
\bm{d}_t &amp;= tanh(\bm{W}^{dd} \cdot \bm{d}_{t-1} + \bm{b}^{dd} +
				 \bm{W}^{yd} \cdot \bm{y}_{t-1} + \bm{b}^{yd} +
				 \bm{W}^{cd} \cdot \bm{C}_t + \bm{b}^{cd}) \\
\bm{y} &amp;= Softmax(\bm{W}^{dy} \cdot \bm{d} + \bm{b}^{dy})
\end{aligned}\]

<p>到这里你能发现注意力机制的什么问题不？</p>

<ul>
  <li>这个注意力机制忽略了位置信息。比如 Tigers love rabbits 和 Rabbits love tigers 会产生一样的注意力分数。</li>
</ul>

<h2 id="三transformer-在-2017-年横空出世">三、Transformer 在 2017 年横空出世</h2>

<p>船长先通过一个动画来看下 Transformer 是举例示意，该图来自 Google 的博客文章 <a href="https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html">《Transformer: A Novel Neural Network Architecture for Language Understanding》</a>：</p>

<p><img src="/img/src/2023-01-04-language-model-5-11.gif" alt="image" /></p>

<p>中文网络里找到的解释得比较好的 blogs、answers，几乎都指向了同一篇博客：Jay Alammar 的<a href="http://jalammar.github.io/illustrated-transformer/">《The Illustrated Transformer》</a>，所以建议读者搭配该篇文章阅读。</p>

<p>Transformer 模型中用到了自注意力（Self-Attention）、多头注意力（Multiple-Head Attention）、残差网络（ResNet）与捷径（Short-Cut）。下面我们先通过第 1 到第 4 小节把几个基本概念讲清楚，然后在第 5 小节讲解整体 Transformer 模型就会好理解很多了。最后第 6 小节我们来一段动手实践。</p>

<h3 id="1自注意力机制self-attention">1、自注意力机制（Self-Attention）</h3>

<p>自注意力是理解 Transformer 的关键，原作者在论文中限于篇幅，没有给出过多的解释。以下是我自己的理解，能够比较通透、符合常识地去理解 Transformer 中的一些神来之笔的概念。</p>

<h4 id="11一段自然语言内容其自身就暗含很多内部关联信息">1.1、一段自然语言内容，其自身就「暗含」很多内部关联信息</h4>

<p>在加入了 Attention 的 Encoder-Decoder 模型中，对输出序列 Y 中的一个词的注意力来自于输入序列 X，那么如果 X 和 Y 相等呢？什么场景会有这个需求？因为我们认为一段文字里某些词就是由于另外某些词而决定的，可以粗暴地理解为「完形填空」的原理。那么这样一段文字，其实就存在其中每个词的自注意力，举个例子：</p>

<blockquote>
  <p>老王是我的主管，我很喜欢他的平易近人。</p>
</blockquote>

<p>对这句话里的「他」，如果基于这句话计算自注意力的话，显然应该给予「老王」最多的注意力。受此启发，我们认为：</p>

<blockquote>
  <p>一段自然语言中，其实暗含了：为了得到关于某方面信息 Q，可以通过关注某些信息 K，进而得到某些信息（V）作为结果。</p>
</blockquote>

<p>Q 就是 query 检索/查询，K、V 分别是 key、value。所以类似于我们在图书检索系统里搜索「NLP书籍」（这是 Q），得到了一本叫《自然语言处理实战》的电子书，书名就是 key，这本电子书就是 value。只是对于自然语言的理解，我们认为任何一段内容里，都自身暗含了很多潜在 Q-K-V 的关联。这是整体受到信息检索领域里 query-key-value 的启发的。</p>

<p>基于这个启发，我们将自注意力的公式表示为：</p>

\[\begin{aligned}
Z = SelfAttention(X) = Attention(Q,K,V)
\end{aligned}\]

<p>X 经过自注意力计算后，得到的「暗含」了大量原数据内部信息的 Z。然后我们拿着这个带有自注意力信息的 Z 进行后续的操作。这里要强调的是，Z 向量中的每个元素 z_i 都与 X 的所有元素有某种关联，而不是只与 x_i 有关联。</p>

<h4 id="12如何计算-qkv">1.2、如何计算 Q、K、V</h4>

<p>Q、K、V 全部来自输入 X 的线性变换：</p>

\[\begin{aligned}
Q &amp;= W^Q \cdot X \\
K &amp;= W^K \cdot X \\
V &amp;= W^V \cdot X
\end{aligned}\]

<p>\(W^Q、W^K、W^V\)  以随机初始化开始，经过训练就会得到非常好的表现。对于  \(X\)  中的每一个词向量  \(x_i\) ，经过这个变换后得到了：</p>

\[\begin{aligned}
q_i &amp;= W^Q \cdot x_i \\
k_i &amp;= W^K \cdot x_i \\
v_i &amp;= W^V \cdot x_i
\end{aligned}\]

<h4 id="13注意力函数如何通过-qv-得到-z">1.3、注意力函数：如何通过 Q、V 得到 Z</h4>

<p>基于上面的启发，我们认为 X 经过自注意力的挖掘后，得到了：</p>

<ul>
  <li>暗含信息 1：一组 query 与一组 key 之间的关联，记作 qk（想一下信息检索系统要用 query 先招到 key）</li>
  <li>暗含信息 2：一组 value</li>
  <li>暗含信息 3：qk 与 value 的某种关联</li>
</ul>

<p>这三组信息，分别如何表示呢？这里又需要一些启发了，因为计算机科学其实是在「模拟还原」现实世界，在 AI 的领域目前的研究方向就是模拟还原人脑的思考。所以这种「模拟还原」都是寻找某一种近似方法，因此不能按照数学、物理的逻辑推理来理解，而应该按照「工程」或者「计算科学」来理解，想想我们大学时学的「计算方法」这门课，因此常需要一些启发来找到某种「表示」。</p>

<p>这里 Transformer 的作者，认为  \(Q\)  和  \(K\)  两个向量之间的关联，是我们在用  \(Q\)  找其在  \(K\)  上的投影，如果  \(Q\) 、 \(K\)  是单位长度的向量，那么这个投影其实可以理解为找「 \(Q\)  和  \(K\)  向量之间的相似度」：</p>

<ul>
  <li>如果  \(Q\)  和  \(K\)  垂直，那么两个向量正交，其点积（Dot Product）为 0；</li>
  <li>如果  \(Q\)  和  \(K\)  平行，那么两个向量点积为两者模积  \(\|Q\|\|K\|\) ；</li>
  <li>如果  \(Q\)  和  \(K\)  呈某个夹角，则点积就是  \(Q\)  在  \(K\)  上的投影的模。</li>
</ul>

<p>因此「暗含信息 1」就可以用「 \(Q\cdot K\) 」再经过 Softmax 归一化来表示。这个表示，是一个所有元素都是 0~1 的矩阵，可以理解成对应注意力机制里的「注意力分数」，也就是一个「注意力分数矩阵（Attention Score Matrix）」。</p>

<p>而「暗含信息 2」则是输入  \(X\)  经过的线性变换后的特征，看做  \(X\)  的另一种表示。然后我们用这个「注意力分数矩阵」来加持一下  \(V\) ，这个点积过程就表示了「暗含信息 3」了。所以我们有了如下公式：</p>

\[\begin{aligned}
Z = Attention(Q,K,V) = Softmax(Q \cdot K^T) \cdot V
\end{aligned}\]

<p>其实到这里，这个注意力函数已经可以用了。有时候，为了避免因为向量维度过大，导致  \(Q \cdot K^T\)  点积结果过大，我们再加一步处理：</p>

\[\begin{aligned}
Z = Attention(Q,K,V) = Softmax(\frac{Q \cdot K^T}{\sqrt{\smash[b]{d_k}}}) \cdot V
\end{aligned}\]

<p>这里  \(d_k\)  是 K 矩阵中向量  \(k_i\)  的维度。这一步修正还有进一步的解释，即如果经过 Softmax 归一化后模型稳定性存在问题。怎么理解？如果假设 Q 和 K 中的每个向量的每一维数据都具有零均值、单位方差，这样输入数据是具有稳定性的，那么如何让「暗含信息 1」计算后仍然具有稳定性呢？即运算结果依然保持零均值、单位方差，就是除以「 \(\sqrt{\smash[b]{d_k}}\) 」。</p>

<p>到这里我们注意到：</p>

<ul>
  <li>K、V 里的每一个向量，都是</li>
</ul>

<h4 id="14其他注意力函数">1.4、其他注意力函数</h4>

<p>为了提醒大家这种暗含信息的表示，都只是计算方法上的一种选择，好坏全靠结果评定，所以包括上面的在内，常见的注意力函数有（甚至你也可以自己定义）：</p>

\[Z = Attention(Q,K,V) =
\begin{cases}
\begin{aligned}
&amp;= Softmax(Q^T K) V \\
&amp;= Softmax(\frac{Q K^T}{\sqrt{\smash[b]{d_k}}}) V \\
&amp;= Softmax(\omega^T tanh(W[q;k])) V \\
&amp;= Softmax(Q^T W K) V \\
&amp;= cosine[Q^T K] V
\end{aligned}
\end{cases}\]

<p>到这里，我们就从原始的输入  \(X\)  得到了一个包含自注意力信息的  \(Z\)  了，后续就可以用  \(Z\)  了。</p>

<h3 id="2多头注意力">2、多头注意力</h3>

<p>到这里我们理解了「自注意力」，而 Transformer 这篇论文通过添加「多头」注意力的机制进一步提升了注意力层。我们先看下它是什么，然后看下它的优点。从本小节开始，本文大量插图引用自<a href="http://jalammar.github.io/illustrated-transformer/">《The Illustrated Transformer》</a>，作者 Jay Alammar 写出一篇非常深入浅出的图解文章，被大量引用，非常出色，再次建议大家去阅读。</p>

<p>Transformer 中用了 8 个头，也就是 8 组不同的 Q-K-V：</p>

\[\begin{aligned}
Q_0 = W_0^Q \cdot X ;\enspace K_0 = &amp;W_0^K \cdot X ;\enspace V_0 = W_0^V \cdot X \\
Q_1 = W_1^Q \cdot X ;\enspace K_1 = &amp;W_0^K \cdot X ;\enspace V_1 = W_1^V \cdot X \\
&amp;.... \\
Q_7 = W_7^Q \cdot X ;\enspace K_7 = &amp;W_0^K \cdot X ;\enspace V_7 = W_7^V \cdot X
\end{aligned}\]

<p>这样我们就能得到 8 个 Z：</p>

\[\begin{aligned}
&amp;Z_0 = Attention(Q_0,K_0,V_0) = Softmax(\frac{Q_0 \cdot K_0^T}{\sqrt{\smash[b]{d_k}}}) \cdot V_0 \\
&amp;Z_1 = Attention(Q_1,K_1,V_1) = Softmax(\frac{Q_1 \cdot K_1^T}{\sqrt{\smash[b]{d_k}}}) \cdot V_1 \\
&amp;... \\
&amp;Z_7 = Attention(Q_7,K_7,V_7) = Softmax(\frac{Q_7 \cdot K_7^T}{\sqrt{\smash[b]{d_k}}}) \cdot V_7 \\
\end{aligned}\]

<p>然后我们把  \(Z_0\)  到  \(Z_7\)  沿着行数不变的方向全部连接起来，如下图所示：</p>

<p><img src="/img/src/2023-01-04-language-model-5-3.png" alt="image" width="464" /></p>

<p>我们再训练一个权重矩阵  \(W^O\) ，然后用上面拼接的  \(Z_{0-7}\)  乘以这个权重矩阵：</p>

<p><img src="/img/src/2023-01-04-language-model-5-4.png" alt="image" width="135" /></p>

<p>于是我们会得到一个 Z 矩阵：</p>

<p><img src="/img/src/2023-01-04-language-model-5-5.png" alt="image" width="100" /></p>

<p>到这里就是多头注意力机制的全部内容，与单头注意力相比，都是为了得到一个 Z 矩阵，但是多头用了多组 Q-K-V，然后经过拼接、乘以权重矩阵得到最后的 Z。我们总览一下整个过程：</p>

<p><img src="/img/src/2023-01-04-language-model-5-6.png" alt="image" width="935" /></p>

<p>通过多头注意力，每个头都会关注到不同的信息，可以如下类似表示：</p>

<p><img src="/img/src/2023-01-04-language-model-5-7.png" alt="image" width="400" /></p>

<p>这通过两种方式提高了注意力层的性能：</p>

<ul>
  <li>多头注意力机制，扩展了模型关注不同位置的能力。 \(Z\)  矩阵中的每个向量  \(z_i\)  包含了与  \(X\)  中所有向量  \(x_i\)  有关的一点编码信息。反过来说，不要认为  \(z_i\)  只与  \(x_i\)  有关。</li>
  <li>多头注意力机制，为注意力层提供了多个「表示子空间 Q-K-V」，以及 Z。这样一个输入矩阵  \(X\) ，就会被表示成 8 种不同的矩阵 Z，都包含了原始数据信息的某种解读暗含其中。</li>
</ul>

<h3 id="3退化现象残差网络与-short-cut">3、退化现象、残差网络与 Short-Cut</h3>

<h4 id="31退化现象">3.1、退化现象</h4>

<p>对于一个 56 层的神经网路，我们很自然地会觉得应该比 20 层的神经网络的效果要好，比如说从误差率（error）的量化角度看。但是华人学者何凯明等人的论文<a href="https://arxiv.org/pdf/1512.03385.pdf">《Deep Residual Learning for Image Recognition》</a>中给我们呈现了相反的结果，而这个问题的原因并不是因为层数多带来的梯度爆炸/梯度消失（毕竟已经用了归一化解决了这个问题），而是因为一种反常的现象，这种现象我们称之为「退化现象」。何凯明等人认为这是因为存在「难以优化好的网络层」。</p>

<h4 id="32恒等映射">3.2、恒等映射</h4>

<p>如果这 36 层还帮了倒忙，那还不如没有，是不是？所以这多出来的 36 个网络层，如果对于提升性能（例如误差率）毫无影响，甚至更进一步，这 36 层前的输入数据，和经过这 36 层后的输出数据，完全相同，那么如果将这 36 层抽象成一个函数  \(f_{36}\) ，这就是一个恒等映射的函数：</p>

\[f_{36}(x) = x\]

<p>回到实际应用中。如果我们对于一个神经网络中的连续 N 层是提升性能，还是降低性能，是未知的，那么则可以建立一个跳过这些层的连接，实现：</p>

<blockquote>
  <p>如果这 N 层可以提升性能，则采用这 N 层；否则就跳过。</p>
</blockquote>

<p>这就像给了这 N 层神经网络一个试错的空间，待我们确认它们的性能后再决定是否采用它们。同时也可以理解成，这些层可以去单独优化，如果性能提升，则不被跳过。</p>

<h4 id="33残差网络residual-network与捷径short-cut">3.3、残差网络（Residual Network）与捷径（Short-Cut）</h4>

<p>如果前面 20 层已经可以实现 99% 的准确率，那么引入了这 36 层能否再提升「残差剩余那 1%」的准确率从而达到 100% 呢？所以这 36 层的网络，就被称为「残差网络（Residual Network，常简称为 ResNet）」，这个叫法非常形象。</p>

<p>而那个可以跳过 N 层残差网络的捷径，则常被称为 Short-Cut，也会被叫做跳跃链接（Skip Conntection），这就解决了上述深度学习中的「退化现象」。</p>

<h3 id="4位置编码positional-embedding">4、位置编码（Positional Embedding）</h3>

<p>还记得我在第二部分最后提到的吗：</p>

<blockquote>
  <p>这个注意力机制忽略了位置信息。比如 Tigers love rabbits 和 Rabbits love tigers 会产生一样的注意力分数。</p>
</blockquote>

<h4 id="41transformer-论文中的三角式位置编码sinusoidal-positional-encoding">4.1、Transformer 论文中的三角式位置编码（Sinusoidal Positional Encoding）</h4>

<p>现在我们来解决这个问题，为每一个输入向量  \(x_i\)  生成一个位置编码向量  \(t_i\) ，这个位置编码向量的维度，与输入向量（词的嵌入式向量表示）的维度是相同的：</p>

<p><img src="/img/src/2023-01-04-language-model-5-8.png" alt="image" width="500" /></p>

<p>Transformer 论文中给出了如下的公式，来计算位置编码向量的每一位的值：</p>

\[\begin{aligned}
P_{pos,2i} &amp;= sin(\frac{pos}{10000^{\frac{2i}{d_{model}}}}) \\
P_{pos,2i+1} &amp;= cos(\frac{pos}{10000^{\frac{2i}{d_{model}}}})
\end{aligned}\]

<p>这样对于一个 embedding，如果它在输入内容中的位置是 pos，那么其编码向量就表示为：</p>

\[\begin{aligned}
[P_{pos,0}, P_{pos,1}, ... , P_{pos,d_x-1}]
\end{aligned}\]

<p>延展开的话，位置编码其实还分为绝对位置编码（Absolute Positional Encoding）、相对位置编码（Relative Positional Encoding）。前者是专门生成位置编码，并想办法融入到输入中，我们上面看到的就是一种。后者是微调 Attention 结构，使得它可以分辨不同位置的数据。另外其实还有一些无法分类到这两种的位置编码方法。</p>

<h4 id="42绝对位置编码">4.2、绝对位置编码</h4>

<p>绝对位置编码，如上面提到的，就是定义一个位置编码向量  \(t_i\) ，通过  \(x_i + t_i\)  就得到了一个含有位置信息的向量。</p>

<ul>
  <li>习得式位置编码（Learned Positional Encoding）：将位置编码当做训练参数，生成一个「最大长度 x 编码维度」的位置编码矩阵，随着训练进行更新。目前 Google BERT、OpenAI GPT 模型都是用的这种位置编码。缺点是「外推性」差，如果文本长度超过之前训练时用的「最大长度」则无法处理。目前有一些给出优化方案的论文，比如「<a href="https://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&amp;mid=2247515573&amp;idx=1&amp;sn=2d719108244ada7db3a535a435631210&amp;chksm=96ea6235a19deb23babde5eaac484d69e4c2f53bab72d2e350f75bed18323eea3cf9be30615b#rd">层次分解位置编码</a>」。</li>
  <li>三角式位置编码（Sinusoidal Positional Encodign）：上面提过了。</li>
  <li>循环式位置编码（Recurrent Positional Encoding）：通过一个 RNN 再接一个 Transformer，那么 RNN 暗含的「顺序」就导致不再需要额外编码了。但这样牺牲了并行性，毕竟 RNN 的两大缺点之一就有这个。</li>
  <li>相乘式位置编码（Product Positional Encoding）：用「 \(x_i \odot t_i\) 」代替「 \(x_i + t_i\) 」。</li>
</ul>

<h4 id="43相对位置编码和其他位置编码">4.3、相对位置编码和其他位置编码</h4>

<p>最早来自于 Google 的论文<a href="https://arxiv.org/abs/1803.02155">《Self-Attention with Relative Position Representations》</a>相对位置编码，考虑的是当前 position 与被 attention 的 position 之前的相对位置。</p>

<ul>
  <li>常见相对位置编码：经典式、XLNET 式、T5 式、DeBERTa 式等。</li>
  <li>其他位置编码：CNN 式、复数式、融合式等。</li>
</ul>

<p>到此我们都是在讲 Encoder，目前我们知道一个 Encoder 可以用如下的示意图表示：</p>

<p><img src="/img/src/2023-01-04-language-model-5-12.png" alt="image" width="680" /></p>

<h3 id="5编码器-encoder-和解码器-decoder">5、编码器 Encoder 和解码器 Decoder</h3>

<h4 id="51encoder-和-decoder-的图示结构">5.1、Encoder 和 Decoder 的图示结构</h4>

<p><img src="/img/src/2023-01-04-language-model-5-15.png" alt="image" width="165" /></p>

<ul>
  <li>第一层是多头注意力层（Multi-Head Attention Layer）。</li>
  <li>第二层是经过一个前馈神经网络（Feed Forward Neural Network，简称 FFNN）。</li>
  <li>这两层，每一层都有「Add &amp; Normalization」和 ResNet。</li>
</ul>

<p><img src="/img/src/2023-01-04-language-model-5-14.png" alt="image" width="179" /></p>

<ul>
  <li>解码器有两个多头注意力层。第一个多头注意力层是 Masked Multi-Head Attention 层，即在自注意力计算的过程中只有前面位置上的内容。第二个多头注意力层买有被 Masked，是个正常多头注意力层。</li>
  <li>可以看出来，第一个注意力层是一个自注意力层（Self Attention Layer），第二个是 Encoder-Decoder Attention 层（它的 K、V 来自 Encoder，Q 来自自注意力层），有些文章里会用这个角度来指代。</li>
  <li>FNN、Add &amp; Norm、ResNet 都与 Encoder 类似。</li>
</ul>

<h4 id="52decoder-的第一个输出结果">5.2、Decoder 的第一个输出结果</h4>

<p>产出第一个最终输出结果的过程：</p>

<ul>
  <li>不需要经过 Masked Multi-Head Attention Layer（自注意力层）。</li>
  <li>只经过 Encoder-Decoder Attention Layer。</li>
</ul>

<p><img src="/img/src/2023-01-04-language-model-5-13.png" alt="image" width="695" /></p>

<p>这样我们就像前面的 Encoder-Decoder Attention 模型一样，得到第一个输出。但是最终的输出结果，还会经过一层「Linear + Softmax」。</p>

<h4 id="53decoder-后续的所有输出">5.3、Decoder 后续的所有输出</h4>

<p>从产出第二个输出结果开始：</p>

<ul>
  <li>Decoder 的自注意力层，会用到前面的输出结果。</li>
  <li>可以看到，这是一个串行过程。</li>
</ul>

<h4 id="54decoder-之后的-linear-和-softmax">5.4、Decoder 之后的 Linear 和 Softmax</h4>

<p>经过所有 Decoder 之后，我们得到了一大堆浮点数的结果。最后的 Linear &amp; Softmax 就是来解决「怎么把它变成文本」的问题的。</p>

<ul>
  <li>Linear 是一个全连接神经网络，把 Decoders 输出的结果投影到一个超大的向量上，我们称之为 logits 向量。</li>
  <li>如果我们的输出词汇表有 1 万个词，那么 logits 向量的每一个维度就有 1 万个单元，每个单元都对应输出词汇表的一个词的概率。</li>
  <li>Softmax 将 logits 向量中的每一个维度都做归一化，这样每个维度都能从 1 万个单元对应的词概率中选出最大的，对应的词汇表里的词，就是输出词。最终得到一个输出字符串。</li>
</ul>

<h3 id="6transformer-模型整体">6、Transformer 模型整体</h3>

<p><img src="/img/src/2023-01-04-language-model-5-16.png" alt="image" width="660" /></p>

<p>最后我们再来整体看一下 Transformer：</p>

<ul>
  <li>首先输入数据生成词的嵌入式向量表示（Embedding），生成位置编码（Positional Encoding，简称 PE）。</li>
  <li>进入 Encoders 部分。先进入多头注意力层（Multi-Head Attention），是自注意力处理，然后进入全连接层（又叫前馈神经网络层），每层都有 ResNet、Add &amp; Norm。</li>
  <li>每一个 Encoder 的输入，都来自前一个 Encoder 的输出，但是第一个 Encoder 的输入就是 Embedding + PE。</li>
  <li>进入 Decoders 部分。先进入第一个多头注意力层（是 Masked 自注意力层），再进入第二个多头注意力层（是 Encoder-Decoder 注意力层），每层都有 ResNet、Add &amp; Norm。</li>
  <li>每一个 Decoder 都有两部分输入。</li>
  <li>Decoder 的第一层（Maksed 多头自注意力层）的输入，都来自前一个 Decoder 的输出，但是第一个 Decoder 是不经过第一层的（因为经过算出来也是 0）。</li>
  <li>Decoder 的第二层（Encoder-Decoder 注意力层）的输入，Q 都来自该 Decoder 的第一层，且每个 Decoder 的这一层的 K、V 都是一样的，均来自最后一个 Encoder。</li>
  <li>最后经过 Linear、Softmax 归一化。</li>
</ul>

<h3 id="7transformer-的性能">7、Transformer 的性能</h3>

<p>Google 在其博客于 2017.08.31 发布如下测试数据：</p>

<table>
  <thead>
    <tr>
      <th><img src="/img/src/2023-01-04-language-model-5-9.png" alt="image" /></th>
      <th><img src="/img/src/2023-01-04-language-model-5-10.png" alt="image" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td> </td>
      <td> </td>
    </tr>
  </tbody>
</table>

<h2 id="四一个基于-tensorflow-架构的-transformer-实现">四、一个基于 TensorFlow 架构的 Transformer 实现</h2>

<p>我们来看看一个简单的 Transformer 模型，就是比较早出现的 Kyubyong 实现的 Transformer 模型：https://github.com/Kyubyong/transformer/tree/master/tf1.2_legacy</p>

<h3 id="1先训练和测试一下-kyubyong-transformer">1、先训练和测试一下 Kyubyong Transformer</h3>

<p>下载一个「德语-英语翻译」的数据集：https://drive.google.com/uc?id=1l5y6Giag9aRPwGtuZHswh3w5v3qEz8D8</p>

<p>把 <code class="language-plaintext highlighter-rouge">de-en</code> 下面的 <code class="language-plaintext highlighter-rouge">tgz</code> 解压后放在 <code class="language-plaintext highlighter-rouge">corpora/</code> 目录下。如果需要先修改超参数，需要修改 <code class="language-plaintext highlighter-rouge">hyperparams.py</code>。然后运行如下命令，生成词汇文件（vocabulary files），默认到 <code class="language-plaintext highlighter-rouge">preprocessed</code> 目录下：</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python prepro.py
</code></pre></div></div>

<p>然后开始训练：</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python train.py
</code></pre></div></div>

<p>也可以跳过训练，直接<a href="https://www.dropbox.com/s/fo5wqgnbmvalwwq/logdir.zip?dl=0">下载预训练过的文件</a>，是一个 <code class="language-plaintext highlighter-rouge">logdir/</code> 目录，把它放到项目根目录下。然后可以对训练出来的结果，运行评价程序啦：</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python eval.py
</code></pre></div></div>

<p>会生成「德语-英语」测试结果文件在 <code class="language-plaintext highlighter-rouge">results/</code> 目录下，内容如下：</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- source: Sie war eine jährige Frau namens Alex
- expected: She was a yearold woman named Alex
- got: She was a &lt;UNK&gt; of vote called &lt;UNK&gt;

- source: Und als ich das hörte war ich erleichtert
- expected: Now when I heard this I was so relieved
- got: And when I was I &lt;UNK&gt; 's

- source: Meine Kommilitonin bekam nämlich einen Brandstifter als ersten Patienten
- expected: My classmate got an arsonist for her first client
- got: Because my first eye was a first show

- source: Das kriege ich hin dachte ich mir
- expected: This I thought I could handle
- got: I would give it to me a day

- source: Aber ich habe es nicht hingekriegt
- expected: But I didn't handle it
- got: But I didn't &lt;UNK&gt; &lt;UNK&gt;

- source: Ich hielt dagegen
- expected: I pushed back
- got: I &lt;UNK&gt;

...

Bleu Score = 6.598452846670836
</code></pre></div></div>

<p>评估结果文件的最后一行是 Bleu Score：</p>

<ul>
  <li>这是用来评估机器翻译质量的一种度量方式。它是由几个不同的 BLEU 分数组成的，每个 BLEU 分数都表示翻译结果中与参考翻译的重叠程度。</li>
  <li>一个常用的 BLEU 分数是 BLEU-4，它计算翻译结果中与参考翻译的 N 元文法语言模型 n-gram（n 为 4）的重叠程度。分数越高表示翻译结果越接近参考翻译。</li>
</ul>

<h3 id="2kyubyong-transformer-源码分析">2、Kyubyong Transformer 源码分析</h3>

<ul>
  <li><code class="language-plaintext highlighter-rouge">hparams.py</code>：超参数都在这里，仅 30 行。将在下面 <code class="language-plaintext highlighter-rouge">2.1</code> 部分解读。</li>
  <li><code class="language-plaintext highlighter-rouge">data_load.py</code>：装载、批处理数据的相关函数，代码仅 92 行。主要在下面 <code class="language-plaintext highlighter-rouge">2.2</code> 部分解读。</li>
  <li><code class="language-plaintext highlighter-rouge">prepro.py</code>：为 source 和 target 创建词汇文件（vocabulary file），代码仅 39 行。下面 <code class="language-plaintext highlighter-rouge">2.3</code> 部分会为大家解读。</li>
  <li><code class="language-plaintext highlighter-rouge">train.py</code>：代码仅 184 行。在下面 <code class="language-plaintext highlighter-rouge">2.4</code> 部分解读。</li>
  <li><code class="language-plaintext highlighter-rouge">modules.py</code>：Encoding / Decoding 网络的构建模块，代码仅 329 行。与 <code class="language-plaintext highlighter-rouge">modules.py</code> 一起会在 <code class="language-plaintext highlighter-rouge">2.4</code> 部分解读。</li>
  <li><code class="language-plaintext highlighter-rouge">eval.py</code>：评估效果，代码仅 82 行。将在 <code class="language-plaintext highlighter-rouge">2.5</code> 部分解读</li>
</ul>

<p>总计 700 多行代码。</p>

<h4 id="21超参数">2.1、超参数</h4>

<p><code class="language-plaintext highlighter-rouge">hyperparams.py</code> 文件中定义了 <code class="language-plaintext highlighter-rouge">Hyperparams</code> 超参数类，其中包含的参数我们逐一来解释一下：</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">source_train</code>：训练数据集的源输入文件，默认是 <code class="language-plaintext highlighter-rouge">'corpora/train.tags.de-en.de'</code></li>
  <li><code class="language-plaintext highlighter-rouge">target_train</code>：训练数据集的目标输出文件，默认是 <code class="language-plaintext highlighter-rouge">'corpora/train.tags.de-en.en'</code></li>
  <li><code class="language-plaintext highlighter-rouge">source_test</code>：测试数据集的源输入文件，默认是 <code class="language-plaintext highlighter-rouge">'corpora/IWSLT16.TED.tst2014.de-en.de.xml'</code></li>
  <li><code class="language-plaintext highlighter-rouge">target_test</code>：测试数据集的目标输出文件，默认是 <code class="language-plaintext highlighter-rouge">'corpora/IWSLT16.TED.tst2014.de-en.en.xml'</code></li>
  <li><code class="language-plaintext highlighter-rouge">batch_size</code>：设置每批数据的大小。</li>
  <li><code class="language-plaintext highlighter-rouge">lr</code>：设置学习率 learning rate。</li>
  <li><code class="language-plaintext highlighter-rouge">logdir</code>：设置日志文件保存的目录。</li>
  <li><code class="language-plaintext highlighter-rouge">maxlen</code></li>
  <li><code class="language-plaintext highlighter-rouge">min_cnt</code></li>
  <li><code class="language-plaintext highlighter-rouge">hidden_units</code>：设置编码器和解码器中隐藏层单元的数量。</li>
  <li><code class="language-plaintext highlighter-rouge">num_blocks</code>：编码器（encoder block）、解码器（decoder block）的数量</li>
  <li><code class="language-plaintext highlighter-rouge">num_epochs</code>：训练过程中迭代的次数。</li>
  <li><code class="language-plaintext highlighter-rouge">num_heads</code>：还记得上面文章里我们提到的 Transformer 中用到了多头注意力吧，这里就是多头注意力的头数。</li>
  <li><code class="language-plaintext highlighter-rouge">droupout_rate</code>：设置 dropout 层的 dropout rate，具体 dropout 请看 2.4.1 部分。</li>
  <li><code class="language-plaintext highlighter-rouge">sinusoid</code>：设置为 <code class="language-plaintext highlighter-rouge">True</code> 时表示使用正弦函数计算位置编码，否则为 <code class="language-plaintext highlighter-rouge">False</code> 时表示直接用 <code class="language-plaintext highlighter-rouge">position</code> 做位置编码。</li>
</ul>

<h4 id="22预处理">2.2、预处理</h4>

<p>文件 <code class="language-plaintext highlighter-rouge">prepro.py</code> 实现了预处理的过程，根据 <code class="language-plaintext highlighter-rouge">hp.source_train</code> 和 <code class="language-plaintext highlighter-rouge">hp.target_train</code> 分别创建 <code class="language-plaintext highlighter-rouge">"de.vocab.tsv"</code> 和 <code class="language-plaintext highlighter-rouge">"en.vocab.tsv"</code> 两个词汇表。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">make_vocab</span><span class="p">(</span><span class="n">fpath</span><span class="p">,</span> <span class="n">fname</span><span class="p">):</span>

    <span class="c1"># 使用 codecs.open 函数读取指定文件路径(fpath)的文本内容，并将其存储在 text 变量中
</span>    <span class="n">text</span> <span class="o">=</span> <span class="n">codecs</span><span class="p">.</span><span class="nb">open</span><span class="p">(</span><span class="n">fpath</span><span class="p">,</span> <span class="s">'r'</span><span class="p">,</span> <span class="s">'utf-8'</span><span class="p">).</span><span class="n">read</span><span class="p">()</span>

    <span class="c1"># 将 text 中的非字母和空格的字符去掉
</span>    <span class="n">text</span> <span class="o">=</span> <span class="n">regex</span><span class="p">.</span><span class="n">sub</span><span class="p">(</span><span class="s">"[^\s\p{Latin}']"</span><span class="p">,</span> <span class="s">""</span><span class="p">,</span> <span class="n">text</span><span class="p">)</span>

    <span class="c1"># 将 text 中的文本按照空格分割，并将每个单词存储在 words 变量中
</span>    <span class="n">words</span> <span class="o">=</span> <span class="n">text</span><span class="p">.</span><span class="n">split</span><span class="p">()</span>

    <span class="c1"># words 中每个单词的词频
</span>    <span class="n">word2cnt</span> <span class="o">=</span> <span class="n">Counter</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>

    <span class="c1"># 检查是否存在 preprocessed 文件夹，如果不存在就创建
</span>    <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">exists</span><span class="p">(</span><span class="s">'preprocessed'</span><span class="p">):</span> <span class="n">os</span><span class="p">.</span><span class="n">mkdir</span><span class="p">(</span><span class="s">'preprocessed'</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">codecs</span><span class="p">.</span><span class="nb">open</span><span class="p">(</span><span class="s">'preprocessed/{}'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">fname</span><span class="p">),</span> <span class="s">'w'</span><span class="p">,</span> <span class="s">'utf-8'</span><span class="p">)</span> <span class="k">as</span> <span class="n">fout</span><span class="p">:</span>

    	<span class="c1"># 按出现次数从多到少的顺序写入每个单词和它的出现次数
</span>    	<span class="c1"># 在文件最前面写入四个特殊字符 &lt;PAD&gt;, &lt;UNK&gt;, &lt;S&gt;, &lt;/S&gt; 分别用于填充，未知单词，句子开始和句子结束
</span>        <span class="n">fout</span><span class="p">.</span><span class="n">write</span><span class="p">(</span><span class="s">"{}</span><span class="se">\t</span><span class="s">1000000000</span><span class="se">\n</span><span class="s">{}</span><span class="se">\t</span><span class="s">1000000000</span><span class="se">\n</span><span class="s">{}</span><span class="se">\t</span><span class="s">1000000000</span><span class="se">\n</span><span class="s">{}</span><span class="se">\t</span><span class="s">1000000000</span><span class="se">\n</span><span class="s">"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="s">"&lt;PAD&gt;"</span><span class="p">,</span> <span class="s">"&lt;UNK&gt;"</span><span class="p">,</span> <span class="s">"&lt;S&gt;"</span><span class="p">,</span> <span class="s">"&lt;/S&gt;"</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">cnt</span> <span class="ow">in</span> <span class="n">word2cnt</span><span class="p">.</span><span class="n">most_common</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">word2cnt</span><span class="p">)):</span>
            <span class="n">fout</span><span class="p">.</span><span class="n">write</span><span class="p">(</span><span class="sa">u</span><span class="s">"{}</span><span class="se">\t</span><span class="s">{}</span><span class="se">\n</span><span class="s">"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">cnt</span><span class="p">))</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">'__main__'</span><span class="p">:</span>
    <span class="n">make_vocab</span><span class="p">(</span><span class="n">hp</span><span class="p">.</span><span class="n">source_train</span><span class="p">,</span> <span class="s">"de.vocab.tsv"</span><span class="p">)</span>
    <span class="n">make_vocab</span><span class="p">(</span><span class="n">hp</span><span class="p">.</span><span class="n">target_train</span><span class="p">,</span> <span class="s">"en.vocab.tsv"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"Done"</span><span class="p">)</span>
</code></pre></div></div>

<ul>
  <li>在主函数中调用 make_vocab 函数，在目录 <code class="language-plaintext highlighter-rouge">preprocessed</code> 生成 <code class="language-plaintext highlighter-rouge">de.vocab.tsv</code> 和 <code class="language-plaintext highlighter-rouge">en.vocab.tsv</code> 两个词汇表文件。</li>
  <li>在函数 <code class="language-plaintext highlighter-rouge">make_vocab</code> 中，先使用 <code class="language-plaintext highlighter-rouge">codecs.open</code> 函数读取指定文件路径 <code class="language-plaintext highlighter-rouge">fpath</code> 的文本内容，并将其存储在 <code class="language-plaintext highlighter-rouge">text</code> 变量中，再使用正则表达式 <code class="language-plaintext highlighter-rouge">regex</code> 将 <code class="language-plaintext highlighter-rouge">text</code> 中的非字母和空格的字符去掉，接着将 <code class="language-plaintext highlighter-rouge">text</code> 中的文本按照空格分割，并将每个单词存储在 <code class="language-plaintext highlighter-rouge">words</code> 变量中。</li>
  <li>接下来，使用 <code class="language-plaintext highlighter-rouge">Counter</code> 函数统计 <code class="language-plaintext highlighter-rouge">words</code> 中每个单词的出现次数，并将统计结果存储在 <code class="language-plaintext highlighter-rouge">word2cnt</code> 变量中。</li>
  <li>最后所有词与词频，存储在 <code class="language-plaintext highlighter-rouge">de.vocab.tsv</code> 和 <code class="language-plaintext highlighter-rouge">en.vocab.tsv</code> 两个文件中。</li>
</ul>

<h4 id="23训练测试数据集的加载">2.3、训练/测试数据集的加载</h4>

<p>我们先看下 <code class="language-plaintext highlighter-rouge">train.py</code>、<code class="language-plaintext highlighter-rouge">data_load.py</code>、<code class="language-plaintext highlighter-rouge">eval.py</code> 三个文件：</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">train.py</code>：该文件包含了 <code class="language-plaintext highlighter-rouge">Graph</code> 类的定义，并在其构造函数中调用 <code class="language-plaintext highlighter-rouge">load_data.py</code> 文件中的 <code class="language-plaintext highlighter-rouge">get_batch_data</code> 函数加载训练数据。</li>
  <li><code class="language-plaintext highlighter-rouge">data_load.py</code>：定义了加载训练数据、加载测试数据的函数。</li>
  <li><code class="language-plaintext highlighter-rouge">eval.py</code>：测试结果的评价函数定义在这个文件里。</li>
</ul>

<p>下面是函数调用的流程：</p>

<div style="text-align: center;">
<div class="graphviz-wrapper">

<!-- Generated by graphviz version 2.43.0 (0)
 -->
<!-- Title: G Pages: 1 -->
<svg role="img" aria-label="graphviz-9559986008ed2e1e47e8729260efda61" width="830pt" height="98pt" viewBox="0.00 0.00 830.00 98.00">
<title>graphviz-9559986008ed2e1e47e8729260efda61</title>
<desc>
digraph G {
	rankdir=LR
	splines=ortho
	node [shape=&quot;box&quot;]

	训练 -&gt; Graph构造函数 -&gt; get_batch_data -&gt; load_train_data
	测试 -&gt; eval -&gt; load_test_data

	load_train_data -&gt; create_data
	load_test_data -&gt; create_data

	create_data -&gt; load_de_vocab
	create_data -&gt; load_en_vocab
}
</desc>

<g id="graph0" class="graph" transform="scale(1 1) rotate(0) translate(4 94)">
<title>G</title>
<polygon fill="white" stroke="transparent" points="-4,4 -4,-94 826,-94 826,4 -4,4" />
<!-- 训练 -->
<g id="node1" class="node">
<title>训练</title>
<polygon fill="none" stroke="black" points="54,-90 0,-90 0,-54 54,-54 54,-90" />
<text text-anchor="middle" x="27" y="-68.3" font-family="Times,serif" font-size="14.00">训练</text>
</g>
<!-- Graph构造函数 -->
<g id="node2" class="node">
<title>Graph构造函数</title>
<polygon fill="none" stroke="black" points="208,-90 90,-90 90,-54 208,-54 208,-90" />
<text text-anchor="middle" x="149" y="-68.3" font-family="Times,serif" font-size="14.00">Graph构造函数</text>
</g>
<!-- 训练&#45;&gt;Graph构造函数 -->
<g id="edge1" class="edge">
<title>训练&#45;&gt;Graph构造函数</title>
<path fill="none" stroke="black" d="M54.08,-72C54.08,-72 79.54,-72 79.54,-72" />
<polygon fill="black" stroke="black" points="79.54,-75.5 89.54,-72 79.54,-68.5 79.54,-75.5" />
</g>
<!-- get_batch_data -->
<g id="node3" class="node">
<title>get_batch_data</title>
<polygon fill="none" stroke="black" points="369,-90 244,-90 244,-54 369,-54 369,-90" />
<text text-anchor="middle" x="306.5" y="-68.3" font-family="Times,serif" font-size="14.00">get_batch_data</text>
</g>
<!-- Graph构造函数&#45;&gt;get_batch_data -->
<g id="edge2" class="edge">
<title>Graph构造函数&#45;&gt;get_batch_data</title>
<path fill="none" stroke="black" d="M208.1,-72C208.1,-72 233.91,-72 233.91,-72" />
<polygon fill="black" stroke="black" points="233.91,-75.5 243.91,-72 233.91,-68.5 233.91,-75.5" />
</g>
<!-- load_train_data -->
<g id="node4" class="node">
<title>load_train_data</title>
<polygon fill="none" stroke="black" points="531,-90 405,-90 405,-54 531,-54 531,-90" />
<text text-anchor="middle" x="468" y="-68.3" font-family="Times,serif" font-size="14.00">load_train_data</text>
</g>
<!-- get_batch_data&#45;&gt;load_train_data -->
<g id="edge3" class="edge">
<title>get_batch_data&#45;&gt;load_train_data</title>
<path fill="none" stroke="black" d="M369.4,-72C369.4,-72 394.74,-72 394.74,-72" />
<polygon fill="black" stroke="black" points="394.74,-75.5 404.74,-72 394.74,-68.5 394.74,-75.5" />
</g>
<!-- create_data -->
<g id="node8" class="node">
<title>create_data</title>
<polygon fill="none" stroke="black" points="667,-63 567,-63 567,-27 667,-27 667,-63" />
<text text-anchor="middle" x="617" y="-41.3" font-family="Times,serif" font-size="14.00">create_data</text>
</g>
<!-- load_train_data&#45;&gt;create_data -->
<g id="edge6" class="edge">
<title>load_train_data&#45;&gt;create_data</title>
<path fill="none" stroke="black" d="M531.19,-58.5C531.19,-58.5 556.81,-58.5 556.81,-58.5" />
<polygon fill="black" stroke="black" points="556.81,-62 566.81,-58.5 556.81,-55 556.81,-62" />
</g>
<!-- 测试 -->
<g id="node5" class="node">
<title>测试</title>
<polygon fill="none" stroke="black" points="176,-36 122,-36 122,0 176,0 176,-36" />
<text text-anchor="middle" x="149" y="-14.3" font-family="Times,serif" font-size="14.00">测试</text>
</g>
<!-- eval -->
<g id="node6" class="node">
<title>eval</title>
<polygon fill="none" stroke="black" points="333.5,-36 279.5,-36 279.5,0 333.5,0 333.5,-36" />
<text text-anchor="middle" x="306.5" y="-14.3" font-family="Times,serif" font-size="14.00">eval</text>
</g>
<!-- 测试&#45;&gt;eval -->
<g id="edge4" class="edge">
<title>测试&#45;&gt;eval</title>
<path fill="none" stroke="black" d="M176.08,-18C176.08,-18 269.25,-18 269.25,-18" />
<polygon fill="black" stroke="black" points="269.25,-21.5 279.25,-18 269.25,-14.5 269.25,-21.5" />
</g>
<!-- load_test_data -->
<g id="node7" class="node">
<title>load_test_data</title>
<polygon fill="none" stroke="black" points="527.5,-36 408.5,-36 408.5,0 527.5,0 527.5,-36" />
<text text-anchor="middle" x="468" y="-14.3" font-family="Times,serif" font-size="14.00">load_test_data</text>
</g>
<!-- eval&#45;&gt;load_test_data -->
<g id="edge5" class="edge">
<title>eval&#45;&gt;load_test_data</title>
<path fill="none" stroke="black" d="M333.53,-18C333.53,-18 398.34,-18 398.34,-18" />
<polygon fill="black" stroke="black" points="398.34,-21.5 408.34,-18 398.34,-14.5 398.34,-21.5" />
</g>
<!-- load_test_data&#45;&gt;create_data -->
<g id="edge7" class="edge">
<title>load_test_data&#45;&gt;create_data</title>
<path fill="none" stroke="black" d="M527.75,-31.5C527.75,-31.5 556.82,-31.5 556.82,-31.5" />
<polygon fill="black" stroke="black" points="556.82,-35 566.82,-31.5 556.81,-28 556.82,-35" />
</g>
<!-- load_de_vocab -->
<g id="node9" class="node">
<title>load_de_vocab</title>
<polygon fill="none" stroke="black" points="822,-90 703,-90 703,-54 822,-54 822,-90" />
<text text-anchor="middle" x="762.5" y="-68.3" font-family="Times,serif" font-size="14.00">load_de_vocab</text>
</g>
<!-- create_data&#45;&gt;load_de_vocab -->
<g id="edge8" class="edge">
<title>create_data&#45;&gt;load_de_vocab</title>
<path fill="none" stroke="black" d="M667.07,-58.5C667.07,-58.5 692.8,-58.5 692.8,-58.5" />
<polygon fill="black" stroke="black" points="692.8,-62 702.8,-58.5 692.8,-55 692.8,-62" />
</g>
<!-- load_en_vocab -->
<g id="node10" class="node">
<title>load_en_vocab</title>
<polygon fill="none" stroke="black" points="822,-36 703,-36 703,0 822,0 822,-36" />
<text text-anchor="middle" x="762.5" y="-14.3" font-family="Times,serif" font-size="14.00">load_en_vocab</text>
</g>
<!-- create_data&#45;&gt;load_en_vocab -->
<g id="edge9" class="edge">
<title>create_data&#45;&gt;load_en_vocab</title>
<path fill="none" stroke="black" d="M667.07,-31.5C667.07,-31.5 692.8,-31.5 692.8,-31.5" />
<polygon fill="black" stroke="black" points="692.8,-35 702.8,-31.5 692.8,-28 692.8,-35" />
</g>
</g>
</svg>
</div>
</div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">load_de_vocab</span><span class="p">():</span>
    <span class="n">vocab</span> <span class="o">=</span> <span class="p">[</span><span class="n">line</span><span class="p">.</span><span class="n">split</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">codecs</span><span class="p">.</span><span class="nb">open</span><span class="p">(</span><span class="s">'preprocessed/de.vocab.tsv'</span><span class="p">,</span> <span class="s">'r'</span><span class="p">,</span> <span class="s">'utf-8'</span><span class="p">).</span><span class="n">read</span><span class="p">().</span><span class="n">splitlines</span><span class="p">()</span> <span class="k">if</span> <span class="nb">int</span><span class="p">(</span><span class="n">line</span><span class="p">.</span><span class="n">split</span><span class="p">()[</span><span class="mi">1</span><span class="p">])</span><span class="o">&gt;=</span><span class="n">hp</span><span class="p">.</span><span class="n">min_cnt</span><span class="p">]</span>
    <span class="n">word2idx</span> <span class="o">=</span> <span class="p">{</span><span class="n">word</span><span class="p">:</span> <span class="n">idx</span> <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">word</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">vocab</span><span class="p">)}</span>
    <span class="n">idx2word</span> <span class="o">=</span> <span class="p">{</span><span class="n">idx</span><span class="p">:</span> <span class="n">word</span> <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">word</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">vocab</span><span class="p">)}</span>
    <span class="k">return</span> <span class="n">word2idx</span><span class="p">,</span> <span class="n">idx2word</span>

<span class="k">def</span> <span class="nf">load_en_vocab</span><span class="p">():</span>
    <span class="n">vocab</span> <span class="o">=</span> <span class="p">[</span><span class="n">line</span><span class="p">.</span><span class="n">split</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">codecs</span><span class="p">.</span><span class="nb">open</span><span class="p">(</span><span class="s">'preprocessed/en.vocab.tsv'</span><span class="p">,</span> <span class="s">'r'</span><span class="p">,</span> <span class="s">'utf-8'</span><span class="p">).</span><span class="n">read</span><span class="p">().</span><span class="n">splitlines</span><span class="p">()</span> <span class="k">if</span> <span class="nb">int</span><span class="p">(</span><span class="n">line</span><span class="p">.</span><span class="n">split</span><span class="p">()[</span><span class="mi">1</span><span class="p">])</span><span class="o">&gt;=</span><span class="n">hp</span><span class="p">.</span><span class="n">min_cnt</span><span class="p">]</span>
    <span class="n">word2idx</span> <span class="o">=</span> <span class="p">{</span><span class="n">word</span><span class="p">:</span> <span class="n">idx</span> <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">word</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">vocab</span><span class="p">)}</span>
    <span class="n">idx2word</span> <span class="o">=</span> <span class="p">{</span><span class="n">idx</span><span class="p">:</span> <span class="n">word</span> <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">word</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">vocab</span><span class="p">)}</span>
    <span class="k">return</span> <span class="n">word2idx</span><span class="p">,</span> <span class="n">idx2word</span>
</code></pre></div></div>

<p>将 <code class="language-plaintext highlighter-rouge">preprocessed/de.vocab.tsv</code> 和 <code class="language-plaintext highlighter-rouge">preprocessed/en.vocab.tsv</code> 中储存的德语、英语的词汇、词频，载入成 <code class="language-plaintext highlighter-rouge">word2idx</code> 和 <code class="language-plaintext highlighter-rouge">idx2word</code>。前者是通过词查询词向量，后者通过词向量查询词。</p>

<p><code class="language-plaintext highlighter-rouge">load_de_vocab</code> 和 <code class="language-plaintext highlighter-rouge">load_en_vocab</code> 函数被 <code class="language-plaintext highlighter-rouge">create_data</code> 函数引用，该函数将输入的源语言和目标语言句子转换为索引表示，并对过长的句子进行截断或填充。详细的解释看下面代码里的注释。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 输入参数是翻译模型的源语言语句、目标语言语句
</span><span class="k">def</span> <span class="nf">create_data</span><span class="p">(</span><span class="n">source_sents</span><span class="p">,</span> <span class="n">target_sents</span><span class="p">):</span>

    <span class="n">de2idx</span><span class="p">,</span> <span class="n">idx2de</span> <span class="o">=</span> <span class="n">load_de_vocab</span><span class="p">()</span>
    <span class="n">en2idx</span><span class="p">,</span> <span class="n">idx2en</span> <span class="o">=</span> <span class="n">load_en_vocab</span><span class="p">()</span>
    
    <span class="c1"># 用 zip 函数将源语言和目标语言句子对应起来，并对句子进行截断或填充
</span>    <span class="n">x_list</span><span class="p">,</span> <span class="n">y_list</span><span class="p">,</span> <span class="n">Sources</span><span class="p">,</span> <span class="n">Targets</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">source_sent</span><span class="p">,</span> <span class="n">target_sent</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">source_sents</span><span class="p">,</span> <span class="n">target_sents</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="n">de2idx</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="p">(</span><span class="n">source_sent</span> <span class="o">+</span> <span class="sa">u</span><span class="s">" &lt;/S&gt;"</span><span class="p">).</span><span class="n">split</span><span class="p">()]</span> <span class="c1"># 1: OOV, &lt;/S&gt;: End of Text
</span>        <span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="n">en2idx</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="p">(</span><span class="n">target_sent</span> <span class="o">+</span> <span class="sa">u</span><span class="s">" &lt;/S&gt;"</span><span class="p">).</span><span class="n">split</span><span class="p">()]</span> 

        <span class="c1"># 将句子的词的编号，原句以及编号后的句子存储下来，以供之后使用
</span>        <span class="k">if</span> <span class="nb">max</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">))</span> <span class="o">&lt;=</span><span class="n">hp</span><span class="p">.</span><span class="n">maxlen</span><span class="p">:</span>

        	<span class="c1"># 将 x 和 y 转换成 numpy 数组并加入 x_list 和 y_list 中
</span>            <span class="n">x_list</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
            <span class="n">y_list</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>

            <span class="c1"># 将原始的 source_sent 和 target_sent 加入 Sources 和 Targets 列表中
</span>            <span class="n">Sources</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">source_sent</span><span class="p">)</span>
            <span class="n">Targets</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">target_sent</span><span class="p">)</span>
    
    <span class="c1"># 对于每个 (x, y) 对，使用 np.lib.pad 函数将 x 和 y 分别用 0 进行填充，直到长度为 hp.maxlen
</span>    <span class="c1"># 这样做的目的是使得每个句子长度都相等，方便后续的训练
</span>    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">x_list</span><span class="p">),</span> <span class="n">hp</span><span class="p">.</span><span class="n">maxlen</span><span class="p">],</span> <span class="n">np</span><span class="p">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">y_list</span><span class="p">),</span> <span class="n">hp</span><span class="p">.</span><span class="n">maxlen</span><span class="p">],</span> <span class="n">np</span><span class="p">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">x_list</span><span class="p">,</span> <span class="n">y_list</span><span class="p">)):</span>
        <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">lib</span><span class="p">.</span><span class="n">pad</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">hp</span><span class="p">.</span><span class="n">maxlen</span><span class="o">-</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)],</span> <span class="s">'constant'</span><span class="p">,</span> <span class="n">constant_values</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
        <span class="n">Y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">lib</span><span class="p">.</span><span class="n">pad</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">hp</span><span class="p">.</span><span class="n">maxlen</span><span class="o">-</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)],</span> <span class="s">'constant'</span><span class="p">,</span> <span class="n">constant_values</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>

    <span class="c1"># 返回转换后的索引表示，以及未经处理的源语言和目标语言句子
</span>    <span class="c1"># X 是原始句子中德语的索引
</span>    <span class="c1"># Y 是原始句子中英语的索引
</span>    <span class="c1"># Sources 是源原始句子列表，并与 X 一一对应
</span>    <span class="c1"># Targets 是目标原始句子列表，并与 Y 一一对应
</span>    <span class="k">return</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">Sources</span><span class="p">,</span> <span class="n">Targets</span>

<span class="c1"># 返回原始句子中德语、英语的索引
</span><span class="k">def</span> <span class="nf">load_train_data</span><span class="p">():</span>
    <span class="n">de_sents</span> <span class="o">=</span> <span class="p">[</span><span class="n">regex</span><span class="p">.</span><span class="n">sub</span><span class="p">(</span><span class="s">"[^\s\p{Latin}']"</span><span class="p">,</span> <span class="s">""</span><span class="p">,</span> <span class="n">line</span><span class="p">)</span> <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">codecs</span><span class="p">.</span><span class="nb">open</span><span class="p">(</span><span class="n">hp</span><span class="p">.</span><span class="n">source_train</span><span class="p">,</span> <span class="s">'r'</span><span class="p">,</span> <span class="s">'utf-8'</span><span class="p">).</span><span class="n">read</span><span class="p">().</span><span class="n">split</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">"</span><span class="p">)</span> <span class="k">if</span> <span class="n">line</span> <span class="ow">and</span> <span class="n">line</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="s">"&lt;"</span><span class="p">]</span>
    <span class="n">en_sents</span> <span class="o">=</span> <span class="p">[</span><span class="n">regex</span><span class="p">.</span><span class="n">sub</span><span class="p">(</span><span class="s">"[^\s\p{Latin}']"</span><span class="p">,</span> <span class="s">""</span><span class="p">,</span> <span class="n">line</span><span class="p">)</span> <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">codecs</span><span class="p">.</span><span class="nb">open</span><span class="p">(</span><span class="n">hp</span><span class="p">.</span><span class="n">target_train</span><span class="p">,</span> <span class="s">'r'</span><span class="p">,</span> <span class="s">'utf-8'</span><span class="p">).</span><span class="n">read</span><span class="p">().</span><span class="n">split</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">"</span><span class="p">)</span> <span class="k">if</span> <span class="n">line</span> <span class="ow">and</span> <span class="n">line</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="s">"&lt;"</span><span class="p">]</span>
    
    <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">Sources</span><span class="p">,</span> <span class="n">Targets</span> <span class="o">=</span> <span class="n">create_data</span><span class="p">(</span><span class="n">de_sents</span><span class="p">,</span> <span class="n">en_sents</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span>
</code></pre></div></div>

<p>下面的 <code class="language-plaintext highlighter-rouge">get_batch_data</code> 则从文本数据中读取并生成 batch：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">get_batch_data</span><span class="p">():</span>
    
    <span class="c1"># 加载数据
</span>    <span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">load_train_data</span><span class="p">()</span>
    
    <span class="c1"># calc total batch count
</span>    <span class="n">num_batch</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">//</span> <span class="n">hp</span><span class="p">.</span><span class="n">batch_size</span>
    
    <span class="c1"># 将 X 和 Y 转换成张量
</span>    <span class="n">X</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="n">Y</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">int32</span><span class="p">)</span>
    
    <span class="c1"># 创建输入队列
</span>    <span class="n">input_queues</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">train</span><span class="p">.</span><span class="n">slice_input_producer</span><span class="p">([</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">])</span>
            
    <span class="c1"># 创建 batch 队列，利用 shuffle_batch 将一组 tensor 随机打乱，并将它们分为多个 batch
</span>    <span class="c1"># 使用 shuffle_batch 是为了防止模型过拟合
</span>    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">train</span><span class="p">.</span><span class="n">shuffle_batch</span><span class="p">(</span><span class="n">input_queues</span><span class="p">,</span>
                                <span class="n">num_threads</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
                                <span class="n">batch_size</span><span class="o">=</span><span class="n">hp</span><span class="p">.</span><span class="n">batch_size</span><span class="p">,</span> 
                                <span class="n">capacity</span><span class="o">=</span><span class="n">hp</span><span class="p">.</span><span class="n">batch_size</span><span class="o">*</span><span class="mi">64</span><span class="p">,</span>   
                                <span class="n">min_after_dequeue</span><span class="o">=</span><span class="n">hp</span><span class="p">.</span><span class="n">batch_size</span><span class="o">*</span><span class="mi">32</span><span class="p">,</span> 
                                <span class="n">allow_smaller_final_batch</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">num_batch</span> <span class="c1"># (N, T), (N, T), ()
</span></code></pre></div></div>

<h4 id="24构建模型并训练">2.4、构建模型并训练</h4>

<p>Graph 的构造函数流程，就是模型的构建流程，下面船长来分析这部分代码。</p>

<div style="text-align: center;">
<div class="graphviz-wrapper">

<!-- Generated by graphviz version 2.43.0 (0)
 -->
<!-- Title: G Pages: 1 -->
<svg role="img" aria-label="graphviz-d43b60bdbb0d309ff41c0875976a1d4b" width="526pt" height="44pt" viewBox="0.00 0.00 526.00 44.00">
<title>graphviz-d43b60bdbb0d309ff41c0875976a1d4b</title>
<desc>
digraph G {
	rankdir=LR
	splines=ortho
	node [shape=&quot;box&quot;]

	Graph构造函数 -&gt; 编码器 -&gt; 解码器 -&gt; Linear -&gt; Softmax
}
</desc>

<g id="graph0" class="graph" transform="scale(1 1) rotate(0) translate(4 40)">
<title>G</title>
<polygon fill="white" stroke="transparent" points="-4,4 -4,-40 522,-40 522,4 -4,4" />
<!-- Graph构造函数 -->
<g id="node1" class="node">
<title>Graph构造函数</title>
<polygon fill="none" stroke="black" points="118,-36 0,-36 0,0 118,0 118,-36" />
<text text-anchor="middle" x="59" y="-14.3" font-family="Times,serif" font-size="14.00">Graph构造函数</text>
</g>
<!-- 编码器 -->
<g id="node2" class="node">
<title>编码器</title>
<polygon fill="none" stroke="black" points="213,-36 154,-36 154,0 213,0 213,-36" />
<text text-anchor="middle" x="183.5" y="-14.3" font-family="Times,serif" font-size="14.00">编码器</text>
</g>
<!-- Graph构造函数&#45;&gt;编码器 -->
<g id="edge1" class="edge">
<title>Graph构造函数&#45;&gt;编码器</title>
<path fill="none" stroke="black" d="M118.33,-18C118.33,-18 143.7,-18 143.7,-18" />
<polygon fill="black" stroke="black" points="143.7,-21.5 153.7,-18 143.7,-14.5 143.7,-21.5" />
</g>
<!-- 解码器 -->
<g id="node3" class="node">
<title>解码器</title>
<polygon fill="none" stroke="black" points="308,-36 249,-36 249,0 308,0 308,-36" />
<text text-anchor="middle" x="278.5" y="-14.3" font-family="Times,serif" font-size="14.00">解码器</text>
</g>
<!-- 编码器&#45;&gt;解码器 -->
<g id="edge2" class="edge">
<title>编码器&#45;&gt;解码器</title>
<path fill="none" stroke="black" d="M213.04,-18C213.04,-18 238.98,-18 238.98,-18" />
<polygon fill="black" stroke="black" points="238.98,-21.5 248.98,-18 238.98,-14.5 238.98,-21.5" />
</g>
<!-- Linear -->
<g id="node4" class="node">
<title>Linear</title>
<polygon fill="none" stroke="black" points="406,-36 344,-36 344,0 406,0 406,-36" />
<text text-anchor="middle" x="375" y="-14.3" font-family="Times,serif" font-size="14.00">Linear</text>
</g>
<!-- 解码器&#45;&gt;Linear -->
<g id="edge3" class="edge">
<title>解码器&#45;&gt;Linear</title>
<path fill="none" stroke="black" d="M308.24,-18C308.24,-18 333.85,-18 333.85,-18" />
<polygon fill="black" stroke="black" points="333.85,-21.5 343.85,-18 333.85,-14.5 333.85,-21.5" />
</g>
<!-- Softmax -->
<g id="node5" class="node">
<title>Softmax</title>
<polygon fill="none" stroke="black" points="518,-36 442,-36 442,0 518,0 518,-36" />
<text text-anchor="middle" x="480" y="-14.3" font-family="Times,serif" font-size="14.00">Softmax</text>
</g>
<!-- Linear&#45;&gt;Softmax -->
<g id="edge4" class="edge">
<title>Linear&#45;&gt;Softmax</title>
<path fill="none" stroke="black" d="M406.22,-18C406.22,-18 431.65,-18 431.65,-18" />
<polygon fill="black" stroke="black" points="431.65,-21.5 441.65,-18 431.65,-14.5 431.65,-21.5" />
</g>
</g>
</svg>
</div>
</div>

<p>整体这个流程，主要涉及 <code class="language-plaintext highlighter-rouge">train.py</code> 文件和 <code class="language-plaintext highlighter-rouge">modules.py</code> 文件。所有模型所需的主要函数定义，都是在 <code class="language-plaintext highlighter-rouge">modules.py</code> 中实现的。我们先看下编码器（Encoder）的流程：</p>

<div style="text-align: center;">
<div class="graphviz-wrapper">

<!-- Generated by graphviz version 2.43.0 (0)
 -->
<!-- Title: G Pages: 1 -->
<svg role="img" aria-label="graphviz-4459d4df0778b105cf972d664023b546" width="169pt" height="332pt" viewBox="0.00 0.00 169.00 332.00">
<title>graphviz-4459d4df0778b105cf972d664023b546</title>
<desc>
digraph G {
	rankdir=BT
	splines=ortho
	node [shape=&quot;box&quot;]

	embedding -&gt; positional_encoding -&gt; dropout -&gt; multihead_attention -&gt; feedforward
}
</desc>

<g id="graph0" class="graph" transform="scale(1 1) rotate(0) translate(4 328)">
<title>G</title>
<polygon fill="white" stroke="transparent" points="-4,4 -4,-328 165,-328 165,4 -4,4" />
<!-- embedding -->
<g id="node1" class="node">
<title>embedding</title>
<polygon fill="none" stroke="black" points="128.5,-36 32.5,-36 32.5,0 128.5,0 128.5,-36" />
<text text-anchor="middle" x="80.5" y="-14.3" font-family="Times,serif" font-size="14.00">embedding</text>
</g>
<!-- positional_encoding -->
<g id="node2" class="node">
<title>positional_encoding</title>
<polygon fill="none" stroke="black" points="159.5,-108 1.5,-108 1.5,-72 159.5,-72 159.5,-108" />
<text text-anchor="middle" x="80.5" y="-86.3" font-family="Times,serif" font-size="14.00">positional_encoding</text>
</g>
<!-- embedding&#45;&gt;positional_encoding -->
<g id="edge1" class="edge">
<title>embedding&#45;&gt;positional_encoding</title>
<path fill="none" stroke="black" d="M80.5,-36.17C80.5,-36.17 80.5,-61.59 80.5,-61.59" />
<polygon fill="black" stroke="black" points="77,-61.59 80.5,-71.59 84,-61.59 77,-61.59" />
</g>
<!-- dropout -->
<g id="node3" class="node">
<title>dropout</title>
<polygon fill="none" stroke="black" points="117,-180 44,-180 44,-144 117,-144 117,-180" />
<text text-anchor="middle" x="80.5" y="-158.3" font-family="Times,serif" font-size="14.00">dropout</text>
</g>
<!-- positional_encoding&#45;&gt;dropout -->
<g id="edge2" class="edge">
<title>positional_encoding&#45;&gt;dropout</title>
<path fill="none" stroke="black" d="M80.5,-108.17C80.5,-108.17 80.5,-133.59 80.5,-133.59" />
<polygon fill="black" stroke="black" points="77,-133.59 80.5,-143.59 84,-133.59 77,-133.59" />
</g>
<!-- multihead_attention -->
<g id="node4" class="node">
<title>multihead_attention</title>
<polygon fill="none" stroke="black" points="161,-252 0,-252 0,-216 161,-216 161,-252" />
<text text-anchor="middle" x="80.5" y="-230.3" font-family="Times,serif" font-size="14.00">multihead_attention</text>
</g>
<!-- dropout&#45;&gt;multihead_attention -->
<g id="edge3" class="edge">
<title>dropout&#45;&gt;multihead_attention</title>
<path fill="none" stroke="black" d="M80.5,-180.17C80.5,-180.17 80.5,-205.59 80.5,-205.59" />
<polygon fill="black" stroke="black" points="77,-205.59 80.5,-215.59 84,-205.59 77,-205.59" />
</g>
<!-- feedforward -->
<g id="node5" class="node">
<title>feedforward</title>
<polygon fill="none" stroke="black" points="132.5,-324 28.5,-324 28.5,-288 132.5,-288 132.5,-324" />
<text text-anchor="middle" x="80.5" y="-302.3" font-family="Times,serif" font-size="14.00">feedforward</text>
</g>
<!-- multihead_attention&#45;&gt;feedforward -->
<g id="edge4" class="edge">
<title>multihead_attention&#45;&gt;feedforward</title>
<path fill="none" stroke="black" d="M80.5,-252.17C80.5,-252.17 80.5,-277.59 80.5,-277.59" />
<polygon fill="black" stroke="black" points="77,-277.59 80.5,-287.59 84,-277.59 77,-277.59" />
</g>
</g>
</svg>
</div>
</div>

<p>下面是 <code class="language-plaintext highlighter-rouge">train.py</code> 中实现的 Transformer 流程，其中的每一段代码，船长都会做详细解释，先不用急。这个流程里，首先定义了编码器，先使用了 Embedding 层将输入数据转换为词向量，使用 Positional Encoding 层对词向量进行位置编码，使用 Dropout 层进行 dropout 操作，然后进行多层 Multihead Attention 和 Feed Forward 操作。</p>

<p>在构建模型前，先执行 <code class="language-plaintext highlighter-rouge">train.py</code> 的主程序段，首先 <code class="language-plaintext highlighter-rouge">if __name__ == '__main__'</code> 这句代码是在 Python 中常用的一种编写方式，它的意思是当一个文件被直接运行时，<code class="language-plaintext highlighter-rouge">if</code> 语句下面的代码会被执行。请看下面代码的注释。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">'__main__'</span><span class="p">:</span>                
    
    <span class="c1"># 加载词汇表   
</span>    <span class="n">de2idx</span><span class="p">,</span> <span class="n">idx2de</span> <span class="o">=</span> <span class="n">load_de_vocab</span><span class="p">()</span>
    <span class="n">en2idx</span><span class="p">,</span> <span class="n">idx2en</span> <span class="o">=</span> <span class="n">load_en_vocab</span><span class="p">()</span>
    
    <span class="c1"># 构建模型并训练
</span>    <span class="n">g</span> <span class="o">=</span> <span class="n">Graph</span><span class="p">(</span><span class="s">"train"</span><span class="p">);</span> <span class="k">print</span><span class="p">(</span><span class="s">"Graph loaded"</span><span class="p">)</span>
    
    <span class="c1"># 创建了一个 Supervisor 对象来管理训练过程
</span>    <span class="n">sv</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">train</span><span class="p">.</span><span class="n">Supervisor</span><span class="p">(</span><span class="n">graph</span><span class="o">=</span><span class="n">g</span><span class="p">.</span><span class="n">graph</span><span class="p">,</span> 
                             <span class="n">logdir</span><span class="o">=</span><span class="n">hp</span><span class="p">.</span><span class="n">logdir</span><span class="p">,</span>
                             <span class="n">save_model_secs</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="c1"># 使用 with 语句打开一个会话
</span>    <span class="k">with</span> <span class="n">sv</span><span class="p">.</span><span class="n">managed_session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>

    	<span class="c1"># 训练迭代 hp.num_epochs 次
</span>        <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">hp</span><span class="p">.</span><span class="n">num_epochs</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span> 
            <span class="k">if</span> <span class="n">sv</span><span class="p">.</span><span class="n">should_stop</span><span class="p">():</span> <span class="k">break</span>

            <span class="c1"># tqdm 是一个 Python 库，用来在循环执行训练操作时在命令行中显示进度条
</span>            <span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">g</span><span class="p">.</span><span class="n">num_batch</span><span class="p">),</span> <span class="n">total</span><span class="o">=</span><span class="n">g</span><span class="p">.</span><span class="n">num_batch</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">70</span><span class="p">,</span> <span class="n">leave</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">unit</span><span class="o">=</span><span class="s">'b'</span><span class="p">):</span>

            	<span class="c1"># 每次迭代都会运行训练操作 g.train_op
</span>                <span class="n">sess</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">g</span><span class="p">.</span><span class="n">train_op</span><span class="p">)</span>

            <span class="c1"># 获取训练的步数，通过 sess.run() 函数获取 global_step 的当前值并赋值给 gs。这样可在后面使用 gs 保存模型时用这个值命名模型
</span>            <span class="n">gs</span> <span class="o">=</span> <span class="n">sess</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">g</span><span class="p">.</span><span class="n">global_step</span><span class="p">)</span>

            <span class="c1"># 每个 epoch 结束时，它使用 saver.save() 函数保存当前模型的状态
</span>            <span class="n">sv</span><span class="p">.</span><span class="n">saver</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="n">hp</span><span class="p">.</span><span class="n">logdir</span> <span class="o">+</span> <span class="s">'/model_epoch_%02d_gs_%d'</span> <span class="o">%</span> <span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="n">gs</span><span class="p">))</span>
    
    <span class="k">print</span><span class="p">(</span><span class="s">"Done"</span><span class="p">)</span>
</code></pre></div></div>

<ul>
  <li><code class="language-plaintext highlighter-rouge">num_epochs</code> 是训练过程中迭代的次数，它表示训练模型需要在训练数据上跑多少遍。每一次迭代都会在训练数据集上进行训练，通常来说，训练数据集会被重复多次迭代，直到达到 <code class="language-plaintext highlighter-rouge">num_epochs</code> 次。这样可以确保模型能够充分地学习数据的特征。设置 <code class="language-plaintext highlighter-rouge">num_epochs</code> 的值过大或过小都会导致模型性能下降。</li>
</ul>

<h5 id="241编码过程">2.4.1、编码过程</h5>

<h6 id="embedding">Embedding</h6>

<p><code class="language-plaintext highlighter-rouge">embedding</code> 用来把输入生成词嵌入向量：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 词语转换为对应的词向量表示
</span><span class="bp">self</span><span class="p">.</span><span class="n">enc</span> <span class="o">=</span> <span class="n">embedding</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">x</span><span class="p">,</span> 
                      <span class="n">vocab_size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">de2idx</span><span class="p">),</span> 
                      <span class="n">num_units</span><span class="o">=</span><span class="n">hp</span><span class="p">.</span><span class="n">hidden_units</span><span class="p">,</span> 
                      <span class="n">scale</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                      <span class="n">scope</span><span class="o">=</span><span class="s">"enc_embed"</span><span class="p">)</span>
</code></pre></div></div>

<ul>
  <li><code class="language-plaintext highlighter-rouge">vocab_size</code> 是词汇表的大小。</li>
  <li><code class="language-plaintext highlighter-rouge">num_units</code> 是词向量的维度。</li>
  <li><code class="language-plaintext highlighter-rouge">scale</code> 是一个布尔值，用来确定是否对词向量进行标准化。</li>
  <li><code class="language-plaintext highlighter-rouge">scope</code> 是变量作用域的名称。</li>
</ul>

<h6 id="key-masks">Key Masks</h6>

<p>接着生成一个 <code class="language-plaintext highlighter-rouge">key_masks</code> 用于在之后的计算中屏蔽掉某些位置的信息，以便模型只关注有效的信息。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">key_masks</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">sign</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="nb">abs</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">enc</span><span class="p">),</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<ul>
  <li>先对 <code class="language-plaintext highlighter-rouge">self.enc</code> 张量进行对每个元素求绝对值的操作</li>
  <li>沿着最后一阶作为轴，进行 <code class="language-plaintext highlighter-rouge">reduce_sum</code> 操作，得到一个 (batch, sequence_length) 形状的张量。</li>
  <li>再进行 <code class="language-plaintext highlighter-rouge">tf.sign</code> 操作，对刚得到的每个元素进行符号函数的变换。</li>
  <li>最后再扩展阶数，变成形状 (batch, sequence_length, 1) 的张量。</li>
</ul>

<h6 id="positional-encoding">Positional Encoding</h6>

<p>下面生成 Transformer 的位置编码：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 位置编码
</span><span class="k">if</span> <span class="n">hp</span><span class="p">.</span><span class="n">sinusoid</span><span class="p">:</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">enc</span> <span class="o">+=</span> <span class="n">positional_encoding</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">x</span><span class="p">,</span>
                      <span class="n">num_units</span><span class="o">=</span><span class="n">hp</span><span class="p">.</span><span class="n">hidden_units</span><span class="p">,</span> 
                      <span class="n">zero_pad</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> 
                      <span class="n">scale</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
                      <span class="n">scope</span><span class="o">=</span><span class="s">"enc_pe"</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">enc</span> <span class="o">+=</span> <span class="n">embedding</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">tile</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="nb">range</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">shape</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">x</span><span class="p">)[</span><span class="mi">1</span><span class="p">]),</span> <span class="mi">0</span><span class="p">),</span>
    							 <span class="p">[</span><span class="n">tf</span><span class="p">.</span><span class="n">shape</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">x</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">]),</span>
                      <span class="n">vocab_size</span><span class="o">=</span><span class="n">hp</span><span class="p">.</span><span class="n">maxlen</span><span class="p">,</span> 
                      <span class="n">num_units</span><span class="o">=</span><span class="n">hp</span><span class="p">.</span><span class="n">hidden_units</span><span class="p">,</span> 
                      <span class="n">zero_pad</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> 
                      <span class="n">scale</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
                      <span class="n">scope</span><span class="o">=</span><span class="s">"enc_pe"</span><span class="p">)</span>
</code></pre></div></div>

<p>如果超参数 <code class="language-plaintext highlighter-rouge">hp.sinusoid=True</code>，使用 <code class="language-plaintext highlighter-rouge">positional_encoding</code> 函数，通过使用正弦和余弦函数来生成位置编码，可以为输入序列添加位置信息。如果 <code class="language-plaintext highlighter-rouge">hp.sinusoid=False</code>，使用 <code class="language-plaintext highlighter-rouge">embedding</code> 函数，通过学习的词嵌入来生成位置编码。</p>

<p>位置编码生成后，用 <code class="language-plaintext highlighter-rouge">key_masks</code> 处理一下。注意 <code class="language-plaintext highlighter-rouge">key_masks</code> 的生成一定要用最初的 <code class="language-plaintext highlighter-rouge">self.enc</code>，所以在前面执行而不是这里：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="bp">self</span><span class="p">.</span><span class="n">enc</span> <span class="o">*=</span> <span class="n">key_masks</span>
</code></pre></div></div>

<p>这个不是矩阵乘法，而是对应元素相乘。这里乘上 <code class="language-plaintext highlighter-rouge">key_masks</code> 的目的是将 <code class="language-plaintext highlighter-rouge">key_masks</code> 中值为 0 的位置对应的 <code class="language-plaintext highlighter-rouge">self.enc</code> 中的元素置为 0，这样就可以排除这些位置对计算的影响。</p>

<h6 id="drop-out">Drop out</h6>

<p>下面调用了 TensorFlow 的 drop out 操作：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="bp">self</span><span class="p">.</span><span class="n">enc</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">dropout</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">enc</span><span class="p">,</span> 
                            <span class="n">rate</span><span class="o">=</span><span class="n">hp</span><span class="p">.</span><span class="n">dropout_rate</span><span class="p">,</span> 
                            <span class="n">training</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">is_training</span><span class="p">))</span>
</code></pre></div></div>

<p>drop out 是一种在深度学习中常用的正则化技巧。它通过在训练过程中随机地「关闭」一些神经元来减少 <strong>过拟合</strong>。这样做是为了防止模型过于依赖于某些特定的特征，而导致在新数据上的表现不佳。</p>

<p>在这个函数中，<code class="language-plaintext highlighter-rouge">dropout</code> 层通过在训练过程中随机地将一些神经元的输出值设置为 0，来减少模型的过拟合。这个函数中使用了一个参数 <code class="language-plaintext highlighter-rouge">rate</code>，表示每个神经元被「关闭」的概率。这样做是为了防止模型过于依赖于某些特定的特征，而导致在新数据上的表现不佳。</p>

<h6 id="encoder-blocks-multi-head-attention--feed-forward">Encoder Blocks: Multi-Head Attention &amp; Feed Forward</h6>

<p>然后看下 encoder blocks 代码：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## Blocks
</span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">hp</span><span class="p">.</span><span class="n">num_blocks</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s">"num_blocks_{}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">i</span><span class="p">)):</span>
        <span class="c1"># 多头注意力
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">enc</span> <span class="o">=</span> <span class="n">multihead_attention</span><span class="p">(</span><span class="n">queries</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">enc</span><span class="p">,</span> 
                                        <span class="n">keys</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">enc</span><span class="p">,</span> 
                                        <span class="n">num_units</span><span class="o">=</span><span class="n">hp</span><span class="p">.</span><span class="n">hidden_units</span><span class="p">,</span> 
                                        <span class="n">num_heads</span><span class="o">=</span><span class="n">hp</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span> 
                                        <span class="n">dropout_rate</span><span class="o">=</span><span class="n">hp</span><span class="p">.</span><span class="n">dropout_rate</span><span class="p">,</span>
                                        <span class="n">is_training</span><span class="o">=</span><span class="n">is_training</span><span class="p">,</span>
                                        <span class="n">causality</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        
        <span class="c1"># 前馈神经网络
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">enc</span> <span class="o">=</span> <span class="n">feedforward</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">enc</span><span class="p">,</span> <span class="n">num_units</span><span class="o">=</span><span class="p">[</span><span class="mi">4</span><span class="o">*</span><span class="n">hp</span><span class="p">.</span><span class="n">hidden_units</span><span class="p">,</span> <span class="n">hp</span><span class="p">.</span><span class="n">hidden_units</span><span class="p">])</span>
</code></pre></div></div>

<p>上述代码是编码器（Encoder）的实现函数调用的流程，也是与船长上面的模型原理介绍一致的，在定义时同样使用了 Embedding 层、Positional Encoding 层、Dropout 层、Multihead Attention 和 Feed Forward 操作。其中 Multihead Attention 在编码、解码中是不一样的，待会儿我们会在 Decoder 部分再提到，有自注意力层和 Encoder-Decoder 层。</p>

<ul>
  <li>超参数 hp.num_blocks 表示 Encoder Blocks 的层数，每一层都有一个 Multi-Head Attention 和一个 Feed Forward。</li>
  <li>这个 Encoder 中的 Multi-Head Attention 是基于自注意力的（注意与后面的 Decoder 部分有区别）</li>
  <li><code class="language-plaintext highlighter-rouge">causality</code> 参数的意思是否使用 Causal Attention，它是 Self-Attention 的一种，但是只使用过去的信息，防止模型获取未来信息的干扰。一般对于预测序列中的某个时间步来说，只关注之前的信息，而不是整个序列的信息。这段代码中 <code class="language-plaintext highlighter-rouge">causality</code> 设置为了 <code class="language-plaintext highlighter-rouge">False</code>，即会关注整个序列的信息。</li>
</ul>

<h5 id="242解码过程">2.4.2、解码过程</h5>

<p>再看一下解码的流程：</p>

<div style="text-align: center;">
<div class="graphviz-wrapper">

<!-- Generated by graphviz version 2.43.0 (0)
 -->
<!-- Title: G Pages: 1 -->
<svg role="img" aria-label="graphviz-0ec29ea83329c971f433bc6641585297" width="372pt" height="404pt" viewBox="0.00 0.00 372.00 404.00">
<title>graphviz-0ec29ea83329c971f433bc6641585297</title>
<desc>
digraph G {
	rankdir=BT
	splines=ortho
	node [shape=&quot;box&quot;]
	decoder_attn1 [label=&quot;multihead_attention (self-attention)&quot;]
	decoder_attn2 [label=&quot;multihead_attention (encoder-decoder attention)&quot;]

	embedding -&gt; positional_encoding -&gt; dropout -&gt; decoder_attn1 -&gt; decoder_attn2 -&gt; feedforward
}
</desc>

<g id="graph0" class="graph" transform="scale(1 1) rotate(0) translate(4 400)">
<title>G</title>
<polygon fill="white" stroke="transparent" points="-4,4 -4,-400 368,-400 368,4 -4,4" />
<!-- decoder_attn1 -->
<g id="node1" class="node">
<title>decoder_attn1</title>
<polygon fill="none" stroke="black" points="317,-252 47,-252 47,-216 317,-216 317,-252" />
<text text-anchor="middle" x="182" y="-230.3" font-family="Times,serif" font-size="14.00">multihead_attention (self&#45;attention)</text>
</g>
<!-- decoder_attn2 -->
<g id="node2" class="node">
<title>decoder_attn2</title>
<polygon fill="none" stroke="black" points="364,-324 0,-324 0,-288 364,-288 364,-324" />
<text text-anchor="middle" x="182" y="-302.3" font-family="Times,serif" font-size="14.00">multihead_attention (encoder&#45;decoder attention)</text>
</g>
<!-- decoder_attn1&#45;&gt;decoder_attn2 -->
<g id="edge4" class="edge">
<title>decoder_attn1&#45;&gt;decoder_attn2</title>
<path fill="none" stroke="black" d="M182,-252.17C182,-252.17 182,-277.59 182,-277.59" />
<polygon fill="black" stroke="black" points="178.5,-277.59 182,-287.59 185.5,-277.59 178.5,-277.59" />
</g>
<!-- feedforward -->
<g id="node6" class="node">
<title>feedforward</title>
<polygon fill="none" stroke="black" points="234,-396 130,-396 130,-360 234,-360 234,-396" />
<text text-anchor="middle" x="182" y="-374.3" font-family="Times,serif" font-size="14.00">feedforward</text>
</g>
<!-- decoder_attn2&#45;&gt;feedforward -->
<g id="edge5" class="edge">
<title>decoder_attn2&#45;&gt;feedforward</title>
<path fill="none" stroke="black" d="M182,-324.17C182,-324.17 182,-349.59 182,-349.59" />
<polygon fill="black" stroke="black" points="178.5,-349.59 182,-359.59 185.5,-349.59 178.5,-349.59" />
</g>
<!-- embedding -->
<g id="node3" class="node">
<title>embedding</title>
<polygon fill="none" stroke="black" points="230,-36 134,-36 134,0 230,0 230,-36" />
<text text-anchor="middle" x="182" y="-14.3" font-family="Times,serif" font-size="14.00">embedding</text>
</g>
<!-- positional_encoding -->
<g id="node4" class="node">
<title>positional_encoding</title>
<polygon fill="none" stroke="black" points="261,-108 103,-108 103,-72 261,-72 261,-108" />
<text text-anchor="middle" x="182" y="-86.3" font-family="Times,serif" font-size="14.00">positional_encoding</text>
</g>
<!-- embedding&#45;&gt;positional_encoding -->
<g id="edge1" class="edge">
<title>embedding&#45;&gt;positional_encoding</title>
<path fill="none" stroke="black" d="M182,-36.17C182,-36.17 182,-61.59 182,-61.59" />
<polygon fill="black" stroke="black" points="178.5,-61.59 182,-71.59 185.5,-61.59 178.5,-61.59" />
</g>
<!-- dropout -->
<g id="node5" class="node">
<title>dropout</title>
<polygon fill="none" stroke="black" points="218.5,-180 145.5,-180 145.5,-144 218.5,-144 218.5,-180" />
<text text-anchor="middle" x="182" y="-158.3" font-family="Times,serif" font-size="14.00">dropout</text>
</g>
<!-- positional_encoding&#45;&gt;dropout -->
<g id="edge2" class="edge">
<title>positional_encoding&#45;&gt;dropout</title>
<path fill="none" stroke="black" d="M182,-108.17C182,-108.17 182,-133.59 182,-133.59" />
<polygon fill="black" stroke="black" points="178.5,-133.59 182,-143.59 185.5,-133.59 178.5,-133.59" />
</g>
<!-- dropout&#45;&gt;decoder_attn1 -->
<g id="edge3" class="edge">
<title>dropout&#45;&gt;decoder_attn1</title>
<path fill="none" stroke="black" d="M182,-180.17C182,-180.17 182,-205.59 182,-205.59" />
<polygon fill="black" stroke="black" points="178.5,-205.59 182,-215.59 185.5,-205.59 178.5,-205.59" />
</g>
</g>
</svg>
</div>
</div>

<h6 id="embedding-1">Embedding</h6>

<p>下面我们逐一看每段代码，主要关注与编码阶段的区别即可：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="bp">self</span><span class="p">.</span><span class="n">dec</span> <span class="o">=</span> <span class="n">embedding</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">decoder_inputs</span><span class="p">,</span> 
                      <span class="n">vocab_size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">en2idx</span><span class="p">),</span> 
                      <span class="n">num_units</span><span class="o">=</span><span class="n">hp</span><span class="p">.</span><span class="n">hidden_units</span><span class="p">,</span>
                      <span class="n">scale</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> 
                      <span class="n">scope</span><span class="o">=</span><span class="s">"dec_embed"</span><span class="p">)</span>
</code></pre></div></div>

<ul>
  <li><code class="language-plaintext highlighter-rouge">embedding</code> 输入用的是 <code class="language-plaintext highlighter-rouge">self.decoder_inputs</code></li>
  <li>词汇表尺寸用翻译后的输出语言英语词汇表长度 <code class="language-plaintext highlighter-rouge">len(en2idx)</code></li>
</ul>

<h6 id="key-masks-1">Key Masks</h6>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">key_masks</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">sign</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="nb">abs</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dec</span><span class="p">),</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<ul>
  <li><code class="language-plaintext highlighter-rouge">key_masks</code> 输入变量用 <code class="language-plaintext highlighter-rouge">self.dec</code>。</li>
</ul>

<h6 id="positional-encoding--drop-out">Positional Encoding &amp; Drop out</h6>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 位置编码
</span><span class="k">if</span> <span class="n">hp</span><span class="p">.</span><span class="n">sinusoid</span><span class="p">:</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">dec</span> <span class="o">+=</span> <span class="n">positional_encoding</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">decoder_inputs</span><span class="p">,</span>
                      <span class="n">vocab_size</span><span class="o">=</span><span class="n">hp</span><span class="p">.</span><span class="n">maxlen</span><span class="p">,</span> 
                      <span class="n">num_units</span><span class="o">=</span><span class="n">hp</span><span class="p">.</span><span class="n">hidden_units</span><span class="p">,</span> 
                      <span class="n">zero_pad</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> 
                      <span class="n">scale</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
                      <span class="n">scope</span><span class="o">=</span><span class="s">"dec_pe"</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">dec</span> <span class="o">+=</span> <span class="n">embedding</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">tile</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="nb">range</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">shape</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">decoder_inputs</span><span class="p">)[</span><span class="mi">1</span><span class="p">]),</span> <span class="mi">0</span><span class="p">),</span>
    							 <span class="p">[</span><span class="n">tf</span><span class="p">.</span><span class="n">shape</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">decoder_inputs</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">]),</span>
                      <span class="n">vocab_size</span><span class="o">=</span><span class="n">hp</span><span class="p">.</span><span class="n">maxlen</span><span class="p">,</span> 
                      <span class="n">num_units</span><span class="o">=</span><span class="n">hp</span><span class="p">.</span><span class="n">hidden_units</span><span class="p">,</span> 
                      <span class="n">zero_pad</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> 
                      <span class="n">scale</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
                      <span class="n">scope</span><span class="o">=</span><span class="s">"dec_pe"</span><span class="p">)</span>

<span class="bp">self</span><span class="p">.</span><span class="n">dec</span> <span class="o">*=</span> <span class="n">key_masks</span>

<span class="bp">self</span><span class="p">.</span><span class="n">dec</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">dropout</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dec</span><span class="p">,</span> 
                            <span class="n">rate</span><span class="o">=</span><span class="n">hp</span><span class="p">.</span><span class="n">dropout_rate</span><span class="p">,</span> 
                            <span class="n">training</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">is_training</span><span class="p">))</span>
</code></pre></div></div>

<ul>
  <li>输入 <code class="language-plaintext highlighter-rouge">self.decoder_inputs</code></li>
  <li>指定 <code class="language-plaintext highlighter-rouge">vocab_size</code> 参数 <code class="language-plaintext highlighter-rouge">hp.maxlen</code></li>
</ul>

<h6 id="decoder-blocks-multi-head-attention--feed-forward">Decoder Blocks: Multi-Head Attention &amp; Feed Forward</h6>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## 解码器模块
</span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">hp</span><span class="p">.</span><span class="n">num_blocks</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s">"num_blocks_{}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">i</span><span class="p">)):</span>
        <span class="c1"># 多头注意力（自注意力）
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">dec</span> <span class="o">=</span> <span class="n">multihead_attention</span><span class="p">(</span><span class="n">queries</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">dec</span><span class="p">,</span> 
                                        <span class="n">keys</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">dec</span><span class="p">,</span> 
                                        <span class="n">num_units</span><span class="o">=</span><span class="n">hp</span><span class="p">.</span><span class="n">hidden_units</span><span class="p">,</span> 
                                        <span class="n">num_heads</span><span class="o">=</span><span class="n">hp</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span> 
                                        <span class="n">dropout_rate</span><span class="o">=</span><span class="n">hp</span><span class="p">.</span><span class="n">dropout_rate</span><span class="p">,</span>
                                        <span class="n">is_training</span><span class="o">=</span><span class="n">is_training</span><span class="p">,</span>
                                        <span class="n">causality</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> 
                                        <span class="n">scope</span><span class="o">=</span><span class="s">"self_attention"</span><span class="p">)</span>
        
        <span class="c1"># 多头注意力（Encoder-Decoder 注意力）
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">dec</span> <span class="o">=</span> <span class="n">multihead_attention</span><span class="p">(</span><span class="n">queries</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">dec</span><span class="p">,</span> 
                                        <span class="n">keys</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">enc</span><span class="p">,</span> 
                                        <span class="n">num_units</span><span class="o">=</span><span class="n">hp</span><span class="p">.</span><span class="n">hidden_units</span><span class="p">,</span> 
                                        <span class="n">num_heads</span><span class="o">=</span><span class="n">hp</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span>
                                        <span class="n">dropout_rate</span><span class="o">=</span><span class="n">hp</span><span class="p">.</span><span class="n">dropout_rate</span><span class="p">,</span>
                                        <span class="n">is_training</span><span class="o">=</span><span class="n">is_training</span><span class="p">,</span> 
                                        <span class="n">causality</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
                                        <span class="n">scope</span><span class="o">=</span><span class="s">"vanilla_attention"</span><span class="p">)</span>

        <span class="c1"># 前馈神经网络
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">dec</span> <span class="o">=</span> <span class="n">feedforward</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dec</span><span class="p">,</span> <span class="n">num_units</span><span class="o">=</span><span class="p">[</span><span class="mi">4</span><span class="o">*</span><span class="n">hp</span><span class="p">.</span><span class="n">hidden_units</span><span class="p">,</span> <span class="n">hp</span><span class="p">.</span><span class="n">hidden_units</span><span class="p">])</span>
</code></pre></div></div>

<ul>
  <li>在用 <code class="language-plaintext highlighter-rouge">multihead_attention</code> 函数解码器模块时，注意传入的参数 <code class="language-plaintext highlighter-rouge">scope</code> 区别，先是自注意力层，用参数 <code class="language-plaintext highlighter-rouge">self_attention</code>，对应的 <code class="language-plaintext highlighter-rouge">queries</code> 是 <code class="language-plaintext highlighter-rouge">self.dec</code>，<code class="language-plaintext highlighter-rouge">keys</code> 也是 <code class="language-plaintext highlighter-rouge">self.dec</code>。再是「Encoder-Decder Attention」用的是参数 <code class="language-plaintext highlighter-rouge">vanilla_attention</code>，对应的 <code class="language-plaintext highlighter-rouge">queries</code> 来自解码器是 <code class="language-plaintext highlighter-rouge">self.dec</code>，但 <code class="language-plaintext highlighter-rouge">keys</code> 来自编码器是是 <code class="language-plaintext highlighter-rouge">self.enc</code>。</li>
</ul>

<h5 id="243embeddingpositional-encodingmulti-head-attentionfeed-forward">2.4.3、Embedding、Positional Encoding、Multi-Head Attention、Feed Forward</h5>

<h6 id="embedding-函数实现">Embedding 函数实现</h6>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">embedding</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> 
              <span class="n">vocab_size</span><span class="p">,</span> 
              <span class="n">num_units</span><span class="p">,</span> 
              <span class="n">zero_pad</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> 
              <span class="n">scale</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
              <span class="n">scope</span><span class="o">=</span><span class="s">"embedding"</span><span class="p">,</span> 
              <span class="n">reuse</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="n">scope</span><span class="p">,</span> <span class="n">reuse</span><span class="o">=</span><span class="n">reuse</span><span class="p">):</span>

    	<span class="c1"># 创建一个名为 `lookup_table`、形状为 (vocab_size, num_units) 的矩阵
</span>        <span class="n">lookup_table</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s">'lookup_table'</span><span class="p">,</span>
                                       <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">float32</span><span class="p">,</span>
                                       <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">num_units</span><span class="p">],</span>
                                       <span class="n">initializer</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">contrib</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">xavier_initializer</span><span class="p">())</span>

        <span class="c1"># lookup_table 的第一行插入一个全零行，作为 PAD 的词向量
</span>        <span class="k">if</span> <span class="n">zero_pad</span><span class="p">:</span>
            <span class="n">lookup_table</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">concat</span><span class="p">((</span><span class="n">tf</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_units</span><span class="p">]),</span>
                                      <span class="n">lookup_table</span><span class="p">[</span><span class="mi">1</span><span class="p">:,</span> <span class="p">:]),</span> <span class="mi">0</span><span class="p">)</span>

        <span class="c1"># 在词向量矩阵 lookup_table 中查找 inputs
</span>        <span class="n">outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">embedding_lookup</span><span class="p">(</span><span class="n">lookup_table</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span>
        
        <span class="c1"># 对输出的词向量进行除以根号 num_units 的操作，可以控制词向量的统计稳定性。
</span>        <span class="k">if</span> <span class="n">scale</span><span class="p">:</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="n">outputs</span> <span class="o">*</span> <span class="p">(</span><span class="n">num_units</span> <span class="o">**</span> <span class="mf">0.5</span><span class="p">)</span> 
            
    <span class="k">return</span> <span class="n">outputs</span>
</code></pre></div></div>

<h6 id="positional-encoding-函数实现">Positional Encoding 函数实现</h6>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">positional_encoding</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span>
                        <span class="n">num_units</span><span class="p">,</span>
                        <span class="n">zero_pad</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                        <span class="n">scale</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                        <span class="n">scope</span><span class="o">=</span><span class="s">"positional_encoding"</span><span class="p">,</span>
                        <span class="n">reuse</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>

    <span class="n">N</span><span class="p">,</span> <span class="n">T</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">.</span><span class="n">get_shape</span><span class="p">().</span><span class="n">as_list</span><span class="p">()</span>
    <span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="n">scope</span><span class="p">,</span> <span class="n">reuse</span><span class="o">=</span><span class="n">reuse</span><span class="p">):</span>

    	<span class="c1"># tf.range(T) 生成一个 0~T-1 的数组
</span>    	<span class="c1"># tf.tile() 将其扩展成 N*T 的矩阵，表示每个词的位置
</span>        <span class="n">position_ind</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">tile</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="nb">range</span><span class="p">(</span><span class="n">T</span><span class="p">),</span> <span class="mi">0</span><span class="p">),</span> <span class="p">[</span><span class="n">N</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>

        <span class="c1"># First part of the PE function: sin and cos argument
</span>        <span class="n">position_enc</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span>
            <span class="p">[</span><span class="n">pos</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="n">power</span><span class="p">(</span><span class="mi">10000</span><span class="p">,</span> <span class="mf">2.</span><span class="o">*</span><span class="n">i</span><span class="o">/</span><span class="n">num_units</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_units</span><span class="p">)]</span>
            <span class="k">for</span> <span class="n">pos</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">T</span><span class="p">)])</span>

        <span class="c1"># 用 numpy 的 sin 和 cos 函数对每个位置进行编码
</span>        <span class="n">position_enc</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">sin</span><span class="p">(</span><span class="n">position_enc</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">])</span>  <span class="c1"># dim 2i
</span>        <span class="n">position_enc</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">cos</span><span class="p">(</span><span class="n">position_enc</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">])</span>  <span class="c1"># dim 2i+1
</span>
        <span class="c1"># 将编码结果转为张量
</span>        <span class="n">lookup_table</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">position_enc</span><span class="p">)</span>

        <span class="c1"># 将编码的结果与位置索引相关联，得到最终的位置编码
</span>        <span class="k">if</span> <span class="n">zero_pad</span><span class="p">:</span>
        	<span class="c1"># 如果 zero_pad 参数为 True，则在编码结果的开头添加一个全 0 的向量
</span>            <span class="n">lookup_table</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">concat</span><span class="p">((</span><span class="n">tf</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_units</span><span class="p">]),</span>
                                      <span class="n">lookup_table</span><span class="p">[</span><span class="mi">1</span><span class="p">:,</span> <span class="p">:]),</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">embedding_lookup</span><span class="p">(</span><span class="n">lookup_table</span><span class="p">,</span> <span class="n">position_ind</span><span class="p">)</span>

        <span class="c1"># scale 参数为 True，则将编码结果乘上 num_units 的平方根
</span>        <span class="k">if</span> <span class="n">scale</span><span class="p">:</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="n">outputs</span> <span class="o">*</span> <span class="n">num_units</span><span class="o">**</span><span class="mf">0.5</span>

        <span class="k">return</span> <span class="n">outputs</span>
</code></pre></div></div>

<h6 id="multi-head-attention-函数实现">Multi-Head Attention 函数实现</h6>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">multihead_attention</span><span class="p">(</span><span class="n">queries</span><span class="p">,</span> 
                        <span class="n">keys</span><span class="p">,</span> 
                        <span class="n">num_units</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> 
                        <span class="n">num_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> 
                        <span class="n">dropout_rate</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                        <span class="n">is_training</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                        <span class="n">causality</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
                        <span class="n">scope</span><span class="o">=</span><span class="s">"multihead_attention"</span><span class="p">,</span> 
                        <span class="n">reuse</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="n">scope</span><span class="p">,</span> <span class="n">reuse</span><span class="o">=</span><span class="n">reuse</span><span class="p">):</span>
        <span class="c1"># Set the fall back option for num_units
</span>        <span class="k">if</span> <span class="n">num_units</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">num_units</span> <span class="o">=</span> <span class="n">queries</span><span class="p">.</span><span class="n">get_shape</span><span class="p">().</span><span class="n">as_list</span><span class="p">()[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        
        <span class="c1"># Linear Projections
</span>        <span class="c1"># 使用三个全连接层对输入的 queries、keys 分别进行线性变换，将其转换为三个维度相同的张量 Q/K/V
</span>        <span class="n">Q</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">dense</span><span class="p">(</span><span class="n">queries</span><span class="p">,</span> <span class="n">num_units</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">relu</span><span class="p">)</span> <span class="c1"># (N, T_q, C)
</span>        <span class="n">K</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">dense</span><span class="p">(</span><span class="n">keys</span><span class="p">,</span> <span class="n">num_units</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">relu</span><span class="p">)</span> <span class="c1"># (N, T_k, C)
</span>        <span class="n">V</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">dense</span><span class="p">(</span><span class="n">keys</span><span class="p">,</span> <span class="n">num_units</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">relu</span><span class="p">)</span> <span class="c1"># (N, T_k, C)
</span>        
        <span class="c1"># Split and concat
</span>        <span class="c1"># 按头数 split Q/K/V，再各自连接起来
</span>        <span class="n">Q_</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">concat</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">split</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="c1"># (h*N, T_q, C/h) 
</span>        <span class="n">K_</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">concat</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">split</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="c1"># (h*N, T_k, C/h) 
</span>        <span class="n">V_</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">concat</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">split</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="c1"># (h*N, T_k, C/h) 
</span>
        <span class="c1"># Multiplication
</span>        <span class="c1"># 计算 Q_, K_, V_ 的点积来获得注意力权重
</span>        <span class="c1"># 其中 Q_ 的维度为 (hN, T_q, C/h)
</span>        <span class="c1"># K_ 的维度为 (hN, T_k, C/h)
</span>        <span class="c1"># 计算出来的结果 outputs 的维度为 (h*N, T_q, T_k)
</span>        <span class="n">outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">Q_</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">K_</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]))</span> <span class="c1"># (h*N, T_q, T_k)
</span>
        <span class="c1"># Scale
</span>        <span class="c1"># 对权重进行 scale，这里除以了 K_ 的第三维的平方根，用于缩放权重
</span>        <span class="n">outputs</span> <span class="o">=</span> <span class="n">outputs</span> <span class="o">/</span> <span class="p">(</span><span class="n">K_</span><span class="p">.</span><span class="n">get_shape</span><span class="p">().</span><span class="n">as_list</span><span class="p">()[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">**</span> <span class="mf">0.5</span><span class="p">)</span>
        
        <span class="c1"># Key Masking
</span>        <span class="c1"># 这里需要将 keys 的有效部分标记出来，将无效部分设置为极小值，以便在之后的 softmax 中被忽略
</span>        <span class="n">key_masks</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">sign</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="nb">abs</span><span class="p">(</span><span class="n">keys</span><span class="p">),</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">))</span> <span class="c1"># (N, T_k)
</span>        <span class="n">key_masks</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">tile</span><span class="p">(</span><span class="n">key_masks</span><span class="p">,</span> <span class="p">[</span><span class="n">num_heads</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span> <span class="c1"># (h*N, T_k)
</span>        <span class="n">key_masks</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">tile</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">key_masks</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">shape</span><span class="p">(</span><span class="n">queries</span><span class="p">)[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">1</span><span class="p">])</span> <span class="c1"># (h*N, T_q, T_k)
</span>        
        <span class="n">paddings</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="o">**</span><span class="mi">32</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">where</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">equal</span><span class="p">(</span><span class="n">key_masks</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">paddings</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)</span> <span class="c1"># (h*N, T_q, T_k)
</span>  
        <span class="c1"># Causality = Future blinding
</span>        <span class="k">if</span> <span class="n">causality</span><span class="p">:</span>

        	<span class="c1"># 创建一个与 outputs[0, :, :] 相同形状的全 1 矩阵
</span>            <span class="n">diag_vals</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:])</span> <span class="c1"># (T_q, T_k)
</span>
            <span class="c1"># 对 diag_vals 进行处理，返回一个下三角线矩阵
</span>            <span class="n">tril</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">LinearOperatorLowerTriangular</span><span class="p">(</span><span class="n">diag_vals</span><span class="p">).</span><span class="n">to_dense</span><span class="p">()</span> <span class="c1"># (T_q, T_k)
</span>            <span class="n">masks</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">tile</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">tril</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">[</span><span class="n">tf</span><span class="p">.</span><span class="n">shape</span><span class="p">(</span><span class="n">outputs</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span> <span class="c1"># (h*N, T_q, T_k)
</span>   
   			<span class="c1"># 将 masks 为 0 的位置的 outputs 值设置为一个非常小的数
</span>   			<span class="c1"># 这样会导致这些位置在之后的计算中对结果产生非常小的影响，从而实现了遮盖未来信息的功能
</span>            <span class="n">paddings</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">masks</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="o">**</span><span class="mi">32</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">where</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">equal</span><span class="p">(</span><span class="n">masks</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">paddings</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)</span> <span class="c1"># (h*N, T_q, T_k)
</span>  
        <span class="c1"># 对于每个头的输出，应用 softmax 激活函数，这样可以得到一个概率分布
</span>        <span class="n">outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span> <span class="c1"># (h*N, T_q, T_k)
</span>         
        <span class="c1"># Query Masking
</span>        <span class="c1"># 对于查询（queries）进行 masking，这样可以避免输入序列后面的词对之前词的影响
</span>        <span class="n">query_masks</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">sign</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="nb">abs</span><span class="p">(</span><span class="n">queries</span><span class="p">),</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">))</span> <span class="c1"># (N, T_q)
</span>        <span class="n">query_masks</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">tile</span><span class="p">(</span><span class="n">query_masks</span><span class="p">,</span> <span class="p">[</span><span class="n">num_heads</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span> <span class="c1"># (h*N, T_q)
</span>        <span class="n">query_masks</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">tile</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">query_masks</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">shape</span><span class="p">(</span><span class="n">keys</span><span class="p">)[</span><span class="mi">1</span><span class="p">]])</span> <span class="c1"># (h*N, T_q, T_k)
</span>        <span class="n">outputs</span> <span class="o">*=</span> <span class="n">query_masks</span> <span class="c1"># broadcasting. (N, T_q, C)
</span>          
        <span class="c1"># Dropouts &amp; Weighted Sum
</span>        <span class="c1"># 对于每个头的输出，应用 dropout 以及进行残差连接
</span>        <span class="n">outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">rate</span><span class="o">=</span><span class="n">dropout_rate</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">is_training</span><span class="p">))</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">V_</span><span class="p">)</span> <span class="c1"># ( h*N, T_q, C/h)
</span>        
        <span class="c1"># Restore shape
</span>        <span class="c1"># 将每个头的输出拼接起来，使用 tf.concat 函数，将不同头的结果按照第二维拼接起来
</span>        <span class="c1"># 得到最终的输出结果，即经过多头注意力计算后的结果
</span>        <span class="n">outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">concat</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">split</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span> <span class="p">)</span> <span class="c1"># (N, T_q, C)
</span>              
        <span class="c1"># Residual connection
</span>        <span class="n">outputs</span> <span class="o">+=</span> <span class="n">queries</span>
              
        <span class="c1"># Normalize
</span>        <span class="n">outputs</span> <span class="o">=</span> <span class="n">normalize</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span> <span class="c1"># (N, T_q, C)
</span> 
    <span class="k">return</span> <span class="n">outputs</span>
</code></pre></div></div>

<h6 id="feed-forward-函数实现">Feed Forward 函数实现</h6>

<p>下面是 <strong>前馈神经网络层</strong> 的定义，这是一个非线性变换，这里用到了一些卷积神经网络（CNN）的知识，我们来看下代码再解释：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">feedforward</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> 
                <span class="n">num_units</span><span class="o">=</span><span class="p">[</span><span class="mi">2048</span><span class="p">,</span> <span class="mi">512</span><span class="p">],</span>
                <span class="n">scope</span><span class="o">=</span><span class="s">"multihead_attention"</span><span class="p">,</span> 
                <span class="n">reuse</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="n">scope</span><span class="p">,</span> <span class="n">reuse</span><span class="o">=</span><span class="n">reuse</span><span class="p">):</span>
        <span class="c1"># Inner layer
</span>        <span class="n">params</span> <span class="o">=</span> <span class="p">{</span><span class="s">"inputs"</span><span class="p">:</span> <span class="n">inputs</span><span class="p">,</span> <span class="s">"filters"</span><span class="p">:</span> <span class="n">num_units</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s">"kernel_size"</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
                  <span class="s">"activation"</span><span class="p">:</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">relu</span><span class="p">,</span> <span class="s">"use_bias"</span><span class="p">:</span> <span class="bp">True</span><span class="p">}</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">conv1d</span><span class="p">(</span><span class="o">**</span><span class="n">params</span><span class="p">)</span>
        
        <span class="c1"># Readout layer
</span>        <span class="n">params</span> <span class="o">=</span> <span class="p">{</span><span class="s">"inputs"</span><span class="p">:</span> <span class="n">outputs</span><span class="p">,</span> <span class="s">"filters"</span><span class="p">:</span> <span class="n">num_units</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="s">"kernel_size"</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
                  <span class="s">"activation"</span><span class="p">:</span> <span class="bp">None</span><span class="p">,</span> <span class="s">"use_bias"</span><span class="p">:</span> <span class="bp">True</span><span class="p">}</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">conv1d</span><span class="p">(</span><span class="o">**</span><span class="n">params</span><span class="p">)</span>
        
        <span class="c1"># 连接一个残差网络 ResNet
</span>        <span class="n">outputs</span> <span class="o">+=</span> <span class="n">inputs</span>
        
        <span class="c1"># 归一化后输出
</span>        <span class="n">outputs</span> <span class="o">=</span> <span class="n">normalize</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">outputs</span>
</code></pre></div></div>

<ul>
  <li>先是使用了一个卷积层（conv1d）作为 inner layer、一个卷积层作为 readout layer，卷积核大小都为 1。</li>
  <li><code class="language-plaintext highlighter-rouge">filters</code> 参数用来控制卷积层中输出通道数量，inner layer 的输出通道数设置为 <code class="language-plaintext highlighter-rouge">num_units[0]</code> ，readout layer 的设置为 <code class="language-plaintext highlighter-rouge">num_units[1]</code>。有时也会把这个解释为神经元数量。这两个的默认分别为 2048、512，调用时传入的是超参数的 <code class="language-plaintext highlighter-rouge">[4 * hidden_units, hidden_units]</code>。</li>
  <li>其中 inner layer 用 <code class="language-plaintext highlighter-rouge">ReLU</code> 作为激活函数，然后连接一个残差网络 RedNet，把 readout layer 的输出加上原始的输入。</li>
  <li>最后使用 <code class="language-plaintext highlighter-rouge">normalize</code> 归一化处理输出，再返回。下面来看下 <code class="language-plaintext highlighter-rouge">normalize</code> 函数。</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">normalize</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> 
              <span class="n">epsilon</span> <span class="o">=</span> <span class="mf">1e-8</span><span class="p">,</span>
              <span class="n">scope</span><span class="o">=</span><span class="s">"ln"</span><span class="p">,</span>
              <span class="n">reuse</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="n">scope</span><span class="p">,</span> <span class="n">reuse</span><span class="o">=</span><span class="n">reuse</span><span class="p">):</span>

    	<span class="c1"># 输入数据的形状
</span>        <span class="n">inputs_shape</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">.</span><span class="n">get_shape</span><span class="p">()</span>
        <span class="n">params_shape</span> <span class="o">=</span> <span class="n">inputs_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">:]</span>
    
    	<span class="c1"># 平均数、方差
</span>        <span class="n">mean</span><span class="p">,</span> <span class="n">variance</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">moments</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">keep_dims</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

        <span class="c1"># 拉伸因子 beta
</span>        <span class="n">beta</span><span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">params_shape</span><span class="p">))</span>

        <span class="c1"># 缩放因子 gamma
</span>        <span class="n">gamma</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="n">params_shape</span><span class="p">))</span>

        <span class="c1"># 归一化：加上一个非常小的 epsilon，是为了防止除以 0
</span>        <span class="n">normalized</span> <span class="o">=</span> <span class="p">(</span><span class="n">inputs</span> <span class="o">-</span> <span class="n">mean</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span> <span class="p">(</span><span class="n">variance</span> <span class="o">+</span> <span class="n">epsilon</span><span class="p">)</span> <span class="o">**</span> <span class="p">(.</span><span class="mi">5</span><span class="p">)</span> <span class="p">)</span>

        <span class="n">outputs</span> <span class="o">=</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">normalized</span> <span class="o">+</span> <span class="n">beta</span>
        
    <span class="k">return</span> <span class="n">outputs</span>
</code></pre></div></div>

<ul>
  <li>该函数实现了 Layer Normalization，用于在深度神经网络中解决数据的不稳定性问题。</li>
</ul>

<h5 id="244编码和解码完成后的操作">2.4.4、编码和解码完成后的操作</h5>

<p>解码器后的 <code class="language-plaintext highlighter-rouge">Linear &amp; Softmax</code>：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 全连接层得到的未经过归一化的概率值
</span><span class="bp">self</span><span class="p">.</span><span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">dense</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dec</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">en2idx</span><span class="p">))</span>

<span class="c1"># 预测的英文单词 idx
</span><span class="bp">self</span><span class="p">.</span><span class="n">preds</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">to_int32</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">arg_max</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">logits</span><span class="p">,</span> <span class="n">dimension</span><span class="o">=-</span><span class="mi">1</span><span class="p">))</span>
<span class="bp">self</span><span class="p">.</span><span class="n">istarget</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">to_float</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">not_equal</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">y</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>

<span class="c1"># 正确预测数量，除以所有样本数，得到准确率
</span><span class="bp">self</span><span class="p">.</span><span class="n">acc</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">to_float</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">equal</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">preds</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">y</span><span class="p">))</span><span class="o">*</span><span class="bp">self</span><span class="p">.</span><span class="n">istarget</span><span class="p">)</span><span class="o">/</span> <span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">istarget</span><span class="p">))</span>

<span class="c1">#  记录了模型的准确率的值，用于 tensorboard 可视化
</span><span class="n">tf</span><span class="p">.</span><span class="n">summary</span><span class="p">.</span><span class="n">scalar</span><span class="p">(</span><span class="s">'acc'</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">acc</span><span class="p">)</span>
</code></pre></div></div>

<p>训练集数据处理时，经过 <code class="language-plaintext highlighter-rouge">Linear &amp; Softmax</code> 之后的最后处理如下。这里用到了 <code class="language-plaintext highlighter-rouge">tf.nn.softmax_cross_entropy_with_logits</code> 交叉熵损失，来计算模型的错误率 <code class="language-plaintext highlighter-rouge">mean_loss</code>，并使用 Adam 优化器 <code class="language-plaintext highlighter-rouge">AdamOptimizer</code> 来优化模型参数。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 使用 label_smoothing 函数对真实标签进行标签平滑，得到 self.y_smoothed
</span><span class="bp">self</span><span class="p">.</span><span class="n">y_smoothed</span> <span class="o">=</span> <span class="n">label_smoothing</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">one_hot</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">y</span><span class="p">,</span> <span class="n">depth</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">en2idx</span><span class="p">)))</span>
</code></pre></div></div>

<p>下面这段代码实现了一种叫做「label Smoothing」的技巧。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">label_smoothing</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>

	<span class="c1"># 获取输入的类别数，并将其赋值给变量 K
</span>    <span class="n">K</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">.</span><span class="n">get_shape</span><span class="p">().</span><span class="n">as_list</span><span class="p">()[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="c1"># number of channels
</span>    <span class="k">return</span> <span class="p">((</span><span class="mi">1</span><span class="o">-</span><span class="n">epsilon</span><span class="p">)</span> <span class="o">*</span> <span class="n">inputs</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">epsilon</span> <span class="o">/</span> <span class="n">K</span><span class="p">)</span>
</code></pre></div></div>

<p>在训练过程中，样本的标签被表示为一个二维矩阵，其中第一维表示样本的编号，第二维表示样本的标签。这个矩阵的形状就是 (样本数, 类别数)，所以类别数对应的就是最后一维。具体到这个模型用例里，第一个维度是德语样本句子数，最后一维就是英语词汇量的大小。</p>

<p>用于解决在训练模型时出现的过拟合问题。在标签平滑中，我们给每个样本的标签加上一些噪声，使得模型不能完全依赖于样本的标签来进行训练，从而减少过拟合的可能性。具体来说，这段代码将输入的标签 <code class="language-plaintext highlighter-rouge">inputs</code> 乘上 <code class="language-plaintext highlighter-rouge">1-epsilon</code>，再加上 <code class="language-plaintext highlighter-rouge">epsilon / K</code>，其中 <code class="language-plaintext highlighter-rouge">epsilon</code> 是平滑因子，<code class="language-plaintext highlighter-rouge">K</code> 是标签类别数（英语词汇量大小）。这样就可以在训练过程中让模型对标签的预测更加平稳，并且降低过拟合的风险。</p>

<p>然后我们看后续的操作。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 对于分类问题来说，常用的损失函数是交叉熵损失
</span><span class="bp">self</span><span class="p">.</span><span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">softmax_cross_entropy_with_logits</span><span class="p">(</span><span class="n">logits</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">y_smoothed</span><span class="p">)</span>
<span class="bp">self</span><span class="p">.</span><span class="n">mean_loss</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">loss</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">istarget</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">istarget</span><span class="p">))</span>

<span class="c1"># Training Scheme
</span><span class="bp">self</span><span class="p">.</span><span class="n">global_step</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s">'global_step'</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="c1"># Adam 优化器 self.optimizer，用于优化损失函数
</span><span class="bp">self</span><span class="p">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">train</span><span class="p">.</span><span class="n">AdamOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">hp</span><span class="p">.</span><span class="n">lr</span><span class="p">,</span> <span class="n">beta1</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">beta2</span><span class="o">=</span><span class="mf">0.98</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">)</span>

<span class="c1"># 使用优化器的 minimize() 函数创建一个训练操作 self.train_op，用于更新模型参数。这个函数会自动计算梯度并应用更新
</span><span class="bp">self</span><span class="p">.</span><span class="n">train_op</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">optimizer</span><span class="p">.</span><span class="n">minimize</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">mean_loss</span><span class="p">,</span> <span class="n">global_step</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">global_step</span><span class="p">)</span>
   
<span class="c1"># 将平均损失写入 TensorFlow 的 Summary 中，用于 tensorboard 可视化
</span><span class="n">tf</span><span class="p">.</span><span class="n">summary</span><span class="p">.</span><span class="n">scalar</span><span class="p">(</span><span class="s">'mean_loss'</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">mean_loss</span><span class="p">)</span>

<span class="c1"># 将所有的 summary 合并到一起，方便在训练过程中写入事件文件
</span><span class="bp">self</span><span class="p">.</span><span class="n">merged</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">summary</span><span class="p">.</span><span class="n">merge_all</span><span class="p">()</span>
</code></pre></div></div>

<h4 id="25效果评价">2.5、效果评价</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">eval</span><span class="p">():</span> 
    <span class="c1"># 创建一个处理测试数据集的 Graph 实例
</span>    <span class="n">g</span> <span class="o">=</span> <span class="n">Graph</span><span class="p">(</span><span class="n">is_training</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"Graph loaded"</span><span class="p">)</span>
    
    <span class="c1"># 加载测试数据
</span>    <span class="n">X</span><span class="p">,</span> <span class="n">Sources</span><span class="p">,</span> <span class="n">Targets</span> <span class="o">=</span> <span class="n">load_test_data</span><span class="p">()</span>
    <span class="n">de2idx</span><span class="p">,</span> <span class="n">idx2de</span> <span class="o">=</span> <span class="n">load_de_vocab</span><span class="p">()</span>
    <span class="n">en2idx</span><span class="p">,</span> <span class="n">idx2en</span> <span class="o">=</span> <span class="n">load_en_vocab</span><span class="p">()</span>
     
    <span class="c1"># Start session         
</span>    <span class="k">with</span> <span class="n">g</span><span class="p">.</span><span class="n">graph</span><span class="p">.</span><span class="n">as_default</span><span class="p">():</span>

    	<span class="c1"># TensorFlow 中用于管理训练的一个类
</span>    	<span class="c1"># 它可以帮助你轻松地管理训练过程中的各种资源，如模型参数、检查点和日志
</span>        <span class="n">sv</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">train</span><span class="p">.</span><span class="n">Supervisor</span><span class="p">()</span>

        <span class="c1"># 创建一个会话
</span>        <span class="k">with</span> <span class="n">sv</span><span class="p">.</span><span class="n">managed_session</span><span class="p">(</span><span class="n">config</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">ConfigProto</span><span class="p">(</span><span class="n">allow_soft_placement</span><span class="o">=</span><span class="bp">True</span><span class="p">))</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>

            <span class="c1"># 恢复模型参数
</span>            <span class="n">sv</span><span class="p">.</span><span class="n">saver</span><span class="p">.</span><span class="n">restore</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">train</span><span class="p">.</span><span class="n">latest_checkpoint</span><span class="p">(</span><span class="n">hp</span><span class="p">.</span><span class="n">logdir</span><span class="p">))</span>
            <span class="k">print</span><span class="p">(</span><span class="s">"Restored!"</span><span class="p">)</span>
              
            <span class="c1"># 获取模型名称
</span>            <span class="n">mname</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="n">hp</span><span class="p">.</span><span class="n">logdir</span> <span class="o">+</span> <span class="s">'/checkpoint'</span><span class="p">,</span> <span class="s">'r'</span><span class="p">).</span><span class="n">read</span><span class="p">().</span><span class="n">split</span><span class="p">(</span><span class="s">'"'</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span> <span class="c1"># model name
</span>             
            <span class="c1">## Inference
</span>            <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">exists</span><span class="p">(</span><span class="s">'results'</span><span class="p">):</span> <span class="n">os</span><span class="p">.</span><span class="n">mkdir</span><span class="p">(</span><span class="s">'results'</span><span class="p">)</span>

            <span class="c1"># 初始化结果文件
</span>            <span class="k">with</span> <span class="n">codecs</span><span class="p">.</span><span class="nb">open</span><span class="p">(</span><span class="s">"results/"</span> <span class="o">+</span> <span class="n">mname</span><span class="p">,</span> <span class="s">"w"</span><span class="p">,</span> <span class="s">"utf-8"</span><span class="p">)</span> <span class="k">as</span> <span class="n">fout</span><span class="p">:</span>
                <span class="n">list_of_refs</span><span class="p">,</span> <span class="n">hypotheses</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>

                <span class="c1"># 循环处理数据
</span>                <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">//</span> <span class="n">hp</span><span class="p">.</span><span class="n">batch_size</span><span class="p">):</span>
                     
                    <span class="c1"># 获取小批量数据
</span>                    <span class="n">x</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="o">*</span><span class="n">hp</span><span class="p">.</span><span class="n">batch_size</span><span class="p">:</span> <span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">hp</span><span class="p">.</span><span class="n">batch_size</span><span class="p">]</span>
                    <span class="n">sources</span> <span class="o">=</span> <span class="n">Sources</span><span class="p">[</span><span class="n">i</span><span class="o">*</span><span class="n">hp</span><span class="p">.</span><span class="n">batch_size</span><span class="p">:</span> <span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">hp</span><span class="p">.</span><span class="n">batch_size</span><span class="p">]</span>
                    <span class="n">targets</span> <span class="o">=</span> <span class="n">Targets</span><span class="p">[</span><span class="n">i</span><span class="o">*</span><span class="n">hp</span><span class="p">.</span><span class="n">batch_size</span><span class="p">:</span> <span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">hp</span><span class="p">.</span><span class="n">batch_size</span><span class="p">]</span>
                     
                    <span class="c1"># 使用自回归推理（Autoregressive inference）得到预测结果
</span>                    <span class="n">preds</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">hp</span><span class="p">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">hp</span><span class="p">.</span><span class="n">maxlen</span><span class="p">),</span> <span class="n">np</span><span class="p">.</span><span class="n">int32</span><span class="p">)</span>
                    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">hp</span><span class="p">.</span><span class="n">maxlen</span><span class="p">):</span>
                        <span class="n">_preds</span> <span class="o">=</span> <span class="n">sess</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">g</span><span class="p">.</span><span class="n">preds</span><span class="p">,</span> <span class="p">{</span><span class="n">g</span><span class="p">.</span><span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span> <span class="n">g</span><span class="p">.</span><span class="n">y</span><span class="p">:</span> <span class="n">preds</span><span class="p">})</span>
                        <span class="n">preds</span><span class="p">[:,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">_preds</span><span class="p">[:,</span> <span class="n">j</span><span class="p">]</span>
                     
                    <span class="c1"># 将预测结果写入文件
</span>                    <span class="k">for</span> <span class="n">source</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">pred</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">sources</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">preds</span><span class="p">):</span> <span class="c1"># sentence-wise
</span>                        <span class="n">got</span> <span class="o">=</span> <span class="s">" "</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">idx2en</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">pred</span><span class="p">).</span><span class="n">split</span><span class="p">(</span><span class="s">"&lt;/S&gt;"</span><span class="p">)[</span><span class="mi">0</span><span class="p">].</span><span class="n">strip</span><span class="p">()</span>
                        <span class="n">fout</span><span class="p">.</span><span class="n">write</span><span class="p">(</span><span class="s">"- source: "</span> <span class="o">+</span> <span class="n">source</span> <span class="o">+</span><span class="s">"</span><span class="se">\n</span><span class="s">"</span><span class="p">)</span>
                        <span class="n">fout</span><span class="p">.</span><span class="n">write</span><span class="p">(</span><span class="s">"- expected: "</span> <span class="o">+</span> <span class="n">target</span> <span class="o">+</span> <span class="s">"</span><span class="se">\n</span><span class="s">"</span><span class="p">)</span>
                        <span class="n">fout</span><span class="p">.</span><span class="n">write</span><span class="p">(</span><span class="s">"- got: "</span> <span class="o">+</span> <span class="n">got</span> <span class="o">+</span> <span class="s">"</span><span class="se">\n\n</span><span class="s">"</span><span class="p">)</span>
                        <span class="n">fout</span><span class="p">.</span><span class="n">flush</span><span class="p">()</span>
                          
                        <span class="c1"># bleu score
</span>                        <span class="n">ref</span> <span class="o">=</span> <span class="n">target</span><span class="p">.</span><span class="n">split</span><span class="p">()</span>
                        <span class="n">hypothesis</span> <span class="o">=</span> <span class="n">got</span><span class="p">.</span><span class="n">split</span><span class="p">()</span>
                        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">ref</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">3</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">hypothesis</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">3</span><span class="p">:</span>
                            <span class="n">list_of_refs</span><span class="p">.</span><span class="n">append</span><span class="p">([</span><span class="n">ref</span><span class="p">])</span>
                            <span class="n">hypotheses</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">hypothesis</span><span class="p">)</span>
              
                <span class="c1"># 计算 BLEU 分数，并将其写入文件
</span>                <span class="n">score</span> <span class="o">=</span> <span class="n">corpus_bleu</span><span class="p">(</span><span class="n">list_of_refs</span><span class="p">,</span> <span class="n">hypotheses</span><span class="p">)</span>
                <span class="n">fout</span><span class="p">.</span><span class="n">write</span><span class="p">(</span><span class="s">"Bleu Score = "</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="mi">100</span><span class="o">*</span><span class="n">score</span><span class="p">))</span>
                                          
<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">'__main__'</span><span class="p">:</span>
    <span class="nb">eval</span><span class="p">()</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"Done"</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="3kyubyong-transformer-的性能表现">3、Kyubyong Transformer 的性能表现</h3>

<p>评估结果文件的最后一行有 Bleu Score = 6.598452846670836 表示这个翻译模型的翻译结果与参考翻译重叠程度比较高，翻译质量较好。不过需要注意的是，BLEU 分数不能完全反映翻译质量，因为它不能评估语法，语义，语调等方面的问题。</p>

<p>另外前面我们在代码中已经将过程数据保存在 logdir 下了，就是为了后续方便可视化，我们可以用 TensorBoard 来可视化，具体使用方法如下：</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>mikecaptain@local <span class="nv">$ </span>tensorboard <span class="nt">--logdir</span> logdir
</code></pre></div></div>

<p>然后在浏览器里查看 <code class="language-plaintext highlighter-rouge">http://localhost:6006</code>，示例如下：</p>

<p><img src="/img/src/2023-01-04-language-model-5-17.gif" alt="image" /></p>

<h3 id="4kyubyong-transformer-模型的一些问题">4、Kyubyong Transformer 模型的一些问题</h3>

<p>我们可以看到这个 Transformer 能够较好地捕捉长距离依赖关系，提高翻译质量。然而，Kyubyong Transformer 的实现存在一些问题。该 Transformer 模型在训练过程中还需要调整许多超参数，如学习率（learning rate）、batch size 等，不同的任务可能需要不同的超参数调整。</p>

<h2 id="参考">参考</h2>

<ul>
  <li>https://arxiv.org/abs/1706.03762</li>
  <li>https://arxiv.org/abs/1512.03385</li>
  <li>https://github.com/Kyubyong/transformer/</li>
  <li>http://jalammar.github.io/illustrated-transformer/</li>
  <li>https://towardsdatascience.com/this-is-how-to-train-better-transformer-models-d54191299978</li>
  <li>《自然语言处理：基于预训练模型的方法》车万翔 等著</li>
  <li>《自然语言处理实战：预训练模型应用及其产品化》安库·A·帕特尔 等著</li>
  <li>https://lilianweng.github.io/posts/2018-06-24-attention/</li>
  <li>https://github.com/lilianweng/transformer-tensorflow/</li>
  <li>《基于深度学习的道路短期交通状态时空序列预测》崔建勋 著</li>
  <li>https://www.zhihu.com/question/325839123</li>
  <li>https://luweikxy.gitbook.io/machine-learning-notes/self-attention-and-transformer</li>
  <li>《Python 深度学习（第 2 版）》弗朗索瓦·肖莱 著</li>
  <li>https://en.wikipedia.org/wiki/Attention_(machine_learning)</li>
  <li>https://zhuanlan.zhihu.com/p/410776234</li>
  <li>https://www.tensorflow.org/tensorboard/get_started</li>
  <li>https://paperswithcode.com/method/multi-head-attention</li>
  <li>https://zhuanlan.zhihu.com/p/48508221</li>
  <li>https://www.joshbelanich.com/self-attention-layer/</li>
  <li>https://learning.rasa.com/transformers/kvq/</li>
  <li>https://zhuanlan.zhihu.com/p/352898810</li>
  <li>https://towardsdatascience.com/beautifully-illustrated-nlp-models-from-rnn-to-transformer-80d69faf2109</li>
  <li>https://medium.com/analytics-vidhya/understanding-q-k-v-in-transformer-self-attention-9a5eddaa5960</li>
</ul>

	</div>
</article>



	  </main>
		
		  <!-- Pagination links -->
      

	  </div>
	    
	    <!-- Footer -->
	    <footer><span>@2022 - MikeCaptain.com</span></footer>


	    <!-- Script -->
      <script src="/js/main.js"></script>	


	</div>
</body>
</html>

<!DOCTYPE html>
<html>

<head>
	<!-- Meta -->
	<meta charset="UTF-8"/>
	<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
	<meta name="generator" content="Jekyll">

	<title>人工智能大模型发展时间脉络梳理</title>
  	<meta name="description" content="麦克船长对于技术、产品、商业等领域的分享|AI,A.I.,NLP,神经网络,人工智能,自然语言处理,BERT,GPT,ChatGPT,OpenAI,阿里巴巴,P9,运营,淘宝,天猫,总监,高管">

	<!-- CSS & fonts -->
	<link rel="stylesheet" href="/css/main.css">

	<!-- RSS -->
	<link href="/atom.xml" type="application/atom+xml" rel="alternate" title="ATOM Feed" />

  	<!-- Favicon -->
 	 <link rel="shortcut icon" type="image/png" href="/img/favicon.png">

 	 <!-- Syntax highlighter -->
  	<link rel="stylesheet" href="/css/syntax.css" />

  	<!--KaTeX-->
  	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
  	<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>
  	<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"></script>
  	<script>
  		document.addEventListener("DOMContentLoaded", function() {
  			renderMathInElement(document.body, {
  				// ...options...
  			});
  		});
  	</script>

  	
  	<!-- KaTeX -->
  	<link rel="stylesheet" href="/assets/plugins/katex.0.11.1/katex.min.css">
  	

  	
  		<script async src="https://www.googletagmanager.com/gtag/js?id=G-CH4708X4R5"></script>
  		<script>
    		window.dataLayer = window.dataLayer || [];
    		function gtag(){dataLayer.push(arguments);}
    		gtag('js', new Date());

    		gtag('config', 'G-CH4708X4R5');
  		</script>
	


</head>

<body>
	<div id="wrap">
	  	
	  	<!-- Navigation -->
	  	<nav id="nav">
	<div id="nav-list">
		<a href="/">Home</a>

		<!-- Nav pages -->
	  <!-- 
	    
	  
	    
	      <a href="/about/" title="关于我">关于我</a>
	    
	  
	    
	  
	    
	  
	    
	      <a href="/booklist/" title="读书行路">读书行路</a>
	    
	  
	    
	  
	    
	  
	    
	  
	    
	      <a href="/categories/" title="Categories">Categories</a>
	    
	  
	    
	  
	    
	  
	    
	  
	    
	      <a href="/target/" title="目标感">目标感</a>
	    
	  
	    
	  
	    
	  
	    
	  
	    
	  
	    
	  
	    
	  
	    
	  
	    
	   -->

	  <!-- Tech category pages -->






  <a href="/category/ai" title="人工智能">人工智能</a>











  <a href="/category/energy" title="能源">能源</a>









  <a href="/category/rt_tech" title="实时技术">实时技术</a>







  <a href="/category/web" title="前端">前端</a>
















<!-- Non-tech category pages -->












  <a href="/category/business" title="商业">商业</a>



  <a href="/category/design" title="设计">设计</a>















  <a href="/category/thinking" title="思考与生活">思考与生活</a>

















	  
        
      
        
          <a href="/about/" title="关于我">关于我</a>
        
      
        
      
        
      
        
          <a href="/booklist/" title="读书行路">读书行路</a>
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
          <a href="/target/" title="目标感">目标感</a>
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    <!-- Nav links -->
	  <!-- <a href="https://github.com/thereviewindex/monochrome/archive/master.zip">Download</a>
<a href="https://github.com/thereviewindex/monochrome">Project on Github</a> -->

	</div>
  
  <!-- Nav footer -->
	
	  <footer>
	
	<span>version 1.0.0</span>

</footer>
	

</nav>

    
    <!-- Icon menu -->
	  <a id="nav-menu">
	  	<div id="menu"></div>
	  </a>

      <!-- Header -->
      
        <header id="header" class="parent justify-spaceBetween">
  <div class="inner w100 relative">
    <span class="f-left">  
      <a href="/">
        <h1>
          <span>Mike</span>Captain
        </h1>
      </a>
    </span>
    <span id="nav-links" class="absolute right bottom">

      <!-- Tech category pages -->






  <a href="/category/ai" title="人工智能">人工智能</a>











  <a href="/category/energy" title="能源">能源</a>









  <a href="/category/rt_tech" title="实时技术">实时技术</a>







  <a href="/category/web" title="前端">前端</a>
















<!-- Non-tech category pages -->












  <a href="/category/business" title="商业">商业</a>



  <a href="/category/design" title="设计">设计</a>















  <a href="/category/thinking" title="思考与生活">思考与生活</a>

















      &nbsp;&nbsp;&nbsp;丨&nbsp;

      <!-- Nav pages -->
      
        
      
        
          <a href="/about/" title="关于我">关于我</a>
        
      
        
      
        
      
        
          <a href="/booklist/" title="读书行路">读书行路</a>
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
          <a href="/target/" title="目标感">目标感</a>
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
      
      <!-- Nav links -->
      <!-- <a href="https://github.com/thereviewindex/monochrome/archive/master.zip">Download</a>
<a href="https://github.com/thereviewindex/monochrome">Project on Github</a> -->

    </span>
  </div>
</header>




      

    <!-- Main content -->
	  <div id="container">
		  
		<main>

			<article id="post-page">
	<h2>人工智能大模型发展时间脉络梳理</h2>		
	<time datetime="2023-01-22T21:00:33+00:00" class="by-line">22 Jan 2023, 香港 | 麦克船长 | 总计 8293 字</time>
	<div class="content">
		<p><strong>本文目录</strong></p>
<ul id="markdown-toc">
  <li><a href="#数据集" id="markdown-toc-数据集">数据集</a></li>
  <li><a href="#一模型理念" id="markdown-toc-一模型理念">一、模型理念</a>    <ul>
      <li><a href="#生成模型生成器" id="markdown-toc-生成模型生成器">生成模型（生成器）</a></li>
      <li><a href="#缩放定律the-scaling-law" id="markdown-toc-缩放定律the-scaling-law">缩放定律（The Scaling Law）</a></li>
      <li><a href="#思维链chain-of-thoughtcot2022-年-1-月" id="markdown-toc-思维链chain-of-thoughtcot2022-年-1-月">思维链（Chain of Thought，CoT，2022 年 1 月）</a></li>
    </ul>
  </li>
  <li><a href="#二训练方法" id="markdown-toc-二训练方法">二、训练方法</a>    <ul>
      <li><a href="#强化学习reinforcement-learning" id="markdown-toc-强化学习reinforcement-learning">强化学习（Reinforcement Learning）</a></li>
      <li><a href="#深度神经网络" id="markdown-toc-深度神经网络">深度神经网络</a></li>
      <li><a href="#自指令self-instruct2022-年-12-月" id="markdown-toc-自指令self-instruct2022-年-12-月">自指令（Self-Instruct，2022 年 12 月）</a></li>
      <li><a href="#指令微调-instruction-fine-tuningift" id="markdown-toc-指令微调-instruction-fine-tuningift">指令微调 (Instruction Fine-Tuning，IFT）</a></li>
      <li><a href="#有监督微调supervised-fine-tuningsft" id="markdown-toc-有监督微调supervised-fine-tuningsft">有监督微调（Supervised Fine-Tuning，SFT）</a></li>
      <li><a href="#人类反馈强化学习reinforcement-learning-from-human-feedbackrlhf" id="markdown-toc-人类反馈强化学习reinforcement-learning-from-human-feedbackrlhf">人类反馈强化学习（Reinforcement Learning from Human Feedback，RLHF）</a></li>
      <li><a href="#ai-反馈强化学习reinforcemen-learning-from-ai-feedbackrlaif" id="markdown-toc-ai-反馈强化学习reinforcemen-learning-from-ai-feedbackrlaif">AI 反馈强化学习（Reinforcemen Learning from AI Feedback，RLAIF）</a></li>
      <li><a href="#自监督self-supervised" id="markdown-toc-自监督self-supervised">自监督（Self-Supervised）</a></li>
      <li><a href="#conventional-fine-tuning" id="markdown-toc-conventional-fine-tuning">Conventional Fine-Tuning</a></li>
    </ul>
  </li>
  <li><a href="#三模型发展" id="markdown-toc-三模型发展">三、模型发展</a>    <ul>
      <li><a href="#google---bert" id="markdown-toc-google---bert">Google - BERT</a></li>
      <li><a href="#google---palm--pathways2022-年-24-月" id="markdown-toc-google---palm--pathways2022-年-24-月">Google - PaLM &amp; Pathways（2022 年 2~4 月）</a></li>
      <li><a href="#google---lamda" id="markdown-toc-google---lamda">Google - LaMDA</a></li>
      <li><a href="#meta---galactica" id="markdown-toc-meta---galactica">Meta - Galactica</a></li>
      <li><a href="#meta---blenderbot" id="markdown-toc-meta---blenderbot">Meta - BlenderBot</a></li>
      <li><a href="#openai---gpt" id="markdown-toc-openai---gpt">OpenAI - GPT</a>        <ul>
          <li><a href="#gpt-32020-年年中" id="markdown-toc-gpt-32020-年年中">GPT-3（2020 年年中）</a></li>
        </ul>
      </li>
      <li><a href="#openai---instructgpt2022-年" id="markdown-toc-openai---instructgpt2022-年">OpenAI - InstructGPT（2022 年）</a>        <ul>
          <li><a href="#基于-gpt-3-以-sft-方法训练出来的-instructgpt" id="markdown-toc-基于-gpt-3-以-sft-方法训练出来的-instructgpt">基于 GPT-3 以 SFT 方法训练出来的 InstructGPT</a></li>
          <li><a href="#基于-gpt-3-以-feedmefeedback-made-easy方法训练出来的-instructgpt" id="markdown-toc-基于-gpt-3-以-feedmefeedback-made-easy方法训练出来的-instructgpt">基于 GPT-3 以 FeedME（Feedback Made Easy）方法训练出来的 InstructGPT</a></li>
          <li><a href="#基于-gpt-3-以-ppo-方法训练" id="markdown-toc-基于-gpt-3-以-ppo-方法训练">基于 GPT-3 以 PPO 方法训练</a></li>
        </ul>
      </li>
      <li><a href="#openai---chatgpt2022-年-11-月底" id="markdown-toc-openai---chatgpt2022-年-11-月底">OpenAI - ChatGPT（2022 年 11 月底）</a></li>
      <li><a href="#deepmind---sparrow" id="markdown-toc-deepmind---sparrow">DeepMind - Sparrow</a></li>
      <li><a href="#anthropic---claude" id="markdown-toc-anthropic---claude">Anthropic - Claude</a></li>
      <li><a href="#laion---open-assistant" id="markdown-toc-laion---open-assistant">LAION - Open Assistant</a></li>
      <li><a href="#google---bard" id="markdown-toc-google---bard">Google - Bard</a></li>
    </ul>
  </li>
  <li><a href="#四各大公司" id="markdown-toc-四各大公司">四、各大公司</a>    <ul>
      <li><a href="#deepmind" id="markdown-toc-deepmind">DeepMind：</a></li>
      <li><a href="#meta" id="markdown-toc-meta">Meta：</a></li>
      <li><a href="#google" id="markdown-toc-google">Google：</a></li>
      <li><a href="#anthropic" id="markdown-toc-anthropic">Anthropic:</a></li>
      <li><a href="#openai" id="markdown-toc-openai">OpenAI：</a></li>
    </ul>
  </li>
  <li><a href="#参考" id="markdown-toc-参考">参考</a></li>
</ul>

<h2 id="数据集">数据集</h2>

<ul>
  <li>Wikipedia 数据集：https://meta.wikimedia.org/wiki/Data_dump_torrents</li>
  <li>C4（Colossal Clean Crawled Corpus）：https://www.tensorflow.org/datasets/catalog/c4</li>
</ul>

<h2 id="一模型理念">一、模型理念</h2>

<h3 id="生成模型生成器">生成模型（生成器）</h3>

<p>Generative Against Network
Boltsmann Machine
Variational Auto-Encode
GPT</p>

<h3 id="缩放定律the-scaling-law">缩放定律（The Scaling Law）</h3>

<ul>
  <li>（OpenAI）的研究者认为语言模型的性能与模型尺寸的关系可以通过对数线性曲线预测，即模型尺寸呈指数增长时，性能会随之线性增加。这种现象被称为语言模型的缩放定律，正如 Kaplan 等人在2020年最初的GPT3文章中讨论的那样。</li>
</ul>

<h3 id="思维链chain-of-thoughtcot2022-年-1-月">思维链（Chain of Thought，CoT，2022 年 1 月）</h3>

<p>正如作者所展示的那样，思维链提示在性能-比例曲线中表现出明显的<strong>相变</strong>。当模型尺寸足够大时，性能会显著提高并明显超越比例曲线。</p>

<p>当使用思维链进行提示时，大模型在复杂推理上的表现明显优于微调，在知识推理上的表现也很有竞争力，并且分布鲁棒性也存在一定的潜力。要达到这样的效果只需要8个左右的示例，意味着范式可能会转变。在 ChatGPT 上线之后，整个领域为之震撼，意识到范式已经转变了。[1]</p>

<h2 id="二训练方法">二、训练方法</h2>

<h3 id="强化学习reinforcement-learning">强化学习（Reinforcement Learning）</h3>

<ul>
  <li>无法使用数据集训练，只能通过真实环境或模拟器产生的数据来学习</li>
  <li>应用：游戏 AI（Game AI）、AlphaGo</li>
</ul>

<h3 id="深度神经网络">深度神经网络</h3>

<ul>
  <li>使用神经网络构建强化学习的方法，就是深度强化学习</li>
</ul>

<h3 id="自指令self-instruct2022-年-12-月">自指令（Self-Instruct，2022 年 12 月）</h3>

<ul>
  <li>在高质量的人类标注数据上，微调基础语言模型。</li>
  <li>论文地址：<a href="https://arxiv.org/abs/2212.10560">《Self-Instruct: Aligning Language Model with Self Generated Instructions》</a></li>
</ul>

<h3 id="指令微调-instruction-fine-tuningift">指令微调 (Instruction Fine-Tuning，IFT）</h3>

<ul>
  <li>few-shot prompting，指令微调可以看作是有监督微调的一个子集。</li>
  <li>instruction + input + output 组成，前面两个组起来是 instance</li>
  <li>input、output 组成的 instance 是可选的，也可以没有，比如开放式生成文本</li>
  <li>IFT 的 training data 通常是人工编写 instruction + bootstrap（语言模型自举）得到的 instances</li>
</ul>

<h3 id="有监督微调supervised-fine-tuningsft">有监督微调（Supervised Fine-Tuning，SFT）</h3>

<ul>
  <li>SFT 阶段经常被用于提高响应的<strong>安全性</strong>，而不是接在 IFT 后面提高指令相应的具体性。</li>
  <li>指令微调可以看作是有监督微调的一个子集。</li>
</ul>

<h3 id="人类反馈强化学习reinforcement-learning-from-human-feedbackrlhf">人类反馈强化学习（Reinforcement Learning from Human Feedback，RLHF）</h3>

<ul>
  <li>使用了 RLHF 的模型：OpenAI 的 InstructGPT、DeepMind 的 Sparrow 和 Anthropic 的 Constitutional AI。</li>
  <li>偏好模型：RLHF 中包含一个偏好模型，该模型用于返回 RL 优化器的标量奖励。</li>
  <li>RLHF：根据人类反馈来对模型的输出响应进行排序标注，用这些带标注的输出响应来训练「偏好模型」，最后通过强化学习训练 Dialogue Agent 来模拟偏好模型。</li>
</ul>

<p>RLHF 的步骤：</p>

<ul>
  <li>预训练一个语言模型 (LM) ；</li>
  <li>聚合问答数据并训练一个奖励模型 (Reward Model，RM) ；</li>
  <li>用强化学习 (RL) 方式微调 LM。</li>
</ul>

<p><img src="/img/src/2023/2023-01-24-sft-sparrow-rules.png" alt="image" /></p>

<h3 id="ai-反馈强化学习reinforcemen-learning-from-ai-feedbackrlaif">AI 反馈强化学习（Reinforcemen Learning from AI Feedback，RLAIF）</h3>

<ul>
  <li>https://arxiv.org/abs/2212.08073</li>
</ul>

<h3 id="自监督self-supervised">自监督（Self-Supervised）</h3>

<h3 id="conventional-fine-tuning">Conventional Fine-Tuning</h3>

<ul>
  <li>论文链接：</li>
</ul>

<h2 id="三模型发展">三、模型发展</h2>

<p>XLNet、RoBERTa、ALBert、Google T5</p>

<h3 id="google---bert">Google - BERT</h3>

<ul>
  <li>论文地址：https://aclanthology.org/N19-1423.pdf</li>
</ul>

<h3 id="google---palm--pathways2022-年-24-月">Google - PaLM &amp; Pathways（2022 年 2~4 月）</h3>

<p>最能体现 Google 技术眼光</p>

<h3 id="google---lamda">Google - LaMDA</h3>

<ul>
  <li>论文地址：https://arxiv.org/abs/2201.08239</li>
</ul>

<h3 id="meta---galactica">Meta - Galactica</h3>

<ul>
  <li>问题：发现会生成错的或有偏见的，这个很危险。</li>
  <li>上线后 3 天下架（2022 年 11 月 18 日）</li>
</ul>

<h3 id="meta---blenderbot">Meta - BlenderBot</h3>

<ul>
  <li>论文地址：https://arxiv.org/abs/2208.03188</li>
</ul>

<h3 id="openai---gpt">OpenAI - GPT</h3>

<h4 id="gpt-32020-年年中">GPT-3（2020 年年中）</h4>

<div style="text-align: center;">
<div class="graphviz-wrapper">

<!-- Generated by graphviz version 2.43.0 (0)
 -->
<!-- Title: G Pages: 1 -->
<svg role="img" aria-label="graphviz-a72ed7c277047eff301cb7f2cfadbbf3" width="1186pt" height="188pt" viewBox="0.00 0.00 1186.48 188.00">
<title>graphviz-a72ed7c277047eff301cb7f2cfadbbf3</title>
<desc>
digraph G {
	rankdir=BT
	splines=ortho

	mtd003[label=&quot;text-davinci-003&quot;]
	mtd002[label=&quot;text-davinci-002&quot;]
	mtd001[label=&quot;text-davinci-001&quot;]

	mtc001[label=&quot;text-curie-001&quot;]

	mtb001[label=&quot;text-babbage-001&quot;]

	mcd001[label=&quot;code-davinci-001&quot;]
	mcd002[label=&quot;code-davinci-002&quot;]

	mdib[label=&quot;davinci-instruct-beta1&quot;]

	mcd002 -&gt; mtd002
	mtd002 -&gt; mtd003
}
</desc>

<g id="graph0" class="graph" transform="scale(1 1) rotate(0) translate(4 184)">
<title>G</title>
<polygon fill="white" stroke="transparent" points="-4,4 -4,-184 1182.48,-184 1182.48,4 -4,4" />
<!-- mtd003 -->
<g id="node1" class="node">
<title>mtd003</title>
<ellipse fill="none" stroke="black" cx="89.04" cy="-162" rx="85.59" ry="18" />
<text text-anchor="middle" x="89.04" y="-158.3" font-family="Times,serif" font-size="14.00">text&#45;davinci&#45;003</text>
</g>
<!-- mtd002 -->
<g id="node2" class="node">
<title>mtd002</title>
<ellipse fill="none" stroke="black" cx="89.04" cy="-90" rx="85.59" ry="18" />
<text text-anchor="middle" x="89.04" y="-86.3" font-family="Times,serif" font-size="14.00">text&#45;davinci&#45;002</text>
</g>
<!-- mtd002&#45;&gt;mtd003 -->
<g id="edge2" class="edge">
<title>mtd002&#45;&gt;mtd003</title>
<path fill="none" stroke="black" d="M89.04,-108.17C89.04,-108.17 89.04,-133.59 89.04,-133.59" />
<polygon fill="black" stroke="black" points="85.54,-133.59 89.04,-143.59 92.54,-133.59 85.54,-133.59" />
</g>
<!-- mtd001 -->
<g id="node3" class="node">
<title>mtd001</title>
<ellipse fill="none" stroke="black" cx="282.04" cy="-18" rx="85.59" ry="18" />
<text text-anchor="middle" x="282.04" y="-14.3" font-family="Times,serif" font-size="14.00">text&#45;davinci&#45;001</text>
</g>
<!-- mtc001 -->
<g id="node4" class="node">
<title>mtc001</title>
<ellipse fill="none" stroke="black" cx="462.04" cy="-18" rx="76.09" ry="18" />
<text text-anchor="middle" x="462.04" y="-14.3" font-family="Times,serif" font-size="14.00">text&#45;curie&#45;001</text>
</g>
<!-- mtb001 -->
<g id="node5" class="node">
<title>mtb001</title>
<ellipse fill="none" stroke="black" cx="648.04" cy="-18" rx="92.08" ry="18" />
<text text-anchor="middle" x="648.04" y="-14.3" font-family="Times,serif" font-size="14.00">text&#45;babbage&#45;001</text>
</g>
<!-- mcd001 -->
<g id="node6" class="node">
<title>mcd001</title>
<ellipse fill="none" stroke="black" cx="847.04" cy="-18" rx="89.08" ry="18" />
<text text-anchor="middle" x="847.04" y="-14.3" font-family="Times,serif" font-size="14.00">code&#45;davinci&#45;001</text>
</g>
<!-- mcd002 -->
<g id="node7" class="node">
<title>mcd002</title>
<ellipse fill="none" stroke="black" cx="89.04" cy="-18" rx="89.08" ry="18" />
<text text-anchor="middle" x="89.04" y="-14.3" font-family="Times,serif" font-size="14.00">code&#45;davinci&#45;002</text>
</g>
<!-- mcd002&#45;&gt;mtd002 -->
<g id="edge1" class="edge">
<title>mcd002&#45;&gt;mtd002</title>
<path fill="none" stroke="black" d="M89.04,-36.17C89.04,-36.17 89.04,-61.59 89.04,-61.59" />
<polygon fill="black" stroke="black" points="85.54,-61.59 89.04,-71.59 92.54,-61.59 85.54,-61.59" />
</g>
<!-- mdib -->
<g id="node8" class="node">
<title>mdib</title>
<ellipse fill="none" stroke="black" cx="1066.04" cy="-18" rx="112.38" ry="18" />
<text text-anchor="middle" x="1066.04" y="-14.3" font-family="Times,serif" font-size="14.00">davinci&#45;instruct&#45;beta1</text>
</g>
</g>
</svg>
</div>
</div>

<p>GPT-3 它不仅仅是一项具体的技术，其实体现的是 LLM 应该往何处去的一个发展理念。自此之后，差距拉得越来越远，ChatGPT 只是这种发展理念差异的一个自然结果。</p>

<ul>
  <li>训练数据集：45 TB</li>
  <li>参数规模：1750 亿，大小超过 700G</li>
</ul>

<h3 id="openai---instructgpt2022-年">OpenAI - InstructGPT（2022 年）</h3>

<ul>
  <li>在性能不妥协的情况下，InstructGPT 只用了 13 亿参数（vs. GPT-3 用了 1750 亿参数），并且采用 RLHF 技术使模型对齐（align）人类，即减少有害内容的输出。</li>
  <li>标注员（labelers）基于 OpenAI 的客户提交给 API 的提示（prompts）进行强化学习。具体地，用的是 2021 年 1 月部署的 Playground（用到 InstructGPT）收到的用户提交的提示（prompts），对这些 prompts 标注员提供一些示例（相当于 few-shot fine-tuning），这样来训练，然后对模型输出的结果排序，我们基于这些输出排序结果来精调 GPT-3 得到了 InstructGPT。</li>
</ul>

<h4 id="基于-gpt-3-以-sft-方法训练出来的-instructgpt">基于 GPT-3 以 SFT 方法训练出来的 InstructGPT</h4>

<ul>
  <li></li>
  <li>模型：davinci-instruct-beta</li>
</ul>

<h4 id="基于-gpt-3-以-feedmefeedback-made-easy方法训练出来的-instructgpt">基于 GPT-3 以 FeedME（Feedback Made Easy）方法训练出来的 InstructGPT</h4>

<ul>
  <li>监督精调（人类写范例）+ RLHF（机器生成结果被人类打分 1~7）</li>
  <li>模型：text-davinci-001, text-davinci-002, text-curie-001, text-babbage-001</li>
</ul>

<h4 id="基于-gpt-3-以-ppo-方法训练">基于 GPT-3 以 PPO 方法训练</h4>

<ul>
  <li>用人类比较反馈的数据作为 RM（奖励模型）进行强化学习</li>
  <li>模型：text-davinci-003</li>
</ul>

<h3 id="openai---chatgpt2022-年-11-月底">OpenAI - ChatGPT（2022 年 11 月底）</h3>

<p>在 2023 年 1 月，Altman 接受的一次采访中提到「ChatGPT 的基本模型已经在 API 中存在很长时间了，大概 10 个月吧，甚至更久。我认为其中一个令人惊讶的情况就是，如果你做出一点微调，让（模型）以特定的方式对人们有所用途，并找出正确的交互范式，那么你就可以得到这个结果。实际上，这并不是一项全新的技术。（让它产生这个效果的）是其他的调整。我认为这一点还没有得到很好的理解。比如，很多人仍然不相信我们，他们认为这一定是GPT-4。」</p>

<h3 id="deepmind---sparrow">DeepMind - Sparrow</h3>

<ul>
  <li>论文地址：https://arxiv.org/abs/2209.14375</li>
</ul>

<h3 id="anthropic---claude">Anthropic - Claude</h3>

<ul>
  <li>论文地址：https://arxiv.org/abs/2204.05862</li>
  <li>Claude vs. ChatGPT：https://scale.com/blog/chatgpt-vs-claude</li>
</ul>

<h3 id="laion---open-assistant">LAION - Open Assistant</h3>

<ul>
  <li>https://github.com/LAION-AI/Open-Assistant</li>
</ul>

<h3 id="google---bard">Google - Bard</h3>

<h2 id="四各大公司">四、各大公司</h2>

<p>在 LLM 这个事情上，感觉梯队很明显，Google 应该是排在第二位，最能体现 Google 技术眼光的是 PaLM 和 Pathways，推出时间大概在 22 年 2 月到 4 月间，同一时期，OpenAI 推出的却是 InstructGPT，从这里就可以看出 Google 和 OpenAI 的差距了，至于为何这么说，你看了我后面的正文后大概能理解。DeepMind 之前的重心一直在强化学习攻克游戏和 AI for science 这些方面，切入LLM 其实很晚，应该是21 年才开始重视这个方向，目前也处于追赶状态。Meta 就更不用说了，重心一直不在 LLM 上，目前感觉也发力开始追赶。</p>

<h3 id="deepmind">DeepMind：</h3>

<h3 id="meta">Meta：</h3>

<h3 id="google">Google：</h3>

<h3 id="anthropic">Anthropic:</h3>

<p>2019 年 OpenAI 与微软完成交易后，OpenAI 研究副总裁 Dario Amodei 不认同公司委身于微软，后于 2021 年包括 Amodei、GPT3 的 Lead Engineer Tom Brown 在内的 11 名 OpenAI 员工离职创立了 Anthropic。2022 年晚些时候，Google 向 Anthropic 投资了 3 亿美元。</p>

<p>与批评者认为 OpenAI 过于莽撞地推出 ChatGPT 而没有考虑足够的安全等问题形成对比的是，Anthropic 强调他们致力于建设「可靠、可控、可解释」的 AI 系统。</p>

<h3 id="openai">OpenAI：</h3>

<ul>
  <li><strong>Alignment</strong>: How can we understand what objective, if any, a model is best understood as pursuing? How do we increase the extent to which that objective is aligned with human preferences, such as via prompt design or fine-tuning?</li>
  <li><strong>Fairness and Representation</strong>: How should performance criteria be established for fairness and representation in language models? How can language models be improved in order to effectively support the goals of fairness and representation in specific, deployed contexts?</li>
  <li><strong>Interdisciplinary Research</strong>: How can AI development draw on insights from other disciplines such as philosophy, cognitive science, and sociolinguistics?</li>
  <li><strong>Interpretability / Transparency</strong>: How do these models work, mechanistically? Can we identify what concepts they’re using, or extract latent knowledge from the model, make inferences about the training procedure, or predict surprising future behavior?</li>
  <li><strong>Misuse Potential</strong>: How can systems like the API be misused? What sorts of ‘red teaming’ approaches can we develop to help us and other AI developers think about responsibly deploying technologies like this?</li>
  <li><strong>Model Exploration</strong>: Models like those served by the API have a variety of capabilities which we have yet to explore. We’re excited by investigations in many areas including model limitations, linguistic properties, commonsense reasoning, and potential uses for many other problems.</li>
  <li><strong>Robustness</strong>: Generative models have uneven capability surfaces, with the potential for surprisingly strong and surprisingly weak areas of capability. How robust are large generative models to “natural” perturbations in the prompt, such as phrasing the same idea in different ways or with/without typos? Can we predict the kinds of domains and tasks for which large generative models are more likely to be robust (or not robust), and how does this relate to the training data? Are there techniques we can use to predict and mitigate worst-case behavior? How can robustness be measured in the context of few-shot learning (e.g. across variations in prompts)? Can we train models so that they satisfy safety properties with a very high level of reliability, even under adversarial inputs?</li>
</ul>

<h2 id="参考">参考</h2>

<ul>
  <li>https://huggingface.co/blog/dialog-agents</li>
  <li>https://yaofu.notion.site/A-Closer-Look-at-Large-Language-Models-Emergent-Abilities-493876b55df5479d80686f68a1abd72f</li>
  <li><a href="https://mp.weixin.qq.com/s/TLQ3TdrB5gLb697AFmjEYQ">《ChatGPT 背后的“功臣”——RLHF 技术详解》</a></li>
  <li>https://platform.openai.com/docs/model-index-for-researchers</li>
  <li>https://openai.com/blog/instruction-following/#rfref18</li>
</ul>

	</div>
</article>



	  </main>
		
		  <!-- Pagination links -->
      

	  </div>
	    
	    <!-- Footer -->
	    <footer>
	<span>
		-<br/><br/>
		船长还不会游泳 at 微信公众号/微博<br/>
		@麦克船长 at 即刻/知乎/小宇宙/掘金/小红书/微信读书<br/>
		@船长模玩 at Bilibili<br/>
		Copyright © 2011-2023, MikeCaptain.com
	</span>
</footer>


	    <!-- Script -->
      <script src="/js/main.js"></script>	


	</div>
</body>
</html>

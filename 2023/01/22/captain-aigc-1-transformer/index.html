<!DOCTYPE html>
<html>

<head>
	<!-- Meta -->
	<meta charset="UTF-8"/>
	<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
	<meta name="generator" content="Jekyll">

	<title>人工智能 LLM 革命前夜：一文读懂横扫自然语言处理的 Transformer 模型</title>
  	<meta name="description" content="基于 RNN 的 Encoder-Decoder 模型存在无法处理过长文本、并行性差的两大痛点。2015 年 Bahdanau 等人在其论文中提出 Attention 机制，再到 2017 年 Transformer 模型的论文《Attention is All You Need》横空出世，其并行速度极快，而且每两个词之间的词间距都是 1。此后 NLP 领域 Transformer 彻底成为主流。如果你已经了解 Encoder-Decoder 模型，本文将基于此带你深入浅出的搞清楚 Attention、Transformer。">

	<!-- CSS & fonts -->
	<link rel="stylesheet" href="/css/main.css">

	<!-- RSS -->
	<link href="/atom.xml" type="application/atom+xml" rel="alternate" title="ATOM Feed" />

  	<!-- Favicon -->
 	 <link rel="shortcut icon" type="image/png" href="/img/favicon.png">

 	 <!-- Syntax highlighter -->
  	<link rel="stylesheet" href="/css/syntax.css" />

  	<!--KaTeX-->
  	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
  	<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>
  	<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"></script>
  	<script>
  		document.addEventListener("DOMContentLoaded", function() {
  			renderMathInElement(document.body, {
  				// ...options...
  			});
  		});
  	</script>

  	
  	<!-- KaTeX -->
  	<link rel="stylesheet" href="/assets/plugins/katex.0.11.1/katex.min.css">
  	

</head>

<body>
	<div id="wrap">
	  	
	  	<!-- Navigation -->
	  	<nav id="nav">
	<div id="nav-list">
		<a href="/">Home</a>

		<!-- Nav pages -->
	  <!-- 
	    
	  
	    
	      <a href="/about/" title="关于我">关于我</a>
	    
	  
	    
	  
	    
	      <a href="/booklist/" title="读书行路">读书行路</a>
	    
	  
	    
	      <a href="/categories/" title="Categories">Categories</a>
	    
	  
	    
	  
	    
	  
	    
	  
	    
	  
	    
	  
	    
	  
	    
	  
	    
	  
	    
	  
	    
	   -->

	  <!-- Tech category pages -->






  <a href="/category/ai" title="人工智能">人工智能</a>











  <a href="/category/rt_tech" title="实时技术">实时技术</a>





  <a href="/category/web" title="前端技术">前端技术</a>














<!-- Non-tech category pages -->


















  <a href="/category/thinking" title="思考与生活">思考与生活</a>















	  
        
      
        
          <a href="/about/" title="关于我">关于我</a>
        
      
        
      
        
          <a href="/booklist/" title="读书行路">读书行路</a>
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    <!-- Nav links -->
	  <!-- <a href="https://github.com/thereviewindex/monochrome/archive/master.zip">Download</a>
<a href="https://github.com/thereviewindex/monochrome">Project on Github</a> -->

	</div>
  
  <!-- Nav footer -->
	
	  <footer>
	
	<span>version 1.0.0</span>

</footer>
	

</nav>

    
    <!-- Icon menu -->
	  <a id="nav-menu">
	  	<div id="menu"></div>
	  </a>

      <!-- Header -->
      
        <header id="header" class="parent justify-spaceBetween">
  <div class="inner w100 relative">
    <span class="f-left">  
      <a href="/">
        <h1>
          <span>Mike</span>Captain
        </h1>
      </a>
    </span>
    <span id="nav-links" class="absolute right bottom">

      <!-- Tech category pages -->






  <a href="/category/ai" title="人工智能">人工智能</a>











  <a href="/category/rt_tech" title="实时技术">实时技术</a>





  <a href="/category/web" title="前端技术">前端技术</a>














<!-- Non-tech category pages -->


















  <a href="/category/thinking" title="思考与生活">思考与生活</a>















      &nbsp;&nbsp;&nbsp;丨&nbsp;

      <!-- Nav pages -->
      
        
      
        
          <a href="/about/" title="关于我">关于我</a>
        
      
        
      
        
          <a href="/booklist/" title="读书行路">读书行路</a>
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
      
      <!-- Nav links -->
      <!-- <a href="https://github.com/thereviewindex/monochrome/archive/master.zip">Download</a>
<a href="https://github.com/thereviewindex/monochrome">Project on Github</a> -->

    </span>
  </div>
</header>




      

    <!-- Main content -->
	  <div id="container">
		  
		<main>

			<article id="post-page">
	<h2>人工智能 LLM 革命前夜：一文读懂横扫自然语言处理的 Transformer 模型</h2>		
	<time datetime="2023-01-22T09:13:09+00:00" class="by-line">22 Jan 2023, 香港 | 麦克船长 | 总计 78105 字</time>
	<div class="content">
		<h2 id="前言">前言</h2>

<p>本文试图从技术角度搞清楚一个问题：<strong>过去一年 AIGC 爆火、过去五年 NLP（自然语言处理）领域突飞猛进的缘起是什么？</strong></p>

<p>这个问题被解答后，将还有两个问题，但暂时本文没有作答：1）如果认为通过图灵测试代表着 AGI（Artificial General Intelligence，通用人工智能）的话，当下 NLP，乃至 AGI 发展到什么程度了？2）未来一些年内，AGI 的发展路线可能会是怎样的？</p>

<p>利用春节时间，写了这么一篇数万字的长文笔记，希望共同爱好的朋友能读完多多指正。我是船涨，网名一直用「麦克船长」，中科大计算机本科毕业后先是做的 RTC 技术、分布式系统等等，干过 Full Stack，后来创业在产品、运营、营销、供应链上折腾了些年后来到阿里，在淘系做过产品、运营。</p>

<h4 id="1我来阿里之后第一个新增爱好是变形金刚模型第二个新增爱好是变形金刚模型">1、我来阿里之后第一个新增爱好是「变形金刚模型」，第二个新增爱好是「变形金刚模型」</h4>

<p>写了个这么冷的梗，其实想说的是，前者指的是著名 IP「变形金刚」相关的手办玩具模型，后者指的是这个引领革命的人工智能语言模型 Transformer。这两个爱好，都与目前从事的电商工作本职没有表面上的直接联系，权当爱好了。</p>

<p>2022 年「生成式 AI」应用取得了突飞猛进的发展，作为一个「古典互联网」从业者，深切地感到这一次 AI 技术可能会带来的颠覆式变革，这让我兴奋又焦虑。2022 年上半年，我从天天特卖业务负责人到大聚划算运营中心负责人，在去年相当长一段时间里在关注直播带货在营销平台的模式命题，一直在思考一个问题：直播电商的高效（更适合的商品演绎方式 + 私域权益 + 冲动购买等」vs. 直播电商的低效（直播分发无人货匹配 + 直播间内千人一面 + 货品状态未知 + 主播不可控等），能否推动一个保留直播的高效，同时解决直播的低效的模式呢？</p>

<p>这里面有大量的内容值得探讨，不过这不是船涨该系列文章的初衷，但这是我为什么开始非常关注 AI 的引子。直播电商的数字人技术基础，有动作捕捉、面部表情模拟、视觉渲染、直播话术生成、语音合成等等。依据第一性原理抽丝剥茧后，我发现尽管动作捕捉、视觉渲染等等很多技术仍有很大挑战，但是从商业视角看真正最影响用户心智的，是直播话术生成和演绎，除了头部主播，绝大多数直播带货在这方面都做的很糟糕，那么这里面就有巨大的「机器学习」生成内容超越非头部的大多数从业者的市场空间，而这完全依赖自然语言处理（NLP）。</p>

<p>这个问题就属于「生成式 AI」的范畴了，国外科技圈叫它「Gen-AI」，即 Generative AI，中国科技圈都叫它「AIGC」，即 AI Generated Content，与 UGC、PGC 相对应。Gen-AI 的叫法更关注主体，具体地说是「生成式 AI 模型」，它是个「内容引擎」。而中国的叫法更关注「内容应用」。</p>

<p>讲到 AIGC 这里，大家熟悉的 ChatGPT 就在 2022 年年底登场了。也是因为 ChatGPT 的破圈，带来了 AIGC 在国内科技圈的关注度暴涨。我从去年年中开始关注「文生图，text2image」领域的明星 Stable Diffusion 开源，进而关注到了 text2image 应用的爆发，包括 Disco Diffusion、MidJourney、DALL·E 2 等等，这些都源于 CV（计算机视觉）领域因为 Diffusion 模型发展带来的技术突破。</p>

<p>AI 生成图片确实非常惊人。我酷爱变形金刚模玩，进而对机甲类都非常喜欢，所以随手生成了几张图，这里贴一下大家看看，分钟级的创作速度。（注意：当下 AI 生成图片主要是基于 Diffusion 的应用发展，AI 生成文本的核心驱动才是 Transformer 模型，此处只是展示）</p>

<table>
  <thead>
    <tr>
      <th><img src="/img/src/2022-12-16-midjourney-first-test-1.png" alt="image" /></th>
      <th><img src="/img/src/2022-12-16-midjourney-first-test-2.png" alt="image" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td> </td>
      <td> </td>
    </tr>
  </tbody>
</table>

<p>但是从第一性原理角度讲，生成图片的应用广度，远远小于生成文本。文本内容的本质是语言文字的理解与生成，人类历史有 600 万年，但是人类文明历史大概就 6000 年，文明的大发展出现在近 2000 多年的原因，主要来自 3500 多年前人类发明了文字。所以 AI 生成文本，意味着 AI 可以用人类熟悉的方式（语言文字）与人类高效协作，这必将引爆生产力革命。而这必将深入影响电商、内容、游戏、云计算、企业服务等众多领域。</p>

<h4 id="2掌握技术基础是当下读懂-ai-脉搏的基本功而这个脉搏将带动各行各业">2、掌握技术基础，是当下读懂 AI 脉搏的基本功，而这个脉搏将带动各行各业</h4>

<p>一旦深入关注 AI、关注 NLP 领域，你就会发现当下仍然处于一个技术发展突破的阶段，不关注技术的情况下来聊 AI、聊 NLP、聊 AIGC，那就只能是一个「爱好者」，而无法深入与这个行业内的弄潮儿对话，更不要提参与其中了。所以这个春节，船涨回归了当年做技术时的初心，翻了一些材料，学习了 NLP 语言模型的关键技术，在此作为技术学习笔记，与大家分享。尽管担心班门弄斧，但是本着费曼老师提倡的输出学习法，我把自己学习梳理的内容抛出来，除了会更帮助到我自己，也能结交一些对此同样在关注的同学们，欢迎感兴趣的同学加我的微信（微信号 sinosuperman）在业余时间和我交流。</p>

<p>本文将包括这几部分：</p>

<ul>
  <li><strong>第一章，主要介绍 Transformer 出现之前的几个主流语言模型，包括 N 元文法（n-gram）、多层感知器（MLP）、卷积神经网络（CNN）、循环神经网络（RNN）。其中 CNN 主要应用领域在计算机视觉，因此没有更详细展开。其他模型也未面面俱到，主要考虑还是一个领域学习者的角度来了解和应用，而非研究。</strong></li>
  <li><strong>第二章，是本文的核心，先介绍了注意力机制（Attention Mechanism），然后基于第一章对此前几大语言模型了解后，我们能更好地理解 Transformer 为什么会带来革命性的影响。</strong></li>
  <li><strong>第三章，是一个 Transformer 的实现版本，基于 Tensorflow。</strong></li>
</ul>

<p>阅读本文，先对你过往的基础知识做了一些假设，如果你暂未了解，可能在阅读时遇到以下内容做一些简单地查询即可：</p>

<ul>
  <li>Word Presentation：自然语言处理中的词表示法，主要涉及 embedding。</li>
  <li>张量：需要一点基础，比如了解张量的形状、升降维度等。但不会涉及到复杂问题，对一阶张量（向量）、二阶张量（矩阵）的简单运算有数学基础即可。对三阶张量，大概能想象出其空间含义即可。语言模型里理解词之间的距离，是有其空间几何意义的。</li>
  <li>技术框架：PyTorch 或 TensorFlow 框架。由于时间和篇幅关系，春节期间梳理这些时，对于框架基础，我主要是 Google 现用现查，询问 ChatGPT 以及在微信读书里直接搜索全文。</li>
</ul>

<p>作为技术笔记难免有纰漏或理解错误，欢迎指正。文中自绘图片用的是 Graphviz，公式生成用的是 KaTeX，贴到 ATA 后难免有一些没有兼容的部分（发现的已做了 fix），望见谅。</p>

<h2 id="第一章--2017-年之前的几个关键-nlp-语言模型">第一章 · 2017 年之前的几个关键 NLP 语言模型</h2>

<p>NLP 的技术基础方面，我认为主要是这两部分：词表示法（Word Presentation）、语言模型（Language Model）。对于词表示法，这里不做详细介绍，基本的思路就是把词表示为向量（一维张量），最基本的 One-Hot、Word2Vec、GloVe、fastText 等。这部分的技术演进也在不断前进，比如本文将要重点介绍的 Transformer 模型里，用到的词表示法是「引入上下文感知的词向量」。</p>

<p>语言模型从早期的 N 元文法（N-Gram，本文要介绍的），到神经网络被提出后最早期的感知器（Perceptron），再到后来席卷计算机视觉（CV）领域的卷积神经网络（CNN），然后出现考虑序列特征的循环神经网络（RNN，包括 Encoder-Decoder 模型），直到 2017 年横空出世的 Transformer，大概分这五个主要阶段。因为本文的重点是 Transformer，所以前面四个模型我会快速概览一下，然后介绍下最朴素的注意力（Attention）机制，基于此再详细介绍下 Transformer，并对一个完整的、精炼实现的代码实例进行精讲。</p>

<h3 id="第-1-节--n-元文法语言模型">第 1 节 · N 元文法语言模型</h3>

<h4 id="11马尔科夫假设markov-assumption与-n-元文法语言模型n-gram-language-model">1.1、马尔科夫假设（Markov Assumption）与 N 元文法语言模型（N-gram Language Model）</h4>

<p>下一个词出现的概率只依赖于它前面 n-1 个词，这种假设被称为「马尔科夫假设（Markov Assumption」。N 元文法，也称为 N-1 阶马尔科夫链。</p>

<ul>
  <li>一元文法（1-gram），unigram，零阶马尔科夫链，不依赖前面任何词；</li>
  <li>二元文法（2-gram），bigram，一阶马尔科夫链，只依赖于前 1 个词；</li>
  <li>三元文法（3-gram），trigram，二阶马尔科夫链，只依赖于前 2 个词；</li>
  <li>……</li>
</ul>

<p>通过前 t-1 个词预测时刻 t 出现某词的概率，用最大似然估计：</p>

\[P(w_t | w_1,w_2...w_{t-1}) = \frac{C(w_1,w_2,...w_t)}{C(w_1,w_2,...w_{t-1})}\]

<p>进一步地，一组词（也就是一个句子）出现的概率就是：</p>

\[P(w_1,w_2,...w_t) = P(w_t | w_1,w_2,...w_{t-1}) \cdot P(w_{t-1} | w_1,w_2,...w_{t-2}) \cdot ... \cdot P(w_1)
			      = \displaystyle\prod_{i=1}^{t-1}P(w_i | w_{1:i-1})\]

<p>为了解决句头、尾逇概率计算问题，我们再引入两个标记 <code class="language-plaintext highlighter-rouge">&lt;BOS&gt;</code> 和 <code class="language-plaintext highlighter-rouge">&lt;EOS&gt;</code> 分别表示 beginning of sentence 和 end of sentence，所以  \(w_0 =\) &lt;BOS&gt;、 \(w_{length + 1} =\) &lt;EOS&gt;，其中 length 是词的数量。</p>

<p>具体地，比如对于 bigram，该模型表示如下：</p>

\[\begin{aligned}
P(w_1,w_2,...w_t) &amp;= \displaystyle\prod_{i=1}^{t-1}P(w_i | w_{i-1}) \\
P(w_t | w_{t-1}) &amp;= \frac{C(w_{t-1}, w_t)}{C(w_{t-1})}
\end{aligned}\]

<ul>
  <li>如果有词出现次数为了 0，这一串乘出来就是 0 了，咋办？</li>
  <li>因为基于马尔科夫假设，所以 N 固定窗口取值，对长距离词依赖的情况会表现很差。</li>
  <li>如果把 N 值取很大来解决长距离词依赖，则会导致严重的数据稀疏（零频太多了），参数规模也会急速爆炸（高维张量计算）。</li>
</ul>

<p>上面的第一个问题，我们引入平滑 / 回退 / 差值等方法来解决，而后面两个问题则是在神经网络模型出现后才更好解决的。</p>

<h4 id="12平滑smoothing-折扣discounting">1.2、平滑（Smoothing）/ 折扣（Discounting）</h4>

<p>虽然限定了窗口 n 大小降低了词概率为 0 的可能性，但当 n-gram 的 n 比较大的时候会有的未登录词问题（Out Of Vocabulary，OOV）。另一方面，训练数据很可能也不是 100% 完备覆盖实际中可能遇到的词的。所以为了避免 0 概率出现，就有了让零平滑过渡为非零的补丁式技术出现。</p>

<p>最简单的平滑技术，就是折扣法（Discounting）。这是一个非常容易想到的办法，就是把整体 100% 的概率腾出一小部分来，给这些零频词（也常把低频词一起考虑）。常见的平滑方法有：加 1 平滑、加 K 平滑、Good-Turing 平滑、Katz 平滑等。</p>

<h5 id="121加-1-平滑--拉普拉斯平滑add-one-discounting--laplace-smoothing">1.2.1、加 1 平滑 / 拉普拉斯平滑（Add-One Discounting / Laplace Smoothing）</h5>

<p>加 1 平滑，就是直接将所有词汇的出现次数都 +1，不止针对零频词、低频词。如果继续拿 bigram 举例来说，模型就会变成：</p>

\[P(w_i | w_{i-1}) = \frac{C_(w_{i-1},w_i) + 1}{\displaystyle\sum_{j=1}^n(C_(w_{i-1},w_j) + 1)} = \frac{C(w_{i-1}, w_i) + 1}{C(w_{i-1}) + |\mathbb{V}|}\]

<table>
  <tbody>
    <tr>
      <td>其中  \(N\)  表示所有词的词频之和， $$</td>
      <td>\mathbb{V}</td>
      <td>$$  表示词汇表的大小。</td>
    </tr>
  </tbody>
</table>

<p>如果当词汇表中的词，很多出现次数都很小，这样对每个词的词频都 +1，结果的偏差影响其实挺大的。换句话说，+1 对于低频词很多的场景，加的太多了，应该加一个更小的数（ 1 &lt; δ &lt; 1）。所以有了下面的「δ 平滑」技术。</p>

<h5 id="122加-k-平滑--δ-平滑add-k-discounting--delta-smoothing">1.2.2、加 K 平滑 / δ 平滑（Add-K Discounting / Delta Smoothing）</h5>

<p>把 +1 换成 δ，我们看下上面 bigram 模型应该变成上面样子：</p>

\[P(w_i | w{i-1}) = \frac{C_(w_{i-1},w_i) + \delta}{\displaystyle\sum_{j=1}^n(C_(w_{i-1},w_j) + \delta)} = \frac{C(w_{i-1}, w_i) + \delta}{C(w_{i-1}) + \delta|\mathbb{V}|}\]

<p>δ 是一个超参数，确定它的值需要用到困惑度（Perplexity，一般用缩写 PPL）。另外，有些文章里也会把这个方法叫做「加 K 平滑，Add-K Smoothing」。</p>

<h5 id="123困惑度perplexity">1.2.3、困惑度（Perplexity）</h5>

<p>对于指定的测试集，困惑度定义为测试集中每一个词概率的几何平均数的倒数，公式如下：</p>

\[\operatorname{PPL}(\mathbb{D}_{test}) = \frac{1}{\sqrt[n]{P(w_1,w_2...w_n)}}\]

<p>把  \(P(w_1,w_2,...w_t) = \displaystyle\prod_{i=1}^{t-1}P(w_i\text{\textbar}w_{i-1})\)  带入上述公式，就得到了 PPL 的计算公式：</p>

\[\operatorname{PPL}(\mathbb{D}_{test}) = (\displaystyle\prod_{i=1}^nP(w_i|w_{1:i-1}))^{-\frac{1}{n}}\]

<h4 id="13回退back-off">1.3、回退（Back-off）</h4>

<p>在多元文法模型中，比如以 3-gram 为例，如果出现某些三元语法概率为零，则不使用零来表示概率，而回退到 2-gram，如下。</p>

\[P(w_i|w_{i-2}w_{i-1}) =
\begin{cases}
P(w_i|w_{i-2}w_{i-1}) &amp; C(w_{i-2}w_{i-1}w_i) &gt; 0 \\
P(w_i|w_{i-1}) &amp; C(w_{i-2}w_{i-1}w_i) = 0 \enspace and \enspace C(w_{i-1}w_i) &gt; 0
\end{cases}\]

<h4 id="14差值interpolation">1.4、差值（Interpolation）</h4>

<p>N 元文法模型如果用回退法，则只考虑了 n-gram 概率为 0 时回退为 n-1 gram，那么自然要问：n-gram 不为零时，是不是也可以按一定权重来考虑 n-1 gram？于是有了插值法。以 3-gram 为例，把 2-gram、1-gram 都考虑进来：</p>

\[P(w_i|w_{i-2}w_{i-1}) = \lambda_1 P(w_i|w_{i-2}w_{i-1}) + \lambda_2 P(w_i|w_{i-1}) + \lambda_3 P(w_i)\]

<h3 id="第-2-节--感知器perceptron">第 2 节 · 感知器（Perceptron）</h3>

<p>N 元文法模型的显著问题，在「马尔科夫假设与 N 元文法语言模型」小节已经提到了。这些问题基本在神经网络模型中被解决，而要了解神经网络模型，就要从感知器（Perceptron）开始。1957 年感知机模型被提出，1959 年多层感知机（MLP）模型被提出。MLP 有时候也被称为 ANN，即 Artificial Neural Network，接下来我们来深入浅出地了解一下，并有一些动手的练习。</p>

<h4 id="21感知器perceptron解决二元分类任务的前馈神经网络">2.1、感知器（Perceptron）：解决二元分类任务的前馈神经网络</h4>

<p>\(x\) 是一个输入向量，\(\omega\) 是一个权重向量（对输入向量里的而每个值分配一个权重值所组成的向量）。举一个具体任务例子，比如如果这两个向量的内积超过某个值，则判断为 1，否则为 0，这其实就是一个分类任务。那么这个最终输出值可以如下表示：</p>

\[y = \begin{cases} 1 &amp; (\omega \cdot x \geq 0) \\ 0 &amp; (\omega \cdot x \lt 0) \end{cases}\]

<p>这就是一个典型的感知器（Perceptron），一般用来解决分类问题。还可以再增加一个偏差项（bias），如下：</p>

\[y = \begin{cases} 1 &amp; (\omega \cdot x + b \geq 0) \\ 0 &amp; (\omega \cdot x + b \lt 0) \end{cases}\]

<p>感知器其实就是一个前馈神经网络，由输入层、输出层组成，没有隐藏层。而且输出是一个二元函数，用于解决二元分类问题。</p>

<h4 id="22线性回归linear-regression从离散值的感知器解决类问题到连续值的线性回归解决回归问题">2.2、线性回归（Linear Regression）：从离散值的感知器（解决类问题），到连续值的线性回归（解决回归问题）</h4>

<p>一般来说，我们认为感知器的输出结果，是离散值。一般来说，我们认为离散值作为输出解决的问题，是分类问题；相应地，连续值解决的问题是回归（Regression）。比如对于上面的感知器，如果我们直接将 \(\omega \cdot x + b\) 作为输出值，则就变成了一个线性回归问题的模型了。</p>

<p>下面我们用 PyTorch 来实现一个线性回归的代码示例，首先我们要了解在 PyTorch 库里有一个非常常用的函数：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">)</span>
</code></pre></div></div>

<p>这个函数在创建时会自动初始化权值和偏置，并且可以通过调用它的 <code class="language-plaintext highlighter-rouge">forward</code> 函数来计算输入数据的线性变换。具体来说，当输入为 <code class="language-plaintext highlighter-rouge">x</code> 时，<code class="language-plaintext highlighter-rouge">forward</code> 函数会计算 \(y = \omega \cdot x + b\)，其中  \(W\)  和  \(b\)  分别是 <code class="language-plaintext highlighter-rouge">nn.Linear</code> 图层的权值和偏置。</p>

<p>我们来一个完整的代码示例：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>

<span class="c1"># 定义模型
</span><span class="k">class</span> <span class="nc">LinearRegression</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">LinearRegression</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">linear</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># 初始化模型
</span><span class="n">model</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">(</span><span class="n">input_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">output_size</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># 定义损失函数和优化器
</span><span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">MSELoss</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>

<span class="c1"># 创建输入特征 X 和标签 y
</span><span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">]])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">([[</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="mi">8</span><span class="p">]])</span>

<span class="c1"># 训练模型
</span><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
    <span class="c1"># 前向传播
</span>    <span class="n">predictions</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

    <span class="c1"># 反向传播
</span>    <span class="n">optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>

<span class="c1"># 创建测试数据 X_test 和标签 y_test
</span><span class="n">X_test</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">([[</span><span class="mi">5</span><span class="p">],</span> <span class="p">[</span><span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="mi">7</span><span class="p">],</span> <span class="p">[</span><span class="mi">8</span><span class="p">]])</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">([[</span><span class="mi">10</span><span class="p">],</span> <span class="p">[</span><span class="mi">12</span><span class="p">],</span> <span class="p">[</span><span class="mi">14</span><span class="p">],</span> <span class="p">[</span><span class="mi">16</span><span class="p">]])</span>

<span class="c1"># 测试模型
</span><span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">'Test loss: </span><span class="si">{</span><span class="n">loss</span><span class="p">:.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>

</code></pre></div></div>

<p>上述代码，一开始先创建一个 <code class="language-plaintext highlighter-rouge">LinearRegression</code> 线性回归模型的类，其中有一个 <code class="language-plaintext highlighter-rouge">forward</code> 前向传播函数，调用时其实就是计算一下输出值 <code class="language-plaintext highlighter-rouge">y</code>。</p>

<p>主程序，一开始创建一个线性回归模型实例，然后定义一个用于评价模型效果的损失函数评价器，和用随机梯度下降（Stochastic Gradient Descent）作为优化器。</p>

<p>然后创建一个输入特征张量，和标签张量。用这组特征和标签进行训练，训练的过程就是根据 <code class="language-plaintext highlighter-rouge">X</code> 计算与测试 <code class="language-plaintext highlighter-rouge">predictions</code> 向量，再把它和 <code class="language-plaintext highlighter-rouge">y</code> 一起给评价器算出损失 <code class="language-plaintext highlighter-rouge">loss</code>，然后进行反向传播。注意反向传播的三行代码：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
<span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
<span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
</code></pre></div></div>

<p>如此训练 100 次（每一次都会黑盒化地更新模型的参数，一个 <code class="language-plaintext highlighter-rouge">epoch</code> 就是一次训练过程，有时也称为 <code class="language-plaintext highlighter-rouge">iteration</code> 或者 <code class="language-plaintext highlighter-rouge">step</code>，不断根据 <code class="language-plaintext highlighter-rouge">loss</code> 训练优化模型参数。</p>

<p>然后我们创建了一组测试特征值张量 <code class="language-plaintext highlighter-rouge">X_test</code>，和测试标签张量 <code class="language-plaintext highlighter-rouge">y_test</code>，然后用它们测试模型性能，把测试特征得到的 <code class="language-plaintext highlighter-rouge">predictions</code> 与 <code class="language-plaintext highlighter-rouge">y_test</code> 共同传给评价器，得到 <code class="language-plaintext highlighter-rouge">loss</code>。在这个例子中我们会得到如下结果：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Test</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">0.0034</span>
</code></pre></div></div>

<h4 id="23逻辑回归logistic-regression没有值域约束的线性回归到限定在一个范围内的逻辑回归常用于分类问题">2.3、逻辑回归（Logistic Regression）：没有值域约束的线性回归，到限定在一个范围内的逻辑回归（常用于分类问题）</h4>

<p>可以看到线性回归问题，输出值是没有范围限定的。如果限定（limit）在特定的  \((0, L)\)  范围内，则就叫做逻辑回归了。那么如何将一个线性回归变成逻辑回归呢？一般通过如下公式变换：</p>

\[y = \frac{L}{1 + e^{-k(z-z_0)}}\]

<p>这样原来的  \(z \in (-\infty, +\infty)\)  就被变换成了  \(y \in (0, L)\)  了。</p>

<ul>
  <li><strong>激活函数</strong>：这种把输出值限定在一个目标范围内的函数，被叫做 <strong>激活函数（Activation Function）</strong>。</li>
  <li><strong>函数的陡峭程度</strong> 由  \(k\)  控制，越大越陡。</li>
  <li>当  \(z = z_0\)  时， \(y = \frac{L}{2}\) 。</li>
</ul>

<p>下面给出一个基于 Python 的 scikit-learn 库的示例代码：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="c1"># 这是 scikit-learn 库里的一个简单的数据集
</span><span class="n">iris</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">()</span>

<span class="c1"># 把 iris 数据集拆分成训练集和测试集两部分
</span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">iris</span><span class="p">.</span><span class="n">data</span><span class="p">,</span> <span class="n">iris</span><span class="p">.</span><span class="n">target</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># 用 scikit-learn 库创建一个逻辑回归模型的实例
</span><span class="n">lr</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">()</span>

<span class="c1"># 用上边 split 出来的训练集数据，训练 lr 模型实例
</span><span class="n">lr</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># 用训练过的模型，拿测试集的输入数据做测试
</span><span class="n">predictions</span> <span class="o">=</span> <span class="n">lr</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># 用测试集的数据验证精确性
</span><span class="n">accuracy</span> <span class="o">=</span> <span class="n">lr</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">accuracy</span><span class="p">)</span>
</code></pre></div></div>

<h4 id="24sigmoid-回归sigmoid-regression归一化的逻辑回归一般用于二元分类任务">2.4、Sigmoid 回归（Sigmoid Regression）：归一化的逻辑回归，一般用于二元分类任务</h4>

<p>当  \(L = 1, k = 1, z_0 = 0\) ，此时的激活函数就是 <strong>Sigmoid</strong> 函数，也常表示为  \(\sigma\)  函数，如下：</p>

\[y = \frac{1}{1 + e^{-z}}\]

<p>Sigmoid 回归的值域，恰好在 (0, 1) 之间，所以常备作为用来归一化的激活函数。而一个线性回归模型，再用 sigmoid 函数归一化，这种也常被称为「Sigmoid 回归」。Sigmoid 这个单词的意思也就是 S 形，我们可以看下它的函数图像如下：</p>

<p><img src="/img/src/2022-12-19-language-model-2.png" alt="image" width="490" /></p>

<p>因为归一化，所以也可以把输出值理解为一个概率。比如我们面对一个二元分类问题，那么输出结果就对应属于这个类别的概率。</p>

<p>这样一个 sigmoid 模型可以表示为：</p>

\[y = Sigmoid(W \cdot x + b)\]

<p>另外 sigmoid 函数的导数（即梯度）是很好算的： \(y' = y \cdot (1-y)\) 。这非常方便用于「梯度下降算法」根据 loss 对模型参数进行优化。Sigmoid 回归，一般用于二元分类任务。那么对于超过二元的情况怎么办呢？这就引出了下面的 Softmax 回归。</p>

<h4 id="25softmax-回归softmax-regression从解决二元任务的-sigmoid到解决多元分类任务的-softmax">2.5、Softmax 回归（Softmax Regression）：从解决二元任务的 sigmoid，到解决多元分类任务的 Softmax</h4>

<p>相对逻辑回归，Softmax 也称为多项逻辑回归。上面说 Sigmoid 一般用于解决二元分类问题，那么多元问题就要用 Softmax 回归了。我们来拿一个具体问题来解释，比如问题是对于任意输入的一个电商商品的图片，来判断这个图片所代表的的商品，属于哪个商品类目。假设我们一共有 100 个类目。那么一个图片比如说其所有像素值作为输入特征值，输出就是一个 100 维的向量 ** \(z\) **，输出向量中的每个值  \(z_i\)  表示属于相对应类目的概率  \(y_i\)  ：</p>

\[y_i = Softmax(z)_i = \frac{e^{z_i}}{e^{z_1} + e^{z_2} + ... + e^{z_100}}\]

<p>那么最后得到的  \(y\)  向量中的每一项就对应这个输入  \(z\)  属于这 100 个类目的各自概率了。所以如果回归到一般问题，这个 Softmax 回归的模型就如下：</p>

\[y = Softmax(W \cdot x + b)\]

<p>对于上面电商商品图片的例子，假设每个图片的尺寸是 512x512，这个模型展开式如下：</p>

\[\begin{bmatrix} y_1 \\ y_2 \\ ... \\ y_{100} \end{bmatrix} = Softmax(\begin{bmatrix} w_{1,1}, &amp; w_{1,2}, &amp; ... &amp; w_{1, 512} \\ w_{2,1}, &amp; w_{2,2}, &amp; ... &amp; w_{2, 512} \\ ... &amp; ... &amp; ... &amp; ... \\ w_{100,1}, &amp; w_{100,2}, &amp; ... &amp; w_{100, 512} \end{bmatrix} \cdot \begin{bmatrix} x_1 \\ x_2 \\ ... \\ x_{512} \end{bmatrix} + \begin{bmatrix} b_1 \\ b_2 \\ ... \\ b_{512} \end{bmatrix})\]

<p>这个对输入向量  \(x\)  执行  \(w \cdot x + b\)  运算，一般也常称为「线性映射/线性变化」。</p>

<h4 id="26多层感知器multi-layer-perceptron">2.6、多层感知器（Multi-Layer Perceptron）</h4>

<p>上面我们遇到的所有任务，都是用线性模型（Linear Models）解决的。有时候问题复杂起来，我们就要引入非线性模型了。</p>

<p>这里我们要介绍一个新的激活函数 —— ReLU（Rectified Linear Unit）—— 一个非线性激活函数，其定义如下：</p>

\[ReLU(z) = max(0, z)\]

<p>比如对于 MNIST 数据集的手写数字分类问题，就是一个典型的非线性的分类任务，下面给出一个示例代码：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">torchvision</span>
<span class="kn">import</span> <span class="nn">torchvision.transforms</span> <span class="k">as</span> <span class="n">transforms</span>

<span class="c1"># 定义多层感知器模型
</span><span class="k">class</span> <span class="nc">MLP</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MLP</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>

<span class="c1"># 超参数
</span><span class="n">input_size</span> <span class="o">=</span> <span class="mi">784</span>
<span class="n">hidden_size</span> <span class="o">=</span> <span class="mi">500</span>
<span class="n">num_classes</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.001</span>

<span class="c1"># 加载 MNIST 数据集
</span><span class="n">train_dataset</span> <span class="o">=</span> <span class="n">datasets</span><span class="p">.</span><span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s">'../../data'</span><span class="p">,</span>
                               <span class="n">train</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                               <span class="n">transform</span><span class="o">=</span><span class="n">transforms</span><span class="p">.</span><span class="n">ToTensor</span><span class="p">(),</span>
                               <span class="n">download</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">test_dataset</span> <span class="o">=</span> <span class="n">datasets</span><span class="p">.</span><span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s">'../../data'</span><span class="p">,</span>
                              <span class="n">train</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
                              <span class="n">transform</span><span class="o">=</span><span class="n">transforms</span><span class="p">.</span><span class="n">ToTensor</span><span class="p">())</span>

<span class="c1"># 数据加载器
</span><span class="n">train_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="o">=</span><span class="n">train_dataset</span><span class="p">,</span>
                                           <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                                           <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">test_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="o">=</span><span class="n">test_dataset</span><span class="p">,</span>
                                          <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                                          <span class="n">shuffle</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>

<span class="c1"># 定义损失函数和优化器
</span><span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>

<span class="c1"># 训练模型
</span><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
        <span class="c1"># 前向传播
</span>        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>

        <span class="c1"># 反向传播
</span>        <span class="n">optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>

    <span class="c1"># 输出训练损失
</span>    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">'Epoch </span><span class="si">{</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="si">}</span><span class="s">, Training Loss: </span><span class="si">{</span><span class="n">loss</span><span class="p">.</span><span class="n">item</span><span class="p">():.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
</code></pre></div></div>

<p>这段代码里，我们能看到 MLP 的模型定义是：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
<span class="n">nn</span><span class="p">.</span><span class="n">ReLU</span><span class="p">()</span>
<span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>
</code></pre></div></div>

<p>与前面的模型示例代码类似，也都用到了反向传播、损失函数评价器、优化器。如果用公式表示的话，就是如下的模型定义：</p>

\[\begin{aligned}
&amp;z = W_1 \cdot x + b_1 \\
&amp;h = ReLU(z) \\
&amp;y = W_2 \cdot h + b_2
\end{aligned}\]

<p>我们知道 MLP 通常是一个输入和输出长度相同的模型，但少数情况下也可以构建输入和输出长度不同的 MLP 模型，比如输入一组序列后，输出是一个离散的分类结果。</p>

<h4 id="27简述如何训练一个模型正向传播与反向传播">2.7、简述如何训练一个模型：正向传播与反向传播</h4>

<p>这是个很重要的议题。但是春节时间有限，这部分只能简写了，我们更多聚焦在语言模型本身。这里简述一下，后续可能会再补全。</p>

<ul>
  <li>训练神经网络，主要包括前向传播、反向传播这两步。</li>
  <li>正向传播，就是将数据输入给模型，基于已确定的一组参数（比如 MLP 中的权重 W、偏置 b 等），得到输出结果。根据输出结果计算损失函数，衡量当前参数下的模型性能。</li>
  <li>反向传播最常用到的是梯度下降法（这里不讨论其他方法），依托损失函数，将其中的参数当做变量来求偏导（计算梯度），沿着梯度下降的方向求解损失函数的极小值，此时的参数可替代此前的参数。这就是对模型优化训练的一个典型过程。</li>
</ul>

<p>引申问题 —— 梯度消失、梯度爆炸问题：因为对损失函数的求偏导，是从输出层向输入层反向基于「数学上的链式法则」计算的，数学上这是个连乘计算，层数越多越容易出现这个问题。这个求导过程可能会出现梯度为零的情况，即梯度消失。也有可能出现梯度值特别大的情况。</p>

<p>解决梯度消失、梯度爆炸问题，又是一个重要议题，这里篇幅所限也难以展开做技术笔记。粗暴的方式比如梯度剪切，Hinton 提出的逐层预训练后再整体精调理论上也 work，本文后续提到的 LSTM、ResNet 等也可以解决问题，我们也还能了解到业界各种解决手段，有机会再与朋友们交流学习。</p>

<h4 id="28mlp-的一个显著问题帮我们引出-cnn-模型">2.8、MLP 的一个显著问题，帮我们引出 CNN 模型</h4>

<p>我们可以看到，在 MLP 中，不论有多少层，某一层的输出向量  \(h_n\)  中的每个值，都会在下一层计算输出向量  \(h_{n+1}\)  的每个值时用到。具体来说，如果对于某一层的输出值如下：</p>

\[h_{n+1} = Softmax(W_{n+1} \cdot h_n + b_{n+1})\]

<p>上一段话里所谓的「用到」，其实就是要针对  \(h_n\)  生成相应的特征值  \(W_{n+1}\)  权重矩阵中的每个行列里的数值和  \(b_{n+1}\) 偏差向量 里的每个值。如果用图画出来，就是：</p>

<div style="text-align: center;">
<div class="graphviz-wrapper">

<!-- Generated by graphviz version 2.43.0 (0)
 -->
<!-- Title: G Pages: 1 -->
<svg role="img" aria-label="graphviz-1b1299448dc08c90d29bebf8b1f045c1" width="428pt" height="116pt" viewBox="0.00 0.00 427.64 116.00">
<title>graphviz-1b1299448dc08c90d29bebf8b1f045c1</title>
<desc>
digraph G {
	rankdir=TB
	a[label=&quot;...&quot;]
	b[label=&quot;...&quot;]
	h_2_1[label=&quot;h_n+1_1&quot;]
	h_2_2[label=&quot;h_n+1_2&quot;]
	h_2_m[label=&quot;h_n+1_m&quot;]

	{rank=same h_n_1 h_n_2 b h_n_m}
	{rank=same h_2_1 h_2_2 a h_2_m}

	h_n_1 -&gt; h_2_1
	h_n_1 -&gt; h_2_2
	h_n_1 -&gt; a
	h_n_1 -&gt; h_2_m

	h_n_1 -&gt; h_2_1
	h_n_2 -&gt; h_2_2
	h_n_2 -&gt; a
	h_n_2 -&gt; h_2_m

	b -&gt; h_2_1
	b -&gt; h_2_2
	b -&gt; a
	b -&gt; h_2_m

	h_n_m -&gt; h_2_1
	h_n_m -&gt; h_2_2
	h_n_m -&gt; a
	h_n_m -&gt; h_2_m
}
</desc>

<g id="graph0" class="graph" transform="scale(1 1) rotate(0) translate(4 112)">
<title>G</title>
<polygon fill="white" stroke="transparent" points="-4,4 -4,-112 423.64,-112 423.64,4 -4,4" />
<!-- a -->
<g id="node1" class="node">
<title>a</title>
<ellipse fill="none" stroke="black" cx="146.7" cy="-18" rx="27" ry="18" />
<text text-anchor="middle" x="146.7" y="-14.3" font-family="Times,serif" font-size="14.00">...</text>
</g>
<!-- b -->
<g id="node2" class="node">
<title>b</title>
<ellipse fill="none" stroke="black" cx="151.7" cy="-90" rx="27" ry="18" />
<text text-anchor="middle" x="151.7" y="-86.3" font-family="Times,serif" font-size="14.00">...</text>
</g>
<!-- b&#45;&gt;a -->
<g id="edge11" class="edge">
<title>b&#45;&gt;a</title>
<path fill="none" stroke="black" d="M150.46,-71.7C149.91,-63.98 149.25,-54.71 148.63,-46.11" />
<polygon fill="black" stroke="black" points="152.12,-45.83 147.92,-36.1 145.14,-46.33 152.12,-45.83" />
</g>
<!-- h_2_1 -->
<g id="node3" class="node">
<title>h_2_1</title>
<ellipse fill="none" stroke="black" cx="50.7" cy="-18" rx="50.89" ry="18" />
<text text-anchor="middle" x="50.7" y="-14.3" font-family="Times,serif" font-size="14.00">h_n+1_1</text>
</g>
<!-- b&#45;&gt;h_2_1 -->
<g id="edge9" class="edge">
<title>b&#45;&gt;h_2_1</title>
<path fill="none" stroke="black" d="M133.64,-76.49C119.14,-66.44 98.46,-52.11 81.38,-40.27" />
<polygon fill="black" stroke="black" points="83.04,-37.16 72.83,-34.34 79.05,-42.91 83.04,-37.16" />
</g>
<!-- h_2_2 -->
<g id="node4" class="node">
<title>h_2_2</title>
<ellipse fill="none" stroke="black" cx="242.7" cy="-18" rx="50.89" ry="18" />
<text text-anchor="middle" x="242.7" y="-14.3" font-family="Times,serif" font-size="14.00">h_n+1_2</text>
</g>
<!-- b&#45;&gt;h_2_2 -->
<g id="edge10" class="edge">
<title>b&#45;&gt;h_2_2</title>
<path fill="none" stroke="black" d="M168.81,-75.83C181.67,-65.94 199.56,-52.18 214.52,-40.67" />
<polygon fill="black" stroke="black" points="216.69,-43.42 222.48,-34.55 212.42,-37.87 216.69,-43.42" />
</g>
<!-- h_2_m -->
<g id="node5" class="node">
<title>h_2_m</title>
<ellipse fill="none" stroke="black" cx="365.7" cy="-18" rx="53.89" ry="18" />
<text text-anchor="middle" x="365.7" y="-14.3" font-family="Times,serif" font-size="14.00">h_n+1_m</text>
</g>
<!-- b&#45;&gt;h_2_m -->
<g id="edge12" class="edge">
<title>b&#45;&gt;h_2_m</title>
<path fill="none" stroke="black" d="M172.78,-78.39C177.62,-76.14 182.79,-73.88 187.7,-72 211.14,-63.03 271.93,-45.36 315.95,-32.9" />
<polygon fill="black" stroke="black" points="316.96,-36.25 325.63,-30.16 315.05,-29.51 316.96,-36.25" />
</g>
<!-- h_n_1 -->
<g id="node6" class="node">
<title>h_n_1</title>
<ellipse fill="none" stroke="black" cx="69.7" cy="-90" rx="37.09" ry="18" />
<text text-anchor="middle" x="69.7" y="-86.3" font-family="Times,serif" font-size="14.00">h_n_1</text>
</g>
<!-- h_n_1&#45;&gt;a -->
<g id="edge3" class="edge">
<title>h_n_1&#45;&gt;a</title>
<path fill="none" stroke="black" d="M86.4,-73.81C97.36,-63.85 111.83,-50.7 123.85,-39.77" />
<polygon fill="black" stroke="black" points="126.28,-42.29 131.33,-32.97 121.57,-37.11 126.28,-42.29" />
</g>
<!-- h_n_1&#45;&gt;h_2_1 -->
<g id="edge1" class="edge">
<title>h_n_1&#45;&gt;h_2_1</title>
<path fill="none" stroke="black" d="M59.35,-72.41C56.39,-64.62 53.56,-55.14 51.51,-46.33" />
<polygon fill="black" stroke="black" points="54.92,-45.55 49.5,-36.45 48.06,-46.94 54.92,-45.55" />
</g>
<!-- h_n_1&#45;&gt;h_2_1 -->
<g id="edge5" class="edge">
<title>h_n_1&#45;&gt;h_2_1</title>
<path fill="none" stroke="black" d="M70.91,-71.7C69.57,-63.7 67.15,-54.02 64.35,-45.15" />
<polygon fill="black" stroke="black" points="67.63,-43.93 61.05,-35.62 61.01,-46.22 67.63,-43.93" />
</g>
<!-- h_n_1&#45;&gt;h_2_2 -->
<g id="edge2" class="edge">
<title>h_n_1&#45;&gt;h_2_2</title>
<path fill="none" stroke="black" d="M97.49,-77.75C125.45,-66.44 168.9,-48.86 200.99,-35.87" />
<polygon fill="black" stroke="black" points="202.6,-39 210.56,-32 199.97,-32.51 202.6,-39" />
</g>
<!-- h_n_1&#45;&gt;h_2_m -->
<g id="edge4" class="edge">
<title>h_n_1&#45;&gt;h_2_m</title>
<path fill="none" stroke="black" d="M97.68,-77.83C103.57,-75.71 109.79,-73.65 115.7,-72 197.22,-49.25 220.23,-55.04 302.7,-36 307.03,-35 311.53,-33.9 316.02,-32.77" />
<polygon fill="black" stroke="black" points="317.01,-36.13 325.81,-30.24 315.26,-29.35 317.01,-36.13" />
</g>
<!-- h_n_2 -->
<g id="node7" class="node">
<title>h_n_2</title>
<ellipse fill="none" stroke="black" cx="331.7" cy="-90" rx="37.09" ry="18" />
<text text-anchor="middle" x="331.7" y="-86.3" font-family="Times,serif" font-size="14.00">h_n_2</text>
</g>
<!-- h_n_2&#45;&gt;a -->
<g id="edge7" class="edge">
<title>h_n_2&#45;&gt;a</title>
<path fill="none" stroke="black" d="M303.01,-78.31C297.28,-76.2 291.3,-74.02 285.7,-72 240.06,-55.59 227.57,-54.38 182.7,-36 180.87,-35.25 179.01,-34.46 177.14,-33.65" />
<polygon fill="black" stroke="black" points="178.4,-30.38 167.85,-29.44 175.52,-36.75 178.4,-30.38" />
</g>
<!-- h_n_2&#45;&gt;h_2_2 -->
<g id="edge6" class="edge">
<title>h_n_2&#45;&gt;h_2_2</title>
<path fill="none" stroke="black" d="M312.82,-74.15C300.65,-64.58 284.6,-51.96 270.93,-41.21" />
<polygon fill="black" stroke="black" points="272.8,-38.23 262.78,-34.8 268.48,-43.73 272.8,-38.23" />
</g>
<!-- h_n_2&#45;&gt;h_2_m -->
<g id="edge8" class="edge">
<title>h_n_2&#45;&gt;h_2_m</title>
<path fill="none" stroke="black" d="M339.75,-72.41C343.72,-64.25 348.59,-54.22 353.04,-45.07" />
<polygon fill="black" stroke="black" points="356.24,-46.48 357.46,-35.96 349.94,-43.42 356.24,-46.48" />
</g>
<!-- h_n_m -->
<g id="node8" class="node">
<title>h_n_m</title>
<ellipse fill="none" stroke="black" cx="236.7" cy="-90" rx="40.09" ry="18" />
<text text-anchor="middle" x="236.7" y="-86.3" font-family="Times,serif" font-size="14.00">h_n_m</text>
</g>
<!-- h_n_m&#45;&gt;a -->
<g id="edge15" class="edge">
<title>h_n_m&#45;&gt;a</title>
<path fill="none" stroke="black" d="M217.17,-73.81C203.86,-63.46 186.11,-49.66 171.76,-38.49" />
<polygon fill="black" stroke="black" points="173.8,-35.65 163.76,-32.27 169.5,-41.17 173.8,-35.65" />
</g>
<!-- h_n_m&#45;&gt;h_2_1 -->
<g id="edge13" class="edge">
<title>h_n_m&#45;&gt;h_2_1</title>
<path fill="none" stroke="black" d="M206.81,-77.75C176.21,-66.24 128.35,-48.22 93.68,-35.18" />
<polygon fill="black" stroke="black" points="94.87,-31.89 84.28,-31.64 92.41,-38.44 94.87,-31.89" />
</g>
<!-- h_n_m&#45;&gt;h_2_2 -->
<g id="edge14" class="edge">
<title>h_n_m&#45;&gt;h_2_2</title>
<path fill="none" stroke="black" d="M238.18,-71.7C238.84,-63.98 239.63,-54.71 240.37,-46.11" />
<polygon fill="black" stroke="black" points="243.86,-46.37 241.23,-36.1 236.89,-45.77 243.86,-46.37" />
</g>
<!-- h_n_m&#45;&gt;h_2_m -->
<g id="edge16" class="edge">
<title>h_n_m&#45;&gt;h_2_m</title>
<path fill="none" stroke="black" d="M261.26,-75.67C280.58,-65.19 307.78,-50.43 329.57,-38.6" />
<polygon fill="black" stroke="black" points="331.42,-41.58 338.54,-33.73 328.08,-35.43 331.42,-41.58" />
</g>
</g>
</svg>
</div>
</div>

<p>可以看到，输入的所有元素都被连接，即被分配权重 w 和偏差项 b，所以这被称为一个「全连接层（<strong>Fully Connected Layer</strong>）」或者「<strong>稠密层（Dense Layer）</strong>」。但是对于一些任务这样做是很蠢的，会付出大量无效的计算。</p>

<p>因此我们需要 focus 在更少量计算成本的模型，于是有了卷积神经网络（CNN）。</p>

<h3 id="第-3-节--卷积神经网络cnn">第 3 节 · 卷积神经网络（CNN）</h3>

<p>MLP 里每一层的每个元素，都要乘以一个独立参数的权重 W，再加上一个偏执 b，这样的神经网络层常被我们叫做「全连接层（Fully Connected Layer）或稠密层（Dence Layer）。但是这样有个显著问题：如果输入内容的局部重要信息只是发生轻微移动并没有丢失，在全连接层处理后，整个输出结果都会发生很大变化 —— 这不合理。</p>

<p>于是我们会想到，如果我们用一个小一些的全连接层，只对重要的局部输入进行处理呢？其实这个思路和 n-gram 是类似的，都是用一个窗口来扫描局部。卷积神经网络（Convolutional Neural Network，CNN）就是基于此诞生的。</p>

<ul>
  <li>卷积核：卷积核是一个小的稠密层，用于提取局部特征，又称其为卷积核（kernel）/ 滤波器（filter）/ 感受野（receptive field / field of view）。</li>
  <li>池化层（Pooling，或称汇聚层）：经过卷积核处理的结果，进一步聚合的过程。对于输入大小不一样的样本，池化后将有相同个数的特征输出。</li>
  <li>提取多个局部特征：一个卷积核只能提取单一类型的局部特征，需要提取多种局部特征则需要多个卷积核。有些文章里你看提到「多个模式」、「多个通道」，其实指的就是多个 kernel 识别多个特征。</li>
  <li>全连接分类层：多个卷积核得到的多个特征，需经过一个全连接的分类层用于最终决策。</li>
</ul>

<p>这样做有几个特性：</p>

<ul>
  <li>本地性（Locality）：输出结果只由一个特定窗口大小区域内的数据决定。</li>
  <li>平移不变性（Translation Invariant）：对同一个特征，扫描不同区域时只用一个 kernel 来计算。</li>
  <li>卷积层的参数规模，与输入输出数据大小无关。</li>
</ul>

<p>CNN 主要的适用领域是计算机视觉。而在 NLP 中，文本数据的维度很高，并且语言的结构比图像更复杂。因此，CNN 一般不适用于处理 NLP 问题。</p>

<h3 id="第-4-节--循环神经网络rnn">第 4 节 · 循环神经网络（RNN）</h3>

<p>RNN（循环神经网络），这是一种强大的神经网络模型，能够预测序列数据，例如文本、语音和时间序列。我们将通过生动的代码示例和实际案例来演示如何使用 RNN，并在日常生活中真实地体验它的功能。您将学习到如何使用 RNN 解决各种机器学习问题，并动手尝试运用 RNN 解决实际问题。这篇文章将为您提供一个完整的 RNN 入门指南，并使您对 RNN 有更深入的了解。</p>

<p>RNN（Recurrent Neural Network）的 R 是 Recurrent 的意思，所以这是一个贷循环的神经网络。首先要明白一点，你并不需要搞懂 CNN 后才能学习 RNN 模型。你只要了解了 MLP 就可以学习 RNN 了。</p>

<h4 id="41经典结构的-rnn">4.1、经典结构的 RNN</h4>

<p><img src="/img/src/2022-12-19-language-model-1.png" alt="image" /></p>

<p>上图这是一个经典结构的 RNN 示意图，Unfold 箭头右侧是展开示意。输入序列（这里用 x 表示）传递给隐藏层（hidden layer，这里用 h 表示），处理完生成输出序列（这里用 o 表示）。序列的下一个词输入时的、上一步隐藏层会一起影响这一步的输出。U、V、W 都表示权重。在这个经典结构理，你可以看到非常重要的一点，就是输入序列长度与输出序列长度是相同的。</p>

<p>这种经典结构的应用场景，比如对一段普通话输入它的四川话版本，比如对视频的每一帧进行处理并输出，等等。</p>

<p>我们知道 RNN 是一个一个序列处理的，每个序列中的数据项都是有序的，所以对于计算一个序列内的所有数据项是无法并行的。但是计算不同序列时，不同序列各自的计算则是可以并行的。如果我们把上一个时刻 t 隐藏层输出的结果（ \(h_{t-1}\) ）传给一个激活函数（比如说用正切函数 <code class="language-plaintext highlighter-rouge">tanh</code> 函数），然后和当下时刻 t 的这个输入（ \(x_{t}\) ）一起，处理后产生一个时刻 t 的输出（ \(h_t\) ）。然后把隐藏层的输出通过多项逻辑回归（Softmax）生成最终的输出值（ \(\bm{y}\) ），我们可以如下表示这个模型：</p>

\[\begin{aligned}
&amp;\bm{h}_t = tanh(\bm{W}^{xh} \cdot \bm{x}_t + \bm{b}^{xh} + \bm{W}^{hh} \cdot \bm{h}_{t-1} + \bm{b}^{hh}) \\
&amp;\bm{y}_t = Softmax(\bm{W}^{hy} \cdot \bm{h_t} + \bm{b}^{hy})
\end{aligned}\]

<p>对应的示意图如下：</p>

<div style="text-align: center;">
<div class="graphviz-wrapper">

<!-- Generated by graphviz version 2.43.0 (0)
 -->
<!-- Title: G Pages: 1 -->
<svg role="img" aria-label="graphviz-34cd77ba92d6e898bab41a54b23f2324" width="278pt" height="188pt" viewBox="0.00 0.00 278.00 188.00">
<title>graphviz-34cd77ba92d6e898bab41a54b23f2324</title>
<desc>
digraph G {
	rankdir=BT
	{rank=same h1 h2 hddd hn}
	{rank=same x1 x2 xddd xn}
	{rank=same y1 y2 yddd yn}
	xddd[label=&quot;...&quot;]
	yddd[label=&quot;...&quot;]
	hddd[label=&quot;...&quot;]

	y1[shape=plaintext]
	y2[shape=plaintext]
	yddd[shape=plaintext]
	yn[shape=plaintext]
	x1[shape=plaintext]
	x2[shape=plaintext]
	xddd[shape=plaintext]
	xn[shape=plaintext]

	h1 -&gt; h2
	h2 -&gt; hddd
	hddd -&gt; hn

	x1 -&gt; h1
	x2 -&gt; h2
	xddd -&gt; hddd
	xn -&gt; hn

	h1 -&gt; y1
	h2 -&gt; y2
	hddd -&gt; yddd
	hn -&gt; yn
}
</desc>

<g id="graph0" class="graph" transform="scale(1 1) rotate(0) translate(4 184)">
<title>G</title>
<polygon fill="white" stroke="transparent" points="-4,4 -4,-184 274,-184 274,4 -4,4" />
<!-- h1 -->
<g id="node1" class="node">
<title>h1</title>
<ellipse fill="none" stroke="black" cx="27" cy="-90" rx="27" ry="18" />
<text text-anchor="middle" x="27" y="-86.3" font-family="Times,serif" font-size="14.00">h1</text>
</g>
<!-- h2 -->
<g id="node2" class="node">
<title>h2</title>
<ellipse fill="none" stroke="black" cx="99" cy="-90" rx="27" ry="18" />
<text text-anchor="middle" x="99" y="-86.3" font-family="Times,serif" font-size="14.00">h2</text>
</g>
<!-- h1&#45;&gt;h2 -->
<g id="edge1" class="edge">
<title>h1&#45;&gt;h2</title>
<path fill="none" stroke="black" d="M54,-90C56.61,-90 59.23,-90 61.84,-90" />
<polygon fill="black" stroke="black" points="61.93,-93.5 71.93,-90 61.93,-86.5 61.93,-93.5" />
</g>
<!-- y1 -->
<g id="node9" class="node">
<title>y1</title>
<text text-anchor="middle" x="27" y="-158.3" font-family="Times,serif" font-size="14.00">y1</text>
</g>
<!-- h1&#45;&gt;y1 -->
<g id="edge8" class="edge">
<title>h1&#45;&gt;y1</title>
<path fill="none" stroke="black" d="M27,-108.3C27,-116.02 27,-125.29 27,-133.89" />
<polygon fill="black" stroke="black" points="23.5,-133.9 27,-143.9 30.5,-133.9 23.5,-133.9" />
</g>
<!-- hddd -->
<g id="node3" class="node">
<title>hddd</title>
<ellipse fill="none" stroke="black" cx="171" cy="-90" rx="27" ry="18" />
<text text-anchor="middle" x="171" y="-86.3" font-family="Times,serif" font-size="14.00">...</text>
</g>
<!-- h2&#45;&gt;hddd -->
<g id="edge2" class="edge">
<title>h2&#45;&gt;hddd</title>
<path fill="none" stroke="black" d="M126,-90C128.61,-90 131.23,-90 133.84,-90" />
<polygon fill="black" stroke="black" points="133.93,-93.5 143.93,-90 133.93,-86.5 133.93,-93.5" />
</g>
<!-- y2 -->
<g id="node10" class="node">
<title>y2</title>
<text text-anchor="middle" x="99" y="-158.3" font-family="Times,serif" font-size="14.00">y2</text>
</g>
<!-- h2&#45;&gt;y2 -->
<g id="edge9" class="edge">
<title>h2&#45;&gt;y2</title>
<path fill="none" stroke="black" d="M99,-108.3C99,-116.02 99,-125.29 99,-133.89" />
<polygon fill="black" stroke="black" points="95.5,-133.9 99,-143.9 102.5,-133.9 95.5,-133.9" />
</g>
<!-- hn -->
<g id="node4" class="node">
<title>hn</title>
<ellipse fill="none" stroke="black" cx="243" cy="-90" rx="27" ry="18" />
<text text-anchor="middle" x="243" y="-86.3" font-family="Times,serif" font-size="14.00">hn</text>
</g>
<!-- hddd&#45;&gt;hn -->
<g id="edge3" class="edge">
<title>hddd&#45;&gt;hn</title>
<path fill="none" stroke="black" d="M198,-90C200.61,-90 203.23,-90 205.84,-90" />
<polygon fill="black" stroke="black" points="205.93,-93.5 215.93,-90 205.93,-86.5 205.93,-93.5" />
</g>
<!-- yddd -->
<g id="node11" class="node">
<title>yddd</title>
<text text-anchor="middle" x="171" y="-158.3" font-family="Times,serif" font-size="14.00">...</text>
</g>
<!-- hddd&#45;&gt;yddd -->
<g id="edge10" class="edge">
<title>hddd&#45;&gt;yddd</title>
<path fill="none" stroke="black" d="M171,-108.3C171,-116.02 171,-125.29 171,-133.89" />
<polygon fill="black" stroke="black" points="167.5,-133.9 171,-143.9 174.5,-133.9 167.5,-133.9" />
</g>
<!-- yn -->
<g id="node12" class="node">
<title>yn</title>
<text text-anchor="middle" x="243" y="-158.3" font-family="Times,serif" font-size="14.00">yn</text>
</g>
<!-- hn&#45;&gt;yn -->
<g id="edge11" class="edge">
<title>hn&#45;&gt;yn</title>
<path fill="none" stroke="black" d="M243,-108.3C243,-116.02 243,-125.29 243,-133.89" />
<polygon fill="black" stroke="black" points="239.5,-133.9 243,-143.9 246.5,-133.9 239.5,-133.9" />
</g>
<!-- x1 -->
<g id="node5" class="node">
<title>x1</title>
<text text-anchor="middle" x="27" y="-14.3" font-family="Times,serif" font-size="14.00">x1</text>
</g>
<!-- x1&#45;&gt;h1 -->
<g id="edge4" class="edge">
<title>x1&#45;&gt;h1</title>
<path fill="none" stroke="black" d="M27,-36.3C27,-44.02 27,-53.29 27,-61.89" />
<polygon fill="black" stroke="black" points="23.5,-61.9 27,-71.9 30.5,-61.9 23.5,-61.9" />
</g>
<!-- x2 -->
<g id="node6" class="node">
<title>x2</title>
<text text-anchor="middle" x="99" y="-14.3" font-family="Times,serif" font-size="14.00">x2</text>
</g>
<!-- x2&#45;&gt;h2 -->
<g id="edge5" class="edge">
<title>x2&#45;&gt;h2</title>
<path fill="none" stroke="black" d="M99,-36.3C99,-44.02 99,-53.29 99,-61.89" />
<polygon fill="black" stroke="black" points="95.5,-61.9 99,-71.9 102.5,-61.9 95.5,-61.9" />
</g>
<!-- xddd -->
<g id="node7" class="node">
<title>xddd</title>
<text text-anchor="middle" x="171" y="-14.3" font-family="Times,serif" font-size="14.00">...</text>
</g>
<!-- xddd&#45;&gt;hddd -->
<g id="edge6" class="edge">
<title>xddd&#45;&gt;hddd</title>
<path fill="none" stroke="black" d="M171,-36.3C171,-44.02 171,-53.29 171,-61.89" />
<polygon fill="black" stroke="black" points="167.5,-61.9 171,-71.9 174.5,-61.9 167.5,-61.9" />
</g>
<!-- xn -->
<g id="node8" class="node">
<title>xn</title>
<text text-anchor="middle" x="243" y="-14.3" font-family="Times,serif" font-size="14.00">xn</text>
</g>
<!-- xn&#45;&gt;hn -->
<g id="edge7" class="edge">
<title>xn&#45;&gt;hn</title>
<path fill="none" stroke="black" d="M243,-36.3C243,-44.02 243,-53.29 243,-61.89" />
<polygon fill="black" stroke="black" points="239.5,-61.9 243,-71.9 246.5,-61.9 239.5,-61.9" />
</g>
</g>
</svg>
</div>
</div>

<p>这种输入和输出数据项数一致的 RNN，一般叫做 N vs. N 的 RNN。如果我们用 PyTorch 来实现一个非常简单的经典 RNN 则如下：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>

<span class="c1"># 创建一个 RNN 实例
# 第一个参数
</span><span class="n">rnn</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">RNN</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>  <span class="c1"># 实例化一个单向单层RNN
</span>
<span class="c1"># 输入是一个形状为 (5, 3, 10) 的张量
# 5 个输入数据项（也可以说是样本）
# 3 个数据项是一个序列，有 3 个 steps
# 每个 step 有 10 个特征
</span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

<span class="c1"># 隐藏层是一个 (1, 5, 20) 的张量
</span><span class="n">h0</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>

<span class="c1"># 调用 rnn 函数后，返回输出、最终的隐藏状态
</span><span class="n">output</span><span class="p">,</span> <span class="n">hn</span> <span class="o">=</span> <span class="n">rnn</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">h0</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">hn</span><span class="p">)</span>
</code></pre></div></div>

<p>我们来解读一下这段代码：</p>

<ul>
  <li>这段代码实例化了一个带有 1 个隐藏层的 RNN 网络。</li>
  <li>它的输入是一个形状为 (5, 3, 10) 的张量，表示有 5 个样本，每个样本有 3 个时间步，每个时间步的特征维度是 10。</li>
  <li>初始隐藏状态是一个形状为 (1, 5, 20) 的张量。</li>
  <li>调用 rnn 函数后，会返回输出和最终的隐藏状态。</li>
  <li>输出的形状是 (5, 3, 20)，表示有 5 个样本，每个样本有 3 个时间步，每个时间步的输出维度是 20。</li>
  <li>最终的隐藏状态的形状是 (1, 5, 20)，表示最后的隐藏状态是 5</li>
</ul>

<p>但是上面的代码示例，并没有自己编写一个具体的 RNN，而是用了默认的 PyTorch 的 RNN，那么下面我们就自己编写一个：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">MikeCaptainRNN</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>

        <span class="c1"># 对于 RNN，输入维度就是序列数
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">input_size</span> <span class="o">=</span> <span class="n">input_size</span>

        <span class="c1"># 隐藏层有多少个节点/神经元，经常将 hidden_size 设置为与序列长度相同
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>

        <span class="c1"># 输入层到隐藏层的 W^{xh} 权重、bias^{xh} 偏置项
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">weight_xh</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">input_size</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.01</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">bias_xh</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">hidden_size</span><span class="p">)</span>

        <span class="c1"># 隐藏层到隐藏层的 W^{hh} 权重、bias^{hh} 偏置项
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">weight_hh</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">hidden_size</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.01</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">bias_hh</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">hidden_size</span><span class="p">)</span>

    <span class="c1"># 前向传播
</span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">h0</span><span class="p">):</span>

    	<span class="c1"># 取出这个张量的形状
</span>        <span class="n">N</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">input_size</span> <span class="o">=</span> <span class="nb">input</span><span class="p">.</span><span class="n">shape</span>

        <span class="c1"># 初始化一个全零张量
</span>        <span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">hidden_size</span><span class="p">)</span>

        <span class="c1"># 处理每个时刻的输入特征
</span>        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">L</span><span class="p">):</span>

        	<span class="c1"># 获得当前时刻的输入特征，[N, input_size, 1]。unsqueeze(n)，在第 n 维上增加一维
</span>            <span class="n">x</span> <span class="o">=</span> <span class="nb">input</span><span class="p">[:,</span> <span class="n">t</span><span class="p">,</span> <span class="p">:].</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>  
            <span class="n">w_xh_batch</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">weight_xh</span><span class="p">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="n">tile</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># [N, hidden_size, input_size]
</span>            <span class="n">w_hh_batch</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">weight_hh</span><span class="p">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="n">tile</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># [N, hidden_size, hidden_size]
</span>
            <span class="c1"># bmm 是矩阵乘法函数
</span>            <span class="n">w_times_x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">w_xh_batch</span><span class="p">,</span> <span class="n">x</span><span class="p">).</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># [N, hidden_size]。squeeze(n)，在第n维上减小一维
</span>            <span class="n">w_times_h</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">w_hh_batch</span><span class="p">,</span> <span class="n">h0</span><span class="p">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)).</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># [N, hidden_size]
</span>            <span class="n">h0</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">w_times_x</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">bias_ih</span> <span class="o">+</span> <span class="n">w_times_h</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">bias_hh</span><span class="p">)</span>
            <span class="n">output</span><span class="p">[:,</span> <span class="n">t</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">h0</span>
        <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">h0</span><span class="p">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div></div>

<p>上述代码中 <code class="language-plaintext highlighter-rouge">weight_xh</code>、<code class="language-plaintext highlighter-rouge">bias_xh</code>。<code class="language-plaintext highlighter-rouge">weighthh</code></p>

<h4 id="42n-vs1-的-rnn">4.2、N vs.1 的 RNN</h4>

<p>上面那个图里，如果只保留最后一个输出，那就是一个 N vs. 1 的 RNN 了。这种的应用场景，比如说判断一个文本序列是英语还是德语，比如根据一个输入序列来判断是一个正向情绪内容还是负向或者中性，或者比如根据一段语音输入序列来判断是哪一首曲子（听歌识曲）。</p>

\[\begin{aligned}
&amp;\bm{h}_t = tanh(\bm{W^{xh}} \cdot \bm{x}_t + \bm{b^{xh}} + \bm{W^{hh}} \cdot \bm{h}_{t-1} + \bm{b^{hh}}) \\
&amp;\bm{y} = Softmax(\bm{W^{hy}} \cdot \bm{h}_n + \bm{b^{hy}})
\end{aligned}\]

<p>即这个模型里，每个序列只有隐藏层对最后一个数据项进行处理时才产生输出  \(h_n\)  如果用示意图表示，则是如下结构：</p>

<div style="text-align: center;">
<div class="graphviz-wrapper">

<!-- Generated by graphviz version 2.43.0 (0)
 -->
<!-- Title: G Pages: 1 -->
<svg role="img" aria-label="graphviz-99506286249ff03a109fde8e4294e12c" width="278pt" height="188pt" viewBox="0.00 0.00 278.00 188.00">
<title>graphviz-99506286249ff03a109fde8e4294e12c</title>
<desc>
digraph G {
	rankdir=BT
	{rank=same h1 h2 hddd hn}
	hddd[label=&quot;...&quot;]
	xddd[label=&quot;...&quot;]

	y[shape=plaintext]
	x1[shape=plaintext]
	x2[shape=plaintext]
	xddd[shape=plaintext]
	xn[shape=plaintext]

	h1 -&gt; h2
	h2 -&gt; hddd
	hddd -&gt; hn

	x1 -&gt; h1
	x2 -&gt; h2
	xn -&gt; hn
	xddd -&gt; hddd

	hn -&gt; y
}
</desc>

<g id="graph0" class="graph" transform="scale(1 1) rotate(0) translate(4 184)">
<title>G</title>
<polygon fill="white" stroke="transparent" points="-4,4 -4,-184 274,-184 274,4 -4,4" />
<!-- h1 -->
<g id="node1" class="node">
<title>h1</title>
<ellipse fill="none" stroke="black" cx="27" cy="-90" rx="27" ry="18" />
<text text-anchor="middle" x="27" y="-86.3" font-family="Times,serif" font-size="14.00">h1</text>
</g>
<!-- h2 -->
<g id="node2" class="node">
<title>h2</title>
<ellipse fill="none" stroke="black" cx="99" cy="-90" rx="27" ry="18" />
<text text-anchor="middle" x="99" y="-86.3" font-family="Times,serif" font-size="14.00">h2</text>
</g>
<!-- h1&#45;&gt;h2 -->
<g id="edge1" class="edge">
<title>h1&#45;&gt;h2</title>
<path fill="none" stroke="black" d="M54,-90C56.61,-90 59.23,-90 61.84,-90" />
<polygon fill="black" stroke="black" points="61.93,-93.5 71.93,-90 61.93,-86.5 61.93,-93.5" />
</g>
<!-- hddd -->
<g id="node3" class="node">
<title>hddd</title>
<ellipse fill="none" stroke="black" cx="171" cy="-90" rx="27" ry="18" />
<text text-anchor="middle" x="171" y="-86.3" font-family="Times,serif" font-size="14.00">...</text>
</g>
<!-- h2&#45;&gt;hddd -->
<g id="edge2" class="edge">
<title>h2&#45;&gt;hddd</title>
<path fill="none" stroke="black" d="M126,-90C128.61,-90 131.23,-90 133.84,-90" />
<polygon fill="black" stroke="black" points="133.93,-93.5 143.93,-90 133.93,-86.5 133.93,-93.5" />
</g>
<!-- hn -->
<g id="node4" class="node">
<title>hn</title>
<ellipse fill="none" stroke="black" cx="243" cy="-90" rx="27" ry="18" />
<text text-anchor="middle" x="243" y="-86.3" font-family="Times,serif" font-size="14.00">hn</text>
</g>
<!-- hddd&#45;&gt;hn -->
<g id="edge3" class="edge">
<title>hddd&#45;&gt;hn</title>
<path fill="none" stroke="black" d="M198,-90C200.61,-90 203.23,-90 205.84,-90" />
<polygon fill="black" stroke="black" points="205.93,-93.5 215.93,-90 205.93,-86.5 205.93,-93.5" />
</g>
<!-- y -->
<g id="node6" class="node">
<title>y</title>
<text text-anchor="middle" x="243" y="-158.3" font-family="Times,serif" font-size="14.00">y</text>
</g>
<!-- hn&#45;&gt;y -->
<g id="edge8" class="edge">
<title>hn&#45;&gt;y</title>
<path fill="none" stroke="black" d="M243,-108.3C243,-116.02 243,-125.29 243,-133.89" />
<polygon fill="black" stroke="black" points="239.5,-133.9 243,-143.9 246.5,-133.9 239.5,-133.9" />
</g>
<!-- xddd -->
<g id="node5" class="node">
<title>xddd</title>
<text text-anchor="middle" x="171" y="-14.3" font-family="Times,serif" font-size="14.00">...</text>
</g>
<!-- xddd&#45;&gt;hddd -->
<g id="edge7" class="edge">
<title>xddd&#45;&gt;hddd</title>
<path fill="none" stroke="black" d="M171,-36.3C171,-44.02 171,-53.29 171,-61.89" />
<polygon fill="black" stroke="black" points="167.5,-61.9 171,-71.9 174.5,-61.9 167.5,-61.9" />
</g>
<!-- x1 -->
<g id="node7" class="node">
<title>x1</title>
<text text-anchor="middle" x="27" y="-14.3" font-family="Times,serif" font-size="14.00">x1</text>
</g>
<!-- x1&#45;&gt;h1 -->
<g id="edge4" class="edge">
<title>x1&#45;&gt;h1</title>
<path fill="none" stroke="black" d="M27,-36.3C27,-44.02 27,-53.29 27,-61.89" />
<polygon fill="black" stroke="black" points="23.5,-61.9 27,-71.9 30.5,-61.9 23.5,-61.9" />
</g>
<!-- x2 -->
<g id="node8" class="node">
<title>x2</title>
<text text-anchor="middle" x="99" y="-14.3" font-family="Times,serif" font-size="14.00">x2</text>
</g>
<!-- x2&#45;&gt;h2 -->
<g id="edge5" class="edge">
<title>x2&#45;&gt;h2</title>
<path fill="none" stroke="black" d="M99,-36.3C99,-44.02 99,-53.29 99,-61.89" />
<polygon fill="black" stroke="black" points="95.5,-61.9 99,-71.9 102.5,-61.9 95.5,-61.9" />
</g>
<!-- xn -->
<g id="node9" class="node">
<title>xn</title>
<text text-anchor="middle" x="243" y="-14.3" font-family="Times,serif" font-size="14.00">xn</text>
</g>
<!-- xn&#45;&gt;hn -->
<g id="edge6" class="edge">
<title>xn&#45;&gt;hn</title>
<path fill="none" stroke="black" d="M243,-36.3C243,-44.02 243,-53.29 243,-61.89" />
<polygon fill="black" stroke="black" points="239.5,-61.9 243,-71.9 246.5,-61.9 239.5,-61.9" />
</g>
</g>
</svg>
</div>
</div>

<h4 id="431-vs-n-的-rnn">4.3、1 vs. N 的 RNN</h4>

<p>反过来，上面那个图里，如果只保留一个 x，那么就是一个 1 vs. N 的 RNN 了。这种场景的应用，比如 AI 创作音乐，还有通过一个 image 提炼或识别某些文本内容输出。</p>

\[\begin{aligned}
&amp;\bm{h}_t = \begin{cases} tanh(\bm{W^{xh}} \cdot \bm{x} + \bm{b^{xh}} + 0 + \bm{b^{hh}}) &amp; (t=1) \\
tanh(0 + \bm{b^{xh}} + \bm{W^{hh}} \cdot \bm{h}_{t-1} + \bm{b^{hh}}) &amp; (t&gt;1) \end{cases} \\
&amp;\bm{y}_t = Softmax(\bm{W^{hy}} \cdot \bm{h}_t + \bm{b^{hy}})
\end{aligned}\]

<p>示意图如下：</p>

<div style="text-align: center;">
<div class="graphviz-wrapper">

<!-- Generated by graphviz version 2.43.0 (0)
 -->
<!-- Title: G Pages: 1 -->
<svg role="img" aria-label="graphviz-aecb5ea5cd91fc1106b18c3c4059fa0a" width="278pt" height="188pt" viewBox="0.00 0.00 278.00 188.00">
<title>graphviz-aecb5ea5cd91fc1106b18c3c4059fa0a</title>
<desc>
digraph G {
	rankdir=BT
	{rank=same h1 h2 hddd hn}
	{rank=same y1 y2 yddd yn}
	hddd[label=&quot;...&quot;]
	yddd[label=&quot;...&quot;]

	y1[shape=plaintext]
	y2[shape=plaintext]
	yddd[shape=plaintext]
	yn[shape=plaintext]
	x[shape=plaintext]

	h1 -&gt; h2
	h2 -&gt; hddd
	hddd -&gt; hn

	x -&gt; h1

	h1 -&gt; y1
	h2 -&gt; y2
	hddd -&gt; yddd
	hn -&gt; yn
}
</desc>

<g id="graph0" class="graph" transform="scale(1 1) rotate(0) translate(4 184)">
<title>G</title>
<polygon fill="white" stroke="transparent" points="-4,4 -4,-184 274,-184 274,4 -4,4" />
<!-- h1 -->
<g id="node1" class="node">
<title>h1</title>
<ellipse fill="none" stroke="black" cx="27" cy="-90" rx="27" ry="18" />
<text text-anchor="middle" x="27" y="-86.3" font-family="Times,serif" font-size="14.00">h1</text>
</g>
<!-- h2 -->
<g id="node2" class="node">
<title>h2</title>
<ellipse fill="none" stroke="black" cx="99" cy="-90" rx="27" ry="18" />
<text text-anchor="middle" x="99" y="-86.3" font-family="Times,serif" font-size="14.00">h2</text>
</g>
<!-- h1&#45;&gt;h2 -->
<g id="edge1" class="edge">
<title>h1&#45;&gt;h2</title>
<path fill="none" stroke="black" d="M54,-90C56.61,-90 59.23,-90 61.84,-90" />
<polygon fill="black" stroke="black" points="61.93,-93.5 71.93,-90 61.93,-86.5 61.93,-93.5" />
</g>
<!-- y1 -->
<g id="node5" class="node">
<title>y1</title>
<text text-anchor="middle" x="27" y="-158.3" font-family="Times,serif" font-size="14.00">y1</text>
</g>
<!-- h1&#45;&gt;y1 -->
<g id="edge5" class="edge">
<title>h1&#45;&gt;y1</title>
<path fill="none" stroke="black" d="M27,-108.3C27,-116.02 27,-125.29 27,-133.89" />
<polygon fill="black" stroke="black" points="23.5,-133.9 27,-143.9 30.5,-133.9 23.5,-133.9" />
</g>
<!-- hddd -->
<g id="node3" class="node">
<title>hddd</title>
<ellipse fill="none" stroke="black" cx="171" cy="-90" rx="27" ry="18" />
<text text-anchor="middle" x="171" y="-86.3" font-family="Times,serif" font-size="14.00">...</text>
</g>
<!-- h2&#45;&gt;hddd -->
<g id="edge2" class="edge">
<title>h2&#45;&gt;hddd</title>
<path fill="none" stroke="black" d="M126,-90C128.61,-90 131.23,-90 133.84,-90" />
<polygon fill="black" stroke="black" points="133.93,-93.5 143.93,-90 133.93,-86.5 133.93,-93.5" />
</g>
<!-- y2 -->
<g id="node6" class="node">
<title>y2</title>
<text text-anchor="middle" x="99" y="-158.3" font-family="Times,serif" font-size="14.00">y2</text>
</g>
<!-- h2&#45;&gt;y2 -->
<g id="edge6" class="edge">
<title>h2&#45;&gt;y2</title>
<path fill="none" stroke="black" d="M99,-108.3C99,-116.02 99,-125.29 99,-133.89" />
<polygon fill="black" stroke="black" points="95.5,-133.9 99,-143.9 102.5,-133.9 95.5,-133.9" />
</g>
<!-- hn -->
<g id="node4" class="node">
<title>hn</title>
<ellipse fill="none" stroke="black" cx="243" cy="-90" rx="27" ry="18" />
<text text-anchor="middle" x="243" y="-86.3" font-family="Times,serif" font-size="14.00">hn</text>
</g>
<!-- hddd&#45;&gt;hn -->
<g id="edge3" class="edge">
<title>hddd&#45;&gt;hn</title>
<path fill="none" stroke="black" d="M198,-90C200.61,-90 203.23,-90 205.84,-90" />
<polygon fill="black" stroke="black" points="205.93,-93.5 215.93,-90 205.93,-86.5 205.93,-93.5" />
</g>
<!-- yddd -->
<g id="node7" class="node">
<title>yddd</title>
<text text-anchor="middle" x="171" y="-158.3" font-family="Times,serif" font-size="14.00">...</text>
</g>
<!-- hddd&#45;&gt;yddd -->
<g id="edge7" class="edge">
<title>hddd&#45;&gt;yddd</title>
<path fill="none" stroke="black" d="M171,-108.3C171,-116.02 171,-125.29 171,-133.89" />
<polygon fill="black" stroke="black" points="167.5,-133.9 171,-143.9 174.5,-133.9 167.5,-133.9" />
</g>
<!-- yn -->
<g id="node8" class="node">
<title>yn</title>
<text text-anchor="middle" x="243" y="-158.3" font-family="Times,serif" font-size="14.00">yn</text>
</g>
<!-- hn&#45;&gt;yn -->
<g id="edge8" class="edge">
<title>hn&#45;&gt;yn</title>
<path fill="none" stroke="black" d="M243,-108.3C243,-116.02 243,-125.29 243,-133.89" />
<polygon fill="black" stroke="black" points="239.5,-133.9 243,-143.9 246.5,-133.9 239.5,-133.9" />
</g>
<!-- x -->
<g id="node9" class="node">
<title>x</title>
<text text-anchor="middle" x="27" y="-14.3" font-family="Times,serif" font-size="14.00">x</text>
</g>
<!-- x&#45;&gt;h1 -->
<g id="edge4" class="edge">
<title>x&#45;&gt;h1</title>
<path fill="none" stroke="black" d="M27,-36.3C27,-44.02 27,-53.29 27,-61.89" />
<polygon fill="black" stroke="black" points="23.5,-61.9 27,-71.9 30.5,-61.9 23.5,-61.9" />
</g>
</g>
</svg>
</div>
</div>

<p>到这里我们可以看到，在 RNN 的隐藏层是能够存储一些有关于输入数据的一些相关内容的，所以也常把 RNN 的隐藏层叫做记忆单元。</p>

<h4 id="44lstmlong-short-term-memory长短时记忆网络">4.4、LSTM（Long Short-Term Memory）长短时记忆网络</h4>

<h5 id="441如何理解这个-short-term-呢">4.4.1、如何理解这个 Short-Term 呢？</h5>

<p>1997 年论文《Long Short-Term Memory》中提出 LSTM 模型。我们先从模型的定义，精确地来理解一下：</p>

\[\begin{aligned}
&amp;\bm{h}_t = \bm{h}_{t-1} + tanh(\bm{W}^{xh} \cdot \bm{x}_t + \bm{b}^{xh} + \bm{W}^{hh} \cdot \bm{h}_{t-1} + \bm{b}^{hh}) \\
&amp;\bm{y}_t = Softmax(\bm{W}^{hy} \cdot \bm{h_t} + \bm{b}^{hy})
\end{aligned}\]

<p>上式中与经典结构的 RNN（输入与输出是 N vs. N）相比，唯一的区别是第一个式子中多了一个「 \(\bm{h}_{t-1}\) 」。如果我们把第一个式子的  \(tanh\)  部分记作  \(u_t\) ：</p>

\[\bm{u}_t = tanh(\bm{W}^{xh} \cdot \bm{x}_t + \bm{b}^{xh} + \bm{W}^{hh} \cdot \bm{h}_{t-1} + \bm{b}^{hh})\]

<p>所以：</p>

\[\bm{h}_t = \bm{h}_{t-1} + \bm{u}_t\]

<p>那么可以展开出如下一组式子：</p>

\[\begin{aligned}
\bm{h}_{k+1} &amp;= \bm{h}_k + \bm{u}_{k+1} \\
\bm{h}_{k+2} &amp;= \bm{h}_{k+1} + \bm{u}_{k+2} \\
&amp;...... \\
\bm{h}_{t-1} &amp;= \bm{h}_{t-2} + \bm{u}_{t-1} \\
\bm{h}_t &amp;= \bm{h}_{t-1} + \bm{u}_t
\end{aligned}\]

<p>如果我们从  \(h_{k+1}\)  到  \(h_n\)  的所有式子左侧相加、右侧相加，我们就得到如下式子：</p>

\[\begin{aligned}
&amp;\bm{h}_{k+1} + ... + \bm{h}_{t-1} + \bm{h}_t \\
= &amp;\bm{h}_k + \bm{h}_{k+1} + ... + \bm{h}_{t-2} + \bm{h}_{t-1} \\+ &amp;\bm{u}_{k+1} + \bm{u}_{k+2} + ... + \bm{u}_{t-1} + \bm{u}_t
\end{aligned}\]

<p>进而推导出：</p>

\[\bm{h}_t = \bm{h}_k + \bm{u}_{k+1} + \bm{u}_{k+2} + ... + \bm{u}_{t-1} + \bm{u}_t\]

<p>从这里我们就可以看到，第 t 时刻的隐藏层输出，直接关联到第 k 时刻的输出，t 到 k 时刻的相关性则用  \(\bm{u}_{k+1}\)  到  \(\bm{u}_t\)  相加表示。也就是有 t-k 的短期（Short Term）记忆。</p>

<h5 id="442引入遗忘门-f输入门-i输出门-o记忆细胞-c">4.4.2、引入遗忘门 f、输入门 i、输出门 o、记忆细胞 c</h5>

<p>如果我们为式子  \(\bm{h}_t = \bm{h}_{t-1} + \bm{u}_t\)  右侧两项分配一个权重呢？就是隐藏层对上一个数据项本身被上一个数据项经过隐藏层计算的结果，这两者做一对权重考虑配比，如下：</p>

\[\begin{aligned}
&amp;\bm{f}_t = sigmoid(\bm{W}^{f,xh} \cdot \bm{x}_t + \bm{b}^{f,xh} + \bm{W}^{f,hh} \cdot \bm{x}_{t-1} + \bm{b}^{f,hh}) \\
&amp;\bm{h}_t = \bm{f}_t \odot \bm{h}_{t-1} + (1 - \bm{f}_t) \odot \bm{u}_t
\end{aligned}\]

<p>其中：</p>

<ul>
  <li>\(\odot\)  是 Hardamard 乘积，即张量的对应元素相乘。</li>
  <li>\(\bm{f}_t\)  是「遗忘门（Forget Gate）」，该值很小时 t-1 时刻的权重就很小，也就是「此刻遗忘上一刻」。该值应根据 t 时刻的输入数据、t-1 时刻数据在隐藏层的输出计算，而且其每个元素必须是 (0, 1) 之间的值，所以可以用 sigmoid 函数来得到该值：</li>
</ul>

<p>但这种方式，对于过去  \(\bm{h}_{t-1}\)  和当下  \(\bm{u}_t\)  形成了互斥，只能此消彼长。但其实过去和当下可能都很重要，有可能都恨不重要，所以我们对过去继续采用  \(\bm{f}_t\)  遗忘门，对当下采用  \(\bm{i}_t\)  输入门（Input Gate）：</p>

\[\begin{aligned}
&amp;\bm{f}_t = sigmoid(\bm{W}^{f,xh} \cdot \bm{x}_t + \bm{b}^{f,xh} + \bm{W}^{f,hh} \cdot \bm{x}_{t-1} + \bm{b}^{f,hh}) \\
&amp;\bm{i}_t = sigmoid(\bm{W}^{i,xh} \cdot \bm{x}_t + \bm{b}^{i,xh} + \bm{W}^{i,hh} \cdot \bm{h}_{t-1} + \bm{b}^{i,hh}) \\
&amp;\bm{h}_t = \bm{f}_t \odot \bm{h}_{t-1} + \bm{i}_t \odot \bm{u}_t
\end{aligned}\]

<p>其中：</p>
<ul>
  <li>与  \(\bm{f}_t\)  类似地，定义输入门  \(\bm{i}_t\)  ，但是注意  \(\bm{f}_t\)  与  \(\bm{h}_{t-1}\)  而非  \(\bm{x}_{t-1}\)  有关。</li>
</ul>

<p>再引入一个输出门：</p>

\[\bm{o}_t = sigmoid(\bm{W}^{o,xh} \cdot \bm{x}_t + \bm{b}^{o,xh} + \bm{W}^{o,hh} \cdot \bm{x}_{t-1} + \bm{b}^{o,hh})\]

<p>再引入记忆细胞  \(\bm{c}_t\) ，它是原来  \(\bm{h}_t\)  的变体，与 t-1 时刻的记忆细胞有遗忘关系（通过遗忘门），与当下时刻有输入门的关系：</p>

\[\bm{c}_t = \bm{f}_t \odot \bm{c}_{t-1} + \bm{i}_t \odot \bm{u}_t\]

<p>那么此时  \(\bm{h}_t\)  ，我们可以把  \(\bm{h}_t\)  变成：</p>

\[\bm{h}_t = \bm{o}_t \odot tanh(\bm{c}_t)\]

<p>记忆细胞这个概念还有有一点点形象的，它存储了过去的一些信息。OK，到此我们整体的 LSTM 模型就变成了这个样子：</p>

\[\begin{aligned}
&amp;\bm{f}_t = sigmoid(\bm{W}^{f,xh} \cdot \bm{x}_t + \bm{b}^{f,xh} + \bm{W}^{f,hh} \cdot \bm{x}_{t-1} + \bm{b}^{f,hh}) \\
&amp;\bm{i}_t = sigmoid(\bm{W}^{i,xh} \cdot \bm{x}_t + \bm{b}^{i,xh} + \bm{W}^{i,hh} \cdot \bm{h}_{t-1} + \bm{b}^{i,hh}) \\
&amp;\bm{o}_t = sigmoid(\bm{W}^{o,xh} \cdot \bm{x}_t + \bm{b}^{o,xh} + \bm{W}^{o,hh} \cdot \bm{x}_{t-1} + \bm{b}^{o,hh}) \\
&amp;\bm{u}_t = tanh(\bm{W}^{xh} \cdot \bm{x}_t + \bm{b}^{xh} + \bm{W}^{hh} \cdot \bm{h}_{t-1} + \bm{b}^{hh}) \\
&amp;\bm{c}_t = \bm{f}_t \odot \bm{c}_{t-1} + \bm{i}_t \odot \bm{u}_t \\
&amp;\bm{h}_t = \bm{o}_t \odot tanh(\bm{c}_t) \\
&amp;\bm{y}_t = Softmax(\bm{W}^{hy} \cdot \bm{h_t} + \bm{b}^{hy})
\end{aligned}\]

<h4 id="45双向循环神经网络双向-lstm">4.5、双向循环神经网络、双向 LSTM</h4>

<p>双向循环神经网络很好理解，就是两个方向都有，例如下图：</p>

<div style="text-align: center;">
<div class="graphviz-wrapper">

<!-- Generated by graphviz version 2.43.0 (0)
 -->
<!-- Title: G Pages: 1 -->
<svg role="img" aria-label="graphviz-5d130f67fc1bf07abc38d62da6ddb01a" width="278pt" height="188pt" viewBox="0.00 0.00 278.00 188.00">
<title>graphviz-5d130f67fc1bf07abc38d62da6ddb01a</title>
<desc>
digraph G {
	rankdir=BT
	{rank=same h1 h2 hddd hn}

	hddd[label=&quot;...&quot;]
	xddd[label=&quot;...&quot;]
	yddd[label=&quot;...&quot;]

	y1[shape=plaintext]
	y2[shape=plaintext]
	yddd[shape=plaintext]
	yn[shape=plaintext]
	x1[shape=plaintext]
	x2[shape=plaintext]
	xddd[shape=plaintext]
	xn[shape=plaintext]

	h1 -&gt; y1
	h2 -&gt; y2
	hddd -&gt; yddd
	hn -&gt; yn

	h1 -&gt; h2
	h2 -&gt; hddd
	hddd -&gt; hn

	hn -&gt; hddd
	hddd -&gt; h2
	h2 -&gt; h1

	x1 -&gt; h1
	x2 -&gt; h2
	xddd -&gt; hddd
	xn -&gt; hn
}
</desc>

<g id="graph0" class="graph" transform="scale(1 1) rotate(0) translate(4 184)">
<title>G</title>
<polygon fill="white" stroke="transparent" points="-4,4 -4,-184 274,-184 274,4 -4,4" />
<!-- h1 -->
<g id="node1" class="node">
<title>h1</title>
<ellipse fill="none" stroke="black" cx="27" cy="-90" rx="27" ry="18" />
<text text-anchor="middle" x="27" y="-86.3" font-family="Times,serif" font-size="14.00">h1</text>
</g>
<!-- h2 -->
<g id="node2" class="node">
<title>h2</title>
<ellipse fill="none" stroke="black" cx="99" cy="-90" rx="27" ry="18" />
<text text-anchor="middle" x="99" y="-86.3" font-family="Times,serif" font-size="14.00">h2</text>
</g>
<!-- h1&#45;&gt;h2 -->
<g id="edge5" class="edge">
<title>h1&#45;&gt;h2</title>
<path fill="none" stroke="black" d="M48.38,-101.27C54.78,-103.22 61.18,-103.89 67.58,-103.28" />
<polygon fill="black" stroke="black" points="68.52,-106.66 77.64,-101.27 67.15,-99.8 68.52,-106.66" />
</g>
<!-- y1 -->
<g id="node7" class="node">
<title>y1</title>
<text text-anchor="middle" x="27" y="-158.3" font-family="Times,serif" font-size="14.00">y1</text>
</g>
<!-- h1&#45;&gt;y1 -->
<g id="edge1" class="edge">
<title>h1&#45;&gt;y1</title>
<path fill="none" stroke="black" d="M27,-108.3C27,-116.02 27,-125.29 27,-133.89" />
<polygon fill="black" stroke="black" points="23.5,-133.9 27,-143.9 30.5,-133.9 23.5,-133.9" />
</g>
<!-- h2&#45;&gt;h1 -->
<g id="edge10" class="edge">
<title>h2&#45;&gt;h1</title>
<path fill="none" stroke="black" d="M77.64,-78.73C71.24,-76.78 64.84,-76.11 58.44,-76.72" />
<polygon fill="black" stroke="black" points="57.49,-73.34 48.38,-78.73 58.87,-80.2 57.49,-73.34" />
</g>
<!-- hddd -->
<g id="node3" class="node">
<title>hddd</title>
<ellipse fill="none" stroke="black" cx="171" cy="-90" rx="27" ry="18" />
<text text-anchor="middle" x="171" y="-86.3" font-family="Times,serif" font-size="14.00">...</text>
</g>
<!-- h2&#45;&gt;hddd -->
<g id="edge6" class="edge">
<title>h2&#45;&gt;hddd</title>
<path fill="none" stroke="black" d="M120.38,-101.27C126.78,-103.22 133.18,-103.89 139.58,-103.28" />
<polygon fill="black" stroke="black" points="140.52,-106.66 149.64,-101.27 139.15,-99.8 140.52,-106.66" />
</g>
<!-- y2 -->
<g id="node8" class="node">
<title>y2</title>
<text text-anchor="middle" x="99" y="-158.3" font-family="Times,serif" font-size="14.00">y2</text>
</g>
<!-- h2&#45;&gt;y2 -->
<g id="edge2" class="edge">
<title>h2&#45;&gt;y2</title>
<path fill="none" stroke="black" d="M99,-108.3C99,-116.02 99,-125.29 99,-133.89" />
<polygon fill="black" stroke="black" points="95.5,-133.9 99,-143.9 102.5,-133.9 95.5,-133.9" />
</g>
<!-- hddd&#45;&gt;h2 -->
<g id="edge9" class="edge">
<title>hddd&#45;&gt;h2</title>
<path fill="none" stroke="black" d="M149.64,-78.73C143.24,-76.78 136.84,-76.11 130.44,-76.72" />
<polygon fill="black" stroke="black" points="129.49,-73.34 120.38,-78.73 130.87,-80.2 129.49,-73.34" />
</g>
<!-- hn -->
<g id="node4" class="node">
<title>hn</title>
<ellipse fill="none" stroke="black" cx="243" cy="-90" rx="27" ry="18" />
<text text-anchor="middle" x="243" y="-86.3" font-family="Times,serif" font-size="14.00">hn</text>
</g>
<!-- hddd&#45;&gt;hn -->
<g id="edge7" class="edge">
<title>hddd&#45;&gt;hn</title>
<path fill="none" stroke="black" d="M192.38,-101.27C198.78,-103.22 205.18,-103.89 211.58,-103.28" />
<polygon fill="black" stroke="black" points="212.52,-106.66 221.64,-101.27 211.15,-99.8 212.52,-106.66" />
</g>
<!-- yddd -->
<g id="node6" class="node">
<title>yddd</title>
<text text-anchor="middle" x="171" y="-158.3" font-family="Times,serif" font-size="14.00">...</text>
</g>
<!-- hddd&#45;&gt;yddd -->
<g id="edge3" class="edge">
<title>hddd&#45;&gt;yddd</title>
<path fill="none" stroke="black" d="M171,-108.3C171,-116.02 171,-125.29 171,-133.89" />
<polygon fill="black" stroke="black" points="167.5,-133.9 171,-143.9 174.5,-133.9 167.5,-133.9" />
</g>
<!-- hn&#45;&gt;hddd -->
<g id="edge8" class="edge">
<title>hn&#45;&gt;hddd</title>
<path fill="none" stroke="black" d="M221.64,-78.73C215.24,-76.78 208.84,-76.11 202.44,-76.72" />
<polygon fill="black" stroke="black" points="201.49,-73.34 192.38,-78.73 202.87,-80.2 201.49,-73.34" />
</g>
<!-- yn -->
<g id="node9" class="node">
<title>yn</title>
<text text-anchor="middle" x="243" y="-158.3" font-family="Times,serif" font-size="14.00">yn</text>
</g>
<!-- hn&#45;&gt;yn -->
<g id="edge4" class="edge">
<title>hn&#45;&gt;yn</title>
<path fill="none" stroke="black" d="M243,-108.3C243,-116.02 243,-125.29 243,-133.89" />
<polygon fill="black" stroke="black" points="239.5,-133.9 243,-143.9 246.5,-133.9 239.5,-133.9" />
</g>
<!-- xddd -->
<g id="node5" class="node">
<title>xddd</title>
<text text-anchor="middle" x="171" y="-14.3" font-family="Times,serif" font-size="14.00">...</text>
</g>
<!-- xddd&#45;&gt;hddd -->
<g id="edge13" class="edge">
<title>xddd&#45;&gt;hddd</title>
<path fill="none" stroke="black" d="M171,-36.3C171,-44.02 171,-53.29 171,-61.89" />
<polygon fill="black" stroke="black" points="167.5,-61.9 171,-71.9 174.5,-61.9 167.5,-61.9" />
</g>
<!-- x1 -->
<g id="node10" class="node">
<title>x1</title>
<text text-anchor="middle" x="27" y="-14.3" font-family="Times,serif" font-size="14.00">x1</text>
</g>
<!-- x1&#45;&gt;h1 -->
<g id="edge11" class="edge">
<title>x1&#45;&gt;h1</title>
<path fill="none" stroke="black" d="M27,-36.3C27,-44.02 27,-53.29 27,-61.89" />
<polygon fill="black" stroke="black" points="23.5,-61.9 27,-71.9 30.5,-61.9 23.5,-61.9" />
</g>
<!-- x2 -->
<g id="node11" class="node">
<title>x2</title>
<text text-anchor="middle" x="99" y="-14.3" font-family="Times,serif" font-size="14.00">x2</text>
</g>
<!-- x2&#45;&gt;h2 -->
<g id="edge12" class="edge">
<title>x2&#45;&gt;h2</title>
<path fill="none" stroke="black" d="M99,-36.3C99,-44.02 99,-53.29 99,-61.89" />
<polygon fill="black" stroke="black" points="95.5,-61.9 99,-71.9 102.5,-61.9 95.5,-61.9" />
</g>
<!-- xn -->
<g id="node12" class="node">
<title>xn</title>
<text text-anchor="middle" x="243" y="-14.3" font-family="Times,serif" font-size="14.00">xn</text>
</g>
<!-- xn&#45;&gt;hn -->
<g id="edge14" class="edge">
<title>xn&#45;&gt;hn</title>
<path fill="none" stroke="black" d="M243,-36.3C243,-44.02 243,-53.29 243,-61.89" />
<polygon fill="black" stroke="black" points="239.5,-61.9 243,-71.9 246.5,-61.9 239.5,-61.9" />
</g>
</g>
</svg>
</div>
</div>

<p>在 PyTorch 中使用 <code class="language-plaintext highlighter-rouge">nn.RNN</code> 就有参数表示双向：</p>

<blockquote>
  <p><code class="language-plaintext highlighter-rouge">bidirectional</code> – If True, becomes a bidirectional RNN. Default: False</p>
</blockquote>

<p><code class="language-plaintext highlighter-rouge">bidirectional</code>：默认设置为 <code class="language-plaintext highlighter-rouge">False</code>。若为 <code class="language-plaintext highlighter-rouge">True</code>，即为双向 RNN。</p>

<h4 id="46堆叠循环神经网络stacked-rnn堆叠长短时记忆网络stacked-lstm">4.6、堆叠循环神经网络（Stacked RNN）、堆叠长短时记忆网络（Stacked LSTM）</h4>

<div style="text-align: center;">
<div class="graphviz-wrapper">

<!-- Generated by graphviz version 2.43.0 (0)
 -->
<!-- Title: G Pages: 1 -->
<svg role="img" aria-label="graphviz-79478ee70f94925103b38a21c70c2539" width="288pt" height="260pt" viewBox="0.00 0.00 288.19 260.00">
<title>graphviz-79478ee70f94925103b38a21c70c2539</title>
<desc>
digraph G {
	rankdir=BT
	{rank=same h11 h12 h1ddd h1n}
	{rank=same h21 h22 h2ddd h2n}

	h1ddd[label=&quot;...&quot;]
	h2ddd[label=&quot;...&quot;]
	xddd[label=&quot;...&quot;]
	yddd[label=&quot;...&quot;]

	y1[shape=plaintext]
	y2[shape=plaintext]
	yddd[shape=plaintext]
	yn[shape=plaintext]
	x1[shape=plaintext]
	x2[shape=plaintext]
	xddd[shape=plaintext]
	xn[shape=plaintext]

	h11 -&gt; y1
	h12 -&gt; y2
	h1ddd -&gt; yddd
	h1n -&gt; yn

	h11 -&gt; h12
	h12 -&gt; h1ddd
	h1ddd -&gt; h1n

	h21 -&gt; h22
	h22 -&gt; h2ddd
	h2ddd -&gt; h2n

	h21 -&gt; h11
	h22 -&gt; h12
	h2ddd -&gt; h1ddd
	h2n -&gt; h1n

	x1 -&gt; h21
	x2 -&gt; h22
	xddd -&gt; h2ddd
	xn -&gt; h2n
}
</desc>

<g id="graph0" class="graph" transform="scale(1 1) rotate(0) translate(4 256)">
<title>G</title>
<polygon fill="white" stroke="transparent" points="-4,4 -4,-256 284.19,-256 284.19,4 -4,4" />
<!-- h11 -->
<g id="node1" class="node">
<title>h11</title>
<ellipse fill="none" stroke="black" cx="28.6" cy="-162" rx="28.7" ry="18" />
<text text-anchor="middle" x="28.6" y="-158.3" font-family="Times,serif" font-size="14.00">h11</text>
</g>
<!-- h12 -->
<g id="node2" class="node">
<title>h12</title>
<ellipse fill="none" stroke="black" cx="103.6" cy="-162" rx="28.7" ry="18" />
<text text-anchor="middle" x="103.6" y="-158.3" font-family="Times,serif" font-size="14.00">h12</text>
</g>
<!-- h11&#45;&gt;h12 -->
<g id="edge5" class="edge">
<title>h11&#45;&gt;h12</title>
<path fill="none" stroke="black" d="M57.31,-162C59.75,-162 62.19,-162 64.63,-162" />
<polygon fill="black" stroke="black" points="64.67,-165.5 74.67,-162 64.67,-158.5 64.67,-165.5" />
</g>
<!-- y1 -->
<g id="node11" class="node">
<title>y1</title>
<text text-anchor="middle" x="28.6" y="-230.3" font-family="Times,serif" font-size="14.00">y1</text>
</g>
<!-- h11&#45;&gt;y1 -->
<g id="edge1" class="edge">
<title>h11&#45;&gt;y1</title>
<path fill="none" stroke="black" d="M28.6,-180.3C28.6,-188.02 28.6,-197.29 28.6,-205.89" />
<polygon fill="black" stroke="black" points="25.1,-205.9 28.6,-215.9 32.1,-205.9 25.1,-205.9" />
</g>
<!-- h1ddd -->
<g id="node3" class="node">
<title>h1ddd</title>
<ellipse fill="none" stroke="black" cx="177.6" cy="-162" rx="27" ry="18" />
<text text-anchor="middle" x="177.6" y="-158.3" font-family="Times,serif" font-size="14.00">...</text>
</g>
<!-- h12&#45;&gt;h1ddd -->
<g id="edge6" class="edge">
<title>h12&#45;&gt;h1ddd</title>
<path fill="none" stroke="black" d="M132.21,-162C134.85,-162 137.49,-162 140.13,-162" />
<polygon fill="black" stroke="black" points="140.3,-165.5 150.3,-162 140.3,-158.5 140.3,-165.5" />
</g>
<!-- y2 -->
<g id="node12" class="node">
<title>y2</title>
<text text-anchor="middle" x="103.6" y="-230.3" font-family="Times,serif" font-size="14.00">y2</text>
</g>
<!-- h12&#45;&gt;y2 -->
<g id="edge2" class="edge">
<title>h12&#45;&gt;y2</title>
<path fill="none" stroke="black" d="M103.6,-180.3C103.6,-188.02 103.6,-197.29 103.6,-205.89" />
<polygon fill="black" stroke="black" points="100.1,-205.9 103.6,-215.9 107.1,-205.9 100.1,-205.9" />
</g>
<!-- h1n -->
<g id="node4" class="node">
<title>h1n</title>
<ellipse fill="none" stroke="black" cx="251.6" cy="-162" rx="28.7" ry="18" />
<text text-anchor="middle" x="251.6" y="-158.3" font-family="Times,serif" font-size="14.00">h1n</text>
</g>
<!-- h1ddd&#45;&gt;h1n -->
<g id="edge7" class="edge">
<title>h1ddd&#45;&gt;h1n</title>
<path fill="none" stroke="black" d="M204.77,-162C207.38,-162 210,-162 212.61,-162" />
<polygon fill="black" stroke="black" points="212.7,-165.5 222.7,-162 212.7,-158.5 212.7,-165.5" />
</g>
<!-- yddd -->
<g id="node10" class="node">
<title>yddd</title>
<text text-anchor="middle" x="177.6" y="-230.3" font-family="Times,serif" font-size="14.00">...</text>
</g>
<!-- h1ddd&#45;&gt;yddd -->
<g id="edge3" class="edge">
<title>h1ddd&#45;&gt;yddd</title>
<path fill="none" stroke="black" d="M177.6,-180.3C177.6,-188.02 177.6,-197.29 177.6,-205.89" />
<polygon fill="black" stroke="black" points="174.1,-205.9 177.6,-215.9 181.1,-205.9 174.1,-205.9" />
</g>
<!-- yn -->
<g id="node13" class="node">
<title>yn</title>
<text text-anchor="middle" x="251.6" y="-230.3" font-family="Times,serif" font-size="14.00">yn</text>
</g>
<!-- h1n&#45;&gt;yn -->
<g id="edge4" class="edge">
<title>h1n&#45;&gt;yn</title>
<path fill="none" stroke="black" d="M251.6,-180.3C251.6,-188.02 251.6,-197.29 251.6,-205.89" />
<polygon fill="black" stroke="black" points="248.1,-205.9 251.6,-215.9 255.1,-205.9 248.1,-205.9" />
</g>
<!-- h21 -->
<g id="node5" class="node">
<title>h21</title>
<ellipse fill="none" stroke="black" cx="28.6" cy="-90" rx="28.7" ry="18" />
<text text-anchor="middle" x="28.6" y="-86.3" font-family="Times,serif" font-size="14.00">h21</text>
</g>
<!-- h21&#45;&gt;h11 -->
<g id="edge11" class="edge">
<title>h21&#45;&gt;h11</title>
<path fill="none" stroke="black" d="M28.6,-108.3C28.6,-116.02 28.6,-125.29 28.6,-133.89" />
<polygon fill="black" stroke="black" points="25.1,-133.9 28.6,-143.9 32.1,-133.9 25.1,-133.9" />
</g>
<!-- h22 -->
<g id="node6" class="node">
<title>h22</title>
<ellipse fill="none" stroke="black" cx="103.6" cy="-90" rx="28.7" ry="18" />
<text text-anchor="middle" x="103.6" y="-86.3" font-family="Times,serif" font-size="14.00">h22</text>
</g>
<!-- h21&#45;&gt;h22 -->
<g id="edge8" class="edge">
<title>h21&#45;&gt;h22</title>
<path fill="none" stroke="black" d="M57.31,-90C59.75,-90 62.19,-90 64.63,-90" />
<polygon fill="black" stroke="black" points="64.67,-93.5 74.67,-90 64.67,-86.5 64.67,-93.5" />
</g>
<!-- h22&#45;&gt;h12 -->
<g id="edge12" class="edge">
<title>h22&#45;&gt;h12</title>
<path fill="none" stroke="black" d="M103.6,-108.3C103.6,-116.02 103.6,-125.29 103.6,-133.89" />
<polygon fill="black" stroke="black" points="100.1,-133.9 103.6,-143.9 107.1,-133.9 100.1,-133.9" />
</g>
<!-- h2ddd -->
<g id="node7" class="node">
<title>h2ddd</title>
<ellipse fill="none" stroke="black" cx="177.6" cy="-90" rx="27" ry="18" />
<text text-anchor="middle" x="177.6" y="-86.3" font-family="Times,serif" font-size="14.00">...</text>
</g>
<!-- h22&#45;&gt;h2ddd -->
<g id="edge9" class="edge">
<title>h22&#45;&gt;h2ddd</title>
<path fill="none" stroke="black" d="M132.21,-90C134.85,-90 137.49,-90 140.13,-90" />
<polygon fill="black" stroke="black" points="140.3,-93.5 150.3,-90 140.3,-86.5 140.3,-93.5" />
</g>
<!-- h2ddd&#45;&gt;h1ddd -->
<g id="edge13" class="edge">
<title>h2ddd&#45;&gt;h1ddd</title>
<path fill="none" stroke="black" d="M177.6,-108.3C177.6,-116.02 177.6,-125.29 177.6,-133.89" />
<polygon fill="black" stroke="black" points="174.1,-133.9 177.6,-143.9 181.1,-133.9 174.1,-133.9" />
</g>
<!-- h2n -->
<g id="node8" class="node">
<title>h2n</title>
<ellipse fill="none" stroke="black" cx="251.6" cy="-90" rx="28.7" ry="18" />
<text text-anchor="middle" x="251.6" y="-86.3" font-family="Times,serif" font-size="14.00">h2n</text>
</g>
<!-- h2ddd&#45;&gt;h2n -->
<g id="edge10" class="edge">
<title>h2ddd&#45;&gt;h2n</title>
<path fill="none" stroke="black" d="M204.77,-90C207.38,-90 210,-90 212.61,-90" />
<polygon fill="black" stroke="black" points="212.7,-93.5 222.7,-90 212.7,-86.5 212.7,-93.5" />
</g>
<!-- h2n&#45;&gt;h1n -->
<g id="edge14" class="edge">
<title>h2n&#45;&gt;h1n</title>
<path fill="none" stroke="black" d="M251.6,-108.3C251.6,-116.02 251.6,-125.29 251.6,-133.89" />
<polygon fill="black" stroke="black" points="248.1,-133.9 251.6,-143.9 255.1,-133.9 248.1,-133.9" />
</g>
<!-- xddd -->
<g id="node9" class="node">
<title>xddd</title>
<text text-anchor="middle" x="177.6" y="-14.3" font-family="Times,serif" font-size="14.00">...</text>
</g>
<!-- xddd&#45;&gt;h2ddd -->
<g id="edge17" class="edge">
<title>xddd&#45;&gt;h2ddd</title>
<path fill="none" stroke="black" d="M177.6,-36.3C177.6,-44.02 177.6,-53.29 177.6,-61.89" />
<polygon fill="black" stroke="black" points="174.1,-61.9 177.6,-71.9 181.1,-61.9 174.1,-61.9" />
</g>
<!-- x1 -->
<g id="node14" class="node">
<title>x1</title>
<text text-anchor="middle" x="28.6" y="-14.3" font-family="Times,serif" font-size="14.00">x1</text>
</g>
<!-- x1&#45;&gt;h21 -->
<g id="edge15" class="edge">
<title>x1&#45;&gt;h21</title>
<path fill="none" stroke="black" d="M28.6,-36.3C28.6,-44.02 28.6,-53.29 28.6,-61.89" />
<polygon fill="black" stroke="black" points="25.1,-61.9 28.6,-71.9 32.1,-61.9 25.1,-61.9" />
</g>
<!-- x2 -->
<g id="node15" class="node">
<title>x2</title>
<text text-anchor="middle" x="103.6" y="-14.3" font-family="Times,serif" font-size="14.00">x2</text>
</g>
<!-- x2&#45;&gt;h22 -->
<g id="edge16" class="edge">
<title>x2&#45;&gt;h22</title>
<path fill="none" stroke="black" d="M103.6,-36.3C103.6,-44.02 103.6,-53.29 103.6,-61.89" />
<polygon fill="black" stroke="black" points="100.1,-61.9 103.6,-71.9 107.1,-61.9 100.1,-61.9" />
</g>
<!-- xn -->
<g id="node16" class="node">
<title>xn</title>
<text text-anchor="middle" x="251.6" y="-14.3" font-family="Times,serif" font-size="14.00">xn</text>
</g>
<!-- xn&#45;&gt;h2n -->
<g id="edge18" class="edge">
<title>xn&#45;&gt;h2n</title>
<path fill="none" stroke="black" d="M251.6,-36.3C251.6,-44.02 251.6,-53.29 251.6,-61.89" />
<polygon fill="black" stroke="black" points="248.1,-61.9 251.6,-71.9 255.1,-61.9 248.1,-61.9" />
</g>
</g>
</svg>
</div>
</div>

<p>在 PyTorch 中使用 <code class="language-plaintext highlighter-rouge">nn.RNN</code> 就有参数表示双向：</p>

<blockquote>
  <p>num_layers – Number of recurrent layers. E.g., setting num_layers=2 would mean stacking two RNNs together to form a stacked RNN, with the second RNN taking in outputs of the first RNN and computing the final results. Default: 1</p>
</blockquote>

<p><code class="language-plaintext highlighter-rouge">num_layers</code>：隐藏层层数，默认设置为 1 层。当 <code class="language-plaintext highlighter-rouge">num_layers</code> &gt;= 2 时，就是一个 stacked RNN 了。</p>

<h4 id="47n-vs-m-的-rnn">4.7、N vs. M 的 RNN</h4>

<p>对于输入序列长度（长度 N）和输出序列长度（长度 M）不一样的 RNN 模型结构，也可以叫做 Encoder-Decoder 模型，也可以叫 Seq2Seq 模型。首先接收输入序列的 Encoder 先将输入序列转成一个隐藏态的上下文表示 C。C 可以只与最后一个隐藏层有关，甚至可以是最后一个隐藏层生成的隐藏态直接设置为 C，C 还可以与所有隐藏层有关。</p>

<p>有了这个 C 之后，再用 Decoder 进行解码，也就是从把 C 作为输入状态开始，生成输出序列。</p>

<div style="text-align: center;">
<div class="graphviz-wrapper">

<!-- Generated by graphviz version 2.43.0 (0)
 -->
<!-- Title: G Pages: 1 -->
<svg role="img" aria-label="graphviz-094de5e41d0af67d4c5617e0f04d7b57" width="638pt" height="188pt" viewBox="0.00 0.00 638.00 188.00">
<title>graphviz-094de5e41d0af67d4c5617e0f04d7b57</title>
<desc>
digraph G {
	rankdir=BT
	{rank=same e1 e2 eddd en C d1 d2 dddd dm}

	eddd[label=&quot;...&quot;]
	dddd[label=&quot;...&quot;]
	xddd[label=&quot;...&quot;]
	yddd[label=&quot;...&quot;]
	C[shape=plaintext]
	x1[shape=plaintext]
	x2[shape=plaintext]
	xddd[shape=plaintext]
	xn[shape=plaintext]
	y1[shape=plaintext]
	y2[shape=plaintext]
	yddd[shape=plaintext]
	yn[shape=plaintext]

	x1 -&gt; e1
	x2 -&gt; e2
	xddd -&gt; eddd
	xn -&gt; en

	e1 -&gt; e2
	e2 -&gt; eddd
	eddd -&gt; en

	en -&gt; C
	C -&gt; d1

	d1 -&gt; y1
	d2 -&gt; y2
	dddd -&gt; yddd
	dm -&gt; yn

	d1 -&gt; d2
	d2 -&gt; dddd
	dddd -&gt; dm
}
</desc>

<g id="graph0" class="graph" transform="scale(1 1) rotate(0) translate(4 184)">
<title>G</title>
<polygon fill="white" stroke="transparent" points="-4,4 -4,-184 634,-184 634,4 -4,4" />
<!-- e1 -->
<g id="node1" class="node">
<title>e1</title>
<ellipse fill="none" stroke="black" cx="27" cy="-90" rx="27" ry="18" />
<text text-anchor="middle" x="27" y="-86.3" font-family="Times,serif" font-size="14.00">e1</text>
</g>
<!-- e2 -->
<g id="node2" class="node">
<title>e2</title>
<ellipse fill="none" stroke="black" cx="99" cy="-90" rx="27" ry="18" />
<text text-anchor="middle" x="99" y="-86.3" font-family="Times,serif" font-size="14.00">e2</text>
</g>
<!-- e1&#45;&gt;e2 -->
<g id="edge5" class="edge">
<title>e1&#45;&gt;e2</title>
<path fill="none" stroke="black" d="M54,-90C56.61,-90 59.23,-90 61.84,-90" />
<polygon fill="black" stroke="black" points="61.93,-93.5 71.93,-90 61.93,-86.5 61.93,-93.5" />
</g>
<!-- eddd -->
<g id="node3" class="node">
<title>eddd</title>
<ellipse fill="none" stroke="black" cx="171" cy="-90" rx="27" ry="18" />
<text text-anchor="middle" x="171" y="-86.3" font-family="Times,serif" font-size="14.00">...</text>
</g>
<!-- e2&#45;&gt;eddd -->
<g id="edge6" class="edge">
<title>e2&#45;&gt;eddd</title>
<path fill="none" stroke="black" d="M126,-90C128.61,-90 131.23,-90 133.84,-90" />
<polygon fill="black" stroke="black" points="133.93,-93.5 143.93,-90 133.93,-86.5 133.93,-93.5" />
</g>
<!-- en -->
<g id="node4" class="node">
<title>en</title>
<ellipse fill="none" stroke="black" cx="243" cy="-90" rx="27" ry="18" />
<text text-anchor="middle" x="243" y="-86.3" font-family="Times,serif" font-size="14.00">en</text>
</g>
<!-- eddd&#45;&gt;en -->
<g id="edge7" class="edge">
<title>eddd&#45;&gt;en</title>
<path fill="none" stroke="black" d="M198,-90C200.61,-90 203.23,-90 205.84,-90" />
<polygon fill="black" stroke="black" points="205.93,-93.5 215.93,-90 205.93,-86.5 205.93,-93.5" />
</g>
<!-- C -->
<g id="node5" class="node">
<title>C</title>
<text text-anchor="middle" x="315" y="-86.3" font-family="Times,serif" font-size="14.00">C</text>
</g>
<!-- en&#45;&gt;C -->
<g id="edge8" class="edge">
<title>en&#45;&gt;C</title>
<path fill="none" stroke="black" d="M270,-90C272.61,-90 275.23,-90 277.84,-90" />
<polygon fill="black" stroke="black" points="277.93,-93.5 287.93,-90 277.93,-86.5 277.93,-93.5" />
</g>
<!-- d1 -->
<g id="node6" class="node">
<title>d1</title>
<ellipse fill="none" stroke="black" cx="387" cy="-90" rx="27" ry="18" />
<text text-anchor="middle" x="387" y="-86.3" font-family="Times,serif" font-size="14.00">d1</text>
</g>
<!-- C&#45;&gt;d1 -->
<g id="edge9" class="edge">
<title>C&#45;&gt;d1</title>
<path fill="none" stroke="black" d="M342.28,-90C344.74,-90 347.19,-90 349.65,-90" />
<polygon fill="black" stroke="black" points="349.75,-93.5 359.75,-90 349.75,-86.5 349.75,-93.5" />
</g>
<!-- d2 -->
<g id="node7" class="node">
<title>d2</title>
<ellipse fill="none" stroke="black" cx="459" cy="-90" rx="27" ry="18" />
<text text-anchor="middle" x="459" y="-86.3" font-family="Times,serif" font-size="14.00">d2</text>
</g>
<!-- d1&#45;&gt;d2 -->
<g id="edge14" class="edge">
<title>d1&#45;&gt;d2</title>
<path fill="none" stroke="black" d="M414,-90C416.61,-90 419.23,-90 421.84,-90" />
<polygon fill="black" stroke="black" points="421.93,-93.5 431.93,-90 421.93,-86.5 421.93,-93.5" />
</g>
<!-- y1 -->
<g id="node15" class="node">
<title>y1</title>
<text text-anchor="middle" x="387" y="-158.3" font-family="Times,serif" font-size="14.00">y1</text>
</g>
<!-- d1&#45;&gt;y1 -->
<g id="edge10" class="edge">
<title>d1&#45;&gt;y1</title>
<path fill="none" stroke="black" d="M387,-108.3C387,-116.02 387,-125.29 387,-133.89" />
<polygon fill="black" stroke="black" points="383.5,-133.9 387,-143.9 390.5,-133.9 383.5,-133.9" />
</g>
<!-- dddd -->
<g id="node8" class="node">
<title>dddd</title>
<ellipse fill="none" stroke="black" cx="531" cy="-90" rx="27" ry="18" />
<text text-anchor="middle" x="531" y="-86.3" font-family="Times,serif" font-size="14.00">...</text>
</g>
<!-- d2&#45;&gt;dddd -->
<g id="edge15" class="edge">
<title>d2&#45;&gt;dddd</title>
<path fill="none" stroke="black" d="M486,-90C488.61,-90 491.23,-90 493.84,-90" />
<polygon fill="black" stroke="black" points="493.93,-93.5 503.93,-90 493.93,-86.5 493.93,-93.5" />
</g>
<!-- y2 -->
<g id="node16" class="node">
<title>y2</title>
<text text-anchor="middle" x="459" y="-158.3" font-family="Times,serif" font-size="14.00">y2</text>
</g>
<!-- d2&#45;&gt;y2 -->
<g id="edge11" class="edge">
<title>d2&#45;&gt;y2</title>
<path fill="none" stroke="black" d="M459,-108.3C459,-116.02 459,-125.29 459,-133.89" />
<polygon fill="black" stroke="black" points="455.5,-133.9 459,-143.9 462.5,-133.9 455.5,-133.9" />
</g>
<!-- dm -->
<g id="node9" class="node">
<title>dm</title>
<ellipse fill="none" stroke="black" cx="603" cy="-90" rx="27" ry="18" />
<text text-anchor="middle" x="603" y="-86.3" font-family="Times,serif" font-size="14.00">dm</text>
</g>
<!-- dddd&#45;&gt;dm -->
<g id="edge16" class="edge">
<title>dddd&#45;&gt;dm</title>
<path fill="none" stroke="black" d="M558,-90C560.61,-90 563.23,-90 565.84,-90" />
<polygon fill="black" stroke="black" points="565.93,-93.5 575.93,-90 565.93,-86.5 565.93,-93.5" />
</g>
<!-- yddd -->
<g id="node11" class="node">
<title>yddd</title>
<text text-anchor="middle" x="531" y="-158.3" font-family="Times,serif" font-size="14.00">...</text>
</g>
<!-- dddd&#45;&gt;yddd -->
<g id="edge12" class="edge">
<title>dddd&#45;&gt;yddd</title>
<path fill="none" stroke="black" d="M531,-108.3C531,-116.02 531,-125.29 531,-133.89" />
<polygon fill="black" stroke="black" points="527.5,-133.9 531,-143.9 534.5,-133.9 527.5,-133.9" />
</g>
<!-- yn -->
<g id="node17" class="node">
<title>yn</title>
<text text-anchor="middle" x="603" y="-158.3" font-family="Times,serif" font-size="14.00">yn</text>
</g>
<!-- dm&#45;&gt;yn -->
<g id="edge13" class="edge">
<title>dm&#45;&gt;yn</title>
<path fill="none" stroke="black" d="M603,-108.3C603,-116.02 603,-125.29 603,-133.89" />
<polygon fill="black" stroke="black" points="599.5,-133.9 603,-143.9 606.5,-133.9 599.5,-133.9" />
</g>
<!-- xddd -->
<g id="node10" class="node">
<title>xddd</title>
<text text-anchor="middle" x="171" y="-14.3" font-family="Times,serif" font-size="14.00">...</text>
</g>
<!-- xddd&#45;&gt;eddd -->
<g id="edge3" class="edge">
<title>xddd&#45;&gt;eddd</title>
<path fill="none" stroke="black" d="M171,-36.3C171,-44.02 171,-53.29 171,-61.89" />
<polygon fill="black" stroke="black" points="167.5,-61.9 171,-71.9 174.5,-61.9 167.5,-61.9" />
</g>
<!-- x1 -->
<g id="node12" class="node">
<title>x1</title>
<text text-anchor="middle" x="27" y="-14.3" font-family="Times,serif" font-size="14.00">x1</text>
</g>
<!-- x1&#45;&gt;e1 -->
<g id="edge1" class="edge">
<title>x1&#45;&gt;e1</title>
<path fill="none" stroke="black" d="M27,-36.3C27,-44.02 27,-53.29 27,-61.89" />
<polygon fill="black" stroke="black" points="23.5,-61.9 27,-71.9 30.5,-61.9 23.5,-61.9" />
</g>
<!-- x2 -->
<g id="node13" class="node">
<title>x2</title>
<text text-anchor="middle" x="99" y="-14.3" font-family="Times,serif" font-size="14.00">x2</text>
</g>
<!-- x2&#45;&gt;e2 -->
<g id="edge2" class="edge">
<title>x2&#45;&gt;e2</title>
<path fill="none" stroke="black" d="M99,-36.3C99,-44.02 99,-53.29 99,-61.89" />
<polygon fill="black" stroke="black" points="95.5,-61.9 99,-71.9 102.5,-61.9 95.5,-61.9" />
</g>
<!-- xn -->
<g id="node14" class="node">
<title>xn</title>
<text text-anchor="middle" x="243" y="-14.3" font-family="Times,serif" font-size="14.00">xn</text>
</g>
<!-- xn&#45;&gt;en -->
<g id="edge4" class="edge">
<title>xn&#45;&gt;en</title>
<path fill="none" stroke="black" d="M243,-36.3C243,-44.02 243,-53.29 243,-61.89" />
<polygon fill="black" stroke="black" points="239.5,-61.9 243,-71.9 246.5,-61.9 239.5,-61.9" />
</g>
</g>
</svg>
</div>
</div>

<p>具体地，可以如下表示：</p>

\[\begin{aligned}
&amp;\bm{C} = Encoder(\bm{X}) \\
&amp;\bm{Y} = Decoder(\bm{C}) \\
\end{aligned}\]

<p>进一步展开：</p>

\[\begin{aligned}
e_t &amp;= Encoder_{LSTM/GRU}(x_t, e_{t-1}) \\
\bm{C} &amp;= f_1(e_n) \\
d_t &amp;= f_2(d_{t-1}, \bm{C}) \\
y_t &amp;= Decoder_{LSTM/GRU}(y_{t-1}, d_{t-1}, \bm{C})
\end{aligned}\]

<p>这种的应用就非常广了，因为大多数时候输入序列与输出序列的长度都是不同的，比如最常见的应用「翻译」，从一个语言翻译成另一个语言；再比如 AI 的一个领域「语音识别」，将语音序列输入后生成所识别的文本内容；还有比如 ChatGPT 这种问答应用等等。</p>

<p>Seq2Seq 模型非常出色，一直到 2018 年之前 NLP 领域里该模型已成为主流。但是它有很显著的问题：</p>

<ul>
  <li>当输入序列很长时，Encoder 生成的 Context 可能就会出现所捕捉的信息不充分的情况，导致 Decoder 最终的输出是不尽如人意的。具体地，毕竟还是 RNN 模型，其词间距过长时还是会有梯度消失问题，根本原因在于用到了「递归」。当递归作用在同一个 weight matrix 上时，使得如果这个矩阵满足条件的话，其最大的特征值要是小于 1 的话，就一定出现梯度消失问题。后来的 LSTM 和 GRU 也仅仅能缓解问题，并不能根本解决。</li>
  <li>并行效果差：每个时刻的结果依赖前一时刻。</li>
</ul>

<h3 id="第-5-节--为什么说-rnn-模型没有体现注意力">第 5 节 · 为什么说 RNN 模型没有体现「注意力」？</h3>

<p>Encoder-Decoder 的一个非常严重的问题，是依赖中间那个 context 向量，则无法处理特别长的输入序列 —— 记忆力不足，会忘事儿。而忘事儿的根本原因，是没有「注意力」。</p>

<p>对于一般的 RNN 模型，Encoder-Decoder 结构并没有体现「注意力」—— 这句话怎么理解？当输入序列经过 Encoder 生成的中间结果（上下文 C），被喂给 Decoder 时，这些中间结果对所生成序列里的哪个词，都没有区别（没有特别关照谁）。这相当于在说：输入序列里的每个词，对于生成任何一个输出的词的影响，是一样的，而不是输出某个词时是聚焦特定的一些输入词。这就是模型没有注意力机制。</p>

<p>人脑的注意力模型，其实是资源分配模型。NLP 领域的注意力模型，是在 2014 年被提出的，后来逐渐成为 NLP 领域的一个广泛应用的机制。可以应用的场景，比如对于一个电商平台中很常见的白底图，其边缘的白色区域都是无用的，那么就不应该被关注（关注权重为 0）。比如机器翻译中，翻译词都是对局部输入重点关注的。</p>

<p>所以 Attention 机制，就是在 Decoder 时，不是所有输出都依赖相同的「上下文  \(\bm{C}_t\) 」，而是时刻 t 的输出，使用  \(\bm{C}_t\) ，而这个  \(\bm{C}_t\)  来自对每个输入数据项根据「注意力」进行的加权。</p>

<h3 id="第-6-节--基于-attention-机制的-encoder-decoder-模型">第 6 节 · 基于 Attention 机制的 Encoder-Decoder 模型</h3>

<p>2015 年 Dzmitry Bahdanau 等人在论文<a href="https://arxiv.org/abs/1409.0473">《Neural Machine Translation by Jointly Learning to Align and Translate》</a> 中提出了「Attention」机制，下面请跟着麦克船长，船长会深入浅出地为你解释清楚。</p>

<p>下图中  \(e_i\)  表示编码器的隐藏层输出， \(d_i\)  表示解码器的隐藏层输出</p>

<div style="text-align: center;">
<div class="graphviz-wrapper">

<!-- Generated by graphviz version 2.43.0 (0)
 -->
<!-- Title: G Pages: 1 -->
<svg role="img" aria-label="graphviz-f66c634a9c7c02915e5610af76c3b1b7" width="436pt" height="336pt" viewBox="0.00 0.00 436.00 336.00">
<title>graphviz-f66c634a9c7c02915e5610af76c3b1b7</title>
<desc>
digraph G {
	rankdir=BT
	splines=ortho
	{rank=same e1 e2 eddd en}
	{rank=same d1 d2 dddd dt0 dt dddd2}

	eddd[label=&quot;...&quot;]
	dddd[label=&quot;...&quot;]
	xddd[label=&quot;...&quot;]
	yddd[label=&quot;...&quot;]
	dt[label=&quot;d_t&quot;]
	dt0[label=&quot;d_t-1&quot;]
	yt[label=&quot;y_t&quot;]
	yt0[label=&quot;y_t-1&quot;]
	Ct[shape=plaintext]
	x1[shape=plaintext]
	x2[shape=plaintext]
	xddd[shape=plaintext]
	xn[shape=plaintext]
	y1[shape=plaintext]
	y2[shape=plaintext]
	yddd[shape=plaintext]
	dddd2[shape=plaintext, label=&quot;&quot;]
	Ct[label=&quot;C_t&quot;, shape=&quot;square&quot;]

	x1 -&gt; e1
	x2 -&gt; e2
	xddd -&gt; eddd
	xn -&gt; en

	e1 -&gt; e2
	e2 -&gt; eddd
	eddd -&gt; en

	Ct -&gt; dt

	d1 -&gt; y1
	d2 -&gt; y2
	dddd -&gt; yddd
	dt0 -&gt; yt0
	dt -&gt; yt

	d1 -&gt; d2
	d2 -&gt; dddd
	dddd -&gt; dt0
	dt0 -&gt; dt

	e1 -&gt; Ct
	e2 -&gt; Ct
	eddd -&gt; Ct
	en -&gt; Ct

	dt -&gt; dddd2
	dt0 -&gt; Ct
}
</desc>

<g id="graph0" class="graph" transform="scale(1 1) rotate(0) translate(4 332)">
<title>G</title>
<polygon fill="white" stroke="transparent" points="-4,4 -4,-332 432,-332 432,4 -4,4" />
<!-- e1 -->
<g id="node1" class="node">
<title>e1</title>
<ellipse fill="none" stroke="black" cx="181" cy="-90" rx="27" ry="18" />
<text text-anchor="middle" x="181" y="-86.3" font-family="Times,serif" font-size="14.00">e1</text>
</g>
<!-- e2 -->
<g id="node2" class="node">
<title>e2</title>
<ellipse fill="none" stroke="black" cx="253" cy="-90" rx="27" ry="18" />
<text text-anchor="middle" x="253" y="-86.3" font-family="Times,serif" font-size="14.00">e2</text>
</g>
<!-- e1&#45;&gt;e2 -->
<g id="edge5" class="edge">
<title>e1&#45;&gt;e2</title>
<path fill="none" stroke="black" d="M208.22,-90C208.22,-90 215.74,-90 215.74,-90" />
<polygon fill="black" stroke="black" points="215.74,-93.5 225.74,-90 215.74,-86.5 215.74,-93.5" />
</g>
<!-- Ct -->
<g id="node15" class="node">
<title>Ct</title>
<polygon fill="none" stroke="black" points="309,-184 269,-184 269,-144 309,-144 309,-184" />
<text text-anchor="middle" x="289" y="-160.3" font-family="Times,serif" font-size="14.00">C_t</text>
</g>
<!-- e1&#45;&gt;Ct -->
<g id="edge18" class="edge">
<title>e1&#45;&gt;Ct</title>
<path fill="none" stroke="black" d="M203,-100.6C203,-121.06 203,-164 203,-164 203,-164 258.62,-164 258.62,-164" />
<polygon fill="black" stroke="black" points="258.62,-167.5 268.62,-164 258.62,-160.5 258.62,-167.5" />
</g>
<!-- eddd -->
<g id="node3" class="node">
<title>eddd</title>
<ellipse fill="none" stroke="black" cx="325" cy="-90" rx="27" ry="18" />
<text text-anchor="middle" x="325" y="-86.3" font-family="Times,serif" font-size="14.00">...</text>
</g>
<!-- e2&#45;&gt;eddd -->
<g id="edge6" class="edge">
<title>e2&#45;&gt;eddd</title>
<path fill="none" stroke="black" d="M280.22,-90C280.22,-90 287.74,-90 287.74,-90" />
<polygon fill="black" stroke="black" points="287.74,-93.5 297.74,-90 287.74,-86.5 287.74,-93.5" />
</g>
<!-- e2&#45;&gt;Ct -->
<g id="edge19" class="edge">
<title>e2&#45;&gt;Ct</title>
<path fill="none" stroke="black" d="M274.5,-100.92C274.5,-100.92 274.5,-133.82 274.5,-133.82" />
<polygon fill="black" stroke="black" points="271,-133.82 274.5,-143.82 278,-133.82 271,-133.82" />
</g>
<!-- en -->
<g id="node4" class="node">
<title>en</title>
<ellipse fill="none" stroke="black" cx="397" cy="-90" rx="27" ry="18" />
<text text-anchor="middle" x="397" y="-86.3" font-family="Times,serif" font-size="14.00">en</text>
</g>
<!-- eddd&#45;&gt;en -->
<g id="edge7" class="edge">
<title>eddd&#45;&gt;en</title>
<path fill="none" stroke="black" d="M352.22,-90C352.22,-90 359.74,-90 359.74,-90" />
<polygon fill="black" stroke="black" points="359.74,-93.5 369.74,-90 359.74,-86.5 359.74,-93.5" />
</g>
<!-- eddd&#45;&gt;Ct -->
<g id="edge20" class="edge">
<title>eddd&#45;&gt;Ct</title>
<path fill="none" stroke="black" d="M303.5,-100.92C303.5,-100.92 303.5,-133.82 303.5,-133.82" />
<polygon fill="black" stroke="black" points="300,-133.82 303.5,-143.82 307,-133.82 300,-133.82" />
</g>
<!-- en&#45;&gt;Ct -->
<g id="edge21" class="edge">
<title>en&#45;&gt;Ct</title>
<path fill="none" stroke="black" d="M399,-108.29C399,-130.21 399,-164 399,-164 399,-164 319.18,-164 319.18,-164" />
<polygon fill="black" stroke="black" points="319.18,-160.5 309.18,-164 319.18,-167.5 319.18,-160.5" />
</g>
<!-- d1 -->
<g id="node5" class="node">
<title>d1</title>
<ellipse fill="none" stroke="black" cx="27" cy="-238" rx="27" ry="18" />
<text text-anchor="middle" x="27" y="-234.3" font-family="Times,serif" font-size="14.00">d1</text>
</g>
<!-- d2 -->
<g id="node6" class="node">
<title>d2</title>
<ellipse fill="none" stroke="black" cx="99" cy="-238" rx="27" ry="18" />
<text text-anchor="middle" x="99" y="-234.3" font-family="Times,serif" font-size="14.00">d2</text>
</g>
<!-- d1&#45;&gt;d2 -->
<g id="edge14" class="edge">
<title>d1&#45;&gt;d2</title>
<path fill="none" stroke="black" d="M54.22,-238C54.22,-238 61.74,-238 61.74,-238" />
<polygon fill="black" stroke="black" points="61.74,-241.5 71.74,-238 61.74,-234.5 61.74,-241.5" />
</g>
<!-- y1 -->
<g id="node19" class="node">
<title>y1</title>
<text text-anchor="middle" x="27" y="-306.3" font-family="Times,serif" font-size="14.00">y1</text>
</g>
<!-- d1&#45;&gt;y1 -->
<g id="edge9" class="edge">
<title>d1&#45;&gt;y1</title>
<path fill="none" stroke="black" d="M27,-256.17C27,-256.17 27,-281.59 27,-281.59" />
<polygon fill="black" stroke="black" points="23.5,-281.59 27,-291.59 30.5,-281.59 23.5,-281.59" />
</g>
<!-- dddd -->
<g id="node7" class="node">
<title>dddd</title>
<ellipse fill="none" stroke="black" cx="171" cy="-238" rx="27" ry="18" />
<text text-anchor="middle" x="171" y="-234.3" font-family="Times,serif" font-size="14.00">...</text>
</g>
<!-- d2&#45;&gt;dddd -->
<g id="edge15" class="edge">
<title>d2&#45;&gt;dddd</title>
<path fill="none" stroke="black" d="M126.22,-238C126.22,-238 133.74,-238 133.74,-238" />
<polygon fill="black" stroke="black" points="133.74,-241.5 143.74,-238 133.74,-234.5 133.74,-241.5" />
</g>
<!-- y2 -->
<g id="node20" class="node">
<title>y2</title>
<text text-anchor="middle" x="99" y="-306.3" font-family="Times,serif" font-size="14.00">y2</text>
</g>
<!-- d2&#45;&gt;y2 -->
<g id="edge10" class="edge">
<title>d2&#45;&gt;y2</title>
<path fill="none" stroke="black" d="M99,-256.17C99,-256.17 99,-281.59 99,-281.59" />
<polygon fill="black" stroke="black" points="95.5,-281.59 99,-291.59 102.5,-281.59 95.5,-281.59" />
</g>
<!-- dt0 -->
<g id="node8" class="node">
<title>dt0</title>
<ellipse fill="none" stroke="black" cx="250" cy="-238" rx="33.6" ry="18" />
<text text-anchor="middle" x="250" y="-234.3" font-family="Times,serif" font-size="14.00">d_t&#45;1</text>
</g>
<!-- dddd&#45;&gt;dt0 -->
<g id="edge16" class="edge">
<title>dddd&#45;&gt;dt0</title>
<path fill="none" stroke="black" d="M198.19,-238C198.19,-238 206.2,-238 206.2,-238" />
<polygon fill="black" stroke="black" points="206.2,-241.5 216.2,-238 206.2,-234.5 206.2,-241.5" />
</g>
<!-- yddd -->
<g id="node12" class="node">
<title>yddd</title>
<text text-anchor="middle" x="171" y="-306.3" font-family="Times,serif" font-size="14.00">...</text>
</g>
<!-- dddd&#45;&gt;yddd -->
<g id="edge11" class="edge">
<title>dddd&#45;&gt;yddd</title>
<path fill="none" stroke="black" d="M171,-256.17C171,-256.17 171,-281.59 171,-281.59" />
<polygon fill="black" stroke="black" points="167.5,-281.59 171,-291.59 174.5,-281.59 167.5,-281.59" />
</g>
<!-- dt -->
<g id="node9" class="node">
<title>dt</title>
<ellipse fill="none" stroke="black" cx="329" cy="-238" rx="27" ry="18" />
<text text-anchor="middle" x="329" y="-234.3" font-family="Times,serif" font-size="14.00">d_t</text>
</g>
<!-- dt0&#45;&gt;dt -->
<g id="edge17" class="edge">
<title>dt0&#45;&gt;dt</title>
<path fill="none" stroke="black" d="M283.96,-238C283.96,-238 291.98,-238 291.98,-238" />
<polygon fill="black" stroke="black" points="291.98,-241.5 301.98,-238 291.98,-234.5 291.98,-241.5" />
</g>
<!-- yt0 -->
<g id="node14" class="node">
<title>yt0</title>
<ellipse fill="none" stroke="black" cx="250" cy="-310" rx="33.29" ry="18" />
<text text-anchor="middle" x="250" y="-306.3" font-family="Times,serif" font-size="14.00">y_t&#45;1</text>
</g>
<!-- dt0&#45;&gt;yt0 -->
<g id="edge12" class="edge">
<title>dt0&#45;&gt;yt0</title>
<path fill="none" stroke="black" d="M250,-256.17C250,-256.17 250,-281.59 250,-281.59" />
<polygon fill="black" stroke="black" points="246.5,-281.59 250,-291.59 253.5,-281.59 246.5,-281.59" />
</g>
<!-- dt0&#45;&gt;Ct -->
<g id="edge23" class="edge">
<title>dt0&#45;&gt;Ct</title>
<path fill="none" stroke="black" d="M276.4,-226.44C276.4,-226.44 276.4,-194.12 276.4,-194.12" />
<polygon fill="black" stroke="black" points="279.9,-194.12 276.4,-184.12 272.9,-194.12 279.9,-194.12" />
</g>
<!-- dddd2 -->
<g id="node10" class="node">
<title>dddd2</title>
</g>
<!-- dt&#45;&gt;dddd2 -->
<g id="edge22" class="edge">
<title>dt&#45;&gt;dddd2</title>
<path fill="none" stroke="black" d="M356.22,-238C356.22,-238 363.74,-238 363.74,-238" />
<polygon fill="black" stroke="black" points="363.74,-241.5 373.74,-238 363.74,-234.5 363.74,-241.5" />
</g>
<!-- yt -->
<g id="node13" class="node">
<title>yt</title>
<ellipse fill="none" stroke="black" cx="329" cy="-310" rx="27" ry="18" />
<text text-anchor="middle" x="329" y="-306.3" font-family="Times,serif" font-size="14.00">y_t</text>
</g>
<!-- dt&#45;&gt;yt -->
<g id="edge13" class="edge">
<title>dt&#45;&gt;yt</title>
<path fill="none" stroke="black" d="M329,-256.17C329,-256.17 329,-281.59 329,-281.59" />
<polygon fill="black" stroke="black" points="325.5,-281.59 329,-291.59 332.5,-281.59 325.5,-281.59" />
</g>
<!-- xddd -->
<g id="node11" class="node">
<title>xddd</title>
<text text-anchor="middle" x="325" y="-14.3" font-family="Times,serif" font-size="14.00">...</text>
</g>
<!-- xddd&#45;&gt;eddd -->
<g id="edge3" class="edge">
<title>xddd&#45;&gt;eddd</title>
<path fill="none" stroke="black" d="M325,-36.17C325,-36.17 325,-61.59 325,-61.59" />
<polygon fill="black" stroke="black" points="321.5,-61.59 325,-71.59 328.5,-61.59 321.5,-61.59" />
</g>
<!-- Ct&#45;&gt;dt -->
<g id="edge8" class="edge">
<title>Ct&#45;&gt;dt</title>
<path fill="none" stroke="black" d="M305.5,-184.22C305.5,-184.22 305.5,-218.8 305.5,-218.8" />
<polygon fill="black" stroke="black" points="302,-218.8 305.5,-228.8 309,-218.8 302,-218.8" />
</g>
<!-- x1 -->
<g id="node16" class="node">
<title>x1</title>
<text text-anchor="middle" x="181" y="-14.3" font-family="Times,serif" font-size="14.00">x1</text>
</g>
<!-- x1&#45;&gt;e1 -->
<g id="edge1" class="edge">
<title>x1&#45;&gt;e1</title>
<path fill="none" stroke="black" d="M181,-36.17C181,-36.17 181,-61.59 181,-61.59" />
<polygon fill="black" stroke="black" points="177.5,-61.59 181,-71.59 184.5,-61.59 177.5,-61.59" />
</g>
<!-- x2 -->
<g id="node17" class="node">
<title>x2</title>
<text text-anchor="middle" x="253" y="-14.3" font-family="Times,serif" font-size="14.00">x2</text>
</g>
<!-- x2&#45;&gt;e2 -->
<g id="edge2" class="edge">
<title>x2&#45;&gt;e2</title>
<path fill="none" stroke="black" d="M253,-36.17C253,-36.17 253,-61.59 253,-61.59" />
<polygon fill="black" stroke="black" points="249.5,-61.59 253,-71.59 256.5,-61.59 249.5,-61.59" />
</g>
<!-- xn -->
<g id="node18" class="node">
<title>xn</title>
<text text-anchor="middle" x="397" y="-14.3" font-family="Times,serif" font-size="14.00">xn</text>
</g>
<!-- xn&#45;&gt;en -->
<g id="edge4" class="edge">
<title>xn&#45;&gt;en</title>
<path fill="none" stroke="black" d="M397,-36.17C397,-36.17 397,-61.59 397,-61.59" />
<polygon fill="black" stroke="black" points="393.5,-61.59 397,-71.59 400.5,-61.59 393.5,-61.59" />
</g>
</g>
</svg>
</div>
</div>

<p>更进一步细化关于  \(\bm{C}_t\)  部分，船长在此引用《基于深度学习的道路短期交通状态时空序列预测》一书中的图：</p>

<p><img src="/img/src/2023-01-04-captain-nlp-5.png" alt="image" /></p>

<p>这个图里的  \(\widetilde{h}_i\)  与上一个图里的  \(d_i\)  对应， \(h_i\)  与上一个图里的  \(e_i\)  对应。</p>

<p>针对时刻  \(t\)  要产出的输出，隐藏层每一个隐藏细胞都与  \(\bm{C}_t\)  有一个权重关系  \(\alpha_{t,i}\)  其中  \(1\le i\le n\) ，这个权重值与「输入项经过编码器后隐藏层后的输出 \(e_i（1\le i\le n）\) 、解码器的前一时刻隐藏层输出  \(d_{t-1}\) 」两者有关：</p>

\[\begin{aligned}
&amp;s_{i,t} = score(\bm{e}_i,\bm{d}_{t-1}) \\
&amp;\alpha_{i,t} = \frac{exp(s_{i,t})}{\textstyle\sum_{j=1}^n exp(s_{j,t})}
\end{aligned}\]

<p>常用的  \(score\)  函数有：</p>

<ul>
  <li>点积（Dot Product）模型： \(s_{i,t} = {\bm{d}_{t-1}}^T \cdot \bm{e}_i\)</li>
  <li>缩放点积（Scaled Dot-Product）模型： \(s_{i,t} = \frac{{\bm{d}_{t-1}}^T \cdot \bm{e}_i}{\sqrt{\smash[b]{dimensions\:of\:d_{t-1}\:or\:e_i}}}\) ，可避免因为向量维度过大导致点积结果太大</li>
</ul>

<p>然后上下文向量就表示成：</p>

\[\begin{aligned}
&amp;\bm{C}_t = \displaystyle\sum_{i=1}^n \alpha_{i,t} \bm{e}_i
\end{aligned}\]

<p>还记得 RNN 那部分里船长讲到的 Encoder-Decoder 模型的公式表示吗？</p>

\[\begin{aligned}
e_t &amp;= Encoder_{LSTM/GRU}(x_t, e_{t-1}) \\
\bm{C} &amp;= f_1(e_n) \\
d_t &amp;= f_2(d_{t-1}, \bm{C}) \\
y_t &amp;= Decoder_{LSTM/GRU}(y_{t-1}, d_{t-1}, \bm{C})
\end{aligned}\]

<p>加入 Attention 机制的 Encoder-Decoder 模型如下。</p>

\[\begin{aligned}
e_t &amp;= Encoder_{LSTM/GRU}(x_t, e_{t-1}) \\
\bm{C}_t &amp;= f_1(e_1,e_2...e_n,d_{t-1}) \\
d_t &amp;= f_2(d_{t-1}, \bm{C}_t) \\
y_t &amp;= Decoder_{LSTM/GRU}(y_{t-1}, d_{t-1}, \bm{C}_t)
\end{aligned}\]

<p>这种同时考虑 Encoder、Decoder 的 Attention，就叫做「Encoder-Decoder Attention」，也常被叫做「Vanilla Attention」。可以看到上面最核心的区别是第二个公式  \(C_t\) 。加入 Attention 后，对所有数据给予不同的注意力分布。具体地，比如我们用如下的函数来定义这个模型：</p>

\[\begin{aligned}
\bm{e} &amp;= tanh(\bm{W}^{xe} \cdot \bm{x} + \bm{b}^{xe}) \\
s_{i,t} &amp;= score(\bm{e}_i,\bm{d}_{t-1}) \\
\alpha_{i,t} &amp;= \frac{e^{s_{i,t}}}{\textstyle\sum_{j=1}^n e^{s_{j,t}}} \\
\bm{C}_t &amp;= \displaystyle\sum_{i=1}^n \alpha_{i,t} \bm{e}_i \\
\bm{d}_t &amp;= tanh(\bm{W}^{dd} \cdot \bm{d}_{t-1} + \bm{b}^{dd} +
				 \bm{W}^{yd} \cdot \bm{y}_{t-1} + \bm{b}^{yd} +
				 \bm{W}^{cd} \cdot \bm{C}_t + \bm{b}^{cd}) \\
\bm{y} &amp;= Softmax(\bm{W}^{dy} \cdot \bm{d} + \bm{b}^{dy})
\end{aligned}\]

<p>到这里你能发现注意力机制的什么问题不？</p>

<ul>
  <li>这个注意力机制忽略了位置信息。比如 Tigers love rabbits 和 Rabbits love tigers 会产生一样的注意力分数。</li>
</ul>

<h2 id="第二章--transformer-在-2017-年横空出世">第二章 · Transformer 在 2017 年横空出世</h2>

<p>船长先通过一个动画来看下 Transformer 是举例示意，该图来自 Google 的博客文章 <a href="https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html">《Transformer: A Novel Neural Network Architecture for Language Understanding》</a>：</p>

<p><img src="/img/src/2023-01-04-language-model-5-11.gif" alt="image" /></p>

<p>中文网络里找到的解释得比较好的 blogs、answers，几乎都指向了同一篇博客：Jay Alammar 的<a href="http://jalammar.github.io/illustrated-transformer/">《The Illustrated Transformer》</a>，所以建议读者搭配该篇文章阅读。</p>

<p>Transformer 模型中用到了自注意力（Self-Attention）、多头注意力（Multiple-Head Attention）、残差网络（ResNet）与捷径（Short-Cut）。下面我们先通过第 1 到第 4 小节把几个基本概念讲清楚，然后在第 5 小节讲解整体 Transformer 模型就会好理解很多了。最后第 6 小节我们来一段动手实践。</p>

<h3 id="第-7-节--自注意力机制self-attention">第 7 节 · 自注意力机制（Self-Attention）</h3>

<p>自注意力是理解 Transformer 的关键，原作者在论文中限于篇幅，没有给出过多的解释。以下是我自己的理解，能够比较通透、符合常识地去理解 Transformer 中的一些神来之笔的概念。</p>

<h4 id="71一段自然语言内容其自身就暗含很多内部关联信息">7.1、一段自然语言内容，其自身就「暗含」很多内部关联信息</h4>

<p>在加入了 Attention 的 Encoder-Decoder 模型中，对输出序列 Y 中的一个词的注意力来自于输入序列 X，那么如果 X 和 Y 相等呢？什么场景会有这个需求？因为我们认为一段文字里某些词就是由于另外某些词而决定的，可以粗暴地理解为「完形填空」的原理。那么这样一段文字，其实就存在其中每个词的自注意力，举个例子：</p>

<blockquote>
  <p>老王是我的主管，我很喜欢他的平易近人。</p>
</blockquote>

<p>对这句话里的「他」，如果基于这句话计算自注意力的话，显然应该给予「老王」最多的注意力。受此启发，我们认为：</p>

<blockquote>
  <p>一段自然语言中，其实暗含了：为了得到关于某方面信息 Q，可以通过关注某些信息 K，进而得到某些信息（V）作为结果。</p>
</blockquote>

<p>Q 就是 query 检索/查询，K、V 分别是 key、value。所以类似于我们在图书检索系统里搜索「NLP书籍」（这是 Q），得到了一本叫《自然语言处理实战》的电子书，书名就是 key，这本电子书就是 value。只是对于自然语言的理解，我们认为任何一段内容里，都自身暗含了很多潜在 Q-K-V 的关联。这是整体受到信息检索领域里 query-key-value 的启发的。</p>

<p>基于这个启发，我们将自注意力的公式表示为：</p>

\[\begin{aligned}
Z = SelfAttention(X) = Attention(Q,K,V)
\end{aligned}\]

<p>X 经过自注意力计算后，得到的「暗含」了大量原数据内部信息的 Z。然后我们拿着这个带有自注意力信息的 Z 进行后续的操作。这里要强调的是，Z 向量中的每个元素 z_i 都与 X 的所有元素有某种关联，而不是只与 x_i 有关联。</p>

<h4 id="72如何计算-qkv">7.2、如何计算 Q、K、V</h4>

<p>Q、K、V 全部来自输入 X 的线性变换：</p>

\[\begin{aligned}
Q &amp;= W^Q \cdot X \\
K &amp;= W^K \cdot X \\
V &amp;= W^V \cdot X
\end{aligned}\]

<p>\(W^Q、W^K、W^V\)  以随机初始化开始，经过训练就会得到非常好的表现。对于  \(X\)  中的每一个词向量  \(x_i\) ，经过这个变换后得到了：</p>

\[\begin{aligned}
q_i &amp;= W^Q \cdot x_i \\
k_i &amp;= W^K \cdot x_i \\
v_i &amp;= W^V \cdot x_i
\end{aligned}\]

<h4 id="73注意力函数如何通过-qv-得到-z">7.3、注意力函数：如何通过 Q、V 得到 Z</h4>

<p>基于上面的启发，我们认为 X 经过自注意力的挖掘后，得到了：</p>

<ul>
  <li>暗含信息 1：一组 query 与一组 key 之间的关联，记作 qk（想一下信息检索系统要用 query 先招到 key）</li>
  <li>暗含信息 2：一组 value</li>
  <li>暗含信息 3：qk 与 value 的某种关联</li>
</ul>

<p>这三组信息，分别如何表示呢？这里又需要一些启发了，因为计算机科学其实是在「模拟还原」现实世界，在 AI 的领域目前的研究方向就是模拟还原人脑的思考。所以这种「模拟还原」都是寻找某一种近似方法，因此不能按照数学、物理的逻辑推理来理解，而应该按照「工程」或者「计算科学」来理解，想想我们大学时学的「计算方法」这门课，因此常需要一些启发来找到某种「表示」。</p>

<p>这里 Transformer 的作者，认为  \(Q\)  和  \(K\)  两个向量之间的关联，是我们在用  \(Q\)  找其在  \(K\)  上的投影，如果  \(Q\) 、 \(K\)  是单位长度的向量，那么这个投影其实可以理解为找「 \(Q\)  和  \(K\)  向量之间的相似度」：</p>

<ul>
  <li>如果  \(Q\)  和  \(K\)  垂直，那么两个向量正交，其点积（Dot Product）为 0；</li>
  <li>如果  \(Q\)  和  \(K\)  平行，那么两个向量点积为两者模积  \(\|Q\|\|K\|\) ；</li>
  <li>如果  \(Q\)  和  \(K\)  呈某个夹角，则点积就是  \(Q\)  在  \(K\)  上的投影的模。</li>
</ul>

<p>因此「暗含信息 1」就可以用「 \(Q\cdot K\) 」再经过 Softmax 归一化来表示。这个表示，是一个所有元素都是 0~1 的矩阵，可以理解成对应注意力机制里的「注意力分数」，也就是一个「注意力分数矩阵（Attention Score Matrix）」。</p>

<p>而「暗含信息 2」则是输入  \(X\)  经过的线性变换后的特征，看做  \(X\)  的另一种表示。然后我们用这个「注意力分数矩阵」来加持一下  \(V\) ，这个点积过程就表示了「暗含信息 3」了。所以我们有了如下公式：</p>

\[\begin{aligned}
Z = Attention(Q,K,V) = Softmax(Q \cdot K^T) \cdot V
\end{aligned}\]

<p>其实到这里，这个注意力函数已经可以用了。有时候，为了避免因为向量维度过大，导致  \(Q \cdot K^T\)  点积结果过大，我们再加一步处理：</p>

\[\begin{aligned}
Z = Attention(Q,K,V) = Softmax(\frac{Q \cdot K^T}{\sqrt{\smash[b]{d_k}}}) \cdot V
\end{aligned}\]

<p>这里  \(d_k\)  是 K 矩阵中向量  \(k_i\)  的维度。这一步修正还有进一步的解释，即如果经过 Softmax 归一化后模型稳定性存在问题。怎么理解？如果假设 Q 和 K 中的每个向量的每一维数据都具有零均值、单位方差，这样输入数据是具有稳定性的，那么如何让「暗含信息 1」计算后仍然具有稳定性呢？即运算结果依然保持零均值、单位方差，就是除以「 \(\sqrt{\smash[b]{d_k}}\) 」。</p>

<p>到这里我们注意到：</p>

<ul>
  <li>K、V 里的每一个向量，都是</li>
</ul>

<h4 id="74其他注意力函数">7.4、其他注意力函数</h4>

<p>为了提醒大家这种暗含信息的表示，都只是计算方法上的一种选择，好坏全靠结果评定，所以包括上面的在内，常见的注意力函数有（甚至你也可以自己定义）：</p>

\[Z = Attention(Q,K,V) =
\begin{cases}
\begin{aligned}
&amp;= Softmax(Q^T K) V \\
&amp;= Softmax(\frac{Q K^T}{\sqrt{\smash[b]{d_k}}}) V \\
&amp;= Softmax(\omega^T tanh(W[q;k])) V \\
&amp;= Softmax(Q^T W K) V \\
&amp;= cosine[Q^T K] V
\end{aligned}
\end{cases}\]

<p>到这里，我们就从原始的输入  \(X\)  得到了一个包含自注意力信息的  \(Z\)  了，后续就可以用  \(Z\)  了。</p>

<h3 id="第-8-节--多头注意力">第 8 节 · 多头注意力</h3>

<p>到这里我们理解了「自注意力」，而 Transformer 这篇论文通过添加「多头」注意力的机制进一步提升了注意力层。我们先看下它是什么，然后看下它的优点。从本小节开始，本文大量插图引用自<a href="http://jalammar.github.io/illustrated-transformer/">《The Illustrated Transformer》</a>，作者 Jay Alammar 写出一篇非常深入浅出的图解文章，被大量引用，非常出色，再次建议大家去阅读。</p>

<p>Transformer 中用了 8 个头，也就是 8 组不同的 Q-K-V：</p>

\[\begin{aligned}
Q_0 = W_0^Q \cdot X ;\enspace K_0 = &amp;W_0^K \cdot X ;\enspace V_0 = W_0^V \cdot X \\
Q_1 = W_1^Q \cdot X ;\enspace K_1 = &amp;W_0^K \cdot X ;\enspace V_1 = W_1^V \cdot X \\
&amp;.... \\
Q_7 = W_7^Q \cdot X ;\enspace K_7 = &amp;W_0^K \cdot X ;\enspace V_7 = W_7^V \cdot X
\end{aligned}\]

<p>这样我们就能得到 8 个 Z：</p>

\[\begin{aligned}
&amp;Z_0 = Attention(Q_0,K_0,V_0) = Softmax(\frac{Q_0 \cdot K_0^T}{\sqrt{\smash[b]{d_k}}}) \cdot V_0 \\
&amp;Z_1 = Attention(Q_1,K_1,V_1) = Softmax(\frac{Q_1 \cdot K_1^T}{\sqrt{\smash[b]{d_k}}}) \cdot V_1 \\
&amp;... \\
&amp;Z_7 = Attention(Q_7,K_7,V_7) = Softmax(\frac{Q_7 \cdot K_7^T}{\sqrt{\smash[b]{d_k}}}) \cdot V_7 \\
\end{aligned}\]

<p>然后我们把  \(Z_0\)  到  \(Z_7\)  沿着行数不变的方向全部连接起来，如下图所示：</p>

<p><img src="/img/src/2023-01-04-language-model-5-3.png" alt="image" width="464" /></p>

<p>我们再训练一个权重矩阵  \(W^O\) ，然后用上面拼接的  \(Z_{0-7}\)  乘以这个权重矩阵：</p>

<p><img src="/img/src/2023-01-04-language-model-5-4.png" alt="image" width="135" /></p>

<p>于是我们会得到一个 Z 矩阵：</p>

<p><img src="/img/src/2023-01-04-language-model-5-5.png" alt="image" width="100" /></p>

<p>到这里就是多头注意力机制的全部内容，与单头注意力相比，都是为了得到一个 Z 矩阵，但是多头用了多组 Q-K-V，然后经过拼接、乘以权重矩阵得到最后的 Z。我们总览一下整个过程：</p>

<p><img src="/img/src/2023-01-04-language-model-5-6.png" alt="image" width="935" /></p>

<p>通过多头注意力，每个头都会关注到不同的信息，可以如下类似表示：</p>

<p><img src="/img/src/2023-01-04-language-model-5-7.png" alt="image" width="400" /></p>

<p>这通过两种方式提高了注意力层的性能：</p>

<ul>
  <li>多头注意力机制，扩展了模型关注不同位置的能力。 \(Z\)  矩阵中的每个向量  \(z_i\)  包含了与  \(X\)  中所有向量  \(x_i\)  有关的一点编码信息。反过来说，不要认为  \(z_i\)  只与  \(x_i\)  有关。</li>
  <li>多头注意力机制，为注意力层提供了多个「表示子空间 Q-K-V」，以及 Z。这样一个输入矩阵  \(X\) ，就会被表示成 8 种不同的矩阵 Z，都包含了原始数据信息的某种解读暗含其中。</li>
</ul>

<h3 id="第-9-节--退化现象残差网络与-short-cut">第 9 节 · 退化现象、残差网络与 Short-Cut</h3>

<h4 id="91退化现象">9.1、退化现象</h4>

<p>对于一个 56 层的神经网路，我们很自然地会觉得应该比 20 层的神经网络的效果要好，比如说从误差率（error）的量化角度看。但是华人学者何凯明等人的论文<a href="https://arxiv.org/pdf/1512.03385.pdf">《Deep Residual Learning for Image Recognition》</a>中给我们呈现了相反的结果，而这个问题的原因并不是因为层数多带来的梯度爆炸/梯度消失（毕竟已经用了归一化解决了这个问题），而是因为一种反常的现象，这种现象我们称之为「退化现象」。何凯明等人认为这是因为存在「难以优化好的网络层」。</p>

<h4 id="92恒等映射">9.2、恒等映射</h4>

<p>如果这 36 层还帮了倒忙，那还不如没有，是不是？所以这多出来的 36 个网络层，如果对于提升性能（例如误差率）毫无影响，甚至更进一步，这 36 层前的输入数据，和经过这 36 层后的输出数据，完全相同，那么如果将这 36 层抽象成一个函数  \(f_{36}\) ，这就是一个恒等映射的函数：</p>

\[f_{36}(x) = x\]

<p>回到实际应用中。如果我们对于一个神经网络中的连续 N 层是提升性能，还是降低性能，是未知的，那么则可以建立一个跳过这些层的连接，实现：</p>

<blockquote>
  <p>如果这 N 层可以提升性能，则采用这 N 层；否则就跳过。</p>
</blockquote>

<p>这就像给了这 N 层神经网络一个试错的空间，待我们确认它们的性能后再决定是否采用它们。同时也可以理解成，这些层可以去单独优化，如果性能提升，则不被跳过。</p>

<h4 id="93残差网络residual-network与捷径short-cut">9.3、残差网络（Residual Network）与捷径（Short-Cut）</h4>

<p>如果前面 20 层已经可以实现 99% 的准确率，那么引入了这 36 层能否再提升「残差剩余那 1%」的准确率从而达到 100% 呢？所以这 36 层的网络，就被称为「残差网络（Residual Network，常简称为 ResNet）」，这个叫法非常形象。</p>

<p>而那个可以跳过 N 层残差网络的捷径，则常被称为 Short-Cut，也会被叫做跳跃链接（Skip Conntection），这就解决了上述深度学习中的「退化现象」。</p>

<h3 id="第-10-节--transformer-的位置编码positional-embedding">第 10 节 · Transformer 的位置编码（Positional Embedding）</h3>

<p>还记得我在第二部分最后提到的吗：</p>

<blockquote>
  <p>这个注意力机制忽略了位置信息。比如 Tigers love rabbits 和 Rabbits love tigers 会产生一样的注意力分数。</p>
</blockquote>

<h4 id="101transformer-论文中的三角式位置编码sinusoidal-positional-encoding">10.1、Transformer 论文中的三角式位置编码（Sinusoidal Positional Encoding）</h4>

<p>现在我们来解决这个问题，为每一个输入向量  \(x_i\)  生成一个位置编码向量  \(t_i\) ，这个位置编码向量的维度，与输入向量（词的嵌入式向量表示）的维度是相同的：</p>

<p><img src="/img/src/2023-01-04-language-model-5-8.png" alt="image" width="500" /></p>

<p>Transformer 论文中给出了如下的公式，来计算位置编码向量的每一位的值：</p>

\[\begin{aligned}
P_{pos,2i} &amp;= sin(\frac{pos}{10000^{\frac{2i}{d_{model}}}}) \\
P_{pos,2i+1} &amp;= cos(\frac{pos}{10000^{\frac{2i}{d_{model}}}})
\end{aligned}\]

<p>这样对于一个 embedding，如果它在输入内容中的位置是 pos，那么其编码向量就表示为：</p>

\[\begin{aligned}
[P_{pos,0}, P_{pos,1}, ... , P_{pos,d_x-1}]
\end{aligned}\]

<p>延展开的话，位置编码其实还分为绝对位置编码（Absolute Positional Encoding）、相对位置编码（Relative Positional Encoding）。前者是专门生成位置编码，并想办法融入到输入中，我们上面看到的就是一种。后者是微调 Attention 结构，使得它可以分辨不同位置的数据。另外其实还有一些无法分类到这两种的位置编码方法。</p>

<h4 id="102绝对位置编码">10.2、绝对位置编码</h4>

<p>绝对位置编码，如上面提到的，就是定义一个位置编码向量  \(t_i\) ，通过  \(x_i + t_i\)  就得到了一个含有位置信息的向量。</p>

<ul>
  <li>习得式位置编码（Learned Positional Encoding）：将位置编码当做训练参数，生成一个「最大长度 x 编码维度」的位置编码矩阵，随着训练进行更新。目前 Google BERT、OpenAI GPT 模型都是用的这种位置编码。缺点是「外推性」差，如果文本长度超过之前训练时用的「最大长度」则无法处理。目前有一些给出优化方案的论文，比如「<a href="https://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&amp;mid=2247515573&amp;idx=1&amp;sn=2d719108244ada7db3a535a435631210&amp;chksm=96ea6235a19deb23babde5eaac484d69e4c2f53bab72d2e350f75bed18323eea3cf9be30615b#rd">层次分解位置编码</a>」。</li>
  <li>三角式位置编码（Sinusoidal Positional Encodign）：上面提过了。</li>
  <li>循环式位置编码（Recurrent Positional Encoding）：通过一个 RNN 再接一个 Transformer，那么 RNN 暗含的「顺序」就导致不再需要额外编码了。但这样牺牲了并行性，毕竟 RNN 的两大缺点之一就有这个。</li>
  <li>相乘式位置编码（Product Positional Encoding）：用「 \(x_i \odot t_i\) 」代替「 \(x_i + t_i\) 」。</li>
</ul>

<h4 id="103相对位置编码和其他位置编码">10.3、相对位置编码和其他位置编码</h4>

<p>最早来自于 Google 的论文<a href="https://arxiv.org/abs/1803.02155">《Self-Attention with Relative Position Representations》</a>相对位置编码，考虑的是当前 position 与被 attention 的 position 之前的相对位置。</p>

<ul>
  <li>常见相对位置编码：经典式、XLNET 式、T5 式、DeBERTa 式等。</li>
  <li>其他位置编码：CNN 式、复数式、融合式等。</li>
</ul>

<p>到此我们都是在讲 Encoder，目前我们知道一个 Encoder 可以用如下的示意图表示：</p>

<p><img src="/img/src/2023-01-04-language-model-5-12.png" alt="image" width="680" /></p>

<h3 id="第-11-节--transformer-的编码器-encoder-和解码器-decoder">第 11 节 · Transformer 的编码器 Encoder 和解码器 Decoder</h3>

<h4 id="111encoder-和-decoder-的图示结构">11.1、Encoder 和 Decoder 的图示结构</h4>

<p><img src="/img/src/2023-01-04-language-model-5-15.png" alt="image" width="165" /></p>

<ul>
  <li>第一层是多头注意力层（Multi-Head Attention Layer）。</li>
  <li>第二层是经过一个前馈神经网络（Feed Forward Neural Network，简称 FFNN）。</li>
  <li>这两层，每一层都有「Add &amp; Normalization」和 ResNet。</li>
</ul>

<p><img src="/img/src/2023-01-04-language-model-5-14.png" alt="image" width="179" /></p>

<ul>
  <li>解码器有两个多头注意力层。第一个多头注意力层是 Masked Multi-Head Attention 层，即在自注意力计算的过程中只有前面位置上的内容。第二个多头注意力层买有被 Masked，是个正常多头注意力层。</li>
  <li>可以看出来，第一个注意力层是一个自注意力层（Self Attention Layer），第二个是 Encoder-Decoder Attention 层（它的 K、V 来自 Encoder，Q 来自自注意力层），有些文章里会用这个角度来指代。</li>
  <li>FNN、Add &amp; Norm、ResNet 都与 Encoder 类似。</li>
</ul>

<h4 id="112decoder-的第一个输出结果">11.2、Decoder 的第一个输出结果</h4>

<p>产出第一个最终输出结果的过程：</p>

<ul>
  <li>不需要经过 Masked Multi-Head Attention Layer（自注意力层）。</li>
  <li>只经过 Encoder-Decoder Attention Layer。</li>
</ul>

<p><img src="/img/src/2023-01-04-language-model-5-13.png" alt="image" width="695" /></p>

<p>这样我们就像前面的 Encoder-Decoder Attention 模型一样，得到第一个输出。但是最终的输出结果，还会经过一层「Linear + Softmax」。</p>

<h4 id="113decoder-后续的所有输出">11.3、Decoder 后续的所有输出</h4>

<p>从产出第二个输出结果开始：</p>

<ul>
  <li>Decoder 的自注意力层，会用到前面的输出结果。</li>
  <li>可以看到，这是一个串行过程。</li>
</ul>

<h4 id="114decoder-之后的-linear-和-softmax">11.4、Decoder 之后的 Linear 和 Softmax</h4>

<p>经过所有 Decoder 之后，我们得到了一大堆浮点数的结果。最后的 Linear &amp; Softmax 就是来解决「怎么把它变成文本」的问题的。</p>

<ul>
  <li>Linear 是一个全连接神经网络，把 Decoders 输出的结果投影到一个超大的向量上，我们称之为 logits 向量。</li>
  <li>如果我们的输出词汇表有 1 万个词，那么 logits 向量的每一个维度就有 1 万个单元，每个单元都对应输出词汇表的一个词的概率。</li>
  <li>Softmax 将 logits 向量中的每一个维度都做归一化，这样每个维度都能从 1 万个单元对应的词概率中选出最大的，对应的词汇表里的词，就是输出词。最终得到一个输出字符串。</li>
</ul>

<h3 id="第-12-节--transformer-模型整体">第 12 节 · Transformer 模型整体</h3>

<p><img src="/img/src/2023-01-04-language-model-5-16.png" alt="image" width="660" /></p>

<p>最后我们再来整体看一下 Transformer：</p>

<ul>
  <li>首先输入数据生成词的嵌入式向量表示（Embedding），生成位置编码（Positional Encoding，简称 PE）。</li>
  <li>进入 Encoders 部分。先进入多头注意力层（Multi-Head Attention），是自注意力处理，然后进入全连接层（又叫前馈神经网络层），每层都有 ResNet、Add &amp; Norm。</li>
  <li>每一个 Encoder 的输入，都来自前一个 Encoder 的输出，但是第一个 Encoder 的输入就是 Embedding + PE。</li>
  <li>进入 Decoders 部分。先进入第一个多头注意力层（是 Masked 自注意力层），再进入第二个多头注意力层（是 Encoder-Decoder 注意力层），每层都有 ResNet、Add &amp; Norm。</li>
  <li>每一个 Decoder 都有两部分输入。</li>
  <li>Decoder 的第一层（Maksed 多头自注意力层）的输入，都来自前一个 Decoder 的输出，但是第一个 Decoder 是不经过第一层的（因为经过算出来也是 0）。</li>
  <li>Decoder 的第二层（Encoder-Decoder 注意力层）的输入，Q 都来自该 Decoder 的第一层，且每个 Decoder 的这一层的 K、V 都是一样的，均来自最后一个 Encoder。</li>
  <li>最后经过 Linear、Softmax 归一化。</li>
</ul>

<h3 id="第-13-节--transformer-的性能">第 13 节 · Transformer 的性能</h3>

<p>Google 在其博客于 2017.08.31 发布如下测试数据：</p>

<table>
  <thead>
    <tr>
      <th><img src="/img/src/2023-01-04-language-model-5-9.png" alt="image" /></th>
      <th><img src="/img/src/2023-01-04-language-model-5-10.png" alt="image" /></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td> </td>
      <td> </td>
    </tr>
  </tbody>
</table>

<h2 id="第三章--一个基于-tensorflow-架构的-transformer-实现">第三章 · 一个基于 TensorFlow 架构的 Transformer 实现</h2>

<p>我们来看看一个简单的 Transformer 模型，就是比较早出现的 Kyubyong 实现的 Transformer 模型：https://github.com/Kyubyong/transformer/tree/master/tf1.2_legacy</p>

<h3 id="第-14-节--先训练和测试一下-kyubyong-transformer">第 14 节 · 先训练和测试一下 Kyubyong Transformer</h3>

<p>下载一个「德语-英语翻译」的数据集：https://drive.google.com/uc?id=1l5y6Giag9aRPwGtuZHswh3w5v3qEz8D8</p>

<p>把 <code class="language-plaintext highlighter-rouge">de-en</code> 下面的 <code class="language-plaintext highlighter-rouge">tgz</code> 解压后放在 <code class="language-plaintext highlighter-rouge">corpora/</code> 目录下。如果需要先修改超参数，需要修改 <code class="language-plaintext highlighter-rouge">hyperparams.py</code>。然后运行如下命令，生成词汇文件（vocabulary files），默认到 <code class="language-plaintext highlighter-rouge">preprocessed</code> 目录下：</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python prepro.py
</code></pre></div></div>

<p>然后开始训练：</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python train.py
</code></pre></div></div>

<p>也可以跳过训练，直接<a href="https://www.dropbox.com/s/fo5wqgnbmvalwwq/logdir.zip?dl=0">下载预训练过的文件</a>，是一个 <code class="language-plaintext highlighter-rouge">logdir/</code> 目录，把它放到项目根目录下。然后可以对训练出来的结果，运行评价程序啦：</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python eval.py
</code></pre></div></div>

<p>会生成「德语-英语」测试结果文件在 <code class="language-plaintext highlighter-rouge">results/</code> 目录下，内容如下：</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- source: Sie war eine jährige Frau namens Alex
- expected: She was a yearold woman named Alex
- got: She was a &lt;UNK&gt; of vote called &lt;UNK&gt;

- source: Und als ich das hörte war ich erleichtert
- expected: Now when I heard this I was so relieved
- got: And when I was I &lt;UNK&gt; 's

- source: Meine Kommilitonin bekam nämlich einen Brandstifter als ersten Patienten
- expected: My classmate got an arsonist for her first client
- got: Because my first eye was a first show

- source: Das kriege ich hin dachte ich mir
- expected: This I thought I could handle
- got: I would give it to me a day

- source: Aber ich habe es nicht hingekriegt
- expected: But I didn't handle it
- got: But I didn't &lt;UNK&gt; &lt;UNK&gt;

- source: Ich hielt dagegen
- expected: I pushed back
- got: I &lt;UNK&gt;

...

Bleu Score = 6.598452846670836
</code></pre></div></div>

<p>评估结果文件的最后一行是 Bleu Score：</p>

<ul>
  <li>这是用来评估机器翻译质量的一种度量方式。它是由几个不同的 BLEU 分数组成的，每个 BLEU 分数都表示翻译结果中与参考翻译的重叠程度。</li>
  <li>一个常用的 BLEU 分数是 BLEU-4，它计算翻译结果中与参考翻译的 N 元文法语言模型 n-gram（n 为 4）的重叠程度。分数越高表示翻译结果越接近参考翻译。</li>
</ul>

<h3 id="第-15-节--kyubyong-transformer-源码分析">第 15 节 · Kyubyong Transformer 源码分析</h3>

<ul>
  <li><code class="language-plaintext highlighter-rouge">hparams.py</code>：超参数都在这里，仅 30 行。将在下面 <code class="language-plaintext highlighter-rouge">2.1</code> 部分解读。</li>
  <li><code class="language-plaintext highlighter-rouge">data_load.py</code>：装载、批处理数据的相关函数，代码仅 92 行。主要在下面 <code class="language-plaintext highlighter-rouge">2.2</code> 部分解读。</li>
  <li><code class="language-plaintext highlighter-rouge">prepro.py</code>：为 source 和 target 创建词汇文件（vocabulary file），代码仅 39 行。下面 <code class="language-plaintext highlighter-rouge">2.3</code> 部分会为大家解读。</li>
  <li><code class="language-plaintext highlighter-rouge">train.py</code>：代码仅 184 行。在下面 <code class="language-plaintext highlighter-rouge">2.4</code> 部分解读。</li>
  <li><code class="language-plaintext highlighter-rouge">modules.py</code>：Encoding / Decoding 网络的构建模块，代码仅 329 行。与 <code class="language-plaintext highlighter-rouge">modules.py</code> 一起会在 <code class="language-plaintext highlighter-rouge">2.4</code> 部分解读。</li>
  <li><code class="language-plaintext highlighter-rouge">eval.py</code>：评估效果，代码仅 82 行。将在 <code class="language-plaintext highlighter-rouge">2.5</code> 部分解读</li>
</ul>

<p>总计 700 多行代码。</p>

<h4 id="151超参数">15.1、超参数</h4>

<p><code class="language-plaintext highlighter-rouge">hyperparams.py</code> 文件中定义了 <code class="language-plaintext highlighter-rouge">Hyperparams</code> 超参数类，其中包含的参数我们逐一来解释一下：</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">source_train</code>：训练数据集的源输入文件，默认是 <code class="language-plaintext highlighter-rouge">'corpora/train.tags.de-en.de'</code></li>
  <li><code class="language-plaintext highlighter-rouge">target_train</code>：训练数据集的目标输出文件，默认是 <code class="language-plaintext highlighter-rouge">'corpora/train.tags.de-en.en'</code></li>
  <li><code class="language-plaintext highlighter-rouge">source_test</code>：测试数据集的源输入文件，默认是 <code class="language-plaintext highlighter-rouge">'corpora/IWSLT16.TED.tst2014.de-en.de.xml'</code></li>
  <li><code class="language-plaintext highlighter-rouge">target_test</code>：测试数据集的目标输出文件，默认是 <code class="language-plaintext highlighter-rouge">'corpora/IWSLT16.TED.tst2014.de-en.en.xml'</code></li>
  <li><code class="language-plaintext highlighter-rouge">batch_size</code>：设置每批数据的大小。</li>
  <li><code class="language-plaintext highlighter-rouge">lr</code>：设置学习率 learning rate。</li>
  <li><code class="language-plaintext highlighter-rouge">logdir</code>：设置日志文件保存的目录。</li>
  <li><code class="language-plaintext highlighter-rouge">maxlen</code></li>
  <li><code class="language-plaintext highlighter-rouge">min_cnt</code></li>
  <li><code class="language-plaintext highlighter-rouge">hidden_units</code>：设置编码器和解码器中隐藏层单元的数量。</li>
  <li><code class="language-plaintext highlighter-rouge">num_blocks</code>：编码器（encoder block）、解码器（decoder block）的数量</li>
  <li><code class="language-plaintext highlighter-rouge">num_epochs</code>：训练过程中迭代的次数。</li>
  <li><code class="language-plaintext highlighter-rouge">num_heads</code>：还记得上面文章里我们提到的 Transformer 中用到了多头注意力吧，这里就是多头注意力的头数。</li>
  <li><code class="language-plaintext highlighter-rouge">droupout_rate</code>：设置 dropout 层的 dropout rate，具体 dropout 请看 2.4.1 部分。</li>
  <li><code class="language-plaintext highlighter-rouge">sinusoid</code>：设置为 <code class="language-plaintext highlighter-rouge">True</code> 时表示使用正弦函数计算位置编码，否则为 <code class="language-plaintext highlighter-rouge">False</code> 时表示直接用 <code class="language-plaintext highlighter-rouge">position</code> 做位置编码。</li>
</ul>

<h4 id="152预处理">15.2、预处理</h4>

<p>文件 <code class="language-plaintext highlighter-rouge">prepro.py</code> 实现了预处理的过程，根据 <code class="language-plaintext highlighter-rouge">hp.source_train</code> 和 <code class="language-plaintext highlighter-rouge">hp.target_train</code> 分别创建 <code class="language-plaintext highlighter-rouge">"de.vocab.tsv"</code> 和 <code class="language-plaintext highlighter-rouge">"en.vocab.tsv"</code> 两个词汇表。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">make_vocab</span><span class="p">(</span><span class="n">fpath</span><span class="p">,</span> <span class="n">fname</span><span class="p">):</span>

    <span class="c1"># 使用 codecs.open 函数读取指定文件路径(fpath)的文本内容，并将其存储在 text 变量中
</span>    <span class="n">text</span> <span class="o">=</span> <span class="n">codecs</span><span class="p">.</span><span class="nb">open</span><span class="p">(</span><span class="n">fpath</span><span class="p">,</span> <span class="s">'r'</span><span class="p">,</span> <span class="s">'utf-8'</span><span class="p">).</span><span class="n">read</span><span class="p">()</span>

    <span class="c1"># 将 text 中的非字母和空格的字符去掉
</span>    <span class="n">text</span> <span class="o">=</span> <span class="n">regex</span><span class="p">.</span><span class="n">sub</span><span class="p">(</span><span class="s">"[^\s\p{Latin}']"</span><span class="p">,</span> <span class="s">""</span><span class="p">,</span> <span class="n">text</span><span class="p">)</span>

    <span class="c1"># 将 text 中的文本按照空格分割，并将每个单词存储在 words 变量中
</span>    <span class="n">words</span> <span class="o">=</span> <span class="n">text</span><span class="p">.</span><span class="n">split</span><span class="p">()</span>

    <span class="c1"># words 中每个单词的词频
</span>    <span class="n">word2cnt</span> <span class="o">=</span> <span class="n">Counter</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>

    <span class="c1"># 检查是否存在 preprocessed 文件夹，如果不存在就创建
</span>    <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">exists</span><span class="p">(</span><span class="s">'preprocessed'</span><span class="p">):</span> <span class="n">os</span><span class="p">.</span><span class="n">mkdir</span><span class="p">(</span><span class="s">'preprocessed'</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">codecs</span><span class="p">.</span><span class="nb">open</span><span class="p">(</span><span class="s">'preprocessed/{}'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">fname</span><span class="p">),</span> <span class="s">'w'</span><span class="p">,</span> <span class="s">'utf-8'</span><span class="p">)</span> <span class="k">as</span> <span class="n">fout</span><span class="p">:</span>

    	<span class="c1"># 按出现次数从多到少的顺序写入每个单词和它的出现次数
</span>    	<span class="c1"># 在文件最前面写入四个特殊字符 &lt;PAD&gt;, &lt;UNK&gt;, &lt;S&gt;, &lt;/S&gt; 分别用于填充，未知单词，句子开始和句子结束
</span>        <span class="n">fout</span><span class="p">.</span><span class="n">write</span><span class="p">(</span><span class="s">"{}</span><span class="se">\t</span><span class="s">1000000000</span><span class="se">\n</span><span class="s">{}</span><span class="se">\t</span><span class="s">1000000000</span><span class="se">\n</span><span class="s">{}</span><span class="se">\t</span><span class="s">1000000000</span><span class="se">\n</span><span class="s">{}</span><span class="se">\t</span><span class="s">1000000000</span><span class="se">\n</span><span class="s">"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="s">"&lt;PAD&gt;"</span><span class="p">,</span> <span class="s">"&lt;UNK&gt;"</span><span class="p">,</span> <span class="s">"&lt;S&gt;"</span><span class="p">,</span> <span class="s">"&lt;/S&gt;"</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">cnt</span> <span class="ow">in</span> <span class="n">word2cnt</span><span class="p">.</span><span class="n">most_common</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">word2cnt</span><span class="p">)):</span>
            <span class="n">fout</span><span class="p">.</span><span class="n">write</span><span class="p">(</span><span class="sa">u</span><span class="s">"{}</span><span class="se">\t</span><span class="s">{}</span><span class="se">\n</span><span class="s">"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">cnt</span><span class="p">))</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">'__main__'</span><span class="p">:</span>
    <span class="n">make_vocab</span><span class="p">(</span><span class="n">hp</span><span class="p">.</span><span class="n">source_train</span><span class="p">,</span> <span class="s">"de.vocab.tsv"</span><span class="p">)</span>
    <span class="n">make_vocab</span><span class="p">(</span><span class="n">hp</span><span class="p">.</span><span class="n">target_train</span><span class="p">,</span> <span class="s">"en.vocab.tsv"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"Done"</span><span class="p">)</span>
</code></pre></div></div>

<ul>
  <li>在主函数中调用 make_vocab 函数，在目录 <code class="language-plaintext highlighter-rouge">preprocessed</code> 生成 <code class="language-plaintext highlighter-rouge">de.vocab.tsv</code> 和 <code class="language-plaintext highlighter-rouge">en.vocab.tsv</code> 两个词汇表文件。</li>
  <li>在函数 <code class="language-plaintext highlighter-rouge">make_vocab</code> 中，先使用 <code class="language-plaintext highlighter-rouge">codecs.open</code> 函数读取指定文件路径 <code class="language-plaintext highlighter-rouge">fpath</code> 的文本内容，并将其存储在 <code class="language-plaintext highlighter-rouge">text</code> 变量中，再使用正则表达式 <code class="language-plaintext highlighter-rouge">regex</code> 将 <code class="language-plaintext highlighter-rouge">text</code> 中的非字母和空格的字符去掉，接着将 <code class="language-plaintext highlighter-rouge">text</code> 中的文本按照空格分割，并将每个单词存储在 <code class="language-plaintext highlighter-rouge">words</code> 变量中。</li>
  <li>接下来，使用 <code class="language-plaintext highlighter-rouge">Counter</code> 函数统计 <code class="language-plaintext highlighter-rouge">words</code> 中每个单词的出现次数，并将统计结果存储在 <code class="language-plaintext highlighter-rouge">word2cnt</code> 变量中。</li>
  <li>最后所有词与词频，存储在 <code class="language-plaintext highlighter-rouge">de.vocab.tsv</code> 和 <code class="language-plaintext highlighter-rouge">en.vocab.tsv</code> 两个文件中。</li>
</ul>

<h4 id="153训练测试数据集的加载">15.3、训练/测试数据集的加载</h4>

<p>我们先看下 <code class="language-plaintext highlighter-rouge">train.py</code>、<code class="language-plaintext highlighter-rouge">data_load.py</code>、<code class="language-plaintext highlighter-rouge">eval.py</code> 三个文件：</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">train.py</code>：该文件包含了 <code class="language-plaintext highlighter-rouge">Graph</code> 类的定义，并在其构造函数中调用 <code class="language-plaintext highlighter-rouge">load_data.py</code> 文件中的 <code class="language-plaintext highlighter-rouge">get_batch_data</code> 函数加载训练数据。</li>
  <li><code class="language-plaintext highlighter-rouge">data_load.py</code>：定义了加载训练数据、加载测试数据的函数。</li>
  <li><code class="language-plaintext highlighter-rouge">eval.py</code>：测试结果的评价函数定义在这个文件里。</li>
</ul>

<p>下面是函数调用的流程：</p>

<div style="text-align: center;">
<div class="graphviz-wrapper">

<!-- Generated by graphviz version 2.43.0 (0)
 -->
<!-- Title: G Pages: 1 -->
<svg role="img" aria-label="graphviz-9559986008ed2e1e47e8729260efda61" width="830pt" height="98pt" viewBox="0.00 0.00 830.00 98.00">
<title>graphviz-9559986008ed2e1e47e8729260efda61</title>
<desc>
digraph G {
	rankdir=LR
	splines=ortho
	node [shape=&quot;box&quot;]

	训练 -&gt; Graph构造函数 -&gt; get_batch_data -&gt; load_train_data
	测试 -&gt; eval -&gt; load_test_data

	load_train_data -&gt; create_data
	load_test_data -&gt; create_data

	create_data -&gt; load_de_vocab
	create_data -&gt; load_en_vocab
}
</desc>

<g id="graph0" class="graph" transform="scale(1 1) rotate(0) translate(4 94)">
<title>G</title>
<polygon fill="white" stroke="transparent" points="-4,4 -4,-94 826,-94 826,4 -4,4" />
<!-- 训练 -->
<g id="node1" class="node">
<title>训练</title>
<polygon fill="none" stroke="black" points="54,-90 0,-90 0,-54 54,-54 54,-90" />
<text text-anchor="middle" x="27" y="-68.3" font-family="Times,serif" font-size="14.00">训练</text>
</g>
<!-- Graph构造函数 -->
<g id="node2" class="node">
<title>Graph构造函数</title>
<polygon fill="none" stroke="black" points="208,-90 90,-90 90,-54 208,-54 208,-90" />
<text text-anchor="middle" x="149" y="-68.3" font-family="Times,serif" font-size="14.00">Graph构造函数</text>
</g>
<!-- 训练&#45;&gt;Graph构造函数 -->
<g id="edge1" class="edge">
<title>训练&#45;&gt;Graph构造函数</title>
<path fill="none" stroke="black" d="M54.08,-72C54.08,-72 79.54,-72 79.54,-72" />
<polygon fill="black" stroke="black" points="79.54,-75.5 89.54,-72 79.54,-68.5 79.54,-75.5" />
</g>
<!-- get_batch_data -->
<g id="node3" class="node">
<title>get_batch_data</title>
<polygon fill="none" stroke="black" points="369,-90 244,-90 244,-54 369,-54 369,-90" />
<text text-anchor="middle" x="306.5" y="-68.3" font-family="Times,serif" font-size="14.00">get_batch_data</text>
</g>
<!-- Graph构造函数&#45;&gt;get_batch_data -->
<g id="edge2" class="edge">
<title>Graph构造函数&#45;&gt;get_batch_data</title>
<path fill="none" stroke="black" d="M208.1,-72C208.1,-72 233.91,-72 233.91,-72" />
<polygon fill="black" stroke="black" points="233.91,-75.5 243.91,-72 233.91,-68.5 233.91,-75.5" />
</g>
<!-- load_train_data -->
<g id="node4" class="node">
<title>load_train_data</title>
<polygon fill="none" stroke="black" points="531,-90 405,-90 405,-54 531,-54 531,-90" />
<text text-anchor="middle" x="468" y="-68.3" font-family="Times,serif" font-size="14.00">load_train_data</text>
</g>
<!-- get_batch_data&#45;&gt;load_train_data -->
<g id="edge3" class="edge">
<title>get_batch_data&#45;&gt;load_train_data</title>
<path fill="none" stroke="black" d="M369.4,-72C369.4,-72 394.74,-72 394.74,-72" />
<polygon fill="black" stroke="black" points="394.74,-75.5 404.74,-72 394.74,-68.5 394.74,-75.5" />
</g>
<!-- create_data -->
<g id="node8" class="node">
<title>create_data</title>
<polygon fill="none" stroke="black" points="667,-63 567,-63 567,-27 667,-27 667,-63" />
<text text-anchor="middle" x="617" y="-41.3" font-family="Times,serif" font-size="14.00">create_data</text>
</g>
<!-- load_train_data&#45;&gt;create_data -->
<g id="edge6" class="edge">
<title>load_train_data&#45;&gt;create_data</title>
<path fill="none" stroke="black" d="M531.19,-58.5C531.19,-58.5 556.81,-58.5 556.81,-58.5" />
<polygon fill="black" stroke="black" points="556.81,-62 566.81,-58.5 556.81,-55 556.81,-62" />
</g>
<!-- 测试 -->
<g id="node5" class="node">
<title>测试</title>
<polygon fill="none" stroke="black" points="176,-36 122,-36 122,0 176,0 176,-36" />
<text text-anchor="middle" x="149" y="-14.3" font-family="Times,serif" font-size="14.00">测试</text>
</g>
<!-- eval -->
<g id="node6" class="node">
<title>eval</title>
<polygon fill="none" stroke="black" points="333.5,-36 279.5,-36 279.5,0 333.5,0 333.5,-36" />
<text text-anchor="middle" x="306.5" y="-14.3" font-family="Times,serif" font-size="14.00">eval</text>
</g>
<!-- 测试&#45;&gt;eval -->
<g id="edge4" class="edge">
<title>测试&#45;&gt;eval</title>
<path fill="none" stroke="black" d="M176.08,-18C176.08,-18 269.25,-18 269.25,-18" />
<polygon fill="black" stroke="black" points="269.25,-21.5 279.25,-18 269.25,-14.5 269.25,-21.5" />
</g>
<!-- load_test_data -->
<g id="node7" class="node">
<title>load_test_data</title>
<polygon fill="none" stroke="black" points="527.5,-36 408.5,-36 408.5,0 527.5,0 527.5,-36" />
<text text-anchor="middle" x="468" y="-14.3" font-family="Times,serif" font-size="14.00">load_test_data</text>
</g>
<!-- eval&#45;&gt;load_test_data -->
<g id="edge5" class="edge">
<title>eval&#45;&gt;load_test_data</title>
<path fill="none" stroke="black" d="M333.53,-18C333.53,-18 398.34,-18 398.34,-18" />
<polygon fill="black" stroke="black" points="398.34,-21.5 408.34,-18 398.34,-14.5 398.34,-21.5" />
</g>
<!-- load_test_data&#45;&gt;create_data -->
<g id="edge7" class="edge">
<title>load_test_data&#45;&gt;create_data</title>
<path fill="none" stroke="black" d="M527.75,-31.5C527.75,-31.5 556.82,-31.5 556.82,-31.5" />
<polygon fill="black" stroke="black" points="556.82,-35 566.82,-31.5 556.81,-28 556.82,-35" />
</g>
<!-- load_de_vocab -->
<g id="node9" class="node">
<title>load_de_vocab</title>
<polygon fill="none" stroke="black" points="822,-90 703,-90 703,-54 822,-54 822,-90" />
<text text-anchor="middle" x="762.5" y="-68.3" font-family="Times,serif" font-size="14.00">load_de_vocab</text>
</g>
<!-- create_data&#45;&gt;load_de_vocab -->
<g id="edge8" class="edge">
<title>create_data&#45;&gt;load_de_vocab</title>
<path fill="none" stroke="black" d="M667.07,-58.5C667.07,-58.5 692.8,-58.5 692.8,-58.5" />
<polygon fill="black" stroke="black" points="692.8,-62 702.8,-58.5 692.8,-55 692.8,-62" />
</g>
<!-- load_en_vocab -->
<g id="node10" class="node">
<title>load_en_vocab</title>
<polygon fill="none" stroke="black" points="822,-36 703,-36 703,0 822,0 822,-36" />
<text text-anchor="middle" x="762.5" y="-14.3" font-family="Times,serif" font-size="14.00">load_en_vocab</text>
</g>
<!-- create_data&#45;&gt;load_en_vocab -->
<g id="edge9" class="edge">
<title>create_data&#45;&gt;load_en_vocab</title>
<path fill="none" stroke="black" d="M667.07,-31.5C667.07,-31.5 692.8,-31.5 692.8,-31.5" />
<polygon fill="black" stroke="black" points="692.8,-35 702.8,-31.5 692.8,-28 692.8,-35" />
</g>
</g>
</svg>
</div>
</div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">load_de_vocab</span><span class="p">():</span>
    <span class="n">vocab</span> <span class="o">=</span> <span class="p">[</span><span class="n">line</span><span class="p">.</span><span class="n">split</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">codecs</span><span class="p">.</span><span class="nb">open</span><span class="p">(</span><span class="s">'preprocessed/de.vocab.tsv'</span><span class="p">,</span> <span class="s">'r'</span><span class="p">,</span> <span class="s">'utf-8'</span><span class="p">).</span><span class="n">read</span><span class="p">().</span><span class="n">splitlines</span><span class="p">()</span> <span class="k">if</span> <span class="nb">int</span><span class="p">(</span><span class="n">line</span><span class="p">.</span><span class="n">split</span><span class="p">()[</span><span class="mi">1</span><span class="p">])</span><span class="o">&gt;=</span><span class="n">hp</span><span class="p">.</span><span class="n">min_cnt</span><span class="p">]</span>
    <span class="n">word2idx</span> <span class="o">=</span> <span class="p">{</span><span class="n">word</span><span class="p">:</span> <span class="n">idx</span> <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">word</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">vocab</span><span class="p">)}</span>
    <span class="n">idx2word</span> <span class="o">=</span> <span class="p">{</span><span class="n">idx</span><span class="p">:</span> <span class="n">word</span> <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">word</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">vocab</span><span class="p">)}</span>
    <span class="k">return</span> <span class="n">word2idx</span><span class="p">,</span> <span class="n">idx2word</span>

<span class="k">def</span> <span class="nf">load_en_vocab</span><span class="p">():</span>
    <span class="n">vocab</span> <span class="o">=</span> <span class="p">[</span><span class="n">line</span><span class="p">.</span><span class="n">split</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">codecs</span><span class="p">.</span><span class="nb">open</span><span class="p">(</span><span class="s">'preprocessed/en.vocab.tsv'</span><span class="p">,</span> <span class="s">'r'</span><span class="p">,</span> <span class="s">'utf-8'</span><span class="p">).</span><span class="n">read</span><span class="p">().</span><span class="n">splitlines</span><span class="p">()</span> <span class="k">if</span> <span class="nb">int</span><span class="p">(</span><span class="n">line</span><span class="p">.</span><span class="n">split</span><span class="p">()[</span><span class="mi">1</span><span class="p">])</span><span class="o">&gt;=</span><span class="n">hp</span><span class="p">.</span><span class="n">min_cnt</span><span class="p">]</span>
    <span class="n">word2idx</span> <span class="o">=</span> <span class="p">{</span><span class="n">word</span><span class="p">:</span> <span class="n">idx</span> <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">word</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">vocab</span><span class="p">)}</span>
    <span class="n">idx2word</span> <span class="o">=</span> <span class="p">{</span><span class="n">idx</span><span class="p">:</span> <span class="n">word</span> <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">word</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">vocab</span><span class="p">)}</span>
    <span class="k">return</span> <span class="n">word2idx</span><span class="p">,</span> <span class="n">idx2word</span>
</code></pre></div></div>

<p>将 <code class="language-plaintext highlighter-rouge">preprocessed/de.vocab.tsv</code> 和 <code class="language-plaintext highlighter-rouge">preprocessed/en.vocab.tsv</code> 中储存的德语、英语的词汇、词频，载入成 <code class="language-plaintext highlighter-rouge">word2idx</code> 和 <code class="language-plaintext highlighter-rouge">idx2word</code>。前者是通过词查询词向量，后者通过词向量查询词。</p>

<p><code class="language-plaintext highlighter-rouge">load_de_vocab</code> 和 <code class="language-plaintext highlighter-rouge">load_en_vocab</code> 函数被 <code class="language-plaintext highlighter-rouge">create_data</code> 函数引用，该函数将输入的源语言和目标语言句子转换为索引表示，并对过长的句子进行截断或填充。详细的解释看下面代码里的注释。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 输入参数是翻译模型的源语言语句、目标语言语句
</span><span class="k">def</span> <span class="nf">create_data</span><span class="p">(</span><span class="n">source_sents</span><span class="p">,</span> <span class="n">target_sents</span><span class="p">):</span>

    <span class="n">de2idx</span><span class="p">,</span> <span class="n">idx2de</span> <span class="o">=</span> <span class="n">load_de_vocab</span><span class="p">()</span>
    <span class="n">en2idx</span><span class="p">,</span> <span class="n">idx2en</span> <span class="o">=</span> <span class="n">load_en_vocab</span><span class="p">()</span>
    
    <span class="c1"># 用 zip 函数将源语言和目标语言句子对应起来，并对句子进行截断或填充
</span>    <span class="n">x_list</span><span class="p">,</span> <span class="n">y_list</span><span class="p">,</span> <span class="n">Sources</span><span class="p">,</span> <span class="n">Targets</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">source_sent</span><span class="p">,</span> <span class="n">target_sent</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">source_sents</span><span class="p">,</span> <span class="n">target_sents</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="n">de2idx</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="p">(</span><span class="n">source_sent</span> <span class="o">+</span> <span class="sa">u</span><span class="s">" &lt;/S&gt;"</span><span class="p">).</span><span class="n">split</span><span class="p">()]</span> <span class="c1"># 1: OOV, &lt;/S&gt;: End of Text
</span>        <span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="n">en2idx</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="p">(</span><span class="n">target_sent</span> <span class="o">+</span> <span class="sa">u</span><span class="s">" &lt;/S&gt;"</span><span class="p">).</span><span class="n">split</span><span class="p">()]</span> 

        <span class="c1"># 将句子的词的编号，原句以及编号后的句子存储下来，以供之后使用
</span>        <span class="k">if</span> <span class="nb">max</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">))</span> <span class="o">&lt;=</span><span class="n">hp</span><span class="p">.</span><span class="n">maxlen</span><span class="p">:</span>

        	<span class="c1"># 将 x 和 y 转换成 numpy 数组并加入 x_list 和 y_list 中
</span>            <span class="n">x_list</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
            <span class="n">y_list</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>

            <span class="c1"># 将原始的 source_sent 和 target_sent 加入 Sources 和 Targets 列表中
</span>            <span class="n">Sources</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">source_sent</span><span class="p">)</span>
            <span class="n">Targets</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">target_sent</span><span class="p">)</span>
    
    <span class="c1"># 对于每个 (x, y) 对，使用 np.lib.pad 函数将 x 和 y 分别用 0 进行填充，直到长度为 hp.maxlen
</span>    <span class="c1"># 这样做的目的是使得每个句子长度都相等，方便后续的训练
</span>    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">x_list</span><span class="p">),</span> <span class="n">hp</span><span class="p">.</span><span class="n">maxlen</span><span class="p">],</span> <span class="n">np</span><span class="p">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">y_list</span><span class="p">),</span> <span class="n">hp</span><span class="p">.</span><span class="n">maxlen</span><span class="p">],</span> <span class="n">np</span><span class="p">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">x_list</span><span class="p">,</span> <span class="n">y_list</span><span class="p">)):</span>
        <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">lib</span><span class="p">.</span><span class="n">pad</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">hp</span><span class="p">.</span><span class="n">maxlen</span><span class="o">-</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)],</span> <span class="s">'constant'</span><span class="p">,</span> <span class="n">constant_values</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
        <span class="n">Y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">lib</span><span class="p">.</span><span class="n">pad</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">hp</span><span class="p">.</span><span class="n">maxlen</span><span class="o">-</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)],</span> <span class="s">'constant'</span><span class="p">,</span> <span class="n">constant_values</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>

    <span class="c1"># 返回转换后的索引表示，以及未经处理的源语言和目标语言句子
</span>    <span class="c1"># X 是原始句子中德语的索引
</span>    <span class="c1"># Y 是原始句子中英语的索引
</span>    <span class="c1"># Sources 是源原始句子列表，并与 X 一一对应
</span>    <span class="c1"># Targets 是目标原始句子列表，并与 Y 一一对应
</span>    <span class="k">return</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">Sources</span><span class="p">,</span> <span class="n">Targets</span>

<span class="c1"># 返回原始句子中德语、英语的索引
</span><span class="k">def</span> <span class="nf">load_train_data</span><span class="p">():</span>
    <span class="n">de_sents</span> <span class="o">=</span> <span class="p">[</span><span class="n">regex</span><span class="p">.</span><span class="n">sub</span><span class="p">(</span><span class="s">"[^\s\p{Latin}']"</span><span class="p">,</span> <span class="s">""</span><span class="p">,</span> <span class="n">line</span><span class="p">)</span> <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">codecs</span><span class="p">.</span><span class="nb">open</span><span class="p">(</span><span class="n">hp</span><span class="p">.</span><span class="n">source_train</span><span class="p">,</span> <span class="s">'r'</span><span class="p">,</span> <span class="s">'utf-8'</span><span class="p">).</span><span class="n">read</span><span class="p">().</span><span class="n">split</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">"</span><span class="p">)</span> <span class="k">if</span> <span class="n">line</span> <span class="ow">and</span> <span class="n">line</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="s">"&lt;"</span><span class="p">]</span>
    <span class="n">en_sents</span> <span class="o">=</span> <span class="p">[</span><span class="n">regex</span><span class="p">.</span><span class="n">sub</span><span class="p">(</span><span class="s">"[^\s\p{Latin}']"</span><span class="p">,</span> <span class="s">""</span><span class="p">,</span> <span class="n">line</span><span class="p">)</span> <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">codecs</span><span class="p">.</span><span class="nb">open</span><span class="p">(</span><span class="n">hp</span><span class="p">.</span><span class="n">target_train</span><span class="p">,</span> <span class="s">'r'</span><span class="p">,</span> <span class="s">'utf-8'</span><span class="p">).</span><span class="n">read</span><span class="p">().</span><span class="n">split</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">"</span><span class="p">)</span> <span class="k">if</span> <span class="n">line</span> <span class="ow">and</span> <span class="n">line</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="s">"&lt;"</span><span class="p">]</span>
    
    <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">Sources</span><span class="p">,</span> <span class="n">Targets</span> <span class="o">=</span> <span class="n">create_data</span><span class="p">(</span><span class="n">de_sents</span><span class="p">,</span> <span class="n">en_sents</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span>
</code></pre></div></div>

<p>下面的 <code class="language-plaintext highlighter-rouge">get_batch_data</code> 则从文本数据中读取并生成 batch：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">get_batch_data</span><span class="p">():</span>
    
    <span class="c1"># 加载数据
</span>    <span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">load_train_data</span><span class="p">()</span>
    
    <span class="c1"># calc total batch count
</span>    <span class="n">num_batch</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">//</span> <span class="n">hp</span><span class="p">.</span><span class="n">batch_size</span>
    
    <span class="c1"># 将 X 和 Y 转换成张量
</span>    <span class="n">X</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="n">Y</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">int32</span><span class="p">)</span>
    
    <span class="c1"># 创建输入队列
</span>    <span class="n">input_queues</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">train</span><span class="p">.</span><span class="n">slice_input_producer</span><span class="p">([</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">])</span>
            
    <span class="c1"># 创建 batch 队列，利用 shuffle_batch 将一组 tensor 随机打乱，并将它们分为多个 batch
</span>    <span class="c1"># 使用 shuffle_batch 是为了防止模型过拟合
</span>    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">train</span><span class="p">.</span><span class="n">shuffle_batch</span><span class="p">(</span><span class="n">input_queues</span><span class="p">,</span>
                                <span class="n">num_threads</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
                                <span class="n">batch_size</span><span class="o">=</span><span class="n">hp</span><span class="p">.</span><span class="n">batch_size</span><span class="p">,</span> 
                                <span class="n">capacity</span><span class="o">=</span><span class="n">hp</span><span class="p">.</span><span class="n">batch_size</span><span class="o">*</span><span class="mi">64</span><span class="p">,</span>   
                                <span class="n">min_after_dequeue</span><span class="o">=</span><span class="n">hp</span><span class="p">.</span><span class="n">batch_size</span><span class="o">*</span><span class="mi">32</span><span class="p">,</span> 
                                <span class="n">allow_smaller_final_batch</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">num_batch</span> <span class="c1"># (N, T), (N, T), ()
</span></code></pre></div></div>

<h4 id="154构建模型并训练">15.4、构建模型并训练</h4>

<p>Graph 的构造函数流程，就是模型的构建流程，下面船长来分析这部分代码。</p>

<div style="text-align: center;">
<div class="graphviz-wrapper">

<!-- Generated by graphviz version 2.43.0 (0)
 -->
<!-- Title: G Pages: 1 -->
<svg role="img" aria-label="graphviz-d43b60bdbb0d309ff41c0875976a1d4b" width="526pt" height="44pt" viewBox="0.00 0.00 526.00 44.00">
<title>graphviz-d43b60bdbb0d309ff41c0875976a1d4b</title>
<desc>
digraph G {
	rankdir=LR
	splines=ortho
	node [shape=&quot;box&quot;]

	Graph构造函数 -&gt; 编码器 -&gt; 解码器 -&gt; Linear -&gt; Softmax
}
</desc>

<g id="graph0" class="graph" transform="scale(1 1) rotate(0) translate(4 40)">
<title>G</title>
<polygon fill="white" stroke="transparent" points="-4,4 -4,-40 522,-40 522,4 -4,4" />
<!-- Graph构造函数 -->
<g id="node1" class="node">
<title>Graph构造函数</title>
<polygon fill="none" stroke="black" points="118,-36 0,-36 0,0 118,0 118,-36" />
<text text-anchor="middle" x="59" y="-14.3" font-family="Times,serif" font-size="14.00">Graph构造函数</text>
</g>
<!-- 编码器 -->
<g id="node2" class="node">
<title>编码器</title>
<polygon fill="none" stroke="black" points="213,-36 154,-36 154,0 213,0 213,-36" />
<text text-anchor="middle" x="183.5" y="-14.3" font-family="Times,serif" font-size="14.00">编码器</text>
</g>
<!-- Graph构造函数&#45;&gt;编码器 -->
<g id="edge1" class="edge">
<title>Graph构造函数&#45;&gt;编码器</title>
<path fill="none" stroke="black" d="M118.33,-18C118.33,-18 143.7,-18 143.7,-18" />
<polygon fill="black" stroke="black" points="143.7,-21.5 153.7,-18 143.7,-14.5 143.7,-21.5" />
</g>
<!-- 解码器 -->
<g id="node3" class="node">
<title>解码器</title>
<polygon fill="none" stroke="black" points="308,-36 249,-36 249,0 308,0 308,-36" />
<text text-anchor="middle" x="278.5" y="-14.3" font-family="Times,serif" font-size="14.00">解码器</text>
</g>
<!-- 编码器&#45;&gt;解码器 -->
<g id="edge2" class="edge">
<title>编码器&#45;&gt;解码器</title>
<path fill="none" stroke="black" d="M213.04,-18C213.04,-18 238.98,-18 238.98,-18" />
<polygon fill="black" stroke="black" points="238.98,-21.5 248.98,-18 238.98,-14.5 238.98,-21.5" />
</g>
<!-- Linear -->
<g id="node4" class="node">
<title>Linear</title>
<polygon fill="none" stroke="black" points="406,-36 344,-36 344,0 406,0 406,-36" />
<text text-anchor="middle" x="375" y="-14.3" font-family="Times,serif" font-size="14.00">Linear</text>
</g>
<!-- 解码器&#45;&gt;Linear -->
<g id="edge3" class="edge">
<title>解码器&#45;&gt;Linear</title>
<path fill="none" stroke="black" d="M308.24,-18C308.24,-18 333.85,-18 333.85,-18" />
<polygon fill="black" stroke="black" points="333.85,-21.5 343.85,-18 333.85,-14.5 333.85,-21.5" />
</g>
<!-- Softmax -->
<g id="node5" class="node">
<title>Softmax</title>
<polygon fill="none" stroke="black" points="518,-36 442,-36 442,0 518,0 518,-36" />
<text text-anchor="middle" x="480" y="-14.3" font-family="Times,serif" font-size="14.00">Softmax</text>
</g>
<!-- Linear&#45;&gt;Softmax -->
<g id="edge4" class="edge">
<title>Linear&#45;&gt;Softmax</title>
<path fill="none" stroke="black" d="M406.22,-18C406.22,-18 431.65,-18 431.65,-18" />
<polygon fill="black" stroke="black" points="431.65,-21.5 441.65,-18 431.65,-14.5 431.65,-21.5" />
</g>
</g>
</svg>
</div>
</div>

<p>整体这个流程，主要涉及 <code class="language-plaintext highlighter-rouge">train.py</code> 文件和 <code class="language-plaintext highlighter-rouge">modules.py</code> 文件。所有模型所需的主要函数定义，都是在 <code class="language-plaintext highlighter-rouge">modules.py</code> 中实现的。我们先看下编码器（Encoder）的流程：</p>

<div style="text-align: center;">
<div class="graphviz-wrapper">

<!-- Generated by graphviz version 2.43.0 (0)
 -->
<!-- Title: G Pages: 1 -->
<svg role="img" aria-label="graphviz-4459d4df0778b105cf972d664023b546" width="169pt" height="332pt" viewBox="0.00 0.00 169.00 332.00">
<title>graphviz-4459d4df0778b105cf972d664023b546</title>
<desc>
digraph G {
	rankdir=BT
	splines=ortho
	node [shape=&quot;box&quot;]

	embedding -&gt; positional_encoding -&gt; dropout -&gt; multihead_attention -&gt; feedforward
}
</desc>

<g id="graph0" class="graph" transform="scale(1 1) rotate(0) translate(4 328)">
<title>G</title>
<polygon fill="white" stroke="transparent" points="-4,4 -4,-328 165,-328 165,4 -4,4" />
<!-- embedding -->
<g id="node1" class="node">
<title>embedding</title>
<polygon fill="none" stroke="black" points="128.5,-36 32.5,-36 32.5,0 128.5,0 128.5,-36" />
<text text-anchor="middle" x="80.5" y="-14.3" font-family="Times,serif" font-size="14.00">embedding</text>
</g>
<!-- positional_encoding -->
<g id="node2" class="node">
<title>positional_encoding</title>
<polygon fill="none" stroke="black" points="159.5,-108 1.5,-108 1.5,-72 159.5,-72 159.5,-108" />
<text text-anchor="middle" x="80.5" y="-86.3" font-family="Times,serif" font-size="14.00">positional_encoding</text>
</g>
<!-- embedding&#45;&gt;positional_encoding -->
<g id="edge1" class="edge">
<title>embedding&#45;&gt;positional_encoding</title>
<path fill="none" stroke="black" d="M80.5,-36.17C80.5,-36.17 80.5,-61.59 80.5,-61.59" />
<polygon fill="black" stroke="black" points="77,-61.59 80.5,-71.59 84,-61.59 77,-61.59" />
</g>
<!-- dropout -->
<g id="node3" class="node">
<title>dropout</title>
<polygon fill="none" stroke="black" points="117,-180 44,-180 44,-144 117,-144 117,-180" />
<text text-anchor="middle" x="80.5" y="-158.3" font-family="Times,serif" font-size="14.00">dropout</text>
</g>
<!-- positional_encoding&#45;&gt;dropout -->
<g id="edge2" class="edge">
<title>positional_encoding&#45;&gt;dropout</title>
<path fill="none" stroke="black" d="M80.5,-108.17C80.5,-108.17 80.5,-133.59 80.5,-133.59" />
<polygon fill="black" stroke="black" points="77,-133.59 80.5,-143.59 84,-133.59 77,-133.59" />
</g>
<!-- multihead_attention -->
<g id="node4" class="node">
<title>multihead_attention</title>
<polygon fill="none" stroke="black" points="161,-252 0,-252 0,-216 161,-216 161,-252" />
<text text-anchor="middle" x="80.5" y="-230.3" font-family="Times,serif" font-size="14.00">multihead_attention</text>
</g>
<!-- dropout&#45;&gt;multihead_attention -->
<g id="edge3" class="edge">
<title>dropout&#45;&gt;multihead_attention</title>
<path fill="none" stroke="black" d="M80.5,-180.17C80.5,-180.17 80.5,-205.59 80.5,-205.59" />
<polygon fill="black" stroke="black" points="77,-205.59 80.5,-215.59 84,-205.59 77,-205.59" />
</g>
<!-- feedforward -->
<g id="node5" class="node">
<title>feedforward</title>
<polygon fill="none" stroke="black" points="132.5,-324 28.5,-324 28.5,-288 132.5,-288 132.5,-324" />
<text text-anchor="middle" x="80.5" y="-302.3" font-family="Times,serif" font-size="14.00">feedforward</text>
</g>
<!-- multihead_attention&#45;&gt;feedforward -->
<g id="edge4" class="edge">
<title>multihead_attention&#45;&gt;feedforward</title>
<path fill="none" stroke="black" d="M80.5,-252.17C80.5,-252.17 80.5,-277.59 80.5,-277.59" />
<polygon fill="black" stroke="black" points="77,-277.59 80.5,-287.59 84,-277.59 77,-277.59" />
</g>
</g>
</svg>
</div>
</div>

<p>下面是 <code class="language-plaintext highlighter-rouge">train.py</code> 中实现的 Transformer 流程，其中的每一段代码，船长都会做详细解释，先不用急。这个流程里，首先定义了编码器，先使用了 Embedding 层将输入数据转换为词向量，使用 Positional Encoding 层对词向量进行位置编码，使用 Dropout 层进行 dropout 操作，然后进行多层 Multihead Attention 和 Feed Forward 操作。</p>

<p>在构建模型前，先执行 <code class="language-plaintext highlighter-rouge">train.py</code> 的主程序段，首先 <code class="language-plaintext highlighter-rouge">if __name__ == '__main__'</code> 这句代码是在 Python 中常用的一种编写方式，它的意思是当一个文件被直接运行时，<code class="language-plaintext highlighter-rouge">if</code> 语句下面的代码会被执行。请看下面代码的注释。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">'__main__'</span><span class="p">:</span>                
    
    <span class="c1"># 加载词汇表   
</span>    <span class="n">de2idx</span><span class="p">,</span> <span class="n">idx2de</span> <span class="o">=</span> <span class="n">load_de_vocab</span><span class="p">()</span>
    <span class="n">en2idx</span><span class="p">,</span> <span class="n">idx2en</span> <span class="o">=</span> <span class="n">load_en_vocab</span><span class="p">()</span>
    
    <span class="c1"># 构建模型并训练
</span>    <span class="n">g</span> <span class="o">=</span> <span class="n">Graph</span><span class="p">(</span><span class="s">"train"</span><span class="p">);</span> <span class="k">print</span><span class="p">(</span><span class="s">"Graph loaded"</span><span class="p">)</span>
    
    <span class="c1"># 创建了一个 Supervisor 对象来管理训练过程
</span>    <span class="n">sv</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">train</span><span class="p">.</span><span class="n">Supervisor</span><span class="p">(</span><span class="n">graph</span><span class="o">=</span><span class="n">g</span><span class="p">.</span><span class="n">graph</span><span class="p">,</span> 
                             <span class="n">logdir</span><span class="o">=</span><span class="n">hp</span><span class="p">.</span><span class="n">logdir</span><span class="p">,</span>
                             <span class="n">save_model_secs</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="c1"># 使用 with 语句打开一个会话
</span>    <span class="k">with</span> <span class="n">sv</span><span class="p">.</span><span class="n">managed_session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>

    	<span class="c1"># 训练迭代 hp.num_epochs 次
</span>        <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">hp</span><span class="p">.</span><span class="n">num_epochs</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span> 
            <span class="k">if</span> <span class="n">sv</span><span class="p">.</span><span class="n">should_stop</span><span class="p">():</span> <span class="k">break</span>

            <span class="c1"># tqdm 是一个 Python 库，用来在循环执行训练操作时在命令行中显示进度条
</span>            <span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">g</span><span class="p">.</span><span class="n">num_batch</span><span class="p">),</span> <span class="n">total</span><span class="o">=</span><span class="n">g</span><span class="p">.</span><span class="n">num_batch</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">70</span><span class="p">,</span> <span class="n">leave</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">unit</span><span class="o">=</span><span class="s">'b'</span><span class="p">):</span>

            	<span class="c1"># 每次迭代都会运行训练操作 g.train_op
</span>                <span class="n">sess</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">g</span><span class="p">.</span><span class="n">train_op</span><span class="p">)</span>

            <span class="c1"># 获取训练的步数，通过 sess.run() 函数获取 global_step 的当前值并赋值给 gs。这样可在后面使用 gs 保存模型时用这个值命名模型
</span>            <span class="n">gs</span> <span class="o">=</span> <span class="n">sess</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">g</span><span class="p">.</span><span class="n">global_step</span><span class="p">)</span>

            <span class="c1"># 每个 epoch 结束时，它使用 saver.save() 函数保存当前模型的状态
</span>            <span class="n">sv</span><span class="p">.</span><span class="n">saver</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="n">hp</span><span class="p">.</span><span class="n">logdir</span> <span class="o">+</span> <span class="s">'/model_epoch_%02d_gs_%d'</span> <span class="o">%</span> <span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="n">gs</span><span class="p">))</span>
    
    <span class="k">print</span><span class="p">(</span><span class="s">"Done"</span><span class="p">)</span>
</code></pre></div></div>

<ul>
  <li><code class="language-plaintext highlighter-rouge">num_epochs</code> 是训练过程中迭代的次数，它表示训练模型需要在训练数据上跑多少遍。每一次迭代都会在训练数据集上进行训练，通常来说，训练数据集会被重复多次迭代，直到达到 <code class="language-plaintext highlighter-rouge">num_epochs</code> 次。这样可以确保模型能够充分地学习数据的特征。设置 <code class="language-plaintext highlighter-rouge">num_epochs</code> 的值过大或过小都会导致模型性能下降。</li>
</ul>

<h5 id="1541编码过程">15.4.1、编码过程</h5>

<h6 id="embedding">Embedding</h6>

<p><code class="language-plaintext highlighter-rouge">embedding</code> 用来把输入生成词嵌入向量：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 词语转换为对应的词向量表示
</span><span class="bp">self</span><span class="p">.</span><span class="n">enc</span> <span class="o">=</span> <span class="n">embedding</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">x</span><span class="p">,</span> 
                      <span class="n">vocab_size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">de2idx</span><span class="p">),</span> 
                      <span class="n">num_units</span><span class="o">=</span><span class="n">hp</span><span class="p">.</span><span class="n">hidden_units</span><span class="p">,</span> 
                      <span class="n">scale</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                      <span class="n">scope</span><span class="o">=</span><span class="s">"enc_embed"</span><span class="p">)</span>
</code></pre></div></div>

<ul>
  <li><code class="language-plaintext highlighter-rouge">vocab_size</code> 是词汇表的大小。</li>
  <li><code class="language-plaintext highlighter-rouge">num_units</code> 是词向量的维度。</li>
  <li><code class="language-plaintext highlighter-rouge">scale</code> 是一个布尔值，用来确定是否对词向量进行标准化。</li>
  <li><code class="language-plaintext highlighter-rouge">scope</code> 是变量作用域的名称。</li>
</ul>

<h6 id="key-masks">Key Masks</h6>

<p>接着生成一个 <code class="language-plaintext highlighter-rouge">key_masks</code> 用于在之后的计算中屏蔽掉某些位置的信息，以便模型只关注有效的信息。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">key_masks</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">sign</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="nb">abs</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">enc</span><span class="p">),</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<ul>
  <li>先对 <code class="language-plaintext highlighter-rouge">self.enc</code> 张量进行对每个元素求绝对值的操作</li>
  <li>沿着最后一阶作为轴，进行 <code class="language-plaintext highlighter-rouge">reduce_sum</code> 操作，得到一个 (batch, sequence_length) 形状的张量。</li>
  <li>再进行 <code class="language-plaintext highlighter-rouge">tf.sign</code> 操作，对刚得到的每个元素进行符号函数的变换。</li>
  <li>最后再扩展阶数，变成形状 (batch, sequence_length, 1) 的张量。</li>
</ul>

<h6 id="positional-encoding">Positional Encoding</h6>

<p>下面生成 Transformer 的位置编码：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 位置编码
</span><span class="k">if</span> <span class="n">hp</span><span class="p">.</span><span class="n">sinusoid</span><span class="p">:</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">enc</span> <span class="o">+=</span> <span class="n">positional_encoding</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">x</span><span class="p">,</span>
                      <span class="n">num_units</span><span class="o">=</span><span class="n">hp</span><span class="p">.</span><span class="n">hidden_units</span><span class="p">,</span> 
                      <span class="n">zero_pad</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> 
                      <span class="n">scale</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
                      <span class="n">scope</span><span class="o">=</span><span class="s">"enc_pe"</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">enc</span> <span class="o">+=</span> <span class="n">embedding</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">tile</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="nb">range</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">shape</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">x</span><span class="p">)[</span><span class="mi">1</span><span class="p">]),</span> <span class="mi">0</span><span class="p">),</span>
    							 <span class="p">[</span><span class="n">tf</span><span class="p">.</span><span class="n">shape</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">x</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">]),</span>
                      <span class="n">vocab_size</span><span class="o">=</span><span class="n">hp</span><span class="p">.</span><span class="n">maxlen</span><span class="p">,</span> 
                      <span class="n">num_units</span><span class="o">=</span><span class="n">hp</span><span class="p">.</span><span class="n">hidden_units</span><span class="p">,</span> 
                      <span class="n">zero_pad</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> 
                      <span class="n">scale</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
                      <span class="n">scope</span><span class="o">=</span><span class="s">"enc_pe"</span><span class="p">)</span>
</code></pre></div></div>

<p>如果超参数 <code class="language-plaintext highlighter-rouge">hp.sinusoid=True</code>，使用 <code class="language-plaintext highlighter-rouge">positional_encoding</code> 函数，通过使用正弦和余弦函数来生成位置编码，可以为输入序列添加位置信息。如果 <code class="language-plaintext highlighter-rouge">hp.sinusoid=False</code>，使用 <code class="language-plaintext highlighter-rouge">embedding</code> 函数，通过学习的词嵌入来生成位置编码。</p>

<p>位置编码生成后，用 <code class="language-plaintext highlighter-rouge">key_masks</code> 处理一下。注意 <code class="language-plaintext highlighter-rouge">key_masks</code> 的生成一定要用最初的 <code class="language-plaintext highlighter-rouge">self.enc</code>，所以在前面执行而不是这里：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="bp">self</span><span class="p">.</span><span class="n">enc</span> <span class="o">*=</span> <span class="n">key_masks</span>
</code></pre></div></div>

<p>这个不是矩阵乘法，而是对应元素相乘。这里乘上 <code class="language-plaintext highlighter-rouge">key_masks</code> 的目的是将 <code class="language-plaintext highlighter-rouge">key_masks</code> 中值为 0 的位置对应的 <code class="language-plaintext highlighter-rouge">self.enc</code> 中的元素置为 0，这样就可以排除这些位置对计算的影响。</p>

<h6 id="drop-out">Drop out</h6>

<p>下面调用了 TensorFlow 的 drop out 操作：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="bp">self</span><span class="p">.</span><span class="n">enc</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">dropout</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">enc</span><span class="p">,</span> 
                            <span class="n">rate</span><span class="o">=</span><span class="n">hp</span><span class="p">.</span><span class="n">dropout_rate</span><span class="p">,</span> 
                            <span class="n">training</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">is_training</span><span class="p">))</span>
</code></pre></div></div>

<p>drop out 是一种在深度学习中常用的正则化技巧。它通过在训练过程中随机地「关闭」一些神经元来减少 <strong>过拟合</strong>。这样做是为了防止模型过于依赖于某些特定的特征，而导致在新数据上的表现不佳。</p>

<p>在这个函数中，<code class="language-plaintext highlighter-rouge">dropout</code> 层通过在训练过程中随机地将一些神经元的输出值设置为 0，来减少模型的过拟合。这个函数中使用了一个参数 <code class="language-plaintext highlighter-rouge">rate</code>，表示每个神经元被「关闭」的概率。这样做是为了防止模型过于依赖于某些特定的特征，而导致在新数据上的表现不佳。</p>

<h6 id="encoder-blocks-multi-head-attention--feed-forward">Encoder Blocks: Multi-Head Attention &amp; Feed Forward</h6>

<p>然后看下 encoder blocks 代码：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## Blocks
</span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">hp</span><span class="p">.</span><span class="n">num_blocks</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s">"num_blocks_{}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">i</span><span class="p">)):</span>
        <span class="c1"># 多头注意力
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">enc</span> <span class="o">=</span> <span class="n">multihead_attention</span><span class="p">(</span><span class="n">queries</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">enc</span><span class="p">,</span> 
                                        <span class="n">keys</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">enc</span><span class="p">,</span> 
                                        <span class="n">num_units</span><span class="o">=</span><span class="n">hp</span><span class="p">.</span><span class="n">hidden_units</span><span class="p">,</span> 
                                        <span class="n">num_heads</span><span class="o">=</span><span class="n">hp</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span> 
                                        <span class="n">dropout_rate</span><span class="o">=</span><span class="n">hp</span><span class="p">.</span><span class="n">dropout_rate</span><span class="p">,</span>
                                        <span class="n">is_training</span><span class="o">=</span><span class="n">is_training</span><span class="p">,</span>
                                        <span class="n">causality</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        
        <span class="c1"># 前馈神经网络
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">enc</span> <span class="o">=</span> <span class="n">feedforward</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">enc</span><span class="p">,</span> <span class="n">num_units</span><span class="o">=</span><span class="p">[</span><span class="mi">4</span><span class="o">*</span><span class="n">hp</span><span class="p">.</span><span class="n">hidden_units</span><span class="p">,</span> <span class="n">hp</span><span class="p">.</span><span class="n">hidden_units</span><span class="p">])</span>
</code></pre></div></div>

<p>上述代码是编码器（Encoder）的实现函数调用的流程，也是与船长上面的模型原理介绍一致的，在定义时同样使用了 Embedding 层、Positional Encoding 层、Dropout 层、Multihead Attention 和 Feed Forward 操作。其中 Multihead Attention 在编码、解码中是不一样的，待会儿我们会在 Decoder 部分再提到，有自注意力层和 Encoder-Decoder 层。</p>

<ul>
  <li>超参数 hp.num_blocks 表示 Encoder Blocks 的层数，每一层都有一个 Multi-Head Attention 和一个 Feed Forward。</li>
  <li>这个 Encoder 中的 Multi-Head Attention 是基于自注意力的（注意与后面的 Decoder 部分有区别）</li>
  <li><code class="language-plaintext highlighter-rouge">causality</code> 参数的意思是否使用 Causal Attention，它是 Self-Attention 的一种，但是只使用过去的信息，防止模型获取未来信息的干扰。一般对于预测序列中的某个时间步来说，只关注之前的信息，而不是整个序列的信息。这段代码中 <code class="language-plaintext highlighter-rouge">causality</code> 设置为了 <code class="language-plaintext highlighter-rouge">False</code>，即会关注整个序列的信息。</li>
</ul>

<h5 id="1542解码过程">15.4.2、解码过程</h5>

<p>再看一下解码的流程：</p>

<div style="text-align: center;">
<div class="graphviz-wrapper">

<!-- Generated by graphviz version 2.43.0 (0)
 -->
<!-- Title: G Pages: 1 -->
<svg role="img" aria-label="graphviz-0ec29ea83329c971f433bc6641585297" width="372pt" height="404pt" viewBox="0.00 0.00 372.00 404.00">
<title>graphviz-0ec29ea83329c971f433bc6641585297</title>
<desc>
digraph G {
	rankdir=BT
	splines=ortho
	node [shape=&quot;box&quot;]
	decoder_attn1 [label=&quot;multihead_attention (self-attention)&quot;]
	decoder_attn2 [label=&quot;multihead_attention (encoder-decoder attention)&quot;]

	embedding -&gt; positional_encoding -&gt; dropout -&gt; decoder_attn1 -&gt; decoder_attn2 -&gt; feedforward
}
</desc>

<g id="graph0" class="graph" transform="scale(1 1) rotate(0) translate(4 400)">
<title>G</title>
<polygon fill="white" stroke="transparent" points="-4,4 -4,-400 368,-400 368,4 -4,4" />
<!-- decoder_attn1 -->
<g id="node1" class="node">
<title>decoder_attn1</title>
<polygon fill="none" stroke="black" points="317,-252 47,-252 47,-216 317,-216 317,-252" />
<text text-anchor="middle" x="182" y="-230.3" font-family="Times,serif" font-size="14.00">multihead_attention (self&#45;attention)</text>
</g>
<!-- decoder_attn2 -->
<g id="node2" class="node">
<title>decoder_attn2</title>
<polygon fill="none" stroke="black" points="364,-324 0,-324 0,-288 364,-288 364,-324" />
<text text-anchor="middle" x="182" y="-302.3" font-family="Times,serif" font-size="14.00">multihead_attention (encoder&#45;decoder attention)</text>
</g>
<!-- decoder_attn1&#45;&gt;decoder_attn2 -->
<g id="edge4" class="edge">
<title>decoder_attn1&#45;&gt;decoder_attn2</title>
<path fill="none" stroke="black" d="M182,-252.17C182,-252.17 182,-277.59 182,-277.59" />
<polygon fill="black" stroke="black" points="178.5,-277.59 182,-287.59 185.5,-277.59 178.5,-277.59" />
</g>
<!-- feedforward -->
<g id="node6" class="node">
<title>feedforward</title>
<polygon fill="none" stroke="black" points="234,-396 130,-396 130,-360 234,-360 234,-396" />
<text text-anchor="middle" x="182" y="-374.3" font-family="Times,serif" font-size="14.00">feedforward</text>
</g>
<!-- decoder_attn2&#45;&gt;feedforward -->
<g id="edge5" class="edge">
<title>decoder_attn2&#45;&gt;feedforward</title>
<path fill="none" stroke="black" d="M182,-324.17C182,-324.17 182,-349.59 182,-349.59" />
<polygon fill="black" stroke="black" points="178.5,-349.59 182,-359.59 185.5,-349.59 178.5,-349.59" />
</g>
<!-- embedding -->
<g id="node3" class="node">
<title>embedding</title>
<polygon fill="none" stroke="black" points="230,-36 134,-36 134,0 230,0 230,-36" />
<text text-anchor="middle" x="182" y="-14.3" font-family="Times,serif" font-size="14.00">embedding</text>
</g>
<!-- positional_encoding -->
<g id="node4" class="node">
<title>positional_encoding</title>
<polygon fill="none" stroke="black" points="261,-108 103,-108 103,-72 261,-72 261,-108" />
<text text-anchor="middle" x="182" y="-86.3" font-family="Times,serif" font-size="14.00">positional_encoding</text>
</g>
<!-- embedding&#45;&gt;positional_encoding -->
<g id="edge1" class="edge">
<title>embedding&#45;&gt;positional_encoding</title>
<path fill="none" stroke="black" d="M182,-36.17C182,-36.17 182,-61.59 182,-61.59" />
<polygon fill="black" stroke="black" points="178.5,-61.59 182,-71.59 185.5,-61.59 178.5,-61.59" />
</g>
<!-- dropout -->
<g id="node5" class="node">
<title>dropout</title>
<polygon fill="none" stroke="black" points="218.5,-180 145.5,-180 145.5,-144 218.5,-144 218.5,-180" />
<text text-anchor="middle" x="182" y="-158.3" font-family="Times,serif" font-size="14.00">dropout</text>
</g>
<!-- positional_encoding&#45;&gt;dropout -->
<g id="edge2" class="edge">
<title>positional_encoding&#45;&gt;dropout</title>
<path fill="none" stroke="black" d="M182,-108.17C182,-108.17 182,-133.59 182,-133.59" />
<polygon fill="black" stroke="black" points="178.5,-133.59 182,-143.59 185.5,-133.59 178.5,-133.59" />
</g>
<!-- dropout&#45;&gt;decoder_attn1 -->
<g id="edge3" class="edge">
<title>dropout&#45;&gt;decoder_attn1</title>
<path fill="none" stroke="black" d="M182,-180.17C182,-180.17 182,-205.59 182,-205.59" />
<polygon fill="black" stroke="black" points="178.5,-205.59 182,-215.59 185.5,-205.59 178.5,-205.59" />
</g>
</g>
</svg>
</div>
</div>

<h6 id="embedding-1">Embedding</h6>

<p>下面我们逐一看每段代码，主要关注与编码阶段的区别即可：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="bp">self</span><span class="p">.</span><span class="n">dec</span> <span class="o">=</span> <span class="n">embedding</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">decoder_inputs</span><span class="p">,</span> 
                      <span class="n">vocab_size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">en2idx</span><span class="p">),</span> 
                      <span class="n">num_units</span><span class="o">=</span><span class="n">hp</span><span class="p">.</span><span class="n">hidden_units</span><span class="p">,</span>
                      <span class="n">scale</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> 
                      <span class="n">scope</span><span class="o">=</span><span class="s">"dec_embed"</span><span class="p">)</span>
</code></pre></div></div>

<ul>
  <li><code class="language-plaintext highlighter-rouge">embedding</code> 输入用的是 <code class="language-plaintext highlighter-rouge">self.decoder_inputs</code></li>
  <li>词汇表尺寸用翻译后的输出语言英语词汇表长度 <code class="language-plaintext highlighter-rouge">len(en2idx)</code></li>
</ul>

<h6 id="key-masks-1">Key Masks</h6>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">key_masks</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">sign</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="nb">abs</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dec</span><span class="p">),</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<ul>
  <li><code class="language-plaintext highlighter-rouge">key_masks</code> 输入变量用 <code class="language-plaintext highlighter-rouge">self.dec</code>。</li>
</ul>

<h6 id="positional-encoding--drop-out">Positional Encoding &amp; Drop out</h6>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 位置编码
</span><span class="k">if</span> <span class="n">hp</span><span class="p">.</span><span class="n">sinusoid</span><span class="p">:</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">dec</span> <span class="o">+=</span> <span class="n">positional_encoding</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">decoder_inputs</span><span class="p">,</span>
                      <span class="n">vocab_size</span><span class="o">=</span><span class="n">hp</span><span class="p">.</span><span class="n">maxlen</span><span class="p">,</span> 
                      <span class="n">num_units</span><span class="o">=</span><span class="n">hp</span><span class="p">.</span><span class="n">hidden_units</span><span class="p">,</span> 
                      <span class="n">zero_pad</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> 
                      <span class="n">scale</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
                      <span class="n">scope</span><span class="o">=</span><span class="s">"dec_pe"</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">dec</span> <span class="o">+=</span> <span class="n">embedding</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">tile</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="nb">range</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">shape</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">decoder_inputs</span><span class="p">)[</span><span class="mi">1</span><span class="p">]),</span> <span class="mi">0</span><span class="p">),</span>
    							 <span class="p">[</span><span class="n">tf</span><span class="p">.</span><span class="n">shape</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">decoder_inputs</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">]),</span>
                      <span class="n">vocab_size</span><span class="o">=</span><span class="n">hp</span><span class="p">.</span><span class="n">maxlen</span><span class="p">,</span> 
                      <span class="n">num_units</span><span class="o">=</span><span class="n">hp</span><span class="p">.</span><span class="n">hidden_units</span><span class="p">,</span> 
                      <span class="n">zero_pad</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> 
                      <span class="n">scale</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
                      <span class="n">scope</span><span class="o">=</span><span class="s">"dec_pe"</span><span class="p">)</span>

<span class="bp">self</span><span class="p">.</span><span class="n">dec</span> <span class="o">*=</span> <span class="n">key_masks</span>

<span class="bp">self</span><span class="p">.</span><span class="n">dec</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">dropout</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dec</span><span class="p">,</span> 
                            <span class="n">rate</span><span class="o">=</span><span class="n">hp</span><span class="p">.</span><span class="n">dropout_rate</span><span class="p">,</span> 
                            <span class="n">training</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">is_training</span><span class="p">))</span>
</code></pre></div></div>

<ul>
  <li>输入 <code class="language-plaintext highlighter-rouge">self.decoder_inputs</code></li>
  <li>指定 <code class="language-plaintext highlighter-rouge">vocab_size</code> 参数 <code class="language-plaintext highlighter-rouge">hp.maxlen</code></li>
</ul>

<h6 id="decoder-blocks-multi-head-attention--feed-forward">Decoder Blocks: Multi-Head Attention &amp; Feed Forward</h6>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## 解码器模块
</span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">hp</span><span class="p">.</span><span class="n">num_blocks</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s">"num_blocks_{}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">i</span><span class="p">)):</span>
        <span class="c1"># 多头注意力（自注意力）
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">dec</span> <span class="o">=</span> <span class="n">multihead_attention</span><span class="p">(</span><span class="n">queries</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">dec</span><span class="p">,</span> 
                                        <span class="n">keys</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">dec</span><span class="p">,</span> 
                                        <span class="n">num_units</span><span class="o">=</span><span class="n">hp</span><span class="p">.</span><span class="n">hidden_units</span><span class="p">,</span> 
                                        <span class="n">num_heads</span><span class="o">=</span><span class="n">hp</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span> 
                                        <span class="n">dropout_rate</span><span class="o">=</span><span class="n">hp</span><span class="p">.</span><span class="n">dropout_rate</span><span class="p">,</span>
                                        <span class="n">is_training</span><span class="o">=</span><span class="n">is_training</span><span class="p">,</span>
                                        <span class="n">causality</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> 
                                        <span class="n">scope</span><span class="o">=</span><span class="s">"self_attention"</span><span class="p">)</span>
        
        <span class="c1"># 多头注意力（Encoder-Decoder 注意力）
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">dec</span> <span class="o">=</span> <span class="n">multihead_attention</span><span class="p">(</span><span class="n">queries</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">dec</span><span class="p">,</span> 
                                        <span class="n">keys</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">enc</span><span class="p">,</span> 
                                        <span class="n">num_units</span><span class="o">=</span><span class="n">hp</span><span class="p">.</span><span class="n">hidden_units</span><span class="p">,</span> 
                                        <span class="n">num_heads</span><span class="o">=</span><span class="n">hp</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span>
                                        <span class="n">dropout_rate</span><span class="o">=</span><span class="n">hp</span><span class="p">.</span><span class="n">dropout_rate</span><span class="p">,</span>
                                        <span class="n">is_training</span><span class="o">=</span><span class="n">is_training</span><span class="p">,</span> 
                                        <span class="n">causality</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
                                        <span class="n">scope</span><span class="o">=</span><span class="s">"vanilla_attention"</span><span class="p">)</span>

        <span class="c1"># 前馈神经网络
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">dec</span> <span class="o">=</span> <span class="n">feedforward</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dec</span><span class="p">,</span> <span class="n">num_units</span><span class="o">=</span><span class="p">[</span><span class="mi">4</span><span class="o">*</span><span class="n">hp</span><span class="p">.</span><span class="n">hidden_units</span><span class="p">,</span> <span class="n">hp</span><span class="p">.</span><span class="n">hidden_units</span><span class="p">])</span>
</code></pre></div></div>

<ul>
  <li>在用 <code class="language-plaintext highlighter-rouge">multihead_attention</code> 函数解码器模块时，注意传入的参数 <code class="language-plaintext highlighter-rouge">scope</code> 区别，先是自注意力层，用参数 <code class="language-plaintext highlighter-rouge">self_attention</code>，对应的 <code class="language-plaintext highlighter-rouge">queries</code> 是 <code class="language-plaintext highlighter-rouge">self.dec</code>，<code class="language-plaintext highlighter-rouge">keys</code> 也是 <code class="language-plaintext highlighter-rouge">self.dec</code>。再是「Encoder-Decder Attention」用的是参数 <code class="language-plaintext highlighter-rouge">vanilla_attention</code>，对应的 <code class="language-plaintext highlighter-rouge">queries</code> 来自解码器是 <code class="language-plaintext highlighter-rouge">self.dec</code>，但 <code class="language-plaintext highlighter-rouge">keys</code> 来自编码器是是 <code class="language-plaintext highlighter-rouge">self.enc</code>。</li>
</ul>

<h5 id="1543embeddingpositional-encodingmulti-head-attentionfeed-forward">15.4.3、Embedding、Positional Encoding、Multi-Head Attention、Feed Forward</h5>

<h6 id="embedding-函数实现">Embedding 函数实现</h6>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">embedding</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> 
              <span class="n">vocab_size</span><span class="p">,</span> 
              <span class="n">num_units</span><span class="p">,</span> 
              <span class="n">zero_pad</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> 
              <span class="n">scale</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
              <span class="n">scope</span><span class="o">=</span><span class="s">"embedding"</span><span class="p">,</span> 
              <span class="n">reuse</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="n">scope</span><span class="p">,</span> <span class="n">reuse</span><span class="o">=</span><span class="n">reuse</span><span class="p">):</span>

    	<span class="c1"># 创建一个名为 `lookup_table`、形状为 (vocab_size, num_units) 的矩阵
</span>        <span class="n">lookup_table</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s">'lookup_table'</span><span class="p">,</span>
                                       <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">float32</span><span class="p">,</span>
                                       <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">num_units</span><span class="p">],</span>
                                       <span class="n">initializer</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">contrib</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">xavier_initializer</span><span class="p">())</span>

        <span class="c1"># lookup_table 的第一行插入一个全零行，作为 PAD 的词向量
</span>        <span class="k">if</span> <span class="n">zero_pad</span><span class="p">:</span>
            <span class="n">lookup_table</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">concat</span><span class="p">((</span><span class="n">tf</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_units</span><span class="p">]),</span>
                                      <span class="n">lookup_table</span><span class="p">[</span><span class="mi">1</span><span class="p">:,</span> <span class="p">:]),</span> <span class="mi">0</span><span class="p">)</span>

        <span class="c1"># 在词向量矩阵 lookup_table 中查找 inputs
</span>        <span class="n">outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">embedding_lookup</span><span class="p">(</span><span class="n">lookup_table</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span>
        
        <span class="c1"># 对输出的词向量进行除以根号 num_units 的操作，可以控制词向量的统计稳定性。
</span>        <span class="k">if</span> <span class="n">scale</span><span class="p">:</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="n">outputs</span> <span class="o">*</span> <span class="p">(</span><span class="n">num_units</span> <span class="o">**</span> <span class="mf">0.5</span><span class="p">)</span> 
            
    <span class="k">return</span> <span class="n">outputs</span>
</code></pre></div></div>

<h6 id="positional-encoding-函数实现">Positional Encoding 函数实现</h6>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">positional_encoding</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span>
                        <span class="n">num_units</span><span class="p">,</span>
                        <span class="n">zero_pad</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                        <span class="n">scale</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                        <span class="n">scope</span><span class="o">=</span><span class="s">"positional_encoding"</span><span class="p">,</span>
                        <span class="n">reuse</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>

    <span class="n">N</span><span class="p">,</span> <span class="n">T</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">.</span><span class="n">get_shape</span><span class="p">().</span><span class="n">as_list</span><span class="p">()</span>
    <span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="n">scope</span><span class="p">,</span> <span class="n">reuse</span><span class="o">=</span><span class="n">reuse</span><span class="p">):</span>

    	<span class="c1"># tf.range(T) 生成一个 0~T-1 的数组
</span>    	<span class="c1"># tf.tile() 将其扩展成 N*T 的矩阵，表示每个词的位置
</span>        <span class="n">position_ind</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">tile</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="nb">range</span><span class="p">(</span><span class="n">T</span><span class="p">),</span> <span class="mi">0</span><span class="p">),</span> <span class="p">[</span><span class="n">N</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>

        <span class="c1"># First part of the PE function: sin and cos argument
</span>        <span class="n">position_enc</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span>
            <span class="p">[</span><span class="n">pos</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="n">power</span><span class="p">(</span><span class="mi">10000</span><span class="p">,</span> <span class="mf">2.</span><span class="o">*</span><span class="n">i</span><span class="o">/</span><span class="n">num_units</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_units</span><span class="p">)]</span>
            <span class="k">for</span> <span class="n">pos</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">T</span><span class="p">)])</span>

        <span class="c1"># 用 numpy 的 sin 和 cos 函数对每个位置进行编码
</span>        <span class="n">position_enc</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">sin</span><span class="p">(</span><span class="n">position_enc</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">])</span>  <span class="c1"># dim 2i
</span>        <span class="n">position_enc</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">cos</span><span class="p">(</span><span class="n">position_enc</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">])</span>  <span class="c1"># dim 2i+1
</span>
        <span class="c1"># 将编码结果转为张量
</span>        <span class="n">lookup_table</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">position_enc</span><span class="p">)</span>

        <span class="c1"># 将编码的结果与位置索引相关联，得到最终的位置编码
</span>        <span class="k">if</span> <span class="n">zero_pad</span><span class="p">:</span>
        	<span class="c1"># 如果 zero_pad 参数为 True，则在编码结果的开头添加一个全 0 的向量
</span>            <span class="n">lookup_table</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">concat</span><span class="p">((</span><span class="n">tf</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_units</span><span class="p">]),</span>
                                      <span class="n">lookup_table</span><span class="p">[</span><span class="mi">1</span><span class="p">:,</span> <span class="p">:]),</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">embedding_lookup</span><span class="p">(</span><span class="n">lookup_table</span><span class="p">,</span> <span class="n">position_ind</span><span class="p">)</span>

        <span class="c1"># scale 参数为 True，则将编码结果乘上 num_units 的平方根
</span>        <span class="k">if</span> <span class="n">scale</span><span class="p">:</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="n">outputs</span> <span class="o">*</span> <span class="n">num_units</span><span class="o">**</span><span class="mf">0.5</span>

        <span class="k">return</span> <span class="n">outputs</span>
</code></pre></div></div>

<h6 id="multi-head-attention-函数实现">Multi-Head Attention 函数实现</h6>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">multihead_attention</span><span class="p">(</span><span class="n">queries</span><span class="p">,</span> 
                        <span class="n">keys</span><span class="p">,</span> 
                        <span class="n">num_units</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> 
                        <span class="n">num_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> 
                        <span class="n">dropout_rate</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                        <span class="n">is_training</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                        <span class="n">causality</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
                        <span class="n">scope</span><span class="o">=</span><span class="s">"multihead_attention"</span><span class="p">,</span> 
                        <span class="n">reuse</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="n">scope</span><span class="p">,</span> <span class="n">reuse</span><span class="o">=</span><span class="n">reuse</span><span class="p">):</span>
        <span class="c1"># Set the fall back option for num_units
</span>        <span class="k">if</span> <span class="n">num_units</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">num_units</span> <span class="o">=</span> <span class="n">queries</span><span class="p">.</span><span class="n">get_shape</span><span class="p">().</span><span class="n">as_list</span><span class="p">()[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        
        <span class="c1"># Linear Projections
</span>        <span class="c1"># 使用三个全连接层对输入的 queries、keys 分别进行线性变换，将其转换为三个维度相同的张量 Q/K/V
</span>        <span class="n">Q</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">dense</span><span class="p">(</span><span class="n">queries</span><span class="p">,</span> <span class="n">num_units</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">relu</span><span class="p">)</span> <span class="c1"># (N, T_q, C)
</span>        <span class="n">K</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">dense</span><span class="p">(</span><span class="n">keys</span><span class="p">,</span> <span class="n">num_units</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">relu</span><span class="p">)</span> <span class="c1"># (N, T_k, C)
</span>        <span class="n">V</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">dense</span><span class="p">(</span><span class="n">keys</span><span class="p">,</span> <span class="n">num_units</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">relu</span><span class="p">)</span> <span class="c1"># (N, T_k, C)
</span>        
        <span class="c1"># Split and concat
</span>        <span class="c1"># 按头数 split Q/K/V，再各自连接起来
</span>        <span class="n">Q_</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">concat</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">split</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="c1"># (h*N, T_q, C/h) 
</span>        <span class="n">K_</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">concat</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">split</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="c1"># (h*N, T_k, C/h) 
</span>        <span class="n">V_</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">concat</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">split</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="c1"># (h*N, T_k, C/h) 
</span>
        <span class="c1"># Multiplication
</span>        <span class="c1"># 计算 Q_, K_, V_ 的点积来获得注意力权重
</span>        <span class="c1"># 其中 Q_ 的维度为 (hN, T_q, C/h)
</span>        <span class="c1"># K_ 的维度为 (hN, T_k, C/h)
</span>        <span class="c1"># 计算出来的结果 outputs 的维度为 (h*N, T_q, T_k)
</span>        <span class="n">outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">Q_</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">K_</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]))</span> <span class="c1"># (h*N, T_q, T_k)
</span>
        <span class="c1"># Scale
</span>        <span class="c1"># 对权重进行 scale，这里除以了 K_ 的第三维的平方根，用于缩放权重
</span>        <span class="n">outputs</span> <span class="o">=</span> <span class="n">outputs</span> <span class="o">/</span> <span class="p">(</span><span class="n">K_</span><span class="p">.</span><span class="n">get_shape</span><span class="p">().</span><span class="n">as_list</span><span class="p">()[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">**</span> <span class="mf">0.5</span><span class="p">)</span>
        
        <span class="c1"># Key Masking
</span>        <span class="c1"># 这里需要将 keys 的有效部分标记出来，将无效部分设置为极小值，以便在之后的 softmax 中被忽略
</span>        <span class="n">key_masks</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">sign</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="nb">abs</span><span class="p">(</span><span class="n">keys</span><span class="p">),</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">))</span> <span class="c1"># (N, T_k)
</span>        <span class="n">key_masks</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">tile</span><span class="p">(</span><span class="n">key_masks</span><span class="p">,</span> <span class="p">[</span><span class="n">num_heads</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span> <span class="c1"># (h*N, T_k)
</span>        <span class="n">key_masks</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">tile</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">key_masks</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">shape</span><span class="p">(</span><span class="n">queries</span><span class="p">)[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">1</span><span class="p">])</span> <span class="c1"># (h*N, T_q, T_k)
</span>        
        <span class="n">paddings</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="o">**</span><span class="mi">32</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">where</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">equal</span><span class="p">(</span><span class="n">key_masks</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">paddings</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)</span> <span class="c1"># (h*N, T_q, T_k)
</span>  
        <span class="c1"># Causality = Future blinding
</span>        <span class="k">if</span> <span class="n">causality</span><span class="p">:</span>

        	<span class="c1"># 创建一个与 outputs[0, :, :] 相同形状的全 1 矩阵
</span>            <span class="n">diag_vals</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:])</span> <span class="c1"># (T_q, T_k)
</span>
            <span class="c1"># 对 diag_vals 进行处理，返回一个下三角线矩阵
</span>            <span class="n">tril</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">LinearOperatorLowerTriangular</span><span class="p">(</span><span class="n">diag_vals</span><span class="p">).</span><span class="n">to_dense</span><span class="p">()</span> <span class="c1"># (T_q, T_k)
</span>            <span class="n">masks</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">tile</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">tril</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">[</span><span class="n">tf</span><span class="p">.</span><span class="n">shape</span><span class="p">(</span><span class="n">outputs</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span> <span class="c1"># (h*N, T_q, T_k)
</span>   
   			<span class="c1"># 将 masks 为 0 的位置的 outputs 值设置为一个非常小的数
</span>   			<span class="c1"># 这样会导致这些位置在之后的计算中对结果产生非常小的影响，从而实现了遮盖未来信息的功能
</span>            <span class="n">paddings</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">masks</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="o">**</span><span class="mi">32</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">where</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">equal</span><span class="p">(</span><span class="n">masks</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">paddings</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)</span> <span class="c1"># (h*N, T_q, T_k)
</span>  
        <span class="c1"># 对于每个头的输出，应用 softmax 激活函数，这样可以得到一个概率分布
</span>        <span class="n">outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span> <span class="c1"># (h*N, T_q, T_k)
</span>         
        <span class="c1"># Query Masking
</span>        <span class="c1"># 对于查询（queries）进行 masking，这样可以避免输入序列后面的词对之前词的影响
</span>        <span class="n">query_masks</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">sign</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="nb">abs</span><span class="p">(</span><span class="n">queries</span><span class="p">),</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">))</span> <span class="c1"># (N, T_q)
</span>        <span class="n">query_masks</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">tile</span><span class="p">(</span><span class="n">query_masks</span><span class="p">,</span> <span class="p">[</span><span class="n">num_heads</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span> <span class="c1"># (h*N, T_q)
</span>        <span class="n">query_masks</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">tile</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">query_masks</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">shape</span><span class="p">(</span><span class="n">keys</span><span class="p">)[</span><span class="mi">1</span><span class="p">]])</span> <span class="c1"># (h*N, T_q, T_k)
</span>        <span class="n">outputs</span> <span class="o">*=</span> <span class="n">query_masks</span> <span class="c1"># broadcasting. (N, T_q, C)
</span>          
        <span class="c1"># Dropouts &amp; Weighted Sum
</span>        <span class="c1"># 对于每个头的输出，应用 dropout 以及进行残差连接
</span>        <span class="n">outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">rate</span><span class="o">=</span><span class="n">dropout_rate</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">is_training</span><span class="p">))</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">V_</span><span class="p">)</span> <span class="c1"># ( h*N, T_q, C/h)
</span>        
        <span class="c1"># Restore shape
</span>        <span class="c1"># 将每个头的输出拼接起来，使用 tf.concat 函数，将不同头的结果按照第二维拼接起来
</span>        <span class="c1"># 得到最终的输出结果，即经过多头注意力计算后的结果
</span>        <span class="n">outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">concat</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">split</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span> <span class="p">)</span> <span class="c1"># (N, T_q, C)
</span>              
        <span class="c1"># Residual connection
</span>        <span class="n">outputs</span> <span class="o">+=</span> <span class="n">queries</span>
              
        <span class="c1"># Normalize
</span>        <span class="n">outputs</span> <span class="o">=</span> <span class="n">normalize</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span> <span class="c1"># (N, T_q, C)
</span> 
    <span class="k">return</span> <span class="n">outputs</span>
</code></pre></div></div>

<h6 id="feed-forward-函数实现">Feed Forward 函数实现</h6>

<p>下面是 <strong>前馈神经网络层</strong> 的定义，这是一个非线性变换，这里用到了一些卷积神经网络（CNN）的知识，我们来看下代码再解释：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">feedforward</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> 
                <span class="n">num_units</span><span class="o">=</span><span class="p">[</span><span class="mi">2048</span><span class="p">,</span> <span class="mi">512</span><span class="p">],</span>
                <span class="n">scope</span><span class="o">=</span><span class="s">"multihead_attention"</span><span class="p">,</span> 
                <span class="n">reuse</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="n">scope</span><span class="p">,</span> <span class="n">reuse</span><span class="o">=</span><span class="n">reuse</span><span class="p">):</span>
        <span class="c1"># Inner layer
</span>        <span class="n">params</span> <span class="o">=</span> <span class="p">{</span><span class="s">"inputs"</span><span class="p">:</span> <span class="n">inputs</span><span class="p">,</span> <span class="s">"filters"</span><span class="p">:</span> <span class="n">num_units</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s">"kernel_size"</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
                  <span class="s">"activation"</span><span class="p">:</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">relu</span><span class="p">,</span> <span class="s">"use_bias"</span><span class="p">:</span> <span class="bp">True</span><span class="p">}</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">conv1d</span><span class="p">(</span><span class="o">**</span><span class="n">params</span><span class="p">)</span>
        
        <span class="c1"># Readout layer
</span>        <span class="n">params</span> <span class="o">=</span> <span class="p">{</span><span class="s">"inputs"</span><span class="p">:</span> <span class="n">outputs</span><span class="p">,</span> <span class="s">"filters"</span><span class="p">:</span> <span class="n">num_units</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="s">"kernel_size"</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
                  <span class="s">"activation"</span><span class="p">:</span> <span class="bp">None</span><span class="p">,</span> <span class="s">"use_bias"</span><span class="p">:</span> <span class="bp">True</span><span class="p">}</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">conv1d</span><span class="p">(</span><span class="o">**</span><span class="n">params</span><span class="p">)</span>
        
        <span class="c1"># 连接一个残差网络 ResNet
</span>        <span class="n">outputs</span> <span class="o">+=</span> <span class="n">inputs</span>
        
        <span class="c1"># 归一化后输出
</span>        <span class="n">outputs</span> <span class="o">=</span> <span class="n">normalize</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">outputs</span>
</code></pre></div></div>

<ul>
  <li>先是使用了一个卷积层（conv1d）作为 inner layer、一个卷积层作为 readout layer，卷积核大小都为 1。</li>
  <li><code class="language-plaintext highlighter-rouge">filters</code> 参数用来控制卷积层中输出通道数量，inner layer 的输出通道数设置为 <code class="language-plaintext highlighter-rouge">num_units[0]</code> ，readout layer 的设置为 <code class="language-plaintext highlighter-rouge">num_units[1]</code>。有时也会把这个解释为神经元数量。这两个的默认分别为 2048、512，调用时传入的是超参数的 <code class="language-plaintext highlighter-rouge">[4 * hidden_units, hidden_units]</code>。</li>
  <li>其中 inner layer 用 <code class="language-plaintext highlighter-rouge">ReLU</code> 作为激活函数，然后连接一个残差网络 RedNet，把 readout layer 的输出加上原始的输入。</li>
  <li>最后使用 <code class="language-plaintext highlighter-rouge">normalize</code> 归一化处理输出，再返回。下面来看下 <code class="language-plaintext highlighter-rouge">normalize</code> 函数。</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">normalize</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> 
              <span class="n">epsilon</span> <span class="o">=</span> <span class="mf">1e-8</span><span class="p">,</span>
              <span class="n">scope</span><span class="o">=</span><span class="s">"ln"</span><span class="p">,</span>
              <span class="n">reuse</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="n">scope</span><span class="p">,</span> <span class="n">reuse</span><span class="o">=</span><span class="n">reuse</span><span class="p">):</span>

    	<span class="c1"># 输入数据的形状
</span>        <span class="n">inputs_shape</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">.</span><span class="n">get_shape</span><span class="p">()</span>
        <span class="n">params_shape</span> <span class="o">=</span> <span class="n">inputs_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">:]</span>
    
    	<span class="c1"># 平均数、方差
</span>        <span class="n">mean</span><span class="p">,</span> <span class="n">variance</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">moments</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">keep_dims</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

        <span class="c1"># 拉伸因子 beta
</span>        <span class="n">beta</span><span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">params_shape</span><span class="p">))</span>

        <span class="c1"># 缩放因子 gamma
</span>        <span class="n">gamma</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="n">params_shape</span><span class="p">))</span>

        <span class="c1"># 归一化：加上一个非常小的 epsilon，是为了防止除以 0
</span>        <span class="n">normalized</span> <span class="o">=</span> <span class="p">(</span><span class="n">inputs</span> <span class="o">-</span> <span class="n">mean</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span> <span class="p">(</span><span class="n">variance</span> <span class="o">+</span> <span class="n">epsilon</span><span class="p">)</span> <span class="o">**</span> <span class="p">(.</span><span class="mi">5</span><span class="p">)</span> <span class="p">)</span>

        <span class="n">outputs</span> <span class="o">=</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">normalized</span> <span class="o">+</span> <span class="n">beta</span>
        
    <span class="k">return</span> <span class="n">outputs</span>
</code></pre></div></div>

<ul>
  <li>该函数实现了 Layer Normalization，用于在深度神经网络中解决数据的不稳定性问题。</li>
</ul>

<h5 id="1544编码和解码完成后的操作">15.4.4、编码和解码完成后的操作</h5>

<p>解码器后的 <code class="language-plaintext highlighter-rouge">Linear &amp; Softmax</code>：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 全连接层得到的未经过归一化的概率值
</span><span class="bp">self</span><span class="p">.</span><span class="n">logits</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="n">dense</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dec</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">en2idx</span><span class="p">))</span>

<span class="c1"># 预测的英文单词 idx
</span><span class="bp">self</span><span class="p">.</span><span class="n">preds</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">to_int32</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">arg_max</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">logits</span><span class="p">,</span> <span class="n">dimension</span><span class="o">=-</span><span class="mi">1</span><span class="p">))</span>
<span class="bp">self</span><span class="p">.</span><span class="n">istarget</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">to_float</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">not_equal</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">y</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>

<span class="c1"># 正确预测数量，除以所有样本数，得到准确率
</span><span class="bp">self</span><span class="p">.</span><span class="n">acc</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">to_float</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">equal</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">preds</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">y</span><span class="p">))</span><span class="o">*</span><span class="bp">self</span><span class="p">.</span><span class="n">istarget</span><span class="p">)</span><span class="o">/</span> <span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">istarget</span><span class="p">))</span>

<span class="c1">#  记录了模型的准确率的值，用于 tensorboard 可视化
</span><span class="n">tf</span><span class="p">.</span><span class="n">summary</span><span class="p">.</span><span class="n">scalar</span><span class="p">(</span><span class="s">'acc'</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">acc</span><span class="p">)</span>
</code></pre></div></div>

<p>训练集数据处理时，经过 <code class="language-plaintext highlighter-rouge">Linear &amp; Softmax</code> 之后的最后处理如下。这里用到了 <code class="language-plaintext highlighter-rouge">tf.nn.softmax_cross_entropy_with_logits</code> 交叉熵损失，来计算模型的错误率 <code class="language-plaintext highlighter-rouge">mean_loss</code>，并使用 Adam 优化器 <code class="language-plaintext highlighter-rouge">AdamOptimizer</code> 来优化模型参数。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 使用 label_smoothing 函数对真实标签进行标签平滑，得到 self.y_smoothed
</span><span class="bp">self</span><span class="p">.</span><span class="n">y_smoothed</span> <span class="o">=</span> <span class="n">label_smoothing</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">one_hot</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">y</span><span class="p">,</span> <span class="n">depth</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">en2idx</span><span class="p">)))</span>
</code></pre></div></div>

<p>下面这段代码实现了一种叫做「label Smoothing」的技巧。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">label_smoothing</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>

	<span class="c1"># 获取输入的类别数，并将其赋值给变量 K
</span>    <span class="n">K</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">.</span><span class="n">get_shape</span><span class="p">().</span><span class="n">as_list</span><span class="p">()[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="c1"># number of channels
</span>    <span class="k">return</span> <span class="p">((</span><span class="mi">1</span><span class="o">-</span><span class="n">epsilon</span><span class="p">)</span> <span class="o">*</span> <span class="n">inputs</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">epsilon</span> <span class="o">/</span> <span class="n">K</span><span class="p">)</span>
</code></pre></div></div>

<p>在训练过程中，样本的标签被表示为一个二维矩阵，其中第一维表示样本的编号，第二维表示样本的标签。这个矩阵的形状就是 (样本数, 类别数)，所以类别数对应的就是最后一维。具体到这个模型用例里，第一个维度是德语样本句子数，最后一维就是英语词汇量的大小。</p>

<p>用于解决在训练模型时出现的过拟合问题。在标签平滑中，我们给每个样本的标签加上一些噪声，使得模型不能完全依赖于样本的标签来进行训练，从而减少过拟合的可能性。具体来说，这段代码将输入的标签 <code class="language-plaintext highlighter-rouge">inputs</code> 乘上 <code class="language-plaintext highlighter-rouge">1-epsilon</code>，再加上 <code class="language-plaintext highlighter-rouge">epsilon / K</code>，其中 <code class="language-plaintext highlighter-rouge">epsilon</code> 是平滑因子，<code class="language-plaintext highlighter-rouge">K</code> 是标签类别数（英语词汇量大小）。这样就可以在训练过程中让模型对标签的预测更加平稳，并且降低过拟合的风险。</p>

<p>然后我们看后续的操作。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 对于分类问题来说，常用的损失函数是交叉熵损失
</span><span class="bp">self</span><span class="p">.</span><span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">softmax_cross_entropy_with_logits</span><span class="p">(</span><span class="n">logits</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">y_smoothed</span><span class="p">)</span>
<span class="bp">self</span><span class="p">.</span><span class="n">mean_loss</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">loss</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">istarget</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">istarget</span><span class="p">))</span>

<span class="c1"># Training Scheme
</span><span class="bp">self</span><span class="p">.</span><span class="n">global_step</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s">'global_step'</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="c1"># Adam 优化器 self.optimizer，用于优化损失函数
</span><span class="bp">self</span><span class="p">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">train</span><span class="p">.</span><span class="n">AdamOptimizer</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">hp</span><span class="p">.</span><span class="n">lr</span><span class="p">,</span> <span class="n">beta1</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">beta2</span><span class="o">=</span><span class="mf">0.98</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">)</span>

<span class="c1"># 使用优化器的 minimize() 函数创建一个训练操作 self.train_op，用于更新模型参数。这个函数会自动计算梯度并应用更新
</span><span class="bp">self</span><span class="p">.</span><span class="n">train_op</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">optimizer</span><span class="p">.</span><span class="n">minimize</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">mean_loss</span><span class="p">,</span> <span class="n">global_step</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">global_step</span><span class="p">)</span>
   
<span class="c1"># 将平均损失写入 TensorFlow 的 Summary 中，用于 tensorboard 可视化
</span><span class="n">tf</span><span class="p">.</span><span class="n">summary</span><span class="p">.</span><span class="n">scalar</span><span class="p">(</span><span class="s">'mean_loss'</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">mean_loss</span><span class="p">)</span>

<span class="c1"># 将所有的 summary 合并到一起，方便在训练过程中写入事件文件
</span><span class="bp">self</span><span class="p">.</span><span class="n">merged</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">summary</span><span class="p">.</span><span class="n">merge_all</span><span class="p">()</span>
</code></pre></div></div>

<h4 id="155效果评价">15.5、效果评价</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">eval</span><span class="p">():</span> 
    <span class="c1"># 创建一个处理测试数据集的 Graph 实例
</span>    <span class="n">g</span> <span class="o">=</span> <span class="n">Graph</span><span class="p">(</span><span class="n">is_training</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"Graph loaded"</span><span class="p">)</span>
    
    <span class="c1"># 加载测试数据
</span>    <span class="n">X</span><span class="p">,</span> <span class="n">Sources</span><span class="p">,</span> <span class="n">Targets</span> <span class="o">=</span> <span class="n">load_test_data</span><span class="p">()</span>
    <span class="n">de2idx</span><span class="p">,</span> <span class="n">idx2de</span> <span class="o">=</span> <span class="n">load_de_vocab</span><span class="p">()</span>
    <span class="n">en2idx</span><span class="p">,</span> <span class="n">idx2en</span> <span class="o">=</span> <span class="n">load_en_vocab</span><span class="p">()</span>
     
    <span class="c1"># Start session         
</span>    <span class="k">with</span> <span class="n">g</span><span class="p">.</span><span class="n">graph</span><span class="p">.</span><span class="n">as_default</span><span class="p">():</span>

    	<span class="c1"># TensorFlow 中用于管理训练的一个类
</span>    	<span class="c1"># 它可以帮助你轻松地管理训练过程中的各种资源，如模型参数、检查点和日志
</span>        <span class="n">sv</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">train</span><span class="p">.</span><span class="n">Supervisor</span><span class="p">()</span>

        <span class="c1"># 创建一个会话
</span>        <span class="k">with</span> <span class="n">sv</span><span class="p">.</span><span class="n">managed_session</span><span class="p">(</span><span class="n">config</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">ConfigProto</span><span class="p">(</span><span class="n">allow_soft_placement</span><span class="o">=</span><span class="bp">True</span><span class="p">))</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>

            <span class="c1"># 恢复模型参数
</span>            <span class="n">sv</span><span class="p">.</span><span class="n">saver</span><span class="p">.</span><span class="n">restore</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">train</span><span class="p">.</span><span class="n">latest_checkpoint</span><span class="p">(</span><span class="n">hp</span><span class="p">.</span><span class="n">logdir</span><span class="p">))</span>
            <span class="k">print</span><span class="p">(</span><span class="s">"Restored!"</span><span class="p">)</span>
              
            <span class="c1"># 获取模型名称
</span>            <span class="n">mname</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="n">hp</span><span class="p">.</span><span class="n">logdir</span> <span class="o">+</span> <span class="s">'/checkpoint'</span><span class="p">,</span> <span class="s">'r'</span><span class="p">).</span><span class="n">read</span><span class="p">().</span><span class="n">split</span><span class="p">(</span><span class="s">'"'</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span> <span class="c1"># model name
</span>             
            <span class="c1">## Inference
</span>            <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">exists</span><span class="p">(</span><span class="s">'results'</span><span class="p">):</span> <span class="n">os</span><span class="p">.</span><span class="n">mkdir</span><span class="p">(</span><span class="s">'results'</span><span class="p">)</span>

            <span class="c1"># 初始化结果文件
</span>            <span class="k">with</span> <span class="n">codecs</span><span class="p">.</span><span class="nb">open</span><span class="p">(</span><span class="s">"results/"</span> <span class="o">+</span> <span class="n">mname</span><span class="p">,</span> <span class="s">"w"</span><span class="p">,</span> <span class="s">"utf-8"</span><span class="p">)</span> <span class="k">as</span> <span class="n">fout</span><span class="p">:</span>
                <span class="n">list_of_refs</span><span class="p">,</span> <span class="n">hypotheses</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>

                <span class="c1"># 循环处理数据
</span>                <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">//</span> <span class="n">hp</span><span class="p">.</span><span class="n">batch_size</span><span class="p">):</span>
                     
                    <span class="c1"># 获取小批量数据
</span>                    <span class="n">x</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="o">*</span><span class="n">hp</span><span class="p">.</span><span class="n">batch_size</span><span class="p">:</span> <span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">hp</span><span class="p">.</span><span class="n">batch_size</span><span class="p">]</span>
                    <span class="n">sources</span> <span class="o">=</span> <span class="n">Sources</span><span class="p">[</span><span class="n">i</span><span class="o">*</span><span class="n">hp</span><span class="p">.</span><span class="n">batch_size</span><span class="p">:</span> <span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">hp</span><span class="p">.</span><span class="n">batch_size</span><span class="p">]</span>
                    <span class="n">targets</span> <span class="o">=</span> <span class="n">Targets</span><span class="p">[</span><span class="n">i</span><span class="o">*</span><span class="n">hp</span><span class="p">.</span><span class="n">batch_size</span><span class="p">:</span> <span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">hp</span><span class="p">.</span><span class="n">batch_size</span><span class="p">]</span>
                     
                    <span class="c1"># 使用自回归推理（Autoregressive inference）得到预测结果
</span>                    <span class="n">preds</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">hp</span><span class="p">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">hp</span><span class="p">.</span><span class="n">maxlen</span><span class="p">),</span> <span class="n">np</span><span class="p">.</span><span class="n">int32</span><span class="p">)</span>
                    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">hp</span><span class="p">.</span><span class="n">maxlen</span><span class="p">):</span>
                        <span class="n">_preds</span> <span class="o">=</span> <span class="n">sess</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">g</span><span class="p">.</span><span class="n">preds</span><span class="p">,</span> <span class="p">{</span><span class="n">g</span><span class="p">.</span><span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span> <span class="n">g</span><span class="p">.</span><span class="n">y</span><span class="p">:</span> <span class="n">preds</span><span class="p">})</span>
                        <span class="n">preds</span><span class="p">[:,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">_preds</span><span class="p">[:,</span> <span class="n">j</span><span class="p">]</span>
                     
                    <span class="c1"># 将预测结果写入文件
</span>                    <span class="k">for</span> <span class="n">source</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">pred</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">sources</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">preds</span><span class="p">):</span> <span class="c1"># sentence-wise
</span>                        <span class="n">got</span> <span class="o">=</span> <span class="s">" "</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">idx2en</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">pred</span><span class="p">).</span><span class="n">split</span><span class="p">(</span><span class="s">"&lt;/S&gt;"</span><span class="p">)[</span><span class="mi">0</span><span class="p">].</span><span class="n">strip</span><span class="p">()</span>
                        <span class="n">fout</span><span class="p">.</span><span class="n">write</span><span class="p">(</span><span class="s">"- source: "</span> <span class="o">+</span> <span class="n">source</span> <span class="o">+</span><span class="s">"</span><span class="se">\n</span><span class="s">"</span><span class="p">)</span>
                        <span class="n">fout</span><span class="p">.</span><span class="n">write</span><span class="p">(</span><span class="s">"- expected: "</span> <span class="o">+</span> <span class="n">target</span> <span class="o">+</span> <span class="s">"</span><span class="se">\n</span><span class="s">"</span><span class="p">)</span>
                        <span class="n">fout</span><span class="p">.</span><span class="n">write</span><span class="p">(</span><span class="s">"- got: "</span> <span class="o">+</span> <span class="n">got</span> <span class="o">+</span> <span class="s">"</span><span class="se">\n\n</span><span class="s">"</span><span class="p">)</span>
                        <span class="n">fout</span><span class="p">.</span><span class="n">flush</span><span class="p">()</span>
                          
                        <span class="c1"># bleu score
</span>                        <span class="n">ref</span> <span class="o">=</span> <span class="n">target</span><span class="p">.</span><span class="n">split</span><span class="p">()</span>
                        <span class="n">hypothesis</span> <span class="o">=</span> <span class="n">got</span><span class="p">.</span><span class="n">split</span><span class="p">()</span>
                        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">ref</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">3</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">hypothesis</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">3</span><span class="p">:</span>
                            <span class="n">list_of_refs</span><span class="p">.</span><span class="n">append</span><span class="p">([</span><span class="n">ref</span><span class="p">])</span>
                            <span class="n">hypotheses</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">hypothesis</span><span class="p">)</span>
              
                <span class="c1"># 计算 BLEU 分数，并将其写入文件
</span>                <span class="n">score</span> <span class="o">=</span> <span class="n">corpus_bleu</span><span class="p">(</span><span class="n">list_of_refs</span><span class="p">,</span> <span class="n">hypotheses</span><span class="p">)</span>
                <span class="n">fout</span><span class="p">.</span><span class="n">write</span><span class="p">(</span><span class="s">"Bleu Score = "</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="mi">100</span><span class="o">*</span><span class="n">score</span><span class="p">))</span>
                                          
<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">'__main__'</span><span class="p">:</span>
    <span class="nb">eval</span><span class="p">()</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"Done"</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="第-16-节--kyubyong-transformer-的性能表现和一些问题">第 16 节 · Kyubyong Transformer 的性能表现和一些问题</h3>

<p>评估结果文件的最后一行有 Bleu Score = 6.598452846670836 表示这个翻译模型的翻译结果与参考翻译重叠程度比较高，翻译质量较好。不过需要注意的是，BLEU 分数不能完全反映翻译质量，因为它不能评估语法，语义，语调等方面的问题。</p>

<p>另外前面我们在代码中已经将过程数据保存在 logdir 下了，就是为了后续方便可视化，我们可以用 TensorBoard 来可视化，具体使用方法如下：</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>mikecaptain@local <span class="nv">$ </span>tensorboard <span class="nt">--logdir</span> logdir
</code></pre></div></div>

<p>然后在浏览器里查看 <code class="language-plaintext highlighter-rouge">http://localhost:6006</code>，示例如下：</p>

<p><img src="/img/src/2023-01-04-language-model-5-17.gif" alt="image" /></p>

<p>我们可以看到这个 Transformer 能够较好地捕捉长距离依赖关系，提高翻译质量。然而，Kyubyong Transformer 的实现存在一些问题。该 Transformer 模型在训练过程中还需要调整许多超参数，如学习率（learning rate）、batch size 等，不同的任务可能需要不同的超参数调整。</p>

<h2 id="结尾--transformer-问世后的这些年">结尾 · Transformer 问世后的这些年</h2>

<p>Transformer 的优势显而易见：</p>

<ul>
  <li>更快 —— 并行性好：在 Transformer 诞生之前，RNN 是 NLP 领域的主流模型，但是 RNN 并行性差（序列串行处理）。</li>
  <li>不健忘 —— 词距离缩短为 1：RNN 模型处理长文本内容已丢失（在 RNN 模型中意味着词的空间距离长）。</li>
  <li>处理不同长度序列：不需要输入数据的序列是固定长度的。</li>
  <li>易于转移学习。</li>
</ul>

<p>因此基于 Transformer 原理的模型，在众多 NLP 任务中都取得了卓越的表现。</p>

<p>说到底机器学习（Machine Learning）领域还是一个实验科学，并且是离工业界极近的实验科学。机器学习看待实验结果的角度，不是为了拿实验结果总结抽象后推动理论科学发展。机器学习的实验结果是要被评价的，其效果有客观量化评估标准。所以机器学习，一切以结果说话。基于 Transformer 架构 Decoder 部分诞生了 OpenAI 的 GPT 大模型，基于其架构的 Encoder 部分诞生了 Google 的 BERT 大模型，他们两个都诞生于 2018 年。这几年基于 Transformer 的各种优化思想不断出现，其集大成者便是 2022 年年底基于 GPT-3.5 或者说基于 InstructGPT 的 ChatGPT。</p>

<p>感谢你有耐心看完本篇近 10 万字长文，因为是船涨的技术笔记，所以对于关键点梳理得细致了些。后续，我讲和大家一起聊聊 AIGC 的当下，如果说本篇内容更像一个教程（对缘起技术的深入），那么后续我们的探讨则可能更像一篇报告了（对眼前学界与业界发展现状的综述），我们将更关注文章「前言」部分的两个议题：1）如果认为通过图灵测试代表着 AGI（Artificial General Intelligence，通用人工智能）的话，当下 NLP，乃至 AGI 发展到什么程度了？2）未来一些年内，AGI 的发展路线可能会是怎样的？</p>

<p>AI 终将颠覆各行各业，阿里人有责任花些时间关注前沿的发展脉搏，欢迎大家在钉钉或微信（id：sinosuperman）上与我交流。</p>

<p>最后，船涨祝大家兔年里，健康又快乐。</p>

<h2 id="参考">参考</h2>

<ul>
  <li>https://web.stanford.edu/~jurafsky/slp3/3.pdf</li>
  <li>https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html</li>
  <li>《自然语言处理：基于预训练模型的方法》车万翔 等著</li>
  <li>https://cs.stanford.edu/people/karpathy/convnetjs/</li>
  <li>https://arxiv.org/abs/1706.03762</li>
  <li>https://arxiv.org/abs/1512.03385</li>
  <li>https://github.com/Kyubyong/transformer/</li>
  <li>http://jalammar.github.io/illustrated-transformer/</li>
  <li>https://towardsdatascience.com/this-is-how-to-train-better-transformer-models-d54191299978</li>
  <li>《自然语言处理实战：预训练模型应用及其产品化》安库·A·帕特尔 等著</li>
  <li>https://lilianweng.github.io/posts/2018-06-24-attention/</li>
  <li>https://github.com/lilianweng/transformer-tensorflow/</li>
  <li>《基于深度学习的道路短期交通状态时空序列预测》崔建勋 著</li>
  <li>https://www.zhihu.com/question/325839123</li>
  <li>https://luweikxy.gitbook.io/machine-learning-notes/self-attention-and-transformer</li>
  <li>《Python 深度学习（第 2 版）》弗朗索瓦·肖莱 著</li>
  <li>https://en.wikipedia.org/wiki/Attention_(machine_learning)</li>
  <li>https://zhuanlan.zhihu.com/p/410776234</li>
  <li>https://www.tensorflow.org/tensorboard/get_started</li>
  <li>https://paperswithcode.com/method/multi-head-attention</li>
  <li>https://zhuanlan.zhihu.com/p/48508221</li>
  <li>https://www.joshbelanich.com/self-attention-layer/</li>
  <li>https://learning.rasa.com/transformers/kvq/</li>
  <li>http://deeplearning.stanford.edu/tutorial/supervised/ConvolutionalNeuralNetwork/</li>
  <li>https://zhuanlan.zhihu.com/p/352898810</li>
  <li>https://towardsdatascience.com/beautifully-illustrated-nlp-models-from-rnn-to-transformer-80d69faf2109</li>
  <li>https://medium.com/analytics-vidhya/understanding-q-k-v-in-transformer-self-attention-9a5eddaa5960</li>
</ul>

	</div>
</article>



	  </main>
		
		  <!-- Pagination links -->
      

	  </div>
	    
	    <!-- Footer -->
	    <footer><span>@2022 - MikeCaptain.com</span></footer>


	    <!-- Script -->
      <script src="/js/main.js"></script>	


	</div>
</body>
</html>

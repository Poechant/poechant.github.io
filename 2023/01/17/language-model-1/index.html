<!DOCTYPE html>
<html>

<head>
	<!-- Meta -->
	<meta charset="UTF-8"/>
	<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
	<meta name="generator" content="Jekyll">

	<title>麦克船长 NLP 语言模型技术笔记 1：N 元文法（N-Gram）</title>
  	<meta name="description" content="麦克船长对于技术、产品、商业等领域的分享|AI,A.I.,NLP,神经网络,人工智能,自然语言处理,BERT,GPT,ChatGPT,OpenAI,阿里巴巴,P9,运营,淘宝,天猫,总监,高管">

	<!-- CSS & fonts -->
	<link rel="stylesheet" href="/css/main.css">

	<!-- RSS -->
	<link href="/atom.xml" type="application/atom+xml" rel="alternate" title="ATOM Feed" />

  	<!-- Favicon -->
 	 <link rel="shortcut icon" type="image/png" href="/img/favicon.png">

 	 <!-- Syntax highlighter -->
  	<link rel="stylesheet" href="/css/syntax.css" />

  	<!--KaTeX-->
  	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
  	<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>
  	<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"></script>
  	<script>
  		document.addEventListener("DOMContentLoaded", function() {
  			renderMathInElement(document.body, {
  				// ...options...
  			});
  		});
  	</script>

  	
  	<!-- KaTeX -->
  	<link rel="stylesheet" href="/assets/plugins/katex.0.11.1/katex.min.css">
  	

  	
  		<script async src="https://www.googletagmanager.com/gtag/js?id=G-CH4708X4R5"></script>
  		<script>
    		window.dataLayer = window.dataLayer || [];
    		function gtag(){dataLayer.push(arguments);}
    		gtag('js', new Date());

    		gtag('config', 'G-CH4708X4R5');
  		</script>
	


</head>

<body>
	<div id="wrap">
	  	
	  	<!-- Navigation -->
	  	<nav id="nav">
	<div id="nav-list">
		<a href="/">Home</a>

		<!-- Nav pages -->
	  <!-- 
	    
	  
	    
	      <a href="/about/" title="关于我">关于我</a>
	    
	  
	    
	  
	    
	  
	    
	      <a href="/booklist/" title="读书行路">读书行路</a>
	    
	  
	    
	  
	    
	  
	    
	  
	    
	      <a href="/categories/" title="Categories">Categories</a>
	    
	  
	    
	  
	    
	  
	    
	  
	    
	      <a href="/target/" title="目标感">目标感</a>
	    
	  
	    
	  
	    
	  
	    
	  
	    
	  
	    
	  
	    
	  
	    
	  
	    
	   -->

	  <!-- Tech category pages -->






  <a href="/category/ai" title="人工智能">人工智能</a>











  <a href="/category/energy" title="能源">能源</a>









  <a href="/category/rt_tech" title="实时技术">实时技术</a>







  <a href="/category/web" title="前端">前端</a>
















<!-- Non-tech category pages -->












  <a href="/category/business" title="商业">商业</a>



  <a href="/category/design" title="设计">设计</a>















  <a href="/category/thinking" title="思考与生活">思考与生活</a>

















	  
        
      
        
          <a href="/about/" title="关于我">关于我</a>
        
      
        
      
        
      
        
          <a href="/booklist/" title="读书行路">读书行路</a>
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
          <a href="/target/" title="目标感">目标感</a>
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    <!-- Nav links -->
	  <!-- <a href="https://github.com/thereviewindex/monochrome/archive/master.zip">Download</a>
<a href="https://github.com/thereviewindex/monochrome">Project on Github</a> -->

	</div>
  
  <!-- Nav footer -->
	
	  <footer>
	
	<span>version 1.0.0</span>

</footer>
	

</nav>

    
    <!-- Icon menu -->
	  <a id="nav-menu">
	  	<div id="menu"></div>
	  </a>

      <!-- Header -->
      
        <header id="header" class="parent justify-spaceBetween">
  <div class="inner w100 relative">
    <span class="f-left">  
      <a href="/">
        <h1>
          <span>Mike</span>Captain
        </h1>
      </a>
    </span>
    <span id="nav-links" class="absolute right bottom">

      <!-- Tech category pages -->






  <a href="/category/ai" title="人工智能">人工智能</a>











  <a href="/category/energy" title="能源">能源</a>









  <a href="/category/rt_tech" title="实时技术">实时技术</a>







  <a href="/category/web" title="前端">前端</a>
















<!-- Non-tech category pages -->












  <a href="/category/business" title="商业">商业</a>



  <a href="/category/design" title="设计">设计</a>















  <a href="/category/thinking" title="思考与生活">思考与生活</a>

















      &nbsp;&nbsp;&nbsp;丨&nbsp;

      <!-- Nav pages -->
      
        
      
        
          <a href="/about/" title="关于我">关于我</a>
        
      
        
      
        
      
        
          <a href="/booklist/" title="读书行路">读书行路</a>
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
          <a href="/target/" title="目标感">目标感</a>
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
      
      <!-- Nav links -->
      <!-- <a href="https://github.com/thereviewindex/monochrome/archive/master.zip">Download</a>
<a href="https://github.com/thereviewindex/monochrome">Project on Github</a> -->

    </span>
  </div>
</header>




      

    <!-- Main content -->
	  <div id="container">
		  
		<main>

			<article id="post-page">
	<h2>麦克船长 NLP 语言模型技术笔记 1：N 元文法（N-Gram）</h2>		
	<time datetime="2023-01-17T21:14:01+00:00" class="by-line">17 Jan 2023, 香港 | 麦克船长 | 总计 3131 字</time>
	<div class="content">
		<p><strong>本文目录</strong></p>
<ul id="markdown-toc">
  <li><a href="#一n-元文法语言模型n-gram-language-model介绍" id="markdown-toc-一n-元文法语言模型n-gram-language-model介绍">一、N 元文法语言模型（N-gram Language Model）介绍</a></li>
  <li><a href="#二平滑技术解决零概率问题" id="markdown-toc-二平滑技术解决零概率问题">二、平滑技术：解决零概率问题</a>    <ul>
      <li><a href="#1加-1-平滑--拉普拉斯平滑add-one-discountinglaplace-smoothing" id="markdown-toc-1加-1-平滑--拉普拉斯平滑add-one-discountinglaplace-smoothing">1、加 1 平滑 / 拉普拉斯平滑（Add-One Discounting、Laplace Smoothing）</a></li>
      <li><a href="#2δ-平滑delta-smoothing" id="markdown-toc-2δ-平滑delta-smoothing">2、δ 平滑（Delta Smoothing）</a></li>
      <li><a href="#3困惑度perplexity" id="markdown-toc-3困惑度perplexity">3、困惑度（Perplexity）</a></li>
    </ul>
  </li>
  <li><a href="#参考" id="markdown-toc-参考">参考：</a></li>
</ul>

<p>NLP 的技术基础部分，我认为主要是这两部分：词表示法（Word Presentation）、语言模型（Language Model）。对于词表示法，这里不做详细介绍，基本的思路就是把词表示为向量（一维张量），最基本的 One-Hot、Word2Vec、GloVe、fastText 等。这部分的技术演进也在不断前进，比如在第 5 篇文章中要介绍的 Transformer 模型里，用到的词表示法是「引入上下文感知的词向量」。</p>

<p>语言模型的发展中，有两个关键阶段。第一个阶段是以 N 元文法为代表的经典模型。第二个阶段是基于神经网络发展的语言包模型。本文先和大家讲下这个经典模型。后续文章将逐一为大家介绍神经网络的 MLP、CNN、RNN 等。</p>

<p>这里先介绍一个词：超参数。一般 NLP 模型的参数都是海量的，比如 OpenAI 的 GPT-3 参数就达到了 1750 亿个。而在模型训练前就要从宏观上整体设定的一些参数，被称为「超参数（hyperparameter）」。</p>

<h3 id="一n-元文法语言模型n-gram-language-model介绍">一、N 元文法语言模型（N-gram Language Model）介绍</h3>

<p>下一个词出现的概率只依赖于它前面 n-1 个词，这种假设被称为「马尔科夫假设（Markov Assumption」。N 元文法，也称为 N-1 阶马尔科夫链。</p>

<ul>
  <li>一元文法（1-gram），unigram，零阶马尔科夫链，不依赖前面任何词；</li>
  <li>二元文法（2-gram），bigram，一阶马尔科夫链，只依赖于前 1 个词；</li>
  <li>三元文法（3-gram），trigram，二阶马尔科夫链，只依赖于前 2 个词；</li>
  <li>……</li>
</ul>

<p>通过前 t-1 个词预测时刻 t 出现某词的概率，用最大似然估计：</p>

\[P(w_t | w_1,w_2...w_{t-1}) = \frac{C(w_1,w_2,...w_t)}{C(w_1,w_2,...w_{t-1})}\]

<p>进一步地，一组词（也就是一个句子）出现的概率就是：</p>

\[P(w_1,w_2,...w_t) = P(w_t | w_1,w_2,...w_{t-1}) \cdot P(w_{t-1} | w_1,w_2,...w_{t-2}) \cdot ... \cdot P(w_1)
			      = \displaystyle\prod_{i=1}^{t-1}P(w_i | w_{1:i-1})\]

<p>为了解决句头、尾逇概率计算问题，我们再引入两个标记 <code class="language-plaintext highlighter-rouge">&lt;BOS&gt;</code> 和 <code class="language-plaintext highlighter-rouge">&lt;EOS&gt;</code> 分别表示 beginning of sentence 和 end of sentence，所以  \(w_0 =\) &lt;BOS&gt;、 \(w_{length + 1} =\) &lt;EOS&gt;，其中 length 是词的数量。</p>

<p>具体地，比如对于 bigram，该模型表示如下：</p>

\[\begin{aligned}
P(w_1,w_2,...w_t) &amp;= \displaystyle\prod_{i=1}^{t-1}P(w_i | w_{i-1}) \\
P(w_t | w_{t-1}) &amp;= \frac{C(w_{t-1}, w_t)}{C(w_{t-1})}
\end{aligned}\]

<ul>
  <li>如果有词出现次数为了 0，这一串乘出来就是 0 了，咋办？</li>
  <li>因为基于马尔科夫假设，所以 N 固定窗口取值，对长距离词依赖的情况会表现很差。</li>
  <li>如果把 N 值取很大来解决长距离词依赖，则会导致严重的数据稀疏（零频太多了），参数规模也会急速爆炸（高维张量计算）。</li>
</ul>

<p>上面的第一个问题，我们引入「平滑技术」来解决，而后面两个问题则是在神经网络模型出现后才更好解决的。</p>

<h3 id="二平滑技术解决零概率问题">二、平滑技术：解决零概率问题</h3>

<p>虽然限定了窗口 n 大小降低了词概率为 0 的可能性，但当 n-gram 的 n 比较大的时候会有的未登录词问题（Out Of Vocabulary，OOV）。另一方面，训练数据很可能也不是 100% 完备覆盖实际中可能遇到的词的。所以为了避免 0 概率出现，就有了让零平滑过渡为非零的补丁式技术出现。</p>

<p>最简单的平滑技术，就是折扣法（Discounting）。这是一个非常容易想到的办法，就是把整体 100% 的概率腾出一小部分来，给这些零频词（也常把低频词一起考虑）。基于此发展出来一些具体的方法如下：</p>

<h4 id="1加-1-平滑--拉普拉斯平滑add-one-discountinglaplace-smoothing">1、加 1 平滑 / 拉普拉斯平滑（Add-One Discounting、Laplace Smoothing）</h4>

<p>加 1 平滑，就是直接将所有词汇的出现次数都 +1，不止针对零频词、低频词。如果继续拿 bigram 举例来说，模型就会变成：</p>

\[P(w_i | w_{i-1}) = \frac{C_(w_{i-1},w_i) + 1}{\displaystyle\sum_{j=1}^n(C_(w_{i-1},w_j) + 1)} = \frac{C(w_{i-1}, w_i) + 1}{C(w_{i-1}) + |\mathbb{V}|}\]

<table>
  <tbody>
    <tr>
      <td>其中  \(N\)  表示所有词的词频之和， $$</td>
      <td>\mathbb{V}</td>
      <td>$$  表示词汇表的大小。</td>
    </tr>
  </tbody>
</table>

<p>如果当词汇表中的词，很多出现次数都很小，这样对每个词的词频都 +1，结果的偏差影响其实挺大的。换句话说，+1 对于低频词很多的场景，加的太多了，应该加一个更小的数（ 1 &lt; δ &lt; 1）。所以有了下面的「δ 平滑」技术。</p>

<h4 id="2δ-平滑delta-smoothing">2、δ 平滑（Delta Smoothing）</h4>

<p>把 +1 换成 δ，我们看下上面 bigram 模型应该变成上面样子：</p>

\[P(w_i | w{i-1}) = \frac{C_(w_{i-1},w_i) + \delta}{\displaystyle\sum_{j=1}^n(C_(w_{i-1},w_j) + \delta)} = \frac{C(w_{i-1}, w_i) + \delta}{C(w_{i-1}) + \delta|\mathbb{V}|}\]

<p>δ 是一个超参数，确定它的值需要用到困惑度（Perplexity，一般用缩写 PPL）。</p>

<h4 id="3困惑度perplexity">3、困惑度（Perplexity）</h4>

<p>对于指定的测试集，困惑度定义为测试集中每一个词概率的几何平均数的倒数，公式如下：</p>

\[\operatorname{PPL}(\mathbb{D}_{test}) = \frac{1}{\sqrt[n]{P(w_1,w_2...w_n)}}\]

<p>把  \(P(w_1,w_2,...w_t) = \displaystyle\prod_{i=1}^{t-1}P(w_i\text{\textbar}w_{i-1})\)  带入上述公式，就得到了 PPL 的计算公式：</p>

\[\operatorname{PPL}(\mathbb{D}_{test}) = (\displaystyle\prod_{i=1}^nP(w_i|w_{1:i-1}))^{-\frac{1}{n}}\]

<h2 id="参考">参考：</h2>

<ul>
  <li>《自然语言处理：基于预训练模型的方法》车万翔 等</li>
  <li>《自然语言处理实战：预训练模型应用及其产品化》安库·A·帕特尔 等</li>
</ul>

	</div>
</article>



	  </main>
		
		  <!-- Pagination links -->
      

	  </div>
	    
	    <!-- Footer -->
	    <footer>
	<span>
		-<br/><br/>
		船长还不会游泳 at 微信公众号/微博<br/>
		@麦克船长 at 即刻/知乎/小宇宙/掘金/小红书/微信读书<br/>
		@船长模玩 at Bilibili<br/>
		Copyright © 2011-2023, MikeCaptain.com
	</span>
</footer>


	    <!-- Script -->
      <script src="/js/main.js"></script>	


	</div>
</body>
</html>

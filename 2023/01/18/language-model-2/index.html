<!DOCTYPE html>
<html>

<head>
	<!-- Meta -->
	<meta charset="UTF-8"/>
	<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
	<meta name="generator" content="Jekyll">

	<title>麦克船长 NLP 语言模型技术笔记 2：多层感知器（MLP）</title>
  	<meta name="description" content="1957 年感知机（Perceptron）模型被提出，1959 年多层感知机（MLP）模型被提出。MLP 有时候也被称为 ANN，即 Artificial Neural Network，接下来我们来深入浅出地了解一下，并有一些动手的练习。">

	<!-- CSS & fonts -->
	<link rel="stylesheet" href="/css/main.css">

	<!-- RSS -->
	<link href="/atom.xml" type="application/atom+xml" rel="alternate" title="ATOM Feed" />

  	<!-- Favicon -->
 	 <link rel="shortcut icon" type="image/png" href="/img/favicon.png">

 	 <!-- Syntax highlighter -->
  	<link rel="stylesheet" href="/css/syntax.css" />

  	<!--KaTeX-->
  	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
  	<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>
  	<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"></script>
  	<script>
  		document.addEventListener("DOMContentLoaded", function() {
  			renderMathInElement(document.body, {
  				// ...options...
  			});
  		});
  	</script>

  	
  	<!-- KaTeX -->
  	<link rel="stylesheet" href="/assets/plugins/katex.0.11.1/katex.min.css">
  	

  	
  		<script async src="https://www.googletagmanager.com/gtag/js?id=G-CH4708X4R5"></script>
  		<script>
    		window.dataLayer = window.dataLayer || [];
    		function gtag(){dataLayer.push(arguments);}
    		gtag('js', new Date());

    		gtag('config', 'G-CH4708X4R5');
  		</script>
	


</head>

<body>
	<div id="wrap">
	  	
	  	<!-- Navigation -->
	  	<nav id="nav">
	<div id="nav-list">
		<a href="/">Home</a>

		<!-- Nav pages -->
	  <!-- 
	    
	  
	    
	      <a href="/about/" title="关于我">关于我</a>
	    
	  
	    
	  
	    
	  
	    
	      <a href="/booklist/" title="读书行路">读书行路</a>
	    
	  
	    
	  
	    
	  
	    
	  
	    
	      <a href="/categories/" title="Categories">Categories</a>
	    
	  
	    
	  
	    
	  
	    
	  
	    
	      <a href="/target/" title="目标感">目标感</a>
	    
	  
	    
	  
	    
	  
	    
	  
	    
	  
	    
	  
	    
	  
	    
	  
	    
	   -->

	  <!-- Tech category pages -->






  <a href="/category/ai" title="人工智能">人工智能</a>











  <a href="/category/energy" title="能源">能源</a>









  <a href="/category/rt_tech" title="实时技术">实时技术</a>







  <a href="/category/web" title="前端">前端</a>
















<!-- Non-tech category pages -->












  <a href="/category/business" title="商业">商业</a>



  <a href="/category/design" title="设计">设计</a>















  <a href="/category/thinking" title="思考与生活">思考与生活</a>

















	  
        
      
        
          <a href="/about/" title="关于我">关于我</a>
        
      
        
      
        
      
        
          <a href="/booklist/" title="读书行路">读书行路</a>
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
          <a href="/target/" title="目标感">目标感</a>
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    <!-- Nav links -->
	  <!-- <a href="https://github.com/thereviewindex/monochrome/archive/master.zip">Download</a>
<a href="https://github.com/thereviewindex/monochrome">Project on Github</a> -->

	</div>
  
  <!-- Nav footer -->
	
	  <footer>
	
	<span>version 1.0.0</span>

</footer>
	

</nav>

    
    <!-- Icon menu -->
	  <a id="nav-menu">
	  	<div id="menu"></div>
	  </a>

      <!-- Header -->
      
        <header id="header" class="parent justify-spaceBetween">
  <div class="inner w100 relative">
    <span class="f-left">  
      <a href="/">
        <h1>
          <span>Mike</span>Captain
        </h1>
      </a>
    </span>
    <span id="nav-links" class="absolute right bottom">

      <!-- Tech category pages -->






  <a href="/category/ai" title="人工智能">人工智能</a>











  <a href="/category/energy" title="能源">能源</a>









  <a href="/category/rt_tech" title="实时技术">实时技术</a>







  <a href="/category/web" title="前端">前端</a>
















<!-- Non-tech category pages -->












  <a href="/category/business" title="商业">商业</a>



  <a href="/category/design" title="设计">设计</a>















  <a href="/category/thinking" title="思考与生活">思考与生活</a>

















      &nbsp;&nbsp;&nbsp;丨&nbsp;

      <!-- Nav pages -->
      
        
      
        
          <a href="/about/" title="关于我">关于我</a>
        
      
        
      
        
      
        
          <a href="/booklist/" title="读书行路">读书行路</a>
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
          <a href="/target/" title="目标感">目标感</a>
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
      
      <!-- Nav links -->
      <!-- <a href="https://github.com/thereviewindex/monochrome/archive/master.zip">Download</a>
<a href="https://github.com/thereviewindex/monochrome">Project on Github</a> -->

    </span>
  </div>
</header>




      

    <!-- Main content -->
	  <div id="container">
		  
		<main>

			<article id="post-page">
	<h2>麦克船长 NLP 语言模型技术笔记 2：多层感知器（MLP）</h2>		
	<time datetime="2023-01-18T19:44:09+00:00" class="by-line">18 Jan 2023, 香港 | 麦克船长 | 总计 9501 字</time>
	<div class="content">
		<p><strong>本文目录</strong></p>
<ul id="markdown-toc">
  <li><a href="#1感知器perceptron解决二元分类任务的前馈神经网络" id="markdown-toc-1感知器perceptron解决二元分类任务的前馈神经网络">1、感知器（Perceptron）：解决二元分类任务的前馈神经网络</a></li>
  <li><a href="#2线性回归linear-regression从离散值的感知器解决类问题到连续值的线性回归解决回归问题" id="markdown-toc-2线性回归linear-regression从离散值的感知器解决类问题到连续值的线性回归解决回归问题">2、线性回归（Linear Regression）：从离散值的感知器（解决类问题），到连续值的线性回归（解决回归问题）</a></li>
  <li><a href="#3逻辑回归logistic-regression没有值域约束的线性回归到限定在一个范围内的逻辑回归常用于分类问题" id="markdown-toc-3逻辑回归logistic-regression没有值域约束的线性回归到限定在一个范围内的逻辑回归常用于分类问题">3、逻辑回归（Logistic Regression）：没有值域约束的线性回归，到限定在一个范围内的逻辑回归（常用于分类问题）</a></li>
  <li><a href="#4sigmoid-回归sigmoid-regression归一化的逻辑回归一般用于二元分类任务" id="markdown-toc-4sigmoid-回归sigmoid-regression归一化的逻辑回归一般用于二元分类任务">4、Sigmoid 回归（Sigmoid Regression）：归一化的逻辑回归，一般用于二元分类任务</a></li>
  <li><a href="#5softmax-回归softmax-regression从解决二元任务的-sigmoid到解决多元分类任务的-softmax" id="markdown-toc-5softmax-回归softmax-regression从解决二元任务的-sigmoid到解决多元分类任务的-softmax">5、Softmax 回归（Softmax Regression）：从解决二元任务的 sigmoid，到解决多元分类任务的 Softmax</a></li>
  <li><a href="#6多层感知器multi-layer-perceptron" id="markdown-toc-6多层感知器multi-layer-perceptron">6、多层感知器（Multi-Layer Perceptron）</a></li>
  <li><a href="#7mlp-的一个显著问题帮我们引出-cnn-模型" id="markdown-toc-7mlp-的一个显著问题帮我们引出-cnn-模型">7、MLP 的一个显著问题，帮我们引出 CNN 模型</a></li>
  <li><a href="#reference" id="markdown-toc-reference">Reference</a></li>
</ul>

<p>本文关键词：感知器、线性回归、逻辑回归、激活函数、Sigmoid 函数/归一化/回归、Softmax 回归。</p>

<p>1957 年感知机（Perceptron）模型被提出，1959 年多层感知机（MLP）模型被提出。MLP 有时候也被称为 ANN，即 Artificial Neural Network，接下来我们来深入浅出地了解一下，并有一些动手的练习。</p>

<h3 id="1感知器perceptron解决二元分类任务的前馈神经网络">1、感知器（Perceptron）：解决二元分类任务的前馈神经网络</h3>

<p>\(x\) 是一个输入向量，\(\omega\) 是一个权重向量（对输入向量里的而每个值分配一个权重值所组成的向量）。举一个具体任务例子，比如如果这两个响亮的内积超过某个值，则判断为 1，否则为 0，这其实就是一个分类任务。那么这个最终输出值可以如下表示：</p>

\[y = \begin{cases} 1 &amp; (\omega \cdot x \geq 0) \\ 0 &amp; (\omega \cdot x \lt 0) \end{cases}\]

<p>这就是一个典型的感知器（Perceptron，一般用来解决分类问题。还可以再增加一个偏差项（bias），如下：</p>

\[y = \begin{cases} 1 &amp; (\omega \cdot x + b \geq 0) \\ 0 &amp; (\omega \cdot x + b \lt 0) \end{cases}\]

<p>感知器其实就是一个前馈神经网络，由输入层、输出层组成，没有隐藏层。而且输出是一个二元函数，用于解决二元分类问题。</p>

<h3 id="2线性回归linear-regression从离散值的感知器解决类问题到连续值的线性回归解决回归问题">2、线性回归（Linear Regression）：从离散值的感知器（解决类问题），到连续值的线性回归（解决回归问题）</h3>

<p>一般来说，我们认为感知器的输出结果，是离散值。一般来说，我们认为离散值作为输出解决的问题，是分类问题；相应地，连续值解决的问题是回归（Regression）。比如对于上面的感知器，如果我们直接将 \(\omega \cdot x + b\) 作为输出值，则就变成了一个线性回归问题的模型了。</p>

<p>下面我们用 PyTorch 来实现一个线性回归的代码示例，首先我们要了解在 PyTorch 库里有一个非常常用的函数：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">)</span>
</code></pre></div></div>

<p>这个函数在创建时会自动初始化权值和偏置，并且可以通过调用它的 <code class="language-plaintext highlighter-rouge">forward</code> 函数来计算输入数据的线性变换。具体来说，当输入为 <code class="language-plaintext highlighter-rouge">x</code> 时，<code class="language-plaintext highlighter-rouge">forward</code> 函数会计算 \(y = \omega \cdot x + b\)，其中  \(W\)  和  \(b\)  分别是 <code class="language-plaintext highlighter-rouge">nn.Linear</code> 图层的权值和偏置。</p>

<p>我们来一个完整的代码示例：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>

<span class="c1"># 定义模型
</span><span class="k">class</span> <span class="nc">LinearRegression</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">LinearRegression</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">linear</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># 初始化模型
</span><span class="n">model</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">(</span><span class="n">input_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">output_size</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># 定义损失函数和优化器
</span><span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">MSELoss</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>

<span class="c1"># 创建输入特征 X 和标签 y
</span><span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">]])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">([[</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="mi">8</span><span class="p">]])</span>

<span class="c1"># 训练模型
</span><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
    <span class="c1"># 前向传播
</span>    <span class="n">predictions</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

    <span class="c1"># 反向传播
</span>    <span class="n">optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>

<span class="c1"># 创建测试数据 X_test 和标签 y_test
</span><span class="n">X_test</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">([[</span><span class="mi">5</span><span class="p">],</span> <span class="p">[</span><span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="mi">7</span><span class="p">],</span> <span class="p">[</span><span class="mi">8</span><span class="p">]])</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">([[</span><span class="mi">10</span><span class="p">],</span> <span class="p">[</span><span class="mi">12</span><span class="p">],</span> <span class="p">[</span><span class="mi">14</span><span class="p">],</span> <span class="p">[</span><span class="mi">16</span><span class="p">]])</span>

<span class="c1"># 测试模型
</span><span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">'Test loss: </span><span class="si">{</span><span class="n">loss</span><span class="p">:.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>

</code></pre></div></div>

<p>上述代码，一开始先创建一个 LinearRegression 线性回归模型的类，其中有一个 <code class="language-plaintext highlighter-rouge">forward</code> 前向传播函数，调用时其实就是计算一下输出值 <code class="language-plaintext highlighter-rouge">y</code>。</p>

<p>主程序，一开始创建一个线性回归模型实例，然后定义一个用于评价模型效果的损失函数评价器，和用随机梯度下降（Stochastic Gradient Descent）作为优化器。</p>

<p>然后创建一个输入特征张量，和标签张量。用这组特征和标签进行训练，训练的过程就是根据 <code class="language-plaintext highlighter-rouge">X</code> 计算与测试 <code class="language-plaintext highlighter-rouge">predictions</code> 向量，再把它和 <code class="language-plaintext highlighter-rouge">y</code> 一起给评价器算出损失 <code class="language-plaintext highlighter-rouge">loss</code>，然后进行反向传播。注意反向传播的三行代码：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
<span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
<span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
</code></pre></div></div>

<p>如此训练 100 次（每一次都会黑盒化地更新模型的参数，一个 <code class="language-plaintext highlighter-rouge">epoch</code> 就是一次训练过程，有时也称为 <code class="language-plaintext highlighter-rouge">iteration</code> 或者 <code class="language-plaintext highlighter-rouge">step</code>，不断根据 <code class="language-plaintext highlighter-rouge">loss</code> 训练优化模型参数。</p>

<p>然后我们创建了一组测试特征值张量 <code class="language-plaintext highlighter-rouge">X_test</code>，和测试标签张量 <code class="language-plaintext highlighter-rouge">y_test</code>，然后用它们测试模型性能，把测试特征得到的 <code class="language-plaintext highlighter-rouge">predictions</code> 与 <code class="language-plaintext highlighter-rouge">y_test</code> 共同传给评价器，得到 <code class="language-plaintext highlighter-rouge">loss</code>。在这个例子中我们会得到如下结果：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Test</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">0.0034</span>
</code></pre></div></div>

<h3 id="3逻辑回归logistic-regression没有值域约束的线性回归到限定在一个范围内的逻辑回归常用于分类问题">3、逻辑回归（Logistic Regression）：没有值域约束的线性回归，到限定在一个范围内的逻辑回归（常用于分类问题）</h3>

<p>可以看到线性回归问题，输出值是没有范围限定的。如果限定（limit）在特定的  \((0, L)\)  范围内，则就叫做逻辑回归了。那么如何将一个线性回归变成逻辑回归呢？一般通过如下公式变换：</p>

\[y = \frac{L}{1 + e^{-k(z-z_0)}}\]

<p>这样原来的  \(z \in (-\infty, +\infty)\)  就被变换成了  \(y \in (0, L)\)  了。</p>

<ul>
  <li><strong>激活函数</strong>：这种把输出值限定在一个目标范围内的函数，被叫做 <strong>激活函数（Activation Function）</strong>。</li>
  <li><strong>函数的陡峭程度</strong> 由  \(k\)  控制，越大越陡。</li>
  <li>当  \(z = z_0\)  时， \(y = \frac{L}{2}\) 。</li>
</ul>

<p>下面给出一个基于 Python 的 scikit-learn 库的示例代码：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="c1"># 这是 scikit-learn 库里的一个简单的数据集
</span><span class="n">iris</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">()</span>

<span class="c1"># 把 iris 数据集拆分成训练集和测试集两部分
</span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">iris</span><span class="p">.</span><span class="n">data</span><span class="p">,</span> <span class="n">iris</span><span class="p">.</span><span class="n">target</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># 用 scikit-learn 库创建一个逻辑回归模型的实例
</span><span class="n">lr</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">()</span>

<span class="c1"># 用上边 split 出来的训练集数据，训练 lr 模型实例
</span><span class="n">lr</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># 用训练过的模型，拿测试集的输入数据做测试
</span><span class="n">predictions</span> <span class="o">=</span> <span class="n">lr</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># 用测试集的数据验证精确性
</span><span class="n">accuracy</span> <span class="o">=</span> <span class="n">lr</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">accuracy</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="4sigmoid-回归sigmoid-regression归一化的逻辑回归一般用于二元分类任务">4、Sigmoid 回归（Sigmoid Regression）：归一化的逻辑回归，一般用于二元分类任务</h3>

<p>当  \(L = 1, k = 1, z_0 = 0\) ，此时的激活函数就是 <strong>Sigmoid</strong> 函数，也常表示为  \(\sigma\)  函数，如下：</p>

\[y = \frac{1}{1 + e^{-z}}\]

<p>Sigmoid 回归的值域，恰好在 (0, 1) 之间，所以常备作为用来归一化的激活函数。而一个线性回归模型，再用 sigmoid 函数归一化，这种也常被称为「Sigmoid 回归」。Sigmoid 这个单词的意思也就是 S 形，我们可以看下它的函数图像如下：</p>

<p><img src="/img/src/2022-12-19-language-model-2.png" alt="image" /></p>

<p>因为归一化，所以也可以把输出值理解为一个概率。比如我们面对一个二元分类问题，那么输出结果就对应属于这个类别的概率。</p>

<p>这样一个 sigmoid 模型可以表示为：</p>

\[\bold{y} = Sigmoid(\bold{W} \cdot \bold{x} + \bold{b})\]

<p>另外 sigmoid 函数的导数（即梯度）是很好算的： \(y' = y \cdot (1-y)\) 。这非常方便用于「梯度下降算法」根据 loss 对模型参数进行优化。Sigmoid 回归，一般用于二元分类任务。那么对于超过二元的情况怎么办呢？这就引出了下面的 Softmax 回归。</p>

<h3 id="5softmax-回归softmax-regression从解决二元任务的-sigmoid到解决多元分类任务的-softmax">5、Softmax 回归（Softmax Regression）：从解决二元任务的 sigmoid，到解决多元分类任务的 Softmax</h3>

<p>相对逻辑回归，Softmax 也称为多项逻辑回归。上面说 Sigmoid 一般用于解决二元分类问题，那么多元问题就要用 Softmax 回归了。我们来拿一个具体问题来解释，比如问题是对于任意输入的一个电商商品的图片，来判断这个图片所代表的的商品，属于哪个商品类目。假设我们一共有 100 个类目。那么一个图片比如说其所有像素值作为输入特征值，输出就是一个 100 维的向量 ** \(z\) **，输出向量中的每个值  \(z_i\)  表示属于相对应类目的概率  \(y_i\)  ：</p>

\[y_i = Softmax(\bold{z})_i = \frac{e^{z_i}}{e^{z_1} + e^{z_2} + ... + e^{z_100}}\]

<p>那么最后得到的  \(y\)  向量中的每一项就对应这个输入  \(z\)  属于这 100 个类目的各自概率了。所以如果回归到一般问题，这个 Softmax 回归的模型就如下：</p>

\[\bold{y} = Softmax(\bold{W} \cdot \bold{x} + \bold{b})\]

<p>对于上面电商商品图片的例子，假设每个图片的尺寸是 512x512，这个模型展开式如下：</p>

\[\begin{bmatrix} y_1 \\ y_2 \\ ... \\ y_{100} \end{bmatrix} = Softmax(\begin{bmatrix} w_{1,1}, &amp; w_{1,2}, &amp; ... &amp; w_{1, 512} \\ w_{2,1}, &amp; w_{2,2}, &amp; ... &amp; w_{2, 512} \\ ... &amp; ... &amp; ... &amp; ... \\ w_{100,1}, &amp; w_{100,2}, &amp; ... &amp; w_{100, 512} \end{bmatrix} \cdot \begin{bmatrix} x_1 \\ x_2 \\ ... \\ x_{512} \end{bmatrix} + \begin{bmatrix} b_1 \\ b_2 \\ ... \\ b_{512} \end{bmatrix})\]

<p>这个对输入向量  \(x\)  执行  \(w \cdot x + b\)  运算，一般也常称为「线性映射/线性变化」。</p>

<h3 id="6多层感知器multi-layer-perceptron">6、多层感知器（Multi-Layer Perceptron）</h3>

<p>上面我们遇到的所有任务，都是用线性模型（Linear Models）解决的。有时候问题复杂起来，我们就要引入非线性模型了。</p>

<p>这里我们要介绍一个新的激活函数 —— ReLU（Rectified Linear Unit）—— 一个非线性激活函数，其定义如下：</p>

\[ReLU(\bold{z}) = max(0, \bold{z})\]

<p>比如对于 MNIST 数据集的手写数字分类问题，就是一个典型的非线性的分类任务，下面给出一个示例代码：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">torchvision</span>
<span class="kn">import</span> <span class="nn">torchvision.transforms</span> <span class="k">as</span> <span class="n">transforms</span>

<span class="c1"># 定义多层感知器模型
</span><span class="k">class</span> <span class="nc">MLP</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MLP</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>

<span class="c1"># 超参数
</span><span class="n">input_size</span> <span class="o">=</span> <span class="mi">784</span>
<span class="n">hidden_size</span> <span class="o">=</span> <span class="mi">500</span>
<span class="n">num_classes</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.001</span>

<span class="c1"># 加载 MNIST 数据集
</span><span class="n">train_dataset</span> <span class="o">=</span> <span class="n">datasets</span><span class="p">.</span><span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s">'../../data'</span><span class="p">,</span>
                               <span class="n">train</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                               <span class="n">transform</span><span class="o">=</span><span class="n">transforms</span><span class="p">.</span><span class="n">ToTensor</span><span class="p">(),</span>
                               <span class="n">download</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">test_dataset</span> <span class="o">=</span> <span class="n">datasets</span><span class="p">.</span><span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s">'../../data'</span><span class="p">,</span>
                              <span class="n">train</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
                              <span class="n">transform</span><span class="o">=</span><span class="n">transforms</span><span class="p">.</span><span class="n">ToTensor</span><span class="p">())</span>

<span class="c1"># 数据加载器
</span><span class="n">train_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="o">=</span><span class="n">train_dataset</span><span class="p">,</span>
                                           <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                                           <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">test_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="o">=</span><span class="n">test_dataset</span><span class="p">,</span>
                                          <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                                          <span class="n">shuffle</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>

<span class="c1"># 定义损失函数和优化器
</span><span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>

<span class="c1"># 训练模型
</span><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
        <span class="c1"># 前向传播
</span>        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>

        <span class="c1"># 反向传播
</span>        <span class="n">optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>

    <span class="c1"># 输出训练损失
</span>    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">'Epoch </span><span class="si">{</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="si">}</span><span class="s">, Training Loss: </span><span class="si">{</span><span class="n">loss</span><span class="p">.</span><span class="n">item</span><span class="p">():.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
</code></pre></div></div>

<p>这段代码里，我们能看到 MLP 的模型定义是：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
<span class="n">nn</span><span class="p">.</span><span class="n">ReLU</span><span class="p">()</span>
<span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>
</code></pre></div></div>

<p>与前面的模型示例代码类似，也都用到了反向传播、损失函数评价器、优化器。如果用公式表示的话，就是如下的模型定义：</p>

\[\begin{aligned}
&amp;\bold{z} = \bold{W}_1 \cdot \bold{x} + \bold{b}_1 \\
&amp;\bold{h} = ReLU(\bold{z}) \\
&amp;\bold{y} = \bold{W}_2 \cdot \bold{h} + \bold{b}_2
\end{aligned}\]

<p>我们知道 MLP 通常是一个输入和输出长度相同的模型，但少数情况下也可以构建输入和输出长度不同的 MLP 模型，比如输入一组序列后，输出是一个离散的分类结果。</p>

<h3 id="7mlp-的一个显著问题帮我们引出-cnn-模型">7、MLP 的一个显著问题，帮我们引出 CNN 模型</h3>

<p>我们可以看到，在 MLP 中，不论有多少层，某一层的输出向量  \(h_n\)  中的每个值，都会在下一层计算输出向量  \(h_{n+1}\)  的每个值时用到。具体来说，如果对于某一层的输出值如下：</p>

\[\bold{h}_{n+1} = Softmax(\bold{W}_{n+1} \cdot \bold{h}_n + \bold{b}_{n+1})\]

<p>上一段话里所谓的「用到」，其实就是要针对  \(h_n\)  生成相应的特征值  \(W_{n+1}\)  权重矩阵中的每个行列里的数值和  \(b_{n+1}\) 偏差向量 里的每个值。如果用图画出来，就是：</p>

<div style="text-align: center;">
<div class="graphviz-wrapper">

<!-- Generated by graphviz version 2.43.0 (0)
 -->
<!-- Title: G Pages: 1 -->
<svg role="img" aria-label="graphviz-1b1299448dc08c90d29bebf8b1f045c1" width="428pt" height="116pt" viewBox="0.00 0.00 427.64 116.00">
<title>graphviz-1b1299448dc08c90d29bebf8b1f045c1</title>
<desc>
digraph G {
	rankdir=TB
	a[label=&quot;...&quot;]
	b[label=&quot;...&quot;]
	h_2_1[label=&quot;h_n+1_1&quot;]
	h_2_2[label=&quot;h_n+1_2&quot;]
	h_2_m[label=&quot;h_n+1_m&quot;]

	{rank=same h_n_1 h_n_2 b h_n_m}
	{rank=same h_2_1 h_2_2 a h_2_m}

	h_n_1 -&gt; h_2_1
	h_n_1 -&gt; h_2_2
	h_n_1 -&gt; a
	h_n_1 -&gt; h_2_m

	h_n_1 -&gt; h_2_1
	h_n_2 -&gt; h_2_2
	h_n_2 -&gt; a
	h_n_2 -&gt; h_2_m

	b -&gt; h_2_1
	b -&gt; h_2_2
	b -&gt; a
	b -&gt; h_2_m

	h_n_m -&gt; h_2_1
	h_n_m -&gt; h_2_2
	h_n_m -&gt; a
	h_n_m -&gt; h_2_m
}
</desc>

<g id="graph0" class="graph" transform="scale(1 1) rotate(0) translate(4 112)">
<title>G</title>
<polygon fill="white" stroke="transparent" points="-4,4 -4,-112 423.64,-112 423.64,4 -4,4" />
<!-- a -->
<g id="node1" class="node">
<title>a</title>
<ellipse fill="none" stroke="black" cx="146.7" cy="-18" rx="27" ry="18" />
<text text-anchor="middle" x="146.7" y="-14.3" font-family="Times,serif" font-size="14.00">...</text>
</g>
<!-- b -->
<g id="node2" class="node">
<title>b</title>
<ellipse fill="none" stroke="black" cx="151.7" cy="-90" rx="27" ry="18" />
<text text-anchor="middle" x="151.7" y="-86.3" font-family="Times,serif" font-size="14.00">...</text>
</g>
<!-- b&#45;&gt;a -->
<g id="edge11" class="edge">
<title>b&#45;&gt;a</title>
<path fill="none" stroke="black" d="M150.46,-71.7C149.91,-63.98 149.25,-54.71 148.63,-46.11" />
<polygon fill="black" stroke="black" points="152.12,-45.83 147.92,-36.1 145.14,-46.33 152.12,-45.83" />
</g>
<!-- h_2_1 -->
<g id="node3" class="node">
<title>h_2_1</title>
<ellipse fill="none" stroke="black" cx="50.7" cy="-18" rx="50.89" ry="18" />
<text text-anchor="middle" x="50.7" y="-14.3" font-family="Times,serif" font-size="14.00">h_n+1_1</text>
</g>
<!-- b&#45;&gt;h_2_1 -->
<g id="edge9" class="edge">
<title>b&#45;&gt;h_2_1</title>
<path fill="none" stroke="black" d="M133.64,-76.49C119.14,-66.44 98.46,-52.11 81.38,-40.27" />
<polygon fill="black" stroke="black" points="83.04,-37.16 72.83,-34.34 79.05,-42.91 83.04,-37.16" />
</g>
<!-- h_2_2 -->
<g id="node4" class="node">
<title>h_2_2</title>
<ellipse fill="none" stroke="black" cx="242.7" cy="-18" rx="50.89" ry="18" />
<text text-anchor="middle" x="242.7" y="-14.3" font-family="Times,serif" font-size="14.00">h_n+1_2</text>
</g>
<!-- b&#45;&gt;h_2_2 -->
<g id="edge10" class="edge">
<title>b&#45;&gt;h_2_2</title>
<path fill="none" stroke="black" d="M168.81,-75.83C181.67,-65.94 199.56,-52.18 214.52,-40.67" />
<polygon fill="black" stroke="black" points="216.69,-43.42 222.48,-34.55 212.42,-37.87 216.69,-43.42" />
</g>
<!-- h_2_m -->
<g id="node5" class="node">
<title>h_2_m</title>
<ellipse fill="none" stroke="black" cx="365.7" cy="-18" rx="53.89" ry="18" />
<text text-anchor="middle" x="365.7" y="-14.3" font-family="Times,serif" font-size="14.00">h_n+1_m</text>
</g>
<!-- b&#45;&gt;h_2_m -->
<g id="edge12" class="edge">
<title>b&#45;&gt;h_2_m</title>
<path fill="none" stroke="black" d="M172.78,-78.39C177.62,-76.14 182.79,-73.88 187.7,-72 211.14,-63.03 271.93,-45.36 315.95,-32.9" />
<polygon fill="black" stroke="black" points="316.96,-36.25 325.63,-30.16 315.05,-29.51 316.96,-36.25" />
</g>
<!-- h_n_1 -->
<g id="node6" class="node">
<title>h_n_1</title>
<ellipse fill="none" stroke="black" cx="69.7" cy="-90" rx="37.09" ry="18" />
<text text-anchor="middle" x="69.7" y="-86.3" font-family="Times,serif" font-size="14.00">h_n_1</text>
</g>
<!-- h_n_1&#45;&gt;a -->
<g id="edge3" class="edge">
<title>h_n_1&#45;&gt;a</title>
<path fill="none" stroke="black" d="M86.4,-73.81C97.36,-63.85 111.83,-50.7 123.85,-39.77" />
<polygon fill="black" stroke="black" points="126.28,-42.29 131.33,-32.97 121.57,-37.11 126.28,-42.29" />
</g>
<!-- h_n_1&#45;&gt;h_2_1 -->
<g id="edge1" class="edge">
<title>h_n_1&#45;&gt;h_2_1</title>
<path fill="none" stroke="black" d="M59.35,-72.41C56.39,-64.62 53.56,-55.14 51.51,-46.33" />
<polygon fill="black" stroke="black" points="54.92,-45.55 49.5,-36.45 48.06,-46.94 54.92,-45.55" />
</g>
<!-- h_n_1&#45;&gt;h_2_1 -->
<g id="edge5" class="edge">
<title>h_n_1&#45;&gt;h_2_1</title>
<path fill="none" stroke="black" d="M70.91,-71.7C69.57,-63.7 67.15,-54.02 64.35,-45.15" />
<polygon fill="black" stroke="black" points="67.63,-43.93 61.05,-35.62 61.01,-46.22 67.63,-43.93" />
</g>
<!-- h_n_1&#45;&gt;h_2_2 -->
<g id="edge2" class="edge">
<title>h_n_1&#45;&gt;h_2_2</title>
<path fill="none" stroke="black" d="M97.49,-77.75C125.45,-66.44 168.9,-48.86 200.99,-35.87" />
<polygon fill="black" stroke="black" points="202.6,-39 210.56,-32 199.97,-32.51 202.6,-39" />
</g>
<!-- h_n_1&#45;&gt;h_2_m -->
<g id="edge4" class="edge">
<title>h_n_1&#45;&gt;h_2_m</title>
<path fill="none" stroke="black" d="M97.68,-77.83C103.57,-75.71 109.79,-73.65 115.7,-72 197.22,-49.25 220.23,-55.04 302.7,-36 307.03,-35 311.53,-33.9 316.02,-32.77" />
<polygon fill="black" stroke="black" points="317.01,-36.13 325.81,-30.24 315.26,-29.35 317.01,-36.13" />
</g>
<!-- h_n_2 -->
<g id="node7" class="node">
<title>h_n_2</title>
<ellipse fill="none" stroke="black" cx="331.7" cy="-90" rx="37.09" ry="18" />
<text text-anchor="middle" x="331.7" y="-86.3" font-family="Times,serif" font-size="14.00">h_n_2</text>
</g>
<!-- h_n_2&#45;&gt;a -->
<g id="edge7" class="edge">
<title>h_n_2&#45;&gt;a</title>
<path fill="none" stroke="black" d="M303.01,-78.31C297.28,-76.2 291.3,-74.02 285.7,-72 240.06,-55.59 227.57,-54.38 182.7,-36 180.87,-35.25 179.01,-34.46 177.14,-33.65" />
<polygon fill="black" stroke="black" points="178.4,-30.38 167.85,-29.44 175.52,-36.75 178.4,-30.38" />
</g>
<!-- h_n_2&#45;&gt;h_2_2 -->
<g id="edge6" class="edge">
<title>h_n_2&#45;&gt;h_2_2</title>
<path fill="none" stroke="black" d="M312.82,-74.15C300.65,-64.58 284.6,-51.96 270.93,-41.21" />
<polygon fill="black" stroke="black" points="272.8,-38.23 262.78,-34.8 268.48,-43.73 272.8,-38.23" />
</g>
<!-- h_n_2&#45;&gt;h_2_m -->
<g id="edge8" class="edge">
<title>h_n_2&#45;&gt;h_2_m</title>
<path fill="none" stroke="black" d="M339.75,-72.41C343.72,-64.25 348.59,-54.22 353.04,-45.07" />
<polygon fill="black" stroke="black" points="356.24,-46.48 357.46,-35.96 349.94,-43.42 356.24,-46.48" />
</g>
<!-- h_n_m -->
<g id="node8" class="node">
<title>h_n_m</title>
<ellipse fill="none" stroke="black" cx="236.7" cy="-90" rx="40.09" ry="18" />
<text text-anchor="middle" x="236.7" y="-86.3" font-family="Times,serif" font-size="14.00">h_n_m</text>
</g>
<!-- h_n_m&#45;&gt;a -->
<g id="edge15" class="edge">
<title>h_n_m&#45;&gt;a</title>
<path fill="none" stroke="black" d="M217.17,-73.81C203.86,-63.46 186.11,-49.66 171.76,-38.49" />
<polygon fill="black" stroke="black" points="173.8,-35.65 163.76,-32.27 169.5,-41.17 173.8,-35.65" />
</g>
<!-- h_n_m&#45;&gt;h_2_1 -->
<g id="edge13" class="edge">
<title>h_n_m&#45;&gt;h_2_1</title>
<path fill="none" stroke="black" d="M206.81,-77.75C176.21,-66.24 128.35,-48.22 93.68,-35.18" />
<polygon fill="black" stroke="black" points="94.87,-31.89 84.28,-31.64 92.41,-38.44 94.87,-31.89" />
</g>
<!-- h_n_m&#45;&gt;h_2_2 -->
<g id="edge14" class="edge">
<title>h_n_m&#45;&gt;h_2_2</title>
<path fill="none" stroke="black" d="M238.18,-71.7C238.84,-63.98 239.63,-54.71 240.37,-46.11" />
<polygon fill="black" stroke="black" points="243.86,-46.37 241.23,-36.1 236.89,-45.77 243.86,-46.37" />
</g>
<!-- h_n_m&#45;&gt;h_2_m -->
<g id="edge16" class="edge">
<title>h_n_m&#45;&gt;h_2_m</title>
<path fill="none" stroke="black" d="M261.26,-75.67C280.58,-65.19 307.78,-50.43 329.57,-38.6" />
<polygon fill="black" stroke="black" points="331.42,-41.58 338.54,-33.73 328.08,-35.43 331.42,-41.58" />
</g>
</g>
</svg>
</div>
</div>

<p>可以看到，输入的所有元素都被连接，即被分配权重 w 和偏差项 b，所以这被称为一个「全连接层（<strong>Fully Connected Layer</strong>）」或者「<strong>稠密层（Dense Layer）</strong>」。但是对于一些任务这样做是很蠢的，会付出大量无效的计算。</p>

<p>因此我们需要 focus 在更少量计算成本的模型，于是有了卷积神经网络（CNN）。关于 CNN 请看本系列博客的第「3」篇。</p>

<h3 id="reference">Reference</h3>

<ul>
  <li>《自然语言处理：基于预训练模型的方法》车万翔 等</li>
  <li>《自然语言处理实战：预训练模型应用及其产品化》安库·A·帕特尔 等</li>
</ul>

	</div>
</article>



	  </main>
		
		  <!-- Pagination links -->
      

	  </div>
	    
	    <!-- Footer -->
	    <footer>
	<span>
		-<br/><br/>
		船长还不会游泳 at 微信公众号/微博<br/>
		@麦克船长 at 即刻/知乎/小宇宙/掘金/小红书/微信读书<br/>
		@船长模玩 at Bilibili<br/>
		Copyright © 2011-2023, MikeCaptain.com
	</span>
</footer>


	    <!-- Script -->
      <script src="/js/main.js"></script>	


	</div>
</body>
</html>

<!DOCTYPE html>
<html>

<head>
	<!-- Meta -->
	<meta charset="UTF-8"/>
	<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
	<meta name="generator" content="Jekyll">

	<title>LLaMA 羊驼家族：大模型世界里的 Linux 内核</title>
  	<meta name="description" content="麦克船长对于技术、产品、商业等领域的分享|AI,A.I.,NLP,神经网络,人工智能,自然语言处理,BERT,GPT,ChatGPT,OpenAI,阿里巴巴,P9,运营,淘宝,天猫,总监,高管">

	<!-- CSS & fonts -->
	<link rel="stylesheet" href="/css/main.css">

	<!-- RSS -->
	<link href="/atom.xml" type="application/atom+xml" rel="alternate" title="ATOM Feed" />

  	<!-- Favicon -->
 	 <link rel="shortcut icon" type="image/png" href="/img/favicon.png">

 	 <!-- Syntax highlighter -->
  	<link rel="stylesheet" href="/css/syntax.css" />

  	<!--KaTeX-->
  	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
  	<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>
  	<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"></script>
  	<script>
  		document.addEventListener("DOMContentLoaded", function() {
  			renderMathInElement(document.body, {
  				// ...options...
  			});
  		});
  	</script>

  	
  	<!-- KaTeX -->
  	<link rel="stylesheet" href="/assets/plugins/katex.0.11.1/katex.min.css">
  	

  	
  		<script async src="https://www.googletagmanager.com/gtag/js?id=G-CH4708X4R5"></script>
  		<script>
    		window.dataLayer = window.dataLayer || [];
    		function gtag(){dataLayer.push(arguments);}
    		gtag('js', new Date());

    		gtag('config', 'G-CH4708X4R5');
  		</script>
	


</head>

<body>
	<div id="wrap">
	  	
	  	<!-- Navigation -->
	  	<nav id="nav">
	<div id="nav-list">
		<a href="/">Home</a>

		<!-- Nav pages -->
	  <!-- 
	    
	  
	    
	      <a href="/about/" title="关于我">关于我</a>
	    
	  
	    
	  
	    
	  
	    
	      <a href="/booklist/" title="读书行路">读书行路</a>
	    
	  
	    
	  
	    
	  
	    
	  
	    
	      <a href="/categories/" title="Categories">Categories</a>
	    
	  
	    
	  
	    
	  
	    
	  
	    
	      <a href="/target/" title="目标感">目标感</a>
	    
	  
	    
	  
	    
	  
	    
	  
	    
	  
	    
	  
	    
	  
	    
	  
	    
	   -->

	  <!-- Tech category pages -->






  <a href="/category/ai" title="人工智能">人工智能</a>











  <a href="/category/energy" title="能源">能源</a>









  <a href="/category/rt_tech" title="实时技术">实时技术</a>







  <a href="/category/web" title="前端">前端</a>
















<!-- Non-tech category pages -->












  <a href="/category/business" title="商业">商业</a>



  <a href="/category/design" title="设计">设计</a>















  <a href="/category/thinking" title="思考与生活">思考与生活</a>

















	  
        
      
        
          <a href="/about/" title="关于我">关于我</a>
        
      
        
      
        
      
        
          <a href="/booklist/" title="读书行路">读书行路</a>
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
          <a href="/target/" title="目标感">目标感</a>
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    <!-- Nav links -->
	  <!-- <a href="https://github.com/thereviewindex/monochrome/archive/master.zip">Download</a>
<a href="https://github.com/thereviewindex/monochrome">Project on Github</a> -->

	</div>
  
  <!-- Nav footer -->
	
	  <footer>
	
	<span>version 1.0.0</span>

</footer>
	

</nav>

    
    <!-- Icon menu -->
	  <a id="nav-menu">
	  	<div id="menu"></div>
	  </a>

      <!-- Header -->
      
        <header id="header" class="parent justify-spaceBetween">
  <div class="inner w100 relative">
    <span class="f-left">  
      <a href="/">
        <h1>
          <span>Mike</span>Captain
        </h1>
      </a>
    </span>
    <span id="nav-links" class="absolute right bottom">

      <!-- Tech category pages -->






  <a href="/category/ai" title="人工智能">人工智能</a>











  <a href="/category/energy" title="能源">能源</a>









  <a href="/category/rt_tech" title="实时技术">实时技术</a>







  <a href="/category/web" title="前端">前端</a>
















<!-- Non-tech category pages -->












  <a href="/category/business" title="商业">商业</a>



  <a href="/category/design" title="设计">设计</a>















  <a href="/category/thinking" title="思考与生活">思考与生活</a>

















      &nbsp;&nbsp;&nbsp;丨&nbsp;

      <!-- Nav pages -->
      
        
      
        
          <a href="/about/" title="关于我">关于我</a>
        
      
        
      
        
      
        
          <a href="/booklist/" title="读书行路">读书行路</a>
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
          <a href="/target/" title="目标感">目标感</a>
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
      
      <!-- Nav links -->
      <!-- <a href="https://github.com/thereviewindex/monochrome/archive/master.zip">Download</a>
<a href="https://github.com/thereviewindex/monochrome">Project on Github</a> -->

    </span>
  </div>
</header>




      

    <!-- Main content -->
	  <div id="container">
		  
		<main>

			<article id="post-page">
	<h2>LLaMA 羊驼家族：大模型世界里的 Linux 内核</h2>		
	<time datetime="2023-03-24T04:40:13+00:00" class="by-line">24 Mar 2023, 杭州 | 麦克船长 | 总计 4798 字</time>
	<div class="content">
		<p>85060909</p>

<h2 id="一llama">一、LLaMA</h2>

<p>目前基于预训练 LLM 研发性能较好的对话机器人，其难点之一是高质量数据。目前调教开源模型，基本都在用 ChatGPT 作为老师，常用的方式就两种：</p>

<ol>
  <li>
    <p>自己准备问题，让老师给答案：研发团队准备一组问题（instruction），然后用 ChatGPT 根据这些问题生成答案（following），这样得到一组 instruction-following 数据作为微调的训练数据。这样做的一个前提是认为 ChatGPT 本身就是高质量对齐的模型。</p>
  </li>
  <li>
    <p>用公开收集的 ChatGPT 的高质量对话作为微调的训练数据，这里的一个好的参考是 ShareGPT，它是一个大量用户分享 ChatGPT 对话内容的网站。</p>
  </li>
</ol>

<h3 id="1llama">1、LLaMA</h3>

<ul>
  <li>Blog：https://ai.facebook.com/blog/large-language-model-llama-meta-ai/</li>
</ul>

<h4 id="11在手机上跑一个-llamacpp">1.1、在手机上跑一个 LLaMA.cpp</h4>

<p>下载 <a href="https://developer.android.com/ndk">Android NDK</a> 到你的本地电脑上，编译一个支持 Android 平台运行的 LLaMA 出来：</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>mikecaptain@local % <span class="nb">mkdir </span>build-android
mikecaptain@local % <span class="nb">cd </span>build-android
mikecaptain@local % <span class="nb">export </span><span class="nv">NDK</span><span class="o">=</span>&lt;your_ndk_directory&gt;
mikecaptain@local % cmake <span class="nt">-DCMAKE_TOOLCHAIN_FILE</span><span class="o">=</span><span class="nv">$NDK</span>/build/cmake/android.toolchain.cmake <span class="nt">-DANDROID_ABI</span><span class="o">=</span>arm64-v8a <span class="nt">-DANDROID_PLATFORM</span><span class="o">=</span>android-23 <span class="nt">-DCMAKE_C_FLAGS</span><span class="o">=</span><span class="nt">-march</span><span class="o">=</span>armv8.4a+dotprod ..
mikecaptain@local % make
</code></pre></div></div>

<p>在 Android 设备上安装 <a href="https://termux.dev/">Termux</a></p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>mikecaptain@local % git clone https://github.com/ggerganov/llama.cpp
mikecaptain@local % <span class="nb">cd </span>llama.cpp
mikecaptain@local % make
</code></pre></div></div>

<h3 id="2llama-家族的-alpaca-7b2023-年-3-月-13-日">2、LLaMA 家族的 Alpaca-7B（2023 年 3 月 13 日）</h3>

<p>Alpaca 是一个在 LLaMA-7B 基础上用 5.2 万条的「instruction-following」微调得到的 LLM，由 Stanford 大学的一个研究团队发布，训练总花费约不到 600 美元。</p>

<ul>
  <li>Blog：https://crfm.stanford.edu/2023/03/13/alpaca.html</li>
  <li>DEMO：https://crfm.stanford.edu/alpaca/</li>
  <li>Code：https://github.com/tatsu-lab/stanford_alpaca</li>
</ul>

<p><img src="https://crfm.stanford.edu/static/img/posts/2023-03-13-alpaca/alpaca_main.jpg" alt="" width="720" /></p>

<ul>
  <li>关于训练数据：发布在 https://github.com/tatsu-lab/stanford_alpaca#data-release ，数据生成过程 https://github.com/tatsu-lab/stanford_alpaca#data-generation-process</li>
</ul>

<h4 id="21alpaca-7b">2.1、Alpaca-7B</h4>

<h3 id="3llama-家族的-gpt4all">3、LLaMA 家族的 GPT4ALL</h3>

<h3 id="4llama-家族的-vicuna-13b2023-年-3-月-31-日">4、LLaMA 家族的 Vicuna-13B（2023 年 3 月 31 日）</h3>

<ul>
  <li>Blog：https://vicuna.lmsys.org/</li>
  <li>DEMO：https://chat.lmsys.org/</li>
  <li>示例：https://vicuna.lmsys.org/eval/</li>
</ul>

<p>Vicuna 是一个在 LLaMA-13B 基础上用了 7 万条 ShareGPT 的对话微调得到的 LLM，由加州大学伯克利分校（UCBerkeley）的一个研究团队主导，联合卡耐基梅隆大学（CMU）、斯坦福大学和加州大学圣地亚哥分校（UCSD）发布，训练总花费约 300 美元。</p>

<ul>
  <li>关于模型参数：截止 2023 年 4 月 3 日暂未发布。</li>
  <li>关于训练数据：根据其官方网页上的声明，研发团队没有发布训练数据的计划。</li>
</ul>

<p><img src="/img/src/2023/04/2023-04-02-llama-01.png" alt="" width="720" /></p>

<p>Clone this repository and navigate to FastChat folder:</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>mikecaptain@localhost % git clone https://github.com/lm-sys/FastChat.git
mikecaptain@localhost % <span class="nb">cd </span>FastChat
</code></pre></div></div>

<p>Install the latest main branch of huggingface/transformers:</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>mikecaptain@localhost % pip3 <span class="nb">install </span>git+https://github.com/huggingface/transformers
</code></pre></div></div>

<p>安装依赖</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>mikecaptain@localhost % pip3 <span class="nb">install</span> <span class="nt">-e</span> <span class="nb">.</span>
</code></pre></div></div>

<p>Launch a controller:</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>mikecaptain@localhost % python3 <span class="nt">-m</span> fastchat.serve.controller
</code></pre></div></div>

<p>Launch a model worker</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>mikecaptain@localhost % python3 <span class="nt">-m</span> fastchat.serve.model_worker <span class="nt">--model-path</span> facebook/opt-1.3b
</code></pre></div></div>

<p>本段参考</p>

<ul>
  <li>https://zhuanlan.zhihu.com/p/618389519</li>
  <li>https://vicuna.lmsys.org/</li>
</ul>

<h3 id="5llama-的局限性">5、LLaMA 的局限性</h3>

<h3 id="6llama-的其他版本">6、LLaMA 的其他版本</h3>

<ul>
  <li>Chinese Vicuna：https://github.com/Facico/Chinese-Vicuna</li>
</ul>

<h2 id="二cerebras-gpt真开源2023-年-3-月-28-日">二、Cerebras-GPT：真·开源（2023 年 3 月 28 日）</h2>

<p>3 月 28 日，芯片初创公司 Cerebras 发布开源大模型 Cereras-GPT 系列，包括 111M、256M、590M、1.3B、2.7B、6.7B、13B 七个参数规模的模型。</p>

<p><img src="https://www.cerebras.net/wp-content/uploads/2023/03/Scaling-laws-blog-banner.png" alt="" /></p>

<h3 id="1真开源">1、真·开源</h3>

<p>模型架构、训练数据、预训练好的模型权重参数、Checkpoints、计算优化训练、License 等全部开源。</p>

<p><img src="https://www.cerebras.net/wp-content/uploads/2023/03/Scaling-laws-blog-comparison.png" alt="" /></p>

<h3 id="2计算优化训练--训练效率提升">2、计算优化训练 —— 训练效率提升</h3>

<p>船长在<a href="https://www.mikecaptain.com/2023/03/06/captain-aigc-2-llm/">《麦克船长 LLM 革命系列 2：破晓》</a>一文中介绍了 2020 年 1 月 OpenAI 提出的「Scaling Law」，指出了训练算力、训练数据规模、模型参数规模指数增长，模型性能表现线性提升的规律。</p>

<p><img src="https://www.mikecaptain.com/img/src/2023/2023-01-23-captain-aigc-2-llm-10.png" alt="" /></p>

<p>2022 年 DeepMind 在提出 Chinchilla（中文意思「龙猫」）模型时，顺便指出 LLM 的训练数据规模（training data）与训练算力（compute）之间的最优关系。</p>

<h2 id="三bloomberggpt2023-年-3-月-30-日">三、BloombergGPT（2023 年 3 月 30 日）</h2>

<h2 id="四chatrwkv一个匹敌-transformer-表现的并行化-rnn-模型">四、ChatRWKV：一个匹敌 Transformer 表现的并行化 RNN 模型</h2>

<h2 id="五glm">五、GLM</h2>

<h2 id="背景">背景</h2>

<h3 id="1chinchilla-ai">1、Chinchilla AI</h3>

<p>2022 年 3 月 DeepMind 在<a href="https://arxiv.org/abs/2203.15556">《Training Compute-Optimal Large Language Models》</a>一文中提到：</p>

<blockquote>
  <p>By training over 400 language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, we find that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled. <br /> 通过在 50~5000 亿个训练数据上（具体说是指 tokens 规模）训练 400 多个语言模型，模型参数规模从 7000 万到超过 160 亿个参数，DeepMind 发现对于计算优化训练（Compute-Optimal Traning），模型规模和训练数据规模应该等比缩放：模型规模翻倍时，训练数据量也应翻倍。</p>
</blockquote>

<p>DeepMind 认为当时很多 LLM 的训练都是不充分的，在暴力堆大模型参数规模时，很多 LLM 的训练数据规模并没有相应放大。DeepMind 顺便在发布该文时提出了 Chinchilla 模型，其参数规模、训练数据规模对比如下：</p>

<table>
  <thead>
    <tr>
      <th>模型</th>
      <th>模型参数规模</th>
      <th>训练数据规模</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>LaMDA</td>
      <td>137B</td>
      <td>168B</td>
    </tr>
    <tr>
      <td>GPT-3</td>
      <td>175B</td>
      <td>300B</td>
    </tr>
    <tr>
      <td>Jurassic</td>
      <td>178B</td>
      <td>300B</td>
    </tr>
    <tr>
      <td>Gopher</td>
      <td>280B</td>
      <td>300B</td>
    </tr>
    <tr>
      <td>MT-NLG 530B</td>
      <td>530B</td>
      <td>270B</td>
    </tr>
    <tr>
      <td>Chinchilla</td>
      <td>70B</td>
      <td>1.4T</td>
    </tr>
  </tbody>
</table>

<p>此前 DeepMind 还发布过一个名为 Gopher 的 LLM（在 Chinchilla 发布前两个月），发布 Chinchilla 时用了与 Gopher 同等规模的预算，但在用更小的模型参数规模情况下，用更多的训练数据，Chinchilla 取得了更好的效果。</p>

<p>发布大语言模型 Chinchilla，官方声称其表现可以超过 GPT-3，并且用更少的</p>

<h3 id="2moemixture-of-experts">2、MoE（Mixture of Experts）</h3>

<p>混合专家系统（Mixture of Experts，简称 MoE）是把处理不同任务的不同神经网络专家系统（Expert Model），通过门控模型（Gating Model），组合成一个系统。针对下游任务，门控模型来决定使用哪个神经网络会得到更好表现。这个技术整体算是一种集成学习技术（Emsemble Learning）。</p>

<h3 id="3physics-of-ai">3、Physics of AI</h3>

<h4 id="参考">参考：</h4>

<ul>
  <li>https://www.youtube.com/watch?v=XLNmgviQHPA</li>
</ul>

<h2 id="参考-1">参考</h2>

<ol>
  <li>https://www.cerebras.net/blog/cerebras-gpt-a-family-of-open-compute-efficient-large-language-models/</li>
  <li>https://arxiv.org/pdf/2203.15556.pdf</li>
  <li>https://vicuna.lmsys.org/</li>
  <li>https://mp.weixin.qq.com/s/Q1jHC5b7NMvrOVTd9dIJCw</li>
  <li>https://zhuanlan.zhihu.com/p/618776565</li>
  <li>https://crfm.stanford.edu/2023/03/13/alpaca.html</li>
  <li>https://medium.com/geekculture/list-of-open-sourced-fine-tuned-large-language-models-llm-8d95a2e0dc76</li>
</ol>

	</div>
</article>



	  </main>
		
		  <!-- Pagination links -->
      

	  </div>
	    
	    <!-- Footer -->
	    <footer>
	<span>
		-<br/><br/>
		船长还不会游泳 at 微信公众号/微博<br/>
		@麦克船长 at 即刻/知乎/小宇宙/掘金/小红书/微信读书<br/>
		@船长模玩 at Bilibili<br/>
		Copyright © 2011-2023, MikeCaptain.com
	</span>
</footer>


	    <!-- Script -->
      <script src="/js/main.js"></script>	


	</div>
</body>
</html>
